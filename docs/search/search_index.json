{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting Started","text":"<p>SDFEst is a method for RGB-D-based shape and pose estimation using discretized signed distance fields.</p>"},{"location":"#installation","title":"Installation","text":"<p>Run</p> <pre><code>pip install sdfest\n</code></pre> <p>to install the latest release of the package.</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>differentiable_renderer<ul> <li>scripts<ul> <li>experiments</li> </ul> </li> <li>sdf_renderer</li> </ul> </li> <li>estimation<ul> <li>losses</li> <li>metrics</li> <li>scripts<ul> <li>play_log</li> <li>real_data</li> <li>rendering_evaluation</li> </ul> </li> <li>simple_setup</li> <li>synthetic</li> </ul> </li> <li>initialization<ul> <li>datasets<ul> <li>dataset_utils</li> <li>generated_dataset</li> <li>nocs_dataset</li> <li>nocs_utils</li> <li>redwood_dataset</li> </ul> </li> <li>pointnet</li> <li>pointset_utils</li> <li>quaternion_utils</li> <li>scripts<ul> <li>train</li> </ul> </li> <li>sdf_pose_network</li> <li>sdf_utils</li> <li>so3grid</li> <li>utils</li> </ul> </li> <li>utils</li> <li>vae<ul> <li>scripts<ul> <li>benchmark</li> <li>benchmark_vae</li> <li>process_shapenet</li> <li>train</li> <li>visualizer</li> </ul> </li> <li>sdf_dataset</li> <li>sdf_utils</li> <li>sdf_vae</li> <li>torch_utils</li> <li>utils</li> </ul> </li> </ul>"},{"location":"reference/utils/","title":"utils","text":"<p>Common utility functions.</p>"},{"location":"reference/utils/#sdfest.utils.load_model_weights","title":"<code>load_model_weights(path, model, map_location, model_weights_url=None)</code>","text":"<p>Load model weights from path or download weights from URL if file does not exist.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to model weights.</p> required <code>model</code> <code>Module</code> <p>Path to model weights.</p> required <code>map_location</code> <code>device</code> <p>See torch.load.</p> required <code>model_weights_url</code> <code>Optional[str]</code> <p>URL to download model weights from if path does not exist.</p> <code>None</code> Source code in <code>sdfest/utils.py</code> <pre><code>def load_model_weights(\n    path: str,\n    model: torch.nn.Module,\n    map_location: torch.device,\n    model_weights_url: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Load model weights from path or download weights from URL if file does not exist.\n\n    Args:\n        path: Path to model weights.\n        model: Path to model weights.\n        map_location: See torch.load.\n        model_weights_url: URL to download model weights from if path does not exist.\n    \"\"\"\n    resolved_path = yoco.resolve_path(\n        path, search_paths=[\".\", \"~/.sdfest/model_weights/\"]\n    )\n\n    if not os.path.exists(resolved_path):\n        if model_weights_url is not None:\n            if not os.path.isabs(resolved_path):\n                resolved_path = os.path.expanduser(\n                    os.path.join(\"~/.sdfest/model_weights\", resolved_path)\n                )\n            os.makedirs(os.path.dirname(resolved_path), exist_ok=True)\n            print(f\"Model weights {resolved_path} not found.\")\n            print(f\"Downloading from {model_weights_url}\")\n            download(model_weights_url, resolved_path)\n        else:\n            print(f\"Model weights {resolved_path} not found. Aborting.\")\n            exit(0)\n\n    state_dict = torch.load(resolved_path, map_location=map_location)\n    model.load_state_dict(state_dict)\n</code></pre>"},{"location":"reference/differentiable_renderer/sdf_renderer/","title":"sdf_renderer","text":"<p>PyTorch interface for diffferentiable renderer.</p> This module provides two functions <ul> <li>render_depth: numpy-based CPU implementation (not recommended, only for development)</li> <li>render_depth_gpu: CUDA implementation (fast)</li> </ul>"},{"location":"reference/differentiable_renderer/sdf_renderer/#sdfest.differentiable_renderer.sdf_renderer.Camera","title":"<code>Camera</code>","text":"<p>Pinhole camera parameters.</p> <p>This class allows conversion between different pixel conventions, i.e., pixel center at (0.5, 0.5) (as common in computer graphics), and (0, 0) as common in computer vision.</p> Source code in <code>sdfest/differentiable_renderer/sdf_renderer.py</code> <pre><code>class Camera:\n    \"\"\"Pinhole camera parameters.\n\n    This class allows conversion between different pixel conventions, i.e., pixel\n    center at (0.5, 0.5) (as common in computer graphics), and (0, 0) as common in\n    computer vision.\n    \"\"\"\n\n    def __init__(\n        self,\n        width: int,\n        height: int,\n        fx: float,\n        fy: float,\n        cx: float,\n        cy: float,\n        s: float = 0.0,\n        pixel_center: float = 0.0,\n    ):\n        \"\"\"Initialize camera parameters.\n\n        Note that the principal point is only fully defined in combination with\n        pixel_center.\n\n        The pixel_center defines the relation between continuous image plane\n        coordinates and discrete pixel coordinates.\n\n        A discrete image coordinate (x, y) will correspond to the continuous\n        image coordinate (x + pixel_center, y + pixel_center). Normally pixel_center\n        will be either 0 or 0.5. During calibration it depends on the convention\n        the point features used to compute the calibration matrix.\n\n        Note that if pixel_center == 0, the corresponding continuous coordinate\n        interval for a pixel are [x-0.5, x+0.5). I.e., proper rounding has to be done\n        to convert from continuous coordinate to the corresponding discrete coordinate.\n\n        For pixel_center == 0.5, the corresponding continuous coordinate interval for a\n        pixel are [x, x+1). I.e., floor is sufficient to convert from continuous\n        coordinate to the corresponding discrete coordinate.\n\n        Args:\n            width: Number of pixels in horizontal direction.\n            height: Number of pixels in vertical direction.\n            fx: Horizontal focal length.\n            fy: Vertical focal length.\n            cx: Principal point x-coordinate.\n            cy: Principal point y-coordinate.\n            s: Skew.\n            pixel_center: The center offset for the provided principal point.\n        \"\"\"\n        # focal length\n        self.fx = fx\n        self.fy = fy\n\n        # principal point\n        self.cx = cx\n        self.cy = cy\n\n        self.pixel_center = pixel_center\n\n        # skew\n        self.s = s\n\n        # image dimensions\n        self.width = width\n        self.height = height\n\n    def get_o3d_pinhole_camera_parameters(self) -&gt; o3d.camera.PinholeCameraParameters():\n        \"\"\"Convert camera to Open3D pinhole camera parameters.\n\n        Open3D camera is at (0,0,0) looking along positive z axis (i.e., positive z\n        values are in front of camera). Open3D expects camera with pixel_center = 0\n        and does not support skew.\n\n        Returns:\n            The pinhole camera parameters.\n        \"\"\"\n        fx, fy, cx, cy, _ = self.get_pinhole_camera_parameters(0)\n        params = o3d.camera.PinholeCameraParameters()\n        params.intrinsic.set_intrinsics(self.width, self.height, fx, fy, cx, cy)\n        params.extrinsic = np.array(\n            [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]\n        )\n        return params\n\n    def get_pinhole_camera_parameters(self, pixel_center: float) -&gt; Tuple:\n        \"\"\"Convert camera to general camera parameters.\n\n        Args:\n            pixel_center:\n                At which ratio of a square the pixel center should be for the resulting\n                parameters. Typically 0 or 0.5. See class documentation for more info.\n        Returns:\n            - fx, fy: The horizontal and vertical focal length\n            - cx, cy:\n                The position of the principal point in continuous image plane\n                coordinates considering the provided pixel center and the pixel center\n                specified during the construction.\n            - s: The skew.\n        \"\"\"\n        cx_corrected = self.cx - self.pixel_center + pixel_center\n        cy_corrected = self.cy - self.pixel_center + pixel_center\n        return self.fx, self.fy, cx_corrected, cy_corrected, self.s\n</code></pre>"},{"location":"reference/differentiable_renderer/sdf_renderer/#sdfest.differentiable_renderer.sdf_renderer.Camera.__init__","title":"<code>__init__(width, height, fx, fy, cx, cy, s=0.0, pixel_center=0.0)</code>","text":"<p>Initialize camera parameters.</p> <p>Note that the principal point is only fully defined in combination with pixel_center.</p> <p>The pixel_center defines the relation between continuous image plane coordinates and discrete pixel coordinates.</p> <p>A discrete image coordinate (x, y) will correspond to the continuous image coordinate (x + pixel_center, y + pixel_center). Normally pixel_center will be either 0 or 0.5. During calibration it depends on the convention the point features used to compute the calibration matrix.</p> <p>Note that if pixel_center == 0, the corresponding continuous coordinate interval for a pixel are [x-0.5, x+0.5). I.e., proper rounding has to be done to convert from continuous coordinate to the corresponding discrete coordinate.</p> <p>For pixel_center == 0.5, the corresponding continuous coordinate interval for a pixel are [x, x+1). I.e., floor is sufficient to convert from continuous coordinate to the corresponding discrete coordinate.</p> <p>Parameters:</p> Name Type Description Default <code>width</code> <code>int</code> <p>Number of pixels in horizontal direction.</p> required <code>height</code> <code>int</code> <p>Number of pixels in vertical direction.</p> required <code>fx</code> <code>float</code> <p>Horizontal focal length.</p> required <code>fy</code> <code>float</code> <p>Vertical focal length.</p> required <code>cx</code> <code>float</code> <p>Principal point x-coordinate.</p> required <code>cy</code> <code>float</code> <p>Principal point y-coordinate.</p> required <code>s</code> <code>float</code> <p>Skew.</p> <code>0.0</code> <code>pixel_center</code> <code>float</code> <p>The center offset for the provided principal point.</p> <code>0.0</code> Source code in <code>sdfest/differentiable_renderer/sdf_renderer.py</code> <pre><code>def __init__(\n    self,\n    width: int,\n    height: int,\n    fx: float,\n    fy: float,\n    cx: float,\n    cy: float,\n    s: float = 0.0,\n    pixel_center: float = 0.0,\n):\n    \"\"\"Initialize camera parameters.\n\n    Note that the principal point is only fully defined in combination with\n    pixel_center.\n\n    The pixel_center defines the relation between continuous image plane\n    coordinates and discrete pixel coordinates.\n\n    A discrete image coordinate (x, y) will correspond to the continuous\n    image coordinate (x + pixel_center, y + pixel_center). Normally pixel_center\n    will be either 0 or 0.5. During calibration it depends on the convention\n    the point features used to compute the calibration matrix.\n\n    Note that if pixel_center == 0, the corresponding continuous coordinate\n    interval for a pixel are [x-0.5, x+0.5). I.e., proper rounding has to be done\n    to convert from continuous coordinate to the corresponding discrete coordinate.\n\n    For pixel_center == 0.5, the corresponding continuous coordinate interval for a\n    pixel are [x, x+1). I.e., floor is sufficient to convert from continuous\n    coordinate to the corresponding discrete coordinate.\n\n    Args:\n        width: Number of pixels in horizontal direction.\n        height: Number of pixels in vertical direction.\n        fx: Horizontal focal length.\n        fy: Vertical focal length.\n        cx: Principal point x-coordinate.\n        cy: Principal point y-coordinate.\n        s: Skew.\n        pixel_center: The center offset for the provided principal point.\n    \"\"\"\n    # focal length\n    self.fx = fx\n    self.fy = fy\n\n    # principal point\n    self.cx = cx\n    self.cy = cy\n\n    self.pixel_center = pixel_center\n\n    # skew\n    self.s = s\n\n    # image dimensions\n    self.width = width\n    self.height = height\n</code></pre>"},{"location":"reference/differentiable_renderer/sdf_renderer/#sdfest.differentiable_renderer.sdf_renderer.Camera.get_o3d_pinhole_camera_parameters","title":"<code>get_o3d_pinhole_camera_parameters()</code>","text":"<p>Convert camera to Open3D pinhole camera parameters.</p> <p>Open3D camera is at (0,0,0) looking along positive z axis (i.e., positive z values are in front of camera). Open3D expects camera with pixel_center = 0 and does not support skew.</p> <p>Returns:</p> Type Description <code>PinholeCameraParameters()</code> <p>The pinhole camera parameters.</p> Source code in <code>sdfest/differentiable_renderer/sdf_renderer.py</code> <pre><code>def get_o3d_pinhole_camera_parameters(self) -&gt; o3d.camera.PinholeCameraParameters():\n    \"\"\"Convert camera to Open3D pinhole camera parameters.\n\n    Open3D camera is at (0,0,0) looking along positive z axis (i.e., positive z\n    values are in front of camera). Open3D expects camera with pixel_center = 0\n    and does not support skew.\n\n    Returns:\n        The pinhole camera parameters.\n    \"\"\"\n    fx, fy, cx, cy, _ = self.get_pinhole_camera_parameters(0)\n    params = o3d.camera.PinholeCameraParameters()\n    params.intrinsic.set_intrinsics(self.width, self.height, fx, fy, cx, cy)\n    params.extrinsic = np.array(\n        [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]\n    )\n    return params\n</code></pre>"},{"location":"reference/differentiable_renderer/sdf_renderer/#sdfest.differentiable_renderer.sdf_renderer.Camera.get_pinhole_camera_parameters","title":"<code>get_pinhole_camera_parameters(pixel_center)</code>","text":"<p>Convert camera to general camera parameters.</p> <p>Parameters:</p> Name Type Description Default <code>pixel_center</code> <code>float</code> <p>At which ratio of a square the pixel center should be for the resulting parameters. Typically 0 or 0.5. See class documentation for more info.</p> required <p>Returns:     - fx, fy: The horizontal and vertical focal length     - cx, cy:         The position of the principal point in continuous image plane         coordinates considering the provided pixel center and the pixel center         specified during the construction.     - s: The skew.</p> Source code in <code>sdfest/differentiable_renderer/sdf_renderer.py</code> <pre><code>def get_pinhole_camera_parameters(self, pixel_center: float) -&gt; Tuple:\n    \"\"\"Convert camera to general camera parameters.\n\n    Args:\n        pixel_center:\n            At which ratio of a square the pixel center should be for the resulting\n            parameters. Typically 0 or 0.5. See class documentation for more info.\n    Returns:\n        - fx, fy: The horizontal and vertical focal length\n        - cx, cy:\n            The position of the principal point in continuous image plane\n            coordinates considering the provided pixel center and the pixel center\n            specified during the construction.\n        - s: The skew.\n    \"\"\"\n    cx_corrected = self.cx - self.pixel_center + pixel_center\n    cy_corrected = self.cy - self.pixel_center + pixel_center\n    return self.fx, self.fy, cx_corrected, cy_corrected, self.s\n</code></pre>"},{"location":"reference/differentiable_renderer/sdf_renderer/#sdfest.differentiable_renderer.sdf_renderer.SDFRendererFunction","title":"<code>SDFRendererFunction</code>","text":"<p>               Bases: <code>Function</code></p> <p>Renderer function for signed distance fields.</p> Source code in <code>sdfest/differentiable_renderer/sdf_renderer.py</code> <pre><code>class SDFRendererFunction(torch.autograd.Function):\n    \"\"\"Renderer function for signed distance fields.\"\"\"\n\n    @staticmethod\n    def forward(\n        ctx,\n        sdf: torch.Tensor,\n        position: torch.Tensor,\n        orientation: torch.Tensor,\n        inv_scale: torch.Tensor,\n        width: Optional[int] = None,\n        height: Optional[int] = None,\n        fov_deg: Optional[float] = None,\n        threshold: Optional[float] = 0.0,\n        camera: Optional[Camera] = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"Render depth image of a 7-DOF discrete signed distance field.\n\n        The SDF position is assumed to be in the camera frame under OpenGL convention.\n\n        That is, camera looks along negative z-axis, y pointing upwards and x to the\n        right. Note that the rendered image will still follow the classical computer\n        vision convention, of first row being up in the camera frame.\n\n        This function internally usese numpy and is very slow due to the fully serial\n        implementation. This is only for testing purposes. Use the GPU version for\n        practical performance.\n\n        Camera can be specified either via camera parameter giving most\n        flexbility or alternatively by providing width, height and fov_deg.\n\n        Args:\n            ctx:\n                Context object to stash information.\n                See https://pytorch.org/docs/stable/notes/extending.html.\n            sdf:\n                Discrete signed distance field with shape (M, M, M).\n                Arbitrary (but uniform) resolutions are supported.\n            position:\n                The position of the signed distance field origin in the camera frame.\n            orientation:\n                The orientation of the SDF as a normalized quaternion.\n            inv_scale:\n                The inverted scale of the SDF. The scale of an SDF the half-width of the\n                full SDF volume.\n            width:\n                Number of pixels in x direction. Recommended to use camera instead.\n            height:\n                Number of pixels in y direction. Recommended to use camera instead.\n            fov_deg:\n                The horizontal field of view (i.e., in x direction).\n                Pixels are assumed to be square, i.e., fx=fy, computed based on width\n                and fov_deg.\n                Recommended to use camera instead.\n            threshold:\n                The distance threshold at which sphere tracing should be stopped.\n                Smaller value will be more accurate, but slower and might potentially\n                lead to holes in the rendering for thin structures in the SDF.\n                Larger values will be faster, but will overestimate the thickness.\n\n                Should always be positive to guarantee convergence.\n            camera:\n                Camera parameters (not supported right now).\n        Returns:\n            The rendered depth image.\n        \"\"\"\n        if None not in [width, height, fov_deg] and camera is not None:\n            raise ValueError(\"Either width+height+fov_dev or camera must be provided.\")\n        if camera is not None:\n            raise NotImplementedError(\n                \"Only width+height+fov_dev currently supported for CPU\"\n            )\n        # for simplicity use numpy internally\n        ctx.save_for_backward(sdf, position, orientation, inv_scale)\n        sdf = sdf.detach().numpy()\n        position = position.detach().numpy()\n        orientation = orientation.detach().numpy()\n        inv_scale = inv_scale.detach().numpy()\n        sdf_object = SDFObject(sdf)\n        image, derivatives = _render_depth(\n            sdf_object,\n            width,\n            height,\n            fov_deg,\n            \"d\",\n            threshold,\n            position,\n            orientation,\n            inv_scale,\n        )\n        ctx.derivatives = derivatives\n        return torch.from_numpy(image)\n\n    @staticmethod\n    def backward(ctx, inp: torch.Tensor):\n        \"\"\"Compute gradients of inputs with respect to the provided gradients.\n\n        Normally called by PyTorch as part of a call to backward() on a loss.\n\n        Args:\n            grad_depth_image:\n        Returns:\n            Gradients of\n                discretized signed distance field, position, orientation, inverted scale\n                followed by None for all the non-supported variables passed to forward.\n        \"\"\"\n        derivatives = ctx.derivatives\n        sdf, pos, quat, inv_s = ctx.saved_tensors\n        g_image = inp.numpy()\n        g_sdf = g_p = g_q = g_is = g_w = g_h = g_fov = g_thresh = g_camera = None\n        g_sdf = torch.zeros_like(sdf)\n        g_p = torch.empty_like(pos)\n        g_q = torch.empty_like(quat)\n        g_is = torch.empty_like(inv_s)\n        g_p[0] = torch.tensor(np.sum(derivatives[\"x\"] * g_image))\n        g_p[1] = torch.tensor(np.sum(derivatives[\"y\"] * g_image))\n        g_p[2] = torch.tensor(np.sum(derivatives[\"z\"] * g_image))\n        g_q[0] = torch.tensor(np.sum(derivatives[\"qx\"] * g_image))\n        g_q[1] = torch.tensor(np.sum(derivatives[\"qy\"] * g_image))\n        g_q[2] = torch.tensor(np.sum(derivatives[\"qz\"] * g_image))\n        g_q[3] = torch.tensor(np.sum(derivatives[\"qw\"] * g_image))\n        g_is = torch.tensor(np.sum(derivatives[\"s_inv\"] * g_image))\n        if \"sdf\" in derivatives:\n            for k, v in derivatives[\"sdf\"].items():\n                g_sdf[k] = torch.tensor(np.sum(v * g_image))\n        return g_sdf, g_p, g_q, g_is, g_w, g_h, g_fov, g_thresh, g_camera\n</code></pre>"},{"location":"reference/differentiable_renderer/sdf_renderer/#sdfest.differentiable_renderer.sdf_renderer.SDFRendererFunction.backward","title":"<code>backward(ctx, inp)</code>  <code>staticmethod</code>","text":"<p>Compute gradients of inputs with respect to the provided gradients.</p> <p>Normally called by PyTorch as part of a call to backward() on a loss.</p> <p>Parameters:</p> Name Type Description Default <code>grad_depth_image</code> required <p>Returns:     Gradients of         discretized signed distance field, position, orientation, inverted scale         followed by None for all the non-supported variables passed to forward.</p> Source code in <code>sdfest/differentiable_renderer/sdf_renderer.py</code> <pre><code>@staticmethod\ndef backward(ctx, inp: torch.Tensor):\n    \"\"\"Compute gradients of inputs with respect to the provided gradients.\n\n    Normally called by PyTorch as part of a call to backward() on a loss.\n\n    Args:\n        grad_depth_image:\n    Returns:\n        Gradients of\n            discretized signed distance field, position, orientation, inverted scale\n            followed by None for all the non-supported variables passed to forward.\n    \"\"\"\n    derivatives = ctx.derivatives\n    sdf, pos, quat, inv_s = ctx.saved_tensors\n    g_image = inp.numpy()\n    g_sdf = g_p = g_q = g_is = g_w = g_h = g_fov = g_thresh = g_camera = None\n    g_sdf = torch.zeros_like(sdf)\n    g_p = torch.empty_like(pos)\n    g_q = torch.empty_like(quat)\n    g_is = torch.empty_like(inv_s)\n    g_p[0] = torch.tensor(np.sum(derivatives[\"x\"] * g_image))\n    g_p[1] = torch.tensor(np.sum(derivatives[\"y\"] * g_image))\n    g_p[2] = torch.tensor(np.sum(derivatives[\"z\"] * g_image))\n    g_q[0] = torch.tensor(np.sum(derivatives[\"qx\"] * g_image))\n    g_q[1] = torch.tensor(np.sum(derivatives[\"qy\"] * g_image))\n    g_q[2] = torch.tensor(np.sum(derivatives[\"qz\"] * g_image))\n    g_q[3] = torch.tensor(np.sum(derivatives[\"qw\"] * g_image))\n    g_is = torch.tensor(np.sum(derivatives[\"s_inv\"] * g_image))\n    if \"sdf\" in derivatives:\n        for k, v in derivatives[\"sdf\"].items():\n            g_sdf[k] = torch.tensor(np.sum(v * g_image))\n    return g_sdf, g_p, g_q, g_is, g_w, g_h, g_fov, g_thresh, g_camera\n</code></pre>"},{"location":"reference/differentiable_renderer/sdf_renderer/#sdfest.differentiable_renderer.sdf_renderer.SDFRendererFunction.forward","title":"<code>forward(ctx, sdf, position, orientation, inv_scale, width=None, height=None, fov_deg=None, threshold=0.0, camera=None)</code>  <code>staticmethod</code>","text":"<p>Render depth image of a 7-DOF discrete signed distance field.</p> <p>The SDF position is assumed to be in the camera frame under OpenGL convention.</p> <p>That is, camera looks along negative z-axis, y pointing upwards and x to the right. Note that the rendered image will still follow the classical computer vision convention, of first row being up in the camera frame.</p> <p>This function internally usese numpy and is very slow due to the fully serial implementation. This is only for testing purposes. Use the GPU version for practical performance.</p> <p>Camera can be specified either via camera parameter giving most flexbility or alternatively by providing width, height and fov_deg.</p> <p>Parameters:</p> Name Type Description Default <code>ctx</code> <p>Context object to stash information. See https://pytorch.org/docs/stable/notes/extending.html.</p> required <code>sdf</code> <code>Tensor</code> <p>Discrete signed distance field with shape (M, M, M). Arbitrary (but uniform) resolutions are supported.</p> required <code>position</code> <code>Tensor</code> <p>The position of the signed distance field origin in the camera frame.</p> required <code>orientation</code> <code>Tensor</code> <p>The orientation of the SDF as a normalized quaternion.</p> required <code>inv_scale</code> <code>Tensor</code> <p>The inverted scale of the SDF. The scale of an SDF the half-width of the full SDF volume.</p> required <code>width</code> <code>Optional[int]</code> <p>Number of pixels in x direction. Recommended to use camera instead.</p> <code>None</code> <code>height</code> <code>Optional[int]</code> <p>Number of pixels in y direction. Recommended to use camera instead.</p> <code>None</code> <code>fov_deg</code> <code>Optional[float]</code> <p>The horizontal field of view (i.e., in x direction). Pixels are assumed to be square, i.e., fx=fy, computed based on width and fov_deg. Recommended to use camera instead.</p> <code>None</code> <code>threshold</code> <code>Optional[float]</code> <p>The distance threshold at which sphere tracing should be stopped. Smaller value will be more accurate, but slower and might potentially lead to holes in the rendering for thin structures in the SDF. Larger values will be faster, but will overestimate the thickness.</p> <p>Should always be positive to guarantee convergence.</p> <code>0.0</code> <code>camera</code> <code>Optional[Camera]</code> <p>Camera parameters (not supported right now).</p> <code>None</code> <p>Returns:     The rendered depth image.</p> Source code in <code>sdfest/differentiable_renderer/sdf_renderer.py</code> <pre><code>@staticmethod\ndef forward(\n    ctx,\n    sdf: torch.Tensor,\n    position: torch.Tensor,\n    orientation: torch.Tensor,\n    inv_scale: torch.Tensor,\n    width: Optional[int] = None,\n    height: Optional[int] = None,\n    fov_deg: Optional[float] = None,\n    threshold: Optional[float] = 0.0,\n    camera: Optional[Camera] = None,\n) -&gt; torch.Tensor:\n    \"\"\"Render depth image of a 7-DOF discrete signed distance field.\n\n    The SDF position is assumed to be in the camera frame under OpenGL convention.\n\n    That is, camera looks along negative z-axis, y pointing upwards and x to the\n    right. Note that the rendered image will still follow the classical computer\n    vision convention, of first row being up in the camera frame.\n\n    This function internally usese numpy and is very slow due to the fully serial\n    implementation. This is only for testing purposes. Use the GPU version for\n    practical performance.\n\n    Camera can be specified either via camera parameter giving most\n    flexbility or alternatively by providing width, height and fov_deg.\n\n    Args:\n        ctx:\n            Context object to stash information.\n            See https://pytorch.org/docs/stable/notes/extending.html.\n        sdf:\n            Discrete signed distance field with shape (M, M, M).\n            Arbitrary (but uniform) resolutions are supported.\n        position:\n            The position of the signed distance field origin in the camera frame.\n        orientation:\n            The orientation of the SDF as a normalized quaternion.\n        inv_scale:\n            The inverted scale of the SDF. The scale of an SDF the half-width of the\n            full SDF volume.\n        width:\n            Number of pixels in x direction. Recommended to use camera instead.\n        height:\n            Number of pixels in y direction. Recommended to use camera instead.\n        fov_deg:\n            The horizontal field of view (i.e., in x direction).\n            Pixels are assumed to be square, i.e., fx=fy, computed based on width\n            and fov_deg.\n            Recommended to use camera instead.\n        threshold:\n            The distance threshold at which sphere tracing should be stopped.\n            Smaller value will be more accurate, but slower and might potentially\n            lead to holes in the rendering for thin structures in the SDF.\n            Larger values will be faster, but will overestimate the thickness.\n\n            Should always be positive to guarantee convergence.\n        camera:\n            Camera parameters (not supported right now).\n    Returns:\n        The rendered depth image.\n    \"\"\"\n    if None not in [width, height, fov_deg] and camera is not None:\n        raise ValueError(\"Either width+height+fov_dev or camera must be provided.\")\n    if camera is not None:\n        raise NotImplementedError(\n            \"Only width+height+fov_dev currently supported for CPU\"\n        )\n    # for simplicity use numpy internally\n    ctx.save_for_backward(sdf, position, orientation, inv_scale)\n    sdf = sdf.detach().numpy()\n    position = position.detach().numpy()\n    orientation = orientation.detach().numpy()\n    inv_scale = inv_scale.detach().numpy()\n    sdf_object = SDFObject(sdf)\n    image, derivatives = _render_depth(\n        sdf_object,\n        width,\n        height,\n        fov_deg,\n        \"d\",\n        threshold,\n        position,\n        orientation,\n        inv_scale,\n    )\n    ctx.derivatives = derivatives\n    return torch.from_numpy(image)\n</code></pre>"},{"location":"reference/differentiable_renderer/sdf_renderer/#sdfest.differentiable_renderer.sdf_renderer.SDFRendererFunctionGPU","title":"<code>SDFRendererFunctionGPU</code>","text":"<p>               Bases: <code>Function</code></p> <p>Renderer function for signed distance fields.</p> Source code in <code>sdfest/differentiable_renderer/sdf_renderer.py</code> <pre><code>class SDFRendererFunctionGPU(torch.autograd.Function):\n    \"\"\"Renderer function for signed distance fields.\"\"\"\n\n    @staticmethod\n    def forward(\n        ctx,\n        sdf: torch.Tensor,\n        position: torch.Tensor,\n        orientation: torch.Tensor,\n        inv_scale: torch.Tensor,\n        threshold: Optional[float] = 0.0,\n        camera: Optional[Camera] = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"Render depth image of a 7-DOF discrete signed distance field on the GPU.\n\n        Also see render_depth_gpu for documentation.\n\n        Args:\n            ctx:\n                Context object to stash information.\n                See https://pytorch.org/docs/stable/notes/extending.html.\n            sdf:\n                Discrete signed distance field with shape (M, M, M).\n                Arbitrary (but uniform) resolutions are supported.\n            position:\n                The position of the signed distance field origin in the camera frame.\n            orientation:\n                The orientation of the SDF as a normalized quaternion.\n            inv_scale:\n                The inverted scale of the SDF. The scale of an SDF the half-width of the\n                full SDF volume.\n            threshold:\n                The distance threshold at which sphere tracing should be stopped.\n                Smaller value will be more accurate, but slower and might potentially\n                lead to holes in the rendering for thin structures in the SDF.\n                Larger values will be faster, but will overestimate the thickness.\n\n                Should always be positive to guarantee convergence.\n            camera:\n                Camera parameters.\n        Returns:\n            The rendered depth image.\n        \"\"\"\n        fx, fy, cx, cy, _ = camera.get_pinhole_camera_parameters(0.5)\n        (image,) = sdf_renderer_cpp.forward(\n            sdf,\n            position,\n            orientation,\n            inv_scale,\n            camera.width,\n            camera.height,\n            cx,\n            cy,\n            fx,\n            fy,\n            threshold,\n        )\n        ctx.save_for_backward(image, sdf, position, orientation, inv_scale)\n        ctx.width = camera.width\n        ctx.height = camera.height\n        ctx.fx = fx\n        ctx.fy = fy\n        ctx.cx = cx\n        ctx.cy = cy\n        return image\n\n    @staticmethod\n    def backward(ctx, grad_depth_image: torch.Tensor):\n        \"\"\"Compute gradients of inputs with respect to the provided gradients.\n\n        Normally called by PyTorch as part of a call to backward() on a loss.\n\n        Args:\n            grad_depth_image:\n        Returns:\n            Gradients of\n                discretized signed distance field, position, orientation, inverted scale\n                followed by None for all the non-supported variables passed to forward.\n        \"\"\"\n        g_sdf = g_p = g_q = g_is = g_thresh = g_camera = None\n        g_sdf, g_p, g_q, g_is = sdf_renderer_cpp.backward(\n            grad_depth_image,\n            *ctx.saved_tensors,\n            ctx.width,\n            ctx.height,\n            ctx.cx,\n            ctx.cy,\n            ctx.fx,\n            ctx.fy\n        )\n        return g_sdf, g_p, g_q, g_is, g_thresh, g_camera\n</code></pre>"},{"location":"reference/differentiable_renderer/sdf_renderer/#sdfest.differentiable_renderer.sdf_renderer.SDFRendererFunctionGPU.backward","title":"<code>backward(ctx, grad_depth_image)</code>  <code>staticmethod</code>","text":"<p>Compute gradients of inputs with respect to the provided gradients.</p> <p>Normally called by PyTorch as part of a call to backward() on a loss.</p> <p>Parameters:</p> Name Type Description Default <code>grad_depth_image</code> <code>Tensor</code> required <p>Returns:     Gradients of         discretized signed distance field, position, orientation, inverted scale         followed by None for all the non-supported variables passed to forward.</p> Source code in <code>sdfest/differentiable_renderer/sdf_renderer.py</code> <pre><code>@staticmethod\ndef backward(ctx, grad_depth_image: torch.Tensor):\n    \"\"\"Compute gradients of inputs with respect to the provided gradients.\n\n    Normally called by PyTorch as part of a call to backward() on a loss.\n\n    Args:\n        grad_depth_image:\n    Returns:\n        Gradients of\n            discretized signed distance field, position, orientation, inverted scale\n            followed by None for all the non-supported variables passed to forward.\n    \"\"\"\n    g_sdf = g_p = g_q = g_is = g_thresh = g_camera = None\n    g_sdf, g_p, g_q, g_is = sdf_renderer_cpp.backward(\n        grad_depth_image,\n        *ctx.saved_tensors,\n        ctx.width,\n        ctx.height,\n        ctx.cx,\n        ctx.cy,\n        ctx.fx,\n        ctx.fy\n    )\n    return g_sdf, g_p, g_q, g_is, g_thresh, g_camera\n</code></pre>"},{"location":"reference/differentiable_renderer/sdf_renderer/#sdfest.differentiable_renderer.sdf_renderer.SDFRendererFunctionGPU.forward","title":"<code>forward(ctx, sdf, position, orientation, inv_scale, threshold=0.0, camera=None)</code>  <code>staticmethod</code>","text":"<p>Render depth image of a 7-DOF discrete signed distance field on the GPU.</p> <p>Also see render_depth_gpu for documentation.</p> <p>Parameters:</p> Name Type Description Default <code>ctx</code> <p>Context object to stash information. See https://pytorch.org/docs/stable/notes/extending.html.</p> required <code>sdf</code> <code>Tensor</code> <p>Discrete signed distance field with shape (M, M, M). Arbitrary (but uniform) resolutions are supported.</p> required <code>position</code> <code>Tensor</code> <p>The position of the signed distance field origin in the camera frame.</p> required <code>orientation</code> <code>Tensor</code> <p>The orientation of the SDF as a normalized quaternion.</p> required <code>inv_scale</code> <code>Tensor</code> <p>The inverted scale of the SDF. The scale of an SDF the half-width of the full SDF volume.</p> required <code>threshold</code> <code>Optional[float]</code> <p>The distance threshold at which sphere tracing should be stopped. Smaller value will be more accurate, but slower and might potentially lead to holes in the rendering for thin structures in the SDF. Larger values will be faster, but will overestimate the thickness.</p> <p>Should always be positive to guarantee convergence.</p> <code>0.0</code> <code>camera</code> <code>Optional[Camera]</code> <p>Camera parameters.</p> <code>None</code> <p>Returns:     The rendered depth image.</p> Source code in <code>sdfest/differentiable_renderer/sdf_renderer.py</code> <pre><code>@staticmethod\ndef forward(\n    ctx,\n    sdf: torch.Tensor,\n    position: torch.Tensor,\n    orientation: torch.Tensor,\n    inv_scale: torch.Tensor,\n    threshold: Optional[float] = 0.0,\n    camera: Optional[Camera] = None,\n) -&gt; torch.Tensor:\n    \"\"\"Render depth image of a 7-DOF discrete signed distance field on the GPU.\n\n    Also see render_depth_gpu for documentation.\n\n    Args:\n        ctx:\n            Context object to stash information.\n            See https://pytorch.org/docs/stable/notes/extending.html.\n        sdf:\n            Discrete signed distance field with shape (M, M, M).\n            Arbitrary (but uniform) resolutions are supported.\n        position:\n            The position of the signed distance field origin in the camera frame.\n        orientation:\n            The orientation of the SDF as a normalized quaternion.\n        inv_scale:\n            The inverted scale of the SDF. The scale of an SDF the half-width of the\n            full SDF volume.\n        threshold:\n            The distance threshold at which sphere tracing should be stopped.\n            Smaller value will be more accurate, but slower and might potentially\n            lead to holes in the rendering for thin structures in the SDF.\n            Larger values will be faster, but will overestimate the thickness.\n\n            Should always be positive to guarantee convergence.\n        camera:\n            Camera parameters.\n    Returns:\n        The rendered depth image.\n    \"\"\"\n    fx, fy, cx, cy, _ = camera.get_pinhole_camera_parameters(0.5)\n    (image,) = sdf_renderer_cpp.forward(\n        sdf,\n        position,\n        orientation,\n        inv_scale,\n        camera.width,\n        camera.height,\n        cx,\n        cy,\n        fx,\n        fy,\n        threshold,\n    )\n    ctx.save_for_backward(image, sdf, position, orientation, inv_scale)\n    ctx.width = camera.width\n    ctx.height = camera.height\n    ctx.fx = fx\n    ctx.fy = fy\n    ctx.cx = cx\n    ctx.cy = cy\n    return image\n</code></pre>"},{"location":"reference/differentiable_renderer/sdf_renderer/#sdfest.differentiable_renderer.sdf_renderer.render_depth_gpu","title":"<code>render_depth_gpu(sdf, position, orientation, inv_scale, width=None, height=None, fov_deg=None, threshold=0.0, camera=None)</code>","text":"<p>Render depth image of a 7-DOF discrete signed distance field on the GPU.</p> <p>The SDF position is assumed to be in the camera frame under OpenGL convention.</p> <p>That is, camera looks along negative z-axis, y pointing upwards and x to the right. Note that the rendered image will still follow the classical computer vision convention, of first row being up in the camera frame.</p> <p>Camera can be specified either via camera parameter giving most flexbility or alternatively by providing width, height and fov_deg.</p> <p>All provided tensors must reside on the GPU.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>Tensor</code> <p>Discrete signed distance field with shape (M, M, M). Arbitrary (but uniform) resolutions are supported.</p> required <code>position</code> <code>Tensor</code> <p>The position of the signed distance field origin in the camera frame.</p> required <code>orientation</code> <code>Tensor</code> <p>The orientation of the SDF as a normalized quaternion.</p> required <code>inv_scale</code> <code>Tensor</code> <p>The inverted scale of the SDF. The scale of an SDF the half-width of the full SDF volume.</p> required <code>width</code> <code>Optional[int]</code> <p>Number of pixels in x direction. Recommended to use camera instead.</p> <code>None</code> <code>height</code> <code>Optional[int]</code> <p>Number of pixels in y direction. Recommended to use camera instead.</p> <code>None</code> <code>fov_deg</code> <code>Optional[float]</code> <p>The horizontal field of view (i.e., in x direction). Pixels are assumed to be square, i.e., fx=fy, computed based on width and fov_deg. Recommended to use camera instead.</p> <code>None</code> <code>threshold</code> <code>Optional[float]</code> <p>The distance threshold at which sphere tracing should be stopped. Smaller value will be more accurate, but slower and might potentially lead to holes in the rendering for thin structures in the SDF. Larger values will be faster, but will overestimate the thickness.</p> <p>Should always be positive to guarantee convergence.</p> <code>0.0</code> <code>camera</code> <code>Optional[Camera]</code> <p>Camera parameters.</p> <code>None</code> <p>Returns:     The rendered depth image.</p> Source code in <code>sdfest/differentiable_renderer/sdf_renderer.py</code> <pre><code>def render_depth_gpu(\n    sdf: torch.Tensor,\n    position: torch.Tensor,\n    orientation: torch.Tensor,\n    inv_scale: torch.Tensor,\n    width: Optional[int] = None,\n    height: Optional[int] = None,\n    fov_deg: Optional[float] = None,\n    threshold: Optional[float] = 0.0,\n    camera: Optional[Camera] = None,\n):\n    \"\"\"Render depth image of a 7-DOF discrete signed distance field on the GPU.\n\n    The SDF position is assumed to be in the camera frame under OpenGL convention.\n\n    That is, camera looks along negative z-axis, y pointing upwards and x to the\n    right. Note that the rendered image will still follow the classical computer\n    vision convention, of first row being up in the camera frame.\n\n    Camera can be specified either via camera parameter giving most\n    flexbility or alternatively by providing width, height and fov_deg.\n\n    All provided tensors must reside on the GPU.\n\n    Args:\n        sdf:\n            Discrete signed distance field with shape (M, M, M).\n            Arbitrary (but uniform) resolutions are supported.\n        position:\n            The position of the signed distance field origin in the camera frame.\n        orientation:\n            The orientation of the SDF as a normalized quaternion.\n        inv_scale:\n            The inverted scale of the SDF. The scale of an SDF the half-width of the\n            full SDF volume.\n        width:\n            Number of pixels in x direction. Recommended to use camera instead.\n        height:\n            Number of pixels in y direction. Recommended to use camera instead.\n        fov_deg:\n            The horizontal field of view (i.e., in x direction).\n            Pixels are assumed to be square, i.e., fx=fy, computed based on width\n            and fov_deg.\n            Recommended to use camera instead.\n        threshold:\n            The distance threshold at which sphere tracing should be stopped.\n            Smaller value will be more accurate, but slower and might potentially\n            lead to holes in the rendering for thin structures in the SDF.\n            Larger values will be faster, but will overestimate the thickness.\n\n            Should always be positive to guarantee convergence.\n        camera:\n            Camera parameters.\n    Returns:\n        The rendered depth image.\n    \"\"\"\n    if None not in [width, height, fov_deg] and camera is not None:\n        raise ValueError(\"Either width+height+fov_dev or camera must be provided.\")\n    if camera is None:\n        f = width / math.tan(fov_deg * math.pi / 180.0 / 2.0) / 2\n        camera = Camera(width, height, f, f, width / 2, height / 2, pixel_center=0.5)\n\n    return SDFRendererFunctionGPU.apply(\n        sdf, position, orientation, inv_scale, threshold, camera\n    )\n</code></pre>"},{"location":"reference/differentiable_renderer/scripts/experiments/","title":"experiments","text":"<p>Experiments with differentiable SDF renderer.</p> Usage <p>python -m sdfest.differentiable_renderer.scripts.experiments --sdf_path data/shapenet_processed/mug_filtered/00001.npy --scale 0.15      --scale_off --pos 0 0 -0.4 --pos_off 0.03 0.03 0.03 --rot 400 90 0 --rot_off 10 10 10 --gpu --visualize 10 --steps 1000</p>"},{"location":"reference/estimation/losses/","title":"losses","text":"<p>Module containing loss functions.</p>"},{"location":"reference/estimation/losses/#sdfest.estimation.losses.nn_loss","title":"<code>nn_loss(points_from, points_to)</code>","text":"<p>Compute the distance to the closest neighbor in the other set of points.</p> <p>Parameters:</p> Name Type Description Default <code>points_from</code> <code>Tensor</code> <p>The first point set. Shape NxD, with N points of dimension D.</p> required <code>points_to</code> <code>Tensor</code> <p>The second point set. Shape MxD, with M points of dimension D.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Squared distance from all points in the points_from set to the closest point in</p> <code>Tensor</code> <p>points to set. Output shape is (N,).</p> Source code in <code>sdfest/estimation/losses.py</code> <pre><code>def nn_loss(points_from: torch.Tensor, points_to: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute the distance to the closest neighbor in the other set of points.\n\n    Params:\n        points_from:\n            The first point set. Shape NxD, with N points of dimension D.\n        points_to:\n            The second point set. Shape MxD, with M points of dimension D.\n\n    Returns:\n        Squared distance from all points in the points_from set to the closest point in\n        points to set. Output shape is (N,).\n    \"\"\"\n    a = torch.sum(points_from ** 2, dim=1)\n    b = torch.mm(points_from, points_to.t())\n    c = torch.sum(points_to ** 2, dim=1)\n\n    # compute the distance matrix\n    d = -2 * b + a.unsqueeze(1) + c.unsqueeze(0)\n    d[d &lt; 0] = 0  # TODO why is it negative sometimes? numerical issues?\n    d, _ = d.min(1)\n    return d\n</code></pre>"},{"location":"reference/estimation/losses/#sdfest.estimation.losses.pc_loss","title":"<code>pc_loss(points, position, orientation, scale, sdf)</code>","text":"<p>Compute trilinerly interpolated SDF value at point positions.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>Tensor</code> <p>pointcloud in camera frame, shape (M, 4)</p> required <code>position</code> <code>Tensor</code> <p>position of SDF center in camera frame, shape (3,)</p> required <code>orientation</code> <code>Tensor</code> <p>quaternion representing orientation of SDF, shape (4,)</p> required <code>scale</code> <code>Tensor</code> <p>half-width of SDF volume</p> required <code>sdf</code> <code>Tensor</code> <p>volumetric signed distance field, shape (res, res, res), assuming same resolution along each axis</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Trilinearly interpolated distance at the passed points 0 if outside of SDF</p> <code>Tensor</code> <p>volume. Distance is in world coordinates (i.e., after scaling the SDF).</p> Source code in <code>sdfest/estimation/losses.py</code> <pre><code>def pc_loss(\n    points: torch.Tensor,\n    position: torch.Tensor,\n    orientation: torch.Tensor,\n    scale: torch.Tensor,\n    sdf: torch.Tensor,\n) -&gt; torch.Tensor:\n    \"\"\"Compute trilinerly interpolated SDF value at point positions.\n\n    Args:\n        points:\n            pointcloud in camera frame, shape (M, 4)\n        position:\n            position of SDF center in camera frame, shape (3,)\n        orientation:\n            quaternion representing orientation of SDF, shape (4,)\n        scale:\n            half-width of SDF volume\n        sdf:\n            volumetric signed distance field, shape (res, res, res),\n            assuming same resolution along each axis\n\n    Returns:\n        Trilinearly interpolated distance at the passed points 0 if outside of SDF\n        volume. Distance is in world coordinates (i.e., after scaling the SDF).\n    \"\"\"\n    q = orientation / torch.norm(orientation)  # to get normalization gradients\n    obj_points = points - position.unsqueeze(0)\n\n    # Quaternion to rotation matrix\n    # Note that we use conjugate here since we want to transform points from\n    # world to object coordinates and the quaternion describes rotation of\n    # object coordinate system in world coordinates.\n    R = obj_points.new_zeros(3, 3)\n\n    R[0, 0] = 1 - 2 * (q[1] * q[1] + q[2] * q[2])\n    R[0, 1] = 2 * (q[0] * q[1] + q[2] * q[3])\n    R[0, 2] = 2 * (q[0] * q[2] - q[3] * q[1])\n\n    R[1, 0] = 2 * (q[0] * q[1] - q[2] * q[3])\n    R[1, 1] = 1 - 2 * (q[0] * q[0] + q[2] * q[2])\n    R[1, 2] = 2 * (q[1] * q[2] + q[3] * q[0])\n\n    R[2, 0] = 2 * (q[0] * q[2] + q[3] * q[1])\n    R[2, 1] = 2 * (q[1] * q[2] - q[3] * q[0])\n    R[2, 2] = 1 - 2 * (q[0] * q[0] + q[1] * q[1])\n\n    obj_points = (R @ obj_points.T).T\n\n    # Transform to canonical coordintes obj_point in [-1,1]^3\n    obj_point = obj_points / scale\n\n    # Compute cell and cell position\n    res = sdf.shape[0]  # assuming same resolution along each axis\n    grid_size = 2.0 / (res - 1)\n    c = torch.floor((obj_point + 1.0) * (res - 1) * 0.5)\n    mask = torch.logical_or(\n        torch.min(c, dim=1)[0] &lt; 0, torch.max(c, dim=1)[0] &gt; res - 2\n    )\n    c = torch.clip(c, 0, res - 2)  # base cell index of each point\n    cell_position = c * grid_size - 1.0  # base cell position of each point\n    sdf_indices = c.new_empty((obj_point.shape[0], 8), dtype=torch.long)\n    sdf_indices[:, 0] = c[:, 0] * res ** 2 + c[:, 1] * res + c[:, 2]\n    sdf_indices[:, 1] = c[:, 0] * res ** 2 + c[:, 1] * res + c[:, 2] + 1\n    sdf_indices[:, 2] = c[:, 0] * res ** 2 + (c[:, 1] + 1) * res + c[:, 2]\n    sdf_indices[:, 3] = c[:, 0] * res ** 2 + (c[:, 1] + 1) * res + c[:, 2] + 1\n    sdf_indices[:, 4] = (c[:, 0] + 1) * res ** 2 + c[:, 1] * res + c[:, 2]\n    sdf_indices[:, 5] = (c[:, 0] + 1) * res ** 2 + c[:, 1] * res + c[:, 2] + 1\n    sdf_indices[:, 6] = (c[:, 0] + 1) * res ** 2 + (c[:, 1] + 1) * res + c[:, 2]\n    sdf_indices[:, 7] = (c[:, 0] + 1) * res ** 2 + (c[:, 1] + 1) * res + c[:, 2] + 1\n    sdf_view = sdf.view([-1])\n    point_cell_position = (obj_point - cell_position) / grid_size  # [0,1]^3\n    sdf_values = torch.take(sdf_view, sdf_indices)\n\n    # trilinear interpolation\n    sdf_value = sdf_values.new_empty(obj_points.shape[0])\n    # sdf_value = obj_point[:, 0]\n    sdf_value = (\n        (\n            sdf_values[:, 0] * (1 - point_cell_position[:, 0])\n            + sdf_values[:, 4] * point_cell_position[:, 0]\n        )\n        * (1 - point_cell_position[:, 1])\n        + (\n            sdf_values[:, 2] * (1 - point_cell_position[:, 0])\n            + sdf_values[:, 6] * point_cell_position[:, 0]\n        )\n        * point_cell_position[:, 1]\n    ) * (1 - point_cell_position[:, 2]) + (\n        (\n            sdf_values[:, 1] * (1 - point_cell_position[:, 0])\n            + sdf_values[:, 5] * point_cell_position[:, 0]\n        )\n        * (1 - point_cell_position[:, 1])\n        + (\n            sdf_values[:, 3] * (1 - point_cell_position[:, 0])\n            + sdf_values[:, 7] * point_cell_position[:, 0]\n        )\n        * point_cell_position[:, 1]\n    ) * point_cell_position[\n        :, 2\n    ]\n    sdf_value[mask] = 0\n    return sdf_value * scale\n</code></pre>"},{"location":"reference/estimation/losses/#sdfest.estimation.losses.point_constraint_loss","title":"<code>point_constraint_loss(orientation_q, source, target)</code>","text":"<p>Compute Euclidean distance between rotated source point and target point.</p> <p>Parameters:</p> Name Type Description Default <code>orientation_q</code> <code>Tensor</code> <p>Orientation of object in world / camera frame as quaternion. Scalar-last convention. Shape (4,).</p> required <code>source</code> <code>Tensor</code> <p>Point in object frame, which will be transformed. (3,).</p> required <code>target</code> <code>Tensor</code> <p>Point in rotated oject frame. Shape (3,).</p> required <p>Returns:     Euclidean norm between R(orientation_q) @ source - target. Scalar.</p> Source code in <code>sdfest/estimation/losses.py</code> <pre><code>def point_constraint_loss(\n    orientation_q: torch.Tensor, source: torch.Tensor, target: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"Compute Euclidean distance between rotated source point and target point.\n\n    Args:\n        orientation_q:\n            Orientation of object in world / camera frame as quaternion.\n            Scalar-last convention. Shape (4,).\n        source: Point in object frame, which will be transformed. (3,).\n        target: Point in rotated oject frame. Shape (3,).\n    Returns:\n        Euclidean norm between R(orientation_q) @ source - target. Scalar.\n    \"\"\"\n    rotated_source = quaternion_utils.quaternion_apply(orientation_q, source)\n    return torch.linalg.norm(rotated_source - target)\n</code></pre>"},{"location":"reference/estimation/metrics/","title":"metrics","text":"<p>Metrics for shape evaluation.</p>"},{"location":"reference/estimation/metrics/#sdfest.estimation.metrics.accuracy_thresh","title":"<code>accuracy_thresh(points_gt, points_rec, threshold, p_norm=2, normalize=False)</code>","text":"<p>Compute thresholded accuracy metric.</p> <p>See FroDO: From Detections to 3D Objects, Ru\u0308nz et al., 2020.</p> <p>Parameters:</p> Name Type Description Default <code>points_gt</code> <code>ndarray</code> <p>set of true points, expected shape (N,3)</p> required <code>points_rec</code> <code>ndarray</code> <p>set of reconstructed points, expected shape (M,3)</p> required <code>threshold</code> <code>float</code> <p>distance threshold to count a point as correct</p> required <code>p_norm</code> <code>int</code> <p>which Minkowski p-norm is used for distance and nearest neighbor query</p> <code>2</code> <code>normalize</code> <code>bool</code> <p>whether to divide distances by Euclidean extent of points_gt</p> <code>False</code> <p>Returns:     Ratio of reconstructed points with closest ground truth point closer than     threshold (in p-norm).</p> Source code in <code>sdfest/estimation/metrics.py</code> <pre><code>def accuracy_thresh(\n    points_gt: np.ndarray,\n    points_rec: np.ndarray,\n    threshold: float,\n    p_norm: int = 2,\n    normalize: bool = False,\n) -&gt; float:\n    \"\"\"Compute thresholded accuracy metric.\n\n    See FroDO: From Detections to 3D Objects, Ru\u0308nz et al., 2020.\n\n    Args:\n        points_gt: set of true points, expected shape (N,3)\n        points_rec: set of reconstructed points, expected shape (M,3)\n        threshold: distance threshold to count a point as correct\n        p_norm: which Minkowski p-norm is used for distance and nearest neighbor query\n        normalize: whether to divide distances by Euclidean extent of points_gt\n    Returns:\n        Ratio of reconstructed points with closest ground truth point closer than\n        threshold (in p-norm).\n    \"\"\"\n    kd_tree = scipy.spatial.KDTree(points_gt)\n    d, _ = kd_tree.query(points_rec, p=p_norm)\n    if normalize:\n        return np.sum(d / extent(points_gt) &lt; threshold) / points_rec.shape[0]\n    else:\n        return np.sum(d &lt; threshold) / points_rec.shape[0]\n</code></pre>"},{"location":"reference/estimation/metrics/#sdfest.estimation.metrics.completeness_thresh","title":"<code>completeness_thresh(points_gt, points_rec, threshold, p_norm=2, normalize=False)</code>","text":"<p>Compute thresholded completion metric.</p> <p>See FroDO: From Detections to 3D Objects, Ru\u0308nz et al., 2020.</p> <p>Parameters:</p> Name Type Description Default <code>points_gt</code> <code>ndarray</code> <p>set of true points, expected shape (N,3)</p> required <code>points_rec</code> <code>ndarray</code> <p>set of reconstructed points, expected shape (M,3)</p> required <code>threshold</code> <code>float</code> <p>distance threshold to count a point as correct</p> required <code>p_norm</code> <code>int</code> <p>which Minkowski p-norm is used for distance and nearest neighbor query</p> <code>2</code> <code>normalize</code> <code>bool</code> <p>whether to divide distances by Euclidean extent of points_gt</p> <code>False</code> <p>Returns:     Ratio of ground truth points with closest reconstructed point closer than     threshold (in p-norm).</p> Source code in <code>sdfest/estimation/metrics.py</code> <pre><code>def completeness_thresh(\n    points_gt: np.ndarray,\n    points_rec: np.ndarray,\n    threshold: float,\n    p_norm: int = 2,\n    normalize: bool = False,\n) -&gt; float:\n    \"\"\"Compute thresholded completion metric.\n\n    See FroDO: From Detections to 3D Objects, Ru\u0308nz et al., 2020.\n\n    Args:\n        points_gt: set of true points, expected shape (N,3)\n        points_rec: set of reconstructed points, expected shape (M,3)\n        threshold: distance threshold to count a point as correct\n        p_norm: which Minkowski p-norm is used for distance and nearest neighbor query\n        normalize: whether to divide distances by Euclidean extent of points_gt\n    Returns:\n        Ratio of ground truth points with closest reconstructed point closer than\n        threshold (in p-norm).\n    \"\"\"\n    kd_tree = scipy.spatial.KDTree(points_rec)\n    d, _ = kd_tree.query(points_gt, p=p_norm)\n    if normalize:\n        return np.sum(d / extent(points_gt) &lt; threshold) / points_gt.shape[0]\n    else:\n        return np.sum(d &lt; threshold) / points_gt.shape[0]\n</code></pre>"},{"location":"reference/estimation/metrics/#sdfest.estimation.metrics.correct_thresh","title":"<code>correct_thresh(position_gt, position_prediction, orientation_gt, orientation_prediction, extent_gt=None, extent_prediction=None, points_gt=None, points_prediction=None, position_threshold=None, degree_threshold=None, iou_3d_threshold=None, fscore_threshold=None, rotational_symmetry_axis=None)</code>","text":"<p>Classify a pose prediction as correct or incorrect.</p> <p>Parameters:</p> Name Type Description Default <code>position_gt</code> <code>ndarray</code> <p>ground truth position, expected shape (3,).</p> required <code>position_prediction</code> <code>ndarray</code> <p>predicted position, expected shape (3,).</p> required <code>position_threshold</code> <code>Optional[float]</code> <p>position threshold in meters, no threshold if None</p> <code>None</code> <code>orientation_q_qt</code> <p>ground truth orientation, scalar-last quaternion, shape (4,)</p> required <code>orientation_q_prediction</code> <p>predicted orientation, scalar-last quaternion, shape (4,)</p> required <code>extent_gt</code> <code>Optional[ndarray]</code> <p>bounding box extents, shape (3,) only used if IoU threshold specified</p> <code>None</code> <code>extent_prediction</code> <code>Optional[ndarray]</code> <p>bounding box extents, shape (3,) only used if IoU threshold specified</p> <code>None</code> <code>point_gt</code> <p>set of true points, expected shape (N,3)</p> required <code>points_rec</code> <p>set of reconstructed points, expected shape (M,3)</p> required <code>degree_threshold</code> <code>Optional[float]</code> <p>orientation threshold in degrees, no threshold if None</p> <code>None</code> <code>iou_3d_threshold</code> <code>Optional[float]</code> <p>3D IoU threshold, no threshold if None</p> <code>None</code> <code>rotational_symmetry_axis</code> <code>Optional[int]</code> <p>Specify axis along which rotation is ignored. If None, no axis is ignored. 0 for x-axis, 1 for y-axis, 2 for z-axis.</p> <code>None</code> <p>Returns:     1 if error is below all provided thresholds.  0 if error is above one provided     threshold.</p> Source code in <code>sdfest/estimation/metrics.py</code> <pre><code>def correct_thresh(\n    position_gt: np.ndarray,\n    position_prediction: np.ndarray,\n    orientation_gt: Rotation,\n    orientation_prediction: Rotation,\n    extent_gt: Optional[np.ndarray] = None,\n    extent_prediction: Optional[np.ndarray] = None,\n    points_gt: Optional[np.ndarray] = None,\n    points_prediction: Optional[np.ndarray] = None,\n    position_threshold: Optional[float] = None,\n    degree_threshold: Optional[float] = None,\n    iou_3d_threshold: Optional[float] = None,\n    fscore_threshold: Optional[float] = None,\n    rotational_symmetry_axis: Optional[int] = None,\n) -&gt; int:\n    \"\"\"Classify a pose prediction as correct or incorrect.\n\n    Args:\n        position_gt: ground truth position, expected shape (3,).\n        position_prediction: predicted position, expected shape (3,).\n        position_threshold: position threshold in meters, no threshold if None\n        orientation_q_qt: ground truth orientation, scalar-last quaternion, shape (4,)\n        orientation_q_prediction:\n            predicted orientation, scalar-last quaternion, shape (4,)\n        extent_gt:\n            bounding box extents, shape (3,)\n            only used if IoU threshold specified\n        extent_prediction:\n            bounding box extents, shape (3,)\n            only used if IoU threshold specified\n        point_gt: set of true points, expected shape (N,3)\n        points_rec: set of reconstructed points, expected shape (M,3)\n        degree_threshold: orientation threshold in degrees, no threshold if None\n        iou_3d_threshold: 3D IoU threshold, no threshold if None\n        rotational_symmetry_axis:\n            Specify axis along which rotation is ignored. If None, no axis is ignored.\n            0 for x-axis, 1 for y-axis, 2 for z-axis.\n    Returns:\n        1 if error is below all provided thresholds.  0 if error is above one provided\n        threshold.\n    \"\"\"\n    if position_threshold is not None:\n        position_error = np.linalg.norm(position_gt - position_prediction)\n        if position_error &gt; position_threshold:\n            return 0\n    if degree_threshold is not None:\n        rad_threshold = degree_threshold * np.pi / 180.0\n        if rotational_symmetry_axis is not None:\n            p = np.array([0.0, 0.0, 0.0])\n            p[rotational_symmetry_axis] = 1.0\n            p1 = orientation_gt.apply(p)\n            p2 = orientation_prediction.apply(p)\n            rad_error = np.arccos(p1 @ p2)\n        else:\n            rad_error = (orientation_gt * orientation_prediction.inv()).magnitude()\n        if rad_error &gt; rad_threshold:\n            return 0\n    if iou_3d_threshold is not None:\n        raise NotImplementedError(\"3D IoU is not impemented yet.\")\n        # TODO implement 3D IoU\n        # starting point for proper implementation: https://github.com/google-research-datasets/Objectron/blob/c06a65165a18396e1e00091981fd1652875c97b5/objectron/dataset/iou.py#L6\n        pass\n    if fscore_threshold is not None:\n        fscore = reconstruction_fscore(points_gt, points_prediction, 0.01)\n        if fscore &lt; fscore_threshold:\n            return 0\n    return 1\n</code></pre>"},{"location":"reference/estimation/metrics/#sdfest.estimation.metrics.extent","title":"<code>extent(points)</code>","text":"<p>Compute largest Euclidean distance between any two points.</p> <p>Parameters:</p> Name Type Description Default <code>points_gt</code> <p>set of true</p> required <code>p_norm</code> <p>which Minkowski p-norm is used for distance and nearest neighbor query</p> required <p>Returns:     Ratio of reconstructed points with closest ground truth point closer than     threshold (in p-norm).</p> Source code in <code>sdfest/estimation/metrics.py</code> <pre><code>def extent(points: np.ndarray) -&gt; float:\n    \"\"\"Compute largest Euclidean distance between any two points.\n\n    Args:\n        points_gt: set of true\n        p_norm: which Minkowski p-norm is used for distance and nearest neighbor query\n    Returns:\n        Ratio of reconstructed points with closest ground truth point closer than\n        threshold (in p-norm).\n    \"\"\"\n    try:\n        hull = scipy.spatial.ConvexHull(points)\n    except scipy.spatial.qhull.QhullError:\n        # fallback to brute force distance matrix\n        return np.max(scipy.spatial.distance_matrix(points, points))\n\n    # this is wasteful, if too slow implement rotating caliper method\n    return np.max(\n        scipy.spatial.distance_matrix(points[hull.vertices], points[hull.vertices])\n    )\n</code></pre>"},{"location":"reference/estimation/metrics/#sdfest.estimation.metrics.mean_accuracy","title":"<code>mean_accuracy(points_gt, points_rec, p_norm=2, normalize=False)</code>","text":"<p>Compute accuracy metric.</p> <p>Accuracy metric is the same as asymmetric chamfer distance from rec to gt.</p> <p>See, for example, Occupancy Networks Learning 3D Reconstruction in Function Space, Mescheder et al., 2019.</p> <p>Parameters:</p> Name Type Description Default <code>points_gt</code> <code>ndarray</code> <p>set of true points, expected shape (N,3)</p> required <code>points_rec</code> <code>ndarray</code> <p>set of reconstructed points, expected shape (M,3)</p> required <code>p_norm</code> <code>int</code> <p>which Minkowski p-norm is used for distance and nearest neighbor query</p> <code>2</code> <code>normalize</code> <code>bool</code> <p>whether to divide result by Euclidean extent of points_gt</p> <code>False</code> <p>Returns:     Arithmetic mean of p-norm from reconstructed points to closest (in p-norm)     ground truth points.</p> Source code in <code>sdfest/estimation/metrics.py</code> <pre><code>def mean_accuracy(\n    points_gt: np.ndarray,\n    points_rec: np.ndarray,\n    p_norm: int = 2,\n    normalize: bool = False,\n) -&gt; float:\n    \"\"\"Compute accuracy metric.\n\n    Accuracy metric is the same as asymmetric chamfer distance from rec to gt.\n\n    See, for example, Occupancy Networks Learning 3D Reconstruction in Function Space,\n    Mescheder et al., 2019.\n\n    Args:\n        points_gt: set of true points, expected shape (N,3)\n        points_rec: set of reconstructed points, expected shape (M,3)\n        p_norm: which Minkowski p-norm is used for distance and nearest neighbor query\n        normalize: whether to divide result by Euclidean extent of points_gt\n    Returns:\n        Arithmetic mean of p-norm from reconstructed points to closest (in p-norm)\n        ground truth points.\n    \"\"\"\n    kd_tree = scipy.spatial.KDTree(points_gt)\n    d, _ = kd_tree.query(points_rec, p=p_norm)\n    if normalize:\n        return np.mean(d) / extent(points_gt)\n    else:\n        return np.mean(d)\n</code></pre>"},{"location":"reference/estimation/metrics/#sdfest.estimation.metrics.mean_completeness","title":"<code>mean_completeness(points_gt, points_rec, p_norm=2, normalize=False)</code>","text":"<p>Compute completeness metric.</p> <p>Completeness metric is the same as asymmetric chamfer distance from gt to rec.</p> <p>See, for example, Occupancy Networks Learning 3D Reconstruction in Function Space, Mescheder et al., 2019.</p> <p>Parameters:</p> Name Type Description Default <code>points_gt</code> <code>ndarray</code> <p>set of true points, expected shape (N,3)</p> required <code>points_rec</code> <code>ndarray</code> <p>set of reconstructed points, expected shape (M,3)</p> required <code>p_norm</code> <code>int</code> <p>which Minkowski p-norm is used for distance and nearest neighbor query</p> <code>2</code> <code>normalize</code> <code>bool</code> <p>whether to divide result by Euclidean extent of points_gt</p> <code>False</code> <p>Returns:     Arithmetic mean of p-norm from ground truth points to closest (in p-norm)     reconstructed points.</p> Source code in <code>sdfest/estimation/metrics.py</code> <pre><code>def mean_completeness(\n    points_gt: np.ndarray,\n    points_rec: np.ndarray,\n    p_norm: int = 2,\n    normalize: bool = False,\n) -&gt; float:\n    \"\"\"Compute completeness metric.\n\n    Completeness metric is the same as asymmetric chamfer distance from gt to rec.\n\n    See, for example, Occupancy Networks Learning 3D Reconstruction in Function Space,\n    Mescheder et al., 2019.\n\n    Args:\n        points_gt: set of true points, expected shape (N,3)\n        points_rec: set of reconstructed points, expected shape (M,3)\n        p_norm: which Minkowski p-norm is used for distance and nearest neighbor query\n        normalize: whether to divide result by Euclidean extent of points_gt\n    Returns:\n        Arithmetic mean of p-norm from ground truth points to closest (in p-norm)\n        reconstructed points.\n    \"\"\"\n    kd_tree = scipy.spatial.KDTree(points_rec)\n    d, _ = kd_tree.query(points_gt, p=p_norm)\n    if normalize:\n        return np.mean(d) / extent(points_gt)\n    else:\n        return np.mean(d)\n</code></pre>"},{"location":"reference/estimation/metrics/#sdfest.estimation.metrics.reconstruction_fscore","title":"<code>reconstruction_fscore(points_gt, points_rec, threshold, p_norm=2, normalize=False)</code>","text":"<p>Compute reconstruction fscore.</p> <p>See What Do Single-View 3D Reconstruction Networks Learn, Tatarchenko, 2019</p> <p>Parameters:</p> Name Type Description Default <code>points_gt</code> <code>ndarray</code> <p>set of true points, expected shape (N,3)</p> required <code>points_rec</code> <code>ndarray</code> <p>set of reconstructed points, expected shape (M,3)</p> required <code>threshold</code> <code>float</code> <p>distance threshold to count a point as correct</p> required <code>p_norm</code> <code>int</code> <p>which Minkowski p-norm is used for distance and nearest neighbor query</p> <code>2</code> <code>normalize</code> <code>bool</code> <p>whether to divide distances by Euclidean extent of points_gt</p> <code>False</code> <p>Returns:     Harmonic mean of precision (thresholded accuracy) and recall (thresholded     completeness).</p> Source code in <code>sdfest/estimation/metrics.py</code> <pre><code>def reconstruction_fscore(\n    points_gt: np.ndarray,\n    points_rec: np.ndarray,\n    threshold: float,\n    p_norm: int = 2,\n    normalize: bool = False,\n) -&gt; float:\n    \"\"\"Compute reconstruction fscore.\n\n    See What Do Single-View 3D Reconstruction Networks Learn, Tatarchenko, 2019\n\n    Args:\n        points_gt: set of true points, expected shape (N,3)\n        points_rec: set of reconstructed points, expected shape (M,3)\n        threshold: distance threshold to count a point as correct\n        p_norm: which Minkowski p-norm is used for distance and nearest neighbor query\n        normalize: whether to divide distances by Euclidean extent of points_gt\n    Returns:\n        Harmonic mean of precision (thresholded accuracy) and recall (thresholded\n        completeness).\n    \"\"\"\n    recall = completeness_thresh(\n        points_gt, points_rec, threshold, p_norm=p_norm, normalize=normalize\n    )\n    precision = accuracy_thresh(\n        points_gt, points_rec, threshold, p_norm=p_norm, normalize=normalize\n    )\n    if recall &lt; 1e-7 or precision &lt; 1e-7:\n        return 0\n    return 2 / (1 / recall + 1 / precision)\n</code></pre>"},{"location":"reference/estimation/metrics/#sdfest.estimation.metrics.symmetric_chamfer","title":"<code>symmetric_chamfer(points_gt, points_rec, p_norm=2, normalize=False)</code>","text":"<p>Compute symmetric chamfer distance.</p> <p>There are various slightly different definitions for the chamfer distance.</p> <p>Note that completeness and accuracy are themselves sometimes referred to as chamfer distances, with symmetric chamfer distance being the combination of the two.</p> <p>Chamfer L1 in the literature (see, for example, Occupancy Networks Learning 3D Reconstruction in Function Space, Mescheder et al., 2019) refers to using arithmetic mean (note that this is actually differently scaled from L1) when combining accuracy and completeness.</p> <p>Parameters:</p> Name Type Description Default <code>points_gt</code> <code>ndarray</code> <p>set of true points, expected shape (N,3)</p> required <code>points_rec</code> <code>ndarray</code> <p>set of reconstructed points, expected shape (M,3)</p> required <code>p_norm</code> <code>int</code> <p>which Minkowski p-norm is used for distance and nearest neighbor query</p> <code>2</code> <code>normalize</code> <code>bool</code> <p>whether to divide result by Euclidean extent of points_gt</p> <code>False</code> <p>Returns:     Arithmetic mean of accuracy and completeness metrics using the specified p-norm.</p> Source code in <code>sdfest/estimation/metrics.py</code> <pre><code>def symmetric_chamfer(\n    points_gt: np.ndarray,\n    points_rec: np.ndarray,\n    p_norm: int = 2,\n    normalize: bool = False,\n) -&gt; float:\n    \"\"\"Compute symmetric chamfer distance.\n\n    There are various slightly different definitions for the chamfer distance.\n\n    Note that completeness and accuracy are themselves sometimes referred to as\n    chamfer distances, with symmetric chamfer distance being the combination of the two.\n\n    Chamfer L1 in the literature (see, for example, Occupancy Networks Learning 3D\n    Reconstruction in Function Space, Mescheder et al., 2019) refers to using\n    arithmetic mean (note that this is actually differently scaled from L1) when\n    combining accuracy and completeness.\n\n    Args:\n        points_gt: set of true points, expected shape (N,3)\n        points_rec: set of reconstructed points, expected shape (M,3)\n        p_norm: which Minkowski p-norm is used for distance and nearest neighbor query\n        normalize: whether to divide result by Euclidean extent of points_gt\n    Returns:\n        Arithmetic mean of accuracy and completeness metrics using the specified p-norm.\n    \"\"\"\n    return (\n        mean_completeness(points_gt, points_rec, p_norm=p_norm, normalize=normalize)\n        + mean_accuracy(points_gt, points_rec, p_norm=p_norm, normalize=normalize)\n    ) / 2\n</code></pre>"},{"location":"reference/estimation/simple_setup/","title":"simple_setup","text":"<p>Modular SDF pose and shape estimation in depth images.</p>"},{"location":"reference/estimation/simple_setup/#sdfest.estimation.simple_setup.NoDepthError","title":"<code>NoDepthError</code>","text":"<p>               Bases: <code>ValueError</code></p> <p>Raised when there is no depth data left after preprocessing.</p> Source code in <code>sdfest/estimation/simple_setup.py</code> <pre><code>class NoDepthError(ValueError):\n    \"\"\"Raised when there is no depth data left after preprocessing.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/estimation/simple_setup/#sdfest.estimation.simple_setup.SDFPipeline","title":"<code>SDFPipeline</code>","text":"<p>SDF pose and shape estimation pipeline.</p> Source code in <code>sdfest/estimation/simple_setup.py</code> <pre><code>class SDFPipeline:\n    \"\"\"SDF pose and shape estimation pipeline.\"\"\"\n\n    def __init__(self, config: dict) -&gt; None:\n        \"\"\"Load and initialize the pipeline.\n\n        Args:\n            config: Configuration dictionary.\n        \"\"\"\n        # improve runtime for conv3d backward\n        # https://github.com/pytorch/pytorch/issues/32370\n        torch.backends.cudnn.enabled = False\n\n        self._parse_config(config)\n\n        self.init_network = SDFPoseNet(\n            INIT_MODULE_DICT[self.init_config[\"backbone_type\"]](\n                **self.init_config[\"backbone\"]\n            ),\n            INIT_MODULE_DICT[self.init_config[\"head_type\"]](\n                shape_dimension=self.vae_config[\"latent_size\"],\n                **self.init_config[\"head\"],\n            ),\n        ).to(self.device)\n        load_model_weights(\n            self.init_config[\"model\"],\n            self.init_network,\n            self.device,\n            self.init_config.get(\"model_url\"),\n        )\n        self.init_network.eval()\n\n        self.resolution = 64\n        self.vae = SDFVAE(\n            sdf_size=64,\n            latent_size=self.vae_config[\"latent_size\"],\n            encoder_dict=self.vae_config[\"encoder\"],\n            decoder_dict=self.vae_config[\"decoder\"],\n            device=self.device,\n        ).to(self.device)\n        load_model_weights(\n            self.vae_config[\"model\"],\n            self.vae,\n            self.device,\n            self.vae_config.get(\"model_url\"),\n        )\n        self.vae.eval()\n\n        self.cam = Camera(**self.camera_config)\n        self.render = lambda sdf, pos, quat, i_s: render_depth_gpu(\n            sdf, pos, quat, i_s, None, None, None, config[\"threshold\"], self.cam\n        )\n        self.config = config\n\n        self.log_data = []\n\n    def _parse_config(self, config: dict) -&gt; None:\n        \"\"\"Parse config dict.\n\n        This function makes sure that all required keys are available.\n        \"\"\"\n        self.device = config[\"device\"]\n        self.init_config = config[\"init\"]\n        self.vae_config = config[\"vae\"] if \"vae\" in config else self.init_config[\"vae\"]\n        self.camera_config = config[\"camera\"]\n        self.result_selection_strategy = config.get(\n            \"result_selection_strategy\", \"last_iteration\"\n        )  # last_iteration | best_inlier_ratio\n        self._relative_inlier_threshold = config.get(\n            \"relative_inlier_threshold\", 0.03\n        )  # relative depth error threshold for pixel to be considered inlier\n        if \"far_field\" in config:\n            self._far_field = config[\"far_field\"] if \"far_field\" in config else None\n\n        self.config = config\n\n    @staticmethod\n    def _compute_gradients(loss: torch.Tensor) -&gt; None:\n        loss.backward()\n\n    def _compute_view_losses(\n        self,\n        depth_input: torch.Tensor,\n        depth_estimate: torch.Tensor,\n        position: torch.Tensor,\n        orientation: torch.Tensor,\n        scale: torch.Tensor,\n        sdf: torch.Tensor,\n    ) -&gt; Tuple[torch.Tensor]:\n        # depth l1\n        overlap_mask = (depth_input &gt; 0) &amp; (depth_estimate &gt; 0)\n        depth_error = torch.abs(depth_estimate - depth_input)\n        # max_depth_error = 0.05\n        # depth_outlier_mask = depth_error &gt; max_depth_error\n        # depth_mask = overlap_mask &amp; ~depth_outlier_mask\n        # depth_error[~overlap_mask] = 0\n        loss_depth = torch.mean(depth_error[overlap_mask])\n\n        # pointcloud l1\n        pointcloud_obs = pointset_utils.depth_to_pointcloud(\n            depth_input, self.cam, normalize=False\n        )\n        pointcloud_error = losses.pc_loss(\n            pointcloud_obs,\n            position,\n            orientation,\n            scale,\n            sdf,\n        )\n        loss_pc = torch.mean(torch.abs(pointcloud_error))\n\n        # nearest neighbor l1\n        # pointcloud_outliers = pointset_utils.depth_to_pointcloud(\n        #     depth_estimate, self.cam, normalize=False, mask=depth_outlier_mask\n        # )\n\n        loss_nn = 0\n\n        # if pointcloud_outliers.shape[0] != 0:\n        # pass\n        # loss_nn += 0\n        # # TODO different gradients for point cloud (not derived by renderer)\n        # outlier_nn_d = losses.nn_loss(pointcloud_outliers, pointcloud_obs)\n        # # only use positive, because sqrt is not differentiable at 0\n        # outlier_nn_d = outlier_nn_d[outlier_nn_d &gt; 0]\n        # loss_nn = loss_nn + torch.mean(torch.sqrt(outlier_nn_d))\n\n        return loss_depth, loss_pc, loss_nn\n\n    def _compute_point_constraint_loss(self, orientation: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute loss for point constraint if specified.\"\"\"\n        if self._point_constraint is not None:\n            loss_point_constraint = losses.point_constraint_loss(\n                orientation_q=orientation[0],\n                source=self._point_constraint[0],\n                target=self._point_constraint[1],\n            )\n            weight = self._point_constraint[2]\n            return weight * loss_point_constraint\n        else:\n            return orientation.new_tensor(0.0)\n\n    def _compute_inlier_ratio(\n        self,\n        depth_input: torch.Tensor,\n        depth_estimate: torch.Tensor,\n    ) -&gt; None:\n        \"\"\"Compute ratio of pixels with small relative depth error.\"\"\"\n        rel_depth_error = torch.abs(depth_input - depth_estimate) / depth_input\n        inlier_mask = rel_depth_error &lt; self._relative_inlier_threshold\n        inliers = torch.count_nonzero(inlier_mask)\n        valid_depth_pixels = torch.count_nonzero(depth_input)\n        inlier_ratio = inliers / valid_depth_pixels\n        return inlier_ratio\n\n    def _update_best_estimate(\n        self,\n        depth_input: torch.Tensor,\n        depth_estimate: torch.Tensor,\n        position: torch.Tensor,\n        orientation: torch.Tensor,\n        scale: torch.Tensor,\n        latent_shape: torch.Tensor,\n    ) -&gt; None:\n        \"\"\"Update the best current estimate by keeping track of inlier ratio.\n\n        Returns:\n            Inlier ratio of this configuration.\n        \"\"\"\n        inlier_ratio = self._compute_inlier_ratio(depth_input, depth_estimate)\n        if self._best_inlier_ratio is None or inlier_ratio &gt; self._best_inlier_ratio:\n            self._best_inlier_ratio = inlier_ratio\n            self._best_position = position\n            self._best_orientation = orientation\n            self._best_scale = scale\n            self._best_latent_shape = latent_shape\n        return inlier_ratio\n\n    def __call__(\n        self,\n        depth_images: torch.Tensor,\n        masks: torch.Tensor,\n        color_images: torch.Tensor,\n        visualize: bool = False,\n        camera_positions: Optional[torch.Tensor] = None,\n        camera_orientations: Optional[torch.Tensor] = None,\n        log_path: Optional[str] = None,\n        shape_optimization: bool = True,\n        animation_path: Optional[str] = None,\n        point_constraint: Optional[Tuple[torch.Tensor]] = None,\n        prior_orientation_distribution: Optional[torch.Tensor] = None,\n        training_orientation_distribution: Optional[torch.Tensor] = None,\n    ) -&gt; tuple:\n        \"\"\"Infer pose, size and latent representation from depth and mask.\n\n        If multiple images are passed the cameras are assumed to be fixed.\n        All tensors should be on the same device as the pipeline.\n\n        Batch dimension N must be provided either for all or none of the arguments.\n\n        Args:\n            depth_images:\n                The depth map containing the distance along the camera's z-axis.\n                Does not have to be masked, necessary preprocessing is done by pipeline.\n                Will be masked and preprocessed in-place (pass copy if full depth is\n                used afterwards).\n                Shape (N, H, W) or (H, W) for a single depth image.\n            masks:\n                binary mask of the object to estimate, same shape as depth_images\n            color_images:\n                the color image (currently only used in visualization),\n                shape (N, H, W, 3) or (H, W, 3), RGB, float, 0-1.\n            visualize: Whether to visualize the intermediate steps and final result.\n            camera_positions:\n                position of camera in world coordinates for each image,\n                if None, (0,0,0) will be assumed for all images,\n                shape (N, 3) or (3,)\n            camera_orientations:\n                orientation of camera in world-frame as normalized quaternion,\n                quaternion is in scalar-last convention,\n                note, that this is the quaternion that transforms a point from camera\n                to world-frame\n                if None, (0,0,0,1) will be assumed for all images,\n                shape (N, 4) or (4,)\n            log_path:\n                file path to write timestamps and intermediate steps to,\n                no logging is performed if None\n            shape_optimization:\n                enable or disable shape optimization during iterative optimization\n            animation_path:\n                file path to write rendering and error visualizations to\n            point_constraint:\n                tuple of source point and rotated target point and weight\n                a loss will be added that penalizes\n                    weight * || rotation @ source - target ||_2\n            prior_orientation_distribution:\n                Prior distribution of orientations used for initialization.\n                If None, distribution of initialization network will not be modified.\n                Only supported for initialization network with discretized orientation\n                representation.\n                Output distribution of initialization network will be adjusted by\n                multiplying with\n                prior_orientation_distribution / training_orientation_distribution\n                and renormalizing.\n                Tensor of shape (N,C,) or (C,) for single image.\n                C being the number of grid cells in the SO3Grid used by the\n                initialization network.\n            training_orientation_distribution:\n                Distribution of orientations used for training initialization network.\n                If None, equal probability for each cell will be assumed.\n                Note this is only approximately the same as a uniform distribution.\n                Only used if prior_orientation_distribution is provided.\n                Tensor of shape (C,). C being the number of grid cells in the SO3Grid\n                used by the initialization network. N not supported, since same\n                training network (and hence distribution) is used independent of view.\n\n        Returns:\n            - 3D pose of SDF center in world frame, shape (1,3,)\n            - Orientation as normalized quaternion, scalar-last convention, shape (1,4,)\n            - Size of SDF as length of half-width, shape (1,)\n            - Latent shape representation of the object, shape (1,latent_size,).\n        \"\"\"\n        # initialize optimization\n        self._best_inlier_ratio = None\n        self._point_constraint = point_constraint\n\n        if animation_path is not None:\n            self._create_animation_folders(animation_path)\n\n        start_time = time.time()  # for logging\n\n        # Add batch dimension if necessary\n        if depth_images.dim() == 2:\n            depth_images = depth_images.unsqueeze(0)\n            masks = masks.unsqueeze(0)\n            color_images = color_images.unsqueeze(0)\n            if camera_positions is not None:\n                camera_positions = camera_positions.unsqueeze(0)\n            if camera_orientations is not None:\n                camera_orientations = camera_orientations.unsqueeze(0)\n            if prior_orientation_distribution is not None:\n                prior_orientation_distribution = (\n                    prior_orientation_distribution.unsqueeze(0)\n                )\n\n        # TODO assert all tensors have expected dimension\n\n        if animation_path is not None:\n            self._save_inputs(animation_path, depth_images, color_images, masks)\n\n        n_imgs = depth_images.shape[0]\n\n        if camera_positions is None:\n            camera_positions = torch.zeros(n_imgs, 3, device=self.device)\n        if camera_orientations is None:\n            camera_orientations = torch.zeros(n_imgs, 4, device=self.device)\n            camera_orientations[:, 3] = 1.0\n\n        with torch.no_grad():\n            self._preprocess_depth(depth_images, masks)\n\n        # store pointcloud without reconstruction\n        if log_path is not None:\n            torch.cuda.synchronize()\n            self._log_data(\n                {\n                    \"timestamp\": time.time() - start_time,\n                    \"depth_images\": depth_images,\n                    \"color_images\": color_images,\n                    \"masks\": masks,\n                    \"color_images\": color_images,\n                    \"camera_positions\": camera_positions,\n                    \"camera_orientations\": camera_orientations,\n                },\n            )\n\n        # Initialization\n        with torch.no_grad():\n            latent_shape, position, scale, orientation = self._nn_init(\n                depth_images,\n                camera_positions,\n                camera_orientations,\n                prior_orientation_distribution,\n                training_orientation_distribution,\n            )\n\n        if log_path is not None:\n            torch.cuda.synchronize()\n            self._log_data(\n                {\n                    \"timestamp\": time.time() - start_time,\n                    \"camera_positions\": camera_positions,\n                    \"camera_orientations\": camera_orientations,\n                    \"latent_shape\": latent_shape,\n                    \"position\": position,\n                    \"scale_inv\": 1 / scale,\n                    \"orientation\": orientation,\n                },\n            )\n\n        if animation_path is not None:\n            self._save_preprocessed_inputs(animation_path, depth_images)\n\n        # Iterative optimization\n        self._current_iteration = 1\n\n        position.requires_grad_()\n        scale.requires_grad_()\n        orientation.requires_grad_()\n        latent_shape.requires_grad_()\n\n        if visualize:\n            fig_vis, axes = plt.subplots(\n                2, 3, sharex=True, sharey=True, figsize=(12, 8)\n            )\n            fig_loss, (loss_ax, inlier_ax) = plt.subplots(1, 2)\n            vmin, vmax = None, None\n\n        depth_losses = []\n        pointcloud_losses = []\n        nn_losses = []\n        point_constraint_losses = []\n        inlier_ratios = []\n        total_losses = []\n\n        opt_vars = [\n            {\"params\": position, \"lr\": 1e-3},\n            {\"params\": orientation, \"lr\": 1e-2},\n            {\"params\": scale, \"lr\": 1e-3},\n            {\"params\": latent_shape, \"lr\": 1e-2},\n        ]\n        optimizer = torch.optim.Adam(opt_vars)\n\n        while self._current_iteration &lt;= self.config[\"max_iterations\"]:\n            optimizer.zero_grad()\n\n            norm_orientation = orientation / torch.sqrt(torch.sum(orientation**2))\n\n            with torch.set_grad_enabled(shape_optimization):\n                sdf = self.vae.decode(latent_shape)\n\n            loss_depth = torch.tensor(0.0, device=self.device, requires_grad=True)\n            loss_pc = torch.tensor(0.0, device=self.device, requires_grad=True)\n            loss_nn = torch.tensor(0.0, device=self.device, requires_grad=True)\n\n            for depth_image, camera_position, camera_orientation in zip(\n                depth_images, camera_positions, camera_orientations\n            ):\n                # transform object to camera frame\n                q_w2c = quaternion_utils.quaternion_invert(camera_orientation)\n                position_c = quaternion_utils.quaternion_apply(\n                    q_w2c, position - camera_position\n                )\n                orientation_c = quaternion_utils.quaternion_multiply(\n                    q_w2c, norm_orientation\n                )\n\n                depth_estimate = self.render(\n                    sdf[0, 0], position_c[0], orientation_c[0], 1 / scale[0]\n                )\n\n                view_loss_depth, view_loss_pc, view_loss_nn = self._compute_view_losses(\n                    depth_image,\n                    depth_estimate,\n                    position_c[0],\n                    orientation_c[0],\n                    scale[0],\n                    sdf[0, 0],\n                )\n                loss_depth = loss_depth + view_loss_depth\n                loss_pc = loss_pc + view_loss_pc\n                loss_nn = loss_nn + view_loss_nn\n\n            loss_point_constraint = self._compute_point_constraint_loss(orientation)\n            loss = (\n                self.config[\"depth_weight\"] * loss_depth\n                + self.config[\"pc_weight\"] * loss_pc\n                + self.config[\"nn_weight\"] * loss_nn\n                + loss_point_constraint\n            )\n\n            self._compute_gradients(loss)\n\n            optimizer.step()\n            optimizer.zero_grad()\n\n            with torch.no_grad():\n                orientation /= torch.sqrt(torch.sum(orientation**2))\n                inlier_ratio = self._update_best_estimate(\n                    depth_image,\n                    depth_estimate,\n                    position,\n                    orientation,\n                    scale,\n                    latent_shape,\n                )\n\n            if visualize:\n                depth_losses.append(loss_depth.item())\n                pointcloud_losses.append(loss_pc.item())\n                nn_losses.append(loss_nn.item())\n                point_constraint_losses.append(loss_point_constraint.item())\n                inlier_ratios.append(inlier_ratio.item())\n                total_losses.append(loss.item())\n\n            if log_path is not None:\n                torch.cuda.synchronize()\n                self._log_data(\n                    {\n                        \"timestamp\": time.time() - start_time,\n                        \"latent_shape\": latent_shape,\n                        \"position\": position,\n                        \"scale_inv\": 1 / scale,\n                        \"orientation\": orientation,\n                    },\n                )\n\n            with torch.no_grad():\n                if animation_path is not None:\n                    self._save_current_state(\n                        depth_images,\n                        animation_path,\n                        camera_positions,\n                        camera_orientations,\n                        position,\n                        orientation,\n                        1 / scale,\n                        sdf,\n                    )\n\n                if visualize and (\n                    self._current_iteration % 10 == 1\n                    or self._current_iteration == self.config[\"max_iterations\"]\n                ):\n                    q_w2c = quaternion_utils.quaternion_invert(camera_orientations[0])\n                    position_c = quaternion_utils.quaternion_apply(\n                        q_w2c, position - camera_positions[0]\n                    )\n                    orientation_c = quaternion_utils.quaternion_multiply(\n                        q_w2c, orientation\n                    )\n\n                    current_depth = self.render(\n                        sdf[0, 0], position_c, orientation_c, 1 / scale\n                    )\n\n                    depth_image = depth_images[0]\n                    color_image = color_images[0]\n\n                    if self._current_iteration == 1:\n                        vmin = depth_image[depth_image != 0].min() * 0.9\n                        vmax = depth_image[depth_image != 0].max()\n                        # show input image\n                        axes[0, 0].clear()\n                        axes[0, 0].imshow(depth_image.cpu(), vmin=vmin, vmax=vmax)\n                        axes[0, 1].imshow(color_image.cpu())\n\n                        # show initial estimate\n                        axes[1, 0].clear()\n                        axes[1, 0].imshow(\n                            current_depth.detach().cpu(), vmin=vmin, vmax=vmax\n                        )\n                        axes[1, 0].set_title(f\"loss {loss.item()}\")\n\n                    # update iterative estimate\n                    # axes[0, 2].clear()\n                    # axes[0, 2].imshow(rendered_error.detach().cpu())\n                    # axes[0, 2].set_title(\"depth_loss\")\n                    # axes[1, 2].clear()\n                    # axes[1, 2].imshow(error_pc.detach().cpu())\n                    # axes[1, 2].set_title(\"pointcloud_loss\")\n\n                    loss_ax.clear()\n                    loss_ax.plot(depth_losses, label=\"Depth\")\n                    loss_ax.plot(pointcloud_losses, label=\"Pointcloud\")\n                    loss_ax.plot(nn_losses, label=\"Nearest Neighbor\")\n                    if self._point_constraint is not None:\n                        loss_ax.plot(point_constraint_losses, label=\"Point constraint\")\n                    loss_ax.plot(total_losses, label=\"Total\")\n                    loss_ax.set_yscale(\"log\")\n                    loss_ax.legend()\n\n                    inlier_ax.clear()\n                    inlier_ax.plot(inlier_ratios, label=\"Inlier Ratio\")\n                    inlier_ax.legend()\n\n                    axes[1, 1].clear()\n                    axes[1, 1].imshow(\n                        current_depth.detach().cpu(), vmin=vmin, vmax=vmax\n                    )\n                    axes[1, 1].set_title(f\"loss {loss.item()}\")\n                    fig_loss.canvas.draw()\n                    fig_vis.canvas.draw()\n                    plt.pause(0.1)\n\n            self._current_iteration += 1\n\n        if visualize:\n            plt.show()\n            plt.close(fig_loss)\n            plt.close(fig_vis)\n\n        if log_path is not None:\n            self._write_log_data(log_path)\n\n        if animation_path is not None:\n            self._create_animations(animation_path)\n\n        if self.result_selection_strategy == \"last_iteration\":\n            return position, orientation, scale, latent_shape\n        elif self.result_selection_strategy == \"best_inlier_ratio\":\n            return (\n                self._best_position,\n                self._best_orientation,\n                self._best_scale,\n                self._best_latent_shape,\n            )\n        else:\n            raise ValueError(\n                f\"Result selection strategy {self.result_selection_strategy} is not\"\n                \"supported.\"\n            )\n\n    def _log_data(self, data: dict) -&gt; None:\n        \"\"\"Add dictionary with associated timestamp to log data list.\"\"\"\n        new_log_data = copy.deepcopy(data)\n        self.log_data.append(new_log_data)\n\n    def _write_log_data(self, file_path: str) -&gt; None:\n        \"\"\"Write current list of log data to file.\"\"\"\n        with open(file_path, \"wb\") as f:\n            pickle.dump({\"config\": self.config, \"log\": self.log_data}, f)\n        self.log_data = []  # reset log\n\n    def generate_depth(\n        self,\n        position: torch.Tensor,\n        orientation: torch.Tensor,\n        scale: torch.Tensor,\n        latent: torch.Tensor,\n    ) -&gt; torch.Tensor:\n        \"\"\"Generate depth image representing positioned object.\"\"\"\n        sdf = self.vae.decode(latent)\n        depth = self.render(sdf[0, 0], position, orientation, 1 / scale)\n        return depth\n\n    def generate_mesh(\n        self, latent: torch.tensor, scale: torch.tensor, complete_mesh: bool = False\n    ) -&gt; synthetic.Mesh:\n        \"\"\"Generate mesh without pose.\n\n        Currently only supports batch size 1.\n\n        Args:\n            latent: Latent shape descriptor, shape (1,L).\n            scale:\n                Relative scale of the signed distance field, (i.e., half-width),\n                shape (1,).\n            complete_mesh:\n                If True, the SDF will be padded with positive values prior to converting\n                it to a mesh. This ensures a watertight mesh is created.\n\n        Returns:\n            Generate mesh by decoding latent shape descriptor and scaling it.\n        \"\"\"\n        with torch.no_grad():\n            sdf = self.vae.decode(latent)\n            if complete_mesh:\n                inc = 2\n                sdf = torch.nn.functional.pad(sdf, (1, 1, 1, 1, 1, 1), value=1.0)\n            else:\n                inc = 0\n            try:\n                sdf = sdf.cpu().numpy()\n                s = 2.0 / (self.resolution - 1)\n                vertices, faces, _, _ = marching_cubes(\n                    sdf[0, 0],\n                    spacing=(\n                        s,\n                        s,\n                        s,\n                    ),\n                    level=self.config[\"iso_threshold\"],\n                )\n\n                c = s * (self.resolution + inc - 1) / 2.0  # move origin to center\n                vertices -= np.array([[c, c, c]])\n\n                mesh = o3d.geometry.TriangleMesh(\n                    vertices=o3d.utility.Vector3dVector(vertices),\n                    triangles=o3d.utility.Vector3iVector(faces),\n                )\n            except KeyError:\n                return None\n            return synthetic.Mesh(mesh=mesh, scale=scale.item(), rel_scale=True)\n\n    def _preprocess_depth(\n        self, depth_images: torch.Tensor, masks: torch.Tensor\n    ) -&gt; None:\n        \"\"\"Preprocesses depth image based on segmentation mask.\n\n        Args:\n            depth_images:\n                the depth images to preprocess, will be modified in place,\n                shape (N, H, W)\n            masks: the masks used for preprocessing, same shape as depth_images\n        \"\"\"\n        # shrink mask\n        # masks = (\n        #     -torch.nn.functional.max_pool2d(\n        #         -masks.double(), kernel_size=9, stride=1, padding=4\n        #     )\n        # ).bool()\n\n        depth_images[~masks] = 0  # set outside of depth to 0\n\n        # remove data far away (should be based on what distances ocurred in training)\n        if self._far_field is not None:\n            depth_images[depth_images &gt; self._far_field] = 0\n\n        # only consider available depth values for outlier detection\n        # masks = torch.logical_and(masks, depth_images != 0)\n\n        # depth_images =\n\n        # remove outliers based on median\n        # plt.imshow(depth_images[0].cpu().numpy())\n        # plt.show()\n        # for mask, depth_image in zip(masks, depth_images):\n        #     median = torch.median(depth_image[mask])\n        #     errors = torch.abs(depth_image[mask] - median)\n\n        #     bins = 100\n        #     hist = torch.histc(errors, bins=bins)\n        #     print(hist)\n        #     zero_indices = torch.nonzero(hist == 0)\n        #     if len(zero_indices):\n        #         threshold = zero_indices[0] / bins * errors.max()\n        #         print(threshold)\n        #         depth_image[torch.abs(depth_image - median) &gt; threshold] = 0\n        # plt.imshow(depth_images[0].cpu().numpy())\n        # plt.show()\n\n    def _nn_init(\n        self,\n        depth_images: torch.Tensor,\n        camera_positions: torch.Tensor,\n        camera_orientations: torch.Tensor,\n        prior_orientation_distribution: Optional[torch.Tensor] = None,\n        training_orientation_distribution: Optional[torch.Tensor] = None,\n    ) -&gt; Tuple:\n        \"\"\"Estimate shape, pose, scale and orientation using initialization network.\n\n        Args:\n            depth_images: the preprocessed depth images, shape (N, H, W)\n            camera_positions:\n                position of camera in world coordinates for each image, shape (N, 3)\n            camera_orientations:\n                orientation of camera in world-frame as normalized quaternion,\n                quaternion is in scalar-last convention, shape (N, 4)\n            prior_orientation_distribution:\n                Prior distribution of orientations used for initialization.\n                If None, distribution of initialization network will not be modified.\n                Only supported for initialization network with discretized orientation\n                representation.\n                Output distribution of initialization network will be adjusted by\n                multiplying with\n                prior_orientation_distribution / training_orientation_distribution\n                and renormalizing.\n                Tensor of shape (N,C,). C being the number of grid cells in the SO3Grid\n                used by the initialization network.\n            training_orientation_distribution:\n                Distribution of orientations used for training initialization network.\n                If None, equal probability for each cell will be assumed.\n                Note this is only approximately the same as a uniform distribution.\n                Only used if prior_orientation_distribution is provided.\n                Tensor of shape (C,). C being the number of grid cells in the SO3Grid\n                used by the initialization network.\n\n        Returns:\n            Tuple comprised of:\n            - Latent shape representation of the object, shape (1, latent_size)\n            - 3D pose of SDF center in camera frame, shape (1, 3)\n            - Size of SDF as length of half-width, (1,)\n            - Orientation of SDF as normalized quaternion (1,4)\n        \"\"\"\n        if (\n            prior_orientation_distribution is not None\n            and self.init_config[\"head\"][\"orientation_repr\"] != \"discretized\"\n        ):\n            raise ValueError(\n                \"prior_orientation_distribution only supported for discretized \"\n                \"orientation representation.\"\n            )\n\n        best = 0\n        best_result = None\n        for i, (depth_image, camera_orientation, camera_position) in enumerate(\n            zip(depth_images, camera_orientations, camera_positions)\n        ):\n            centroid = None\n            if self.init_config[\"backbone_type\"] == \"VanillaPointNet\":\n                inp = pointset_utils.depth_to_pointcloud(\n                    depth_image, self.cam, normalize=False\n                )\n                if len(inp) == 0:\n                    raise NoDepthError\n                if self.init_config[\"normalize_pose\"]:\n                    inp, centroid = pointset_utils.normalize_points(inp)\n            else:\n                inp = depth_image\n\n            inp = inp.unsqueeze(0)\n            latent_shape, position, scale, orientation_repr = self.init_network(inp)\n\n            if self.config[\"mean_shape\"]:\n                latent_shape = latent_shape.new_zeros(latent_shape.shape)\n\n            if centroid is not None:\n                position += centroid\n\n            if self.init_config[\"head\"][\"orientation_repr\"] == \"discretized\":\n                posterior_orientation_dist = torch.softmax(orientation_repr, -1)\n\n                if prior_orientation_distribution is not None:\n                    posterior_orientation_dist = self._adjust_categorical_posterior(\n                        posterior=posterior_orientation_dist,\n                        prior=prior_orientation_distribution[i],\n                        train_prior=training_orientation_distribution,\n                    )\n\n                orientation_camera = torch.tensor(\n                    self.init_network._head._grid.index_to_quat(\n                        posterior_orientation_dist.argmax().item()\n                    ),\n                    dtype=torch.float,\n                    device=self.device,\n                ).unsqueeze(0)\n            elif self.init_config[\"head\"][\"orientation_repr\"] == \"quaternion\":\n                orientation_camera = orientation_repr\n            else:\n                raise NotImplementedError(\"Orientation representation is not supported\")\n\n            # output are in camera frame, transform to world frame\n            position_world = (\n                quaternion_utils.quaternion_apply(camera_orientation, position)\n                + camera_position\n            )\n            orientation_world = quaternion_utils.quaternion_multiply(\n                camera_orientation, orientation_camera\n            )\n\n            if self.config[\"init_view\"] == \"first\":\n                return latent_shape, position_world, scale, orientation_world\n            elif self.config[\"init_view\"] == \"best\":\n                if self.init_config[\"head\"][\"orientation_repr\"] != \"discretized\":\n                    raise NotImplementedError(\n                        '\"best\" init strategy only supported with discretized '\n                        \"orientation representation\"\n                    )\n                maximum = posterior_orientation_dist.max()\n                if maximum &gt; best:\n                    best = maximum\n                    best_result = latent_shape, position_world, scale, orientation_world\n            else:\n                raise NotImplementedError(\n                    'Only \"first\" and \"best\" strategies are currently supported'\n                )\n\n        return best_result\n\n    def _generate_uniform_quaternion(self) -&gt; torch.tensor:\n        \"\"\"Generate a uniform quaternion.\n\n        Following the method from K. Shoemake, Uniform Random Rotations, 1992.\n\n        See: http://planning.cs.uiuc.edu/node198.html\n\n        Returns:\n            Uniformly distributed unit quaternion on the estimator's device.\n        \"\"\"\n        u1, u2, u3 = random.random(), random.random(), random.random()\n        return (\n            torch.tensor(\n                [\n                    math.sqrt(1 - u1) * math.sin(2 * math.pi * u2),\n                    math.sqrt(1 - u1) * math.cos(2 * math.pi * u2),\n                    math.sqrt(u1) * math.sin(2 * math.pi * u3),\n                    math.sqrt(u1) * math.cos(2 * math.pi * u3),\n                ]\n            )\n            .unsqueeze(0)\n            .to(self.device)\n        )\n\n    def _create_animation_folders(self, animation_path: str) -&gt; None:\n        \"\"\"Create subfolders to store animation frames.\"\"\"\n        os.makedirs(animation_path)\n        depth_path = os.path.join(animation_path, \"depth\")\n        os.makedirs(depth_path)\n        error_path = os.path.join(animation_path, \"depth_error\")\n        os.makedirs(error_path)\n        sdf_path = os.path.join(animation_path, \"sdf\")\n        os.makedirs(sdf_path)\n\n    def _save_inputs(\n        self,\n        animation_path: str,\n        color_images: torch.Tensor,\n        depth_images: torch.Tensor,\n        instance_masks: torch.Tensor,\n    ) -&gt; None:\n        color_path = os.path.join(animation_path, \"color_input.png\")\n        fig, ax = plt.subplots()\n        ax.imshow(color_images[0].cpu().numpy())\n        fig.savefig(color_path)\n        plt.close(fig)\n        depth_path = os.path.join(animation_path, \"depth_input.png\")\n        fig, ax = plt.subplots()\n        ax.imshow(depth_images[0].cpu().numpy())\n        fig.savefig(depth_path)\n        plt.close(fig)\n        mask_path = os.path.join(animation_path, \"mask.png\")\n        fig, ax = plt.subplots()\n        ax.imshow(instance_masks[0].cpu().numpy())\n        fig.savefig(mask_path)\n        plt.close(fig)\n\n    def _save_preprocessed_inputs(\n        self,\n        animation_path: str,\n        depth_images: torch.Tensor,\n    ) -&gt; None:\n        depth_path = os.path.join(animation_path, \"preprocessed_depth_input.png\")\n        fig, ax = plt.subplots()\n        ax.imshow(depth_images[0].cpu().numpy())\n        fig.savefig(depth_path)\n        plt.close(fig)\n\n    def _save_current_state(\n        self,\n        depth_images: torch.Tensor,\n        animation_path: str,\n        camera_positions: torch.Tensor,\n        camera_orientations: torch.Tensor,\n        position: torch.Tensor,\n        orientation: torch.Tensor,\n        scale_inv: torch.Tensor,\n        sdf: torch.Tensor,\n    ) -&gt; None:\n        q_w2c = quaternion_utils.quaternion_invert(camera_orientations[0])\n        position_c = quaternion_utils.quaternion_apply(\n            q_w2c, position - camera_positions[0]\n        )\n        orientation_c = quaternion_utils.quaternion_multiply(q_w2c, orientation)\n        current_depth = self.render(sdf[0, 0], position_c, orientation_c, scale_inv)\n        depth_path = os.path.join(\n            animation_path, \"depth\", f\"{self._current_iteration:06}.png\"\n        )\n        fig, ax = plt.subplots()\n        ax.imshow(current_depth.cpu().numpy(), interpolation=\"none\")\n        fig.savefig(depth_path)\n        plt.close(fig)\n\n        error_image = torch.abs(current_depth - depth_images[0])\n        error_image[depth_images[0] == 0] = 0\n        error_image[current_depth == 0] = 0\n        error_path = os.path.join(\n            animation_path, \"depth_error\", f\"{self._current_iteration:06}.png\"\n        )\n        fig, ax = plt.subplots()\n        ax.imshow(error_image.cpu().numpy(), interpolation=\"none\")\n        fig.savefig(error_path)\n        plt.close(fig)\n\n        unscaled_threshold = self.config[\"threshold\"] * scale_inv.item()\n        mesh = sdf_utils.mesh_from_sdf(\n            sdf[0, 0].cpu().numpy(),\n            unscaled_threshold,\n            complete_mesh=True,\n        )\n        sdf_path = os.path.join(\n            animation_path, \"sdf\", f\"{self._current_iteration:06}.png\"\n        )\n\n        # map y -&gt; z; z -&gt; y\n        transform = np.eye(4)\n        transform[0:3, 0:3] = np.array([[-1, 0, 0], [0, 0, 1], [0, 1, 0]])\n        fig, ax = plt.subplots()\n        sdf_utils.plot_mesh(mesh, transform=transform, plot_object=ax)\n        fig.savefig(sdf_path)\n        plt.close(fig)\n\n    def _create_animations(self, animation_path: str) -&gt; None:\n        names = [\"sdf\", \"depth\", \"depth_error\"]\n        for name in names:\n            frame_folder = os.path.join(animation_path, name)\n            video_name = os.path.join(animation_path, f\"{name}.mp4\")\n            ffmpeg.input(\n                os.path.join(frame_folder, \"*.png\"), pattern_type=\"glob\", framerate=30\n            ).output(video_name).run()\n\n    @staticmethod\n    def _adjust_categorical_posterior(\n        posterior: torch.Tensor, prior: torch.Tensor, train_prior: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"Adjust categorical posterior distribution.\n\n        Posterior is calculated with a train_prior\n\n        Args:\n            posterior:\n                Posterior distribution computed assuming train_prior.\n                Shape (..., K). K being number of categories.\n            prior:\n                The desired new prior distribution.\n                Same shape as posterior.\n            train_prior:\n                The prior distribution used to compute the posterior.\n                If None, equal probability for each category will be assumed.\n                Same shape as posterior.\n\n        Returns:\n            The categorical posterior, adjusted such that prior is prior, instead of\n            train_prior.\n            Same shape as posterior.\n        \"\"\"\n        adjusted_posterior = posterior.clone()\n        # adjust if prior different from training\n        adjusted_posterior *= prior\n        if train_prior is not None:\n            adjusted_posterior /= train_prior\n        adjusted_posterior = torch.nn.functional.normalize(\n            adjusted_posterior, p=1, dim=-1\n        )\n        return adjusted_posterior\n</code></pre>"},{"location":"reference/estimation/simple_setup/#sdfest.estimation.simple_setup.SDFPipeline.__call__","title":"<code>__call__(depth_images, masks, color_images, visualize=False, camera_positions=None, camera_orientations=None, log_path=None, shape_optimization=True, animation_path=None, point_constraint=None, prior_orientation_distribution=None, training_orientation_distribution=None)</code>","text":"<p>Infer pose, size and latent representation from depth and mask.</p> <p>If multiple images are passed the cameras are assumed to be fixed. All tensors should be on the same device as the pipeline.</p> <p>Batch dimension N must be provided either for all or none of the arguments.</p> <p>Parameters:</p> Name Type Description Default <code>depth_images</code> <code>Tensor</code> <p>The depth map containing the distance along the camera's z-axis. Does not have to be masked, necessary preprocessing is done by pipeline. Will be masked and preprocessed in-place (pass copy if full depth is used afterwards). Shape (N, H, W) or (H, W) for a single depth image.</p> required <code>masks</code> <code>Tensor</code> <p>binary mask of the object to estimate, same shape as depth_images</p> required <code>color_images</code> <code>Tensor</code> <p>the color image (currently only used in visualization), shape (N, H, W, 3) or (H, W, 3), RGB, float, 0-1.</p> required <code>visualize</code> <code>bool</code> <p>Whether to visualize the intermediate steps and final result.</p> <code>False</code> <code>camera_positions</code> <code>Optional[Tensor]</code> <p>position of camera in world coordinates for each image, if None, (0,0,0) will be assumed for all images, shape (N, 3) or (3,)</p> <code>None</code> <code>camera_orientations</code> <code>Optional[Tensor]</code> <p>orientation of camera in world-frame as normalized quaternion, quaternion is in scalar-last convention, note, that this is the quaternion that transforms a point from camera to world-frame if None, (0,0,0,1) will be assumed for all images, shape (N, 4) or (4,)</p> <code>None</code> <code>log_path</code> <code>Optional[str]</code> <p>file path to write timestamps and intermediate steps to, no logging is performed if None</p> <code>None</code> <code>shape_optimization</code> <code>bool</code> <p>enable or disable shape optimization during iterative optimization</p> <code>True</code> <code>animation_path</code> <code>Optional[str]</code> <p>file path to write rendering and error visualizations to</p> <code>None</code> <code>point_constraint</code> <code>Optional[Tuple[Tensor]]</code> <p>tuple of source point and rotated target point and weight a loss will be added that penalizes     weight * || rotation @ source - target ||_2</p> <code>None</code> <code>prior_orientation_distribution</code> <code>Optional[Tensor]</code> <p>Prior distribution of orientations used for initialization. If None, distribution of initialization network will not be modified. Only supported for initialization network with discretized orientation representation. Output distribution of initialization network will be adjusted by multiplying with prior_orientation_distribution / training_orientation_distribution and renormalizing. Tensor of shape (N,C,) or (C,) for single image. C being the number of grid cells in the SO3Grid used by the initialization network.</p> <code>None</code> <code>training_orientation_distribution</code> <code>Optional[Tensor]</code> <p>Distribution of orientations used for training initialization network. If None, equal probability for each cell will be assumed. Note this is only approximately the same as a uniform distribution. Only used if prior_orientation_distribution is provided. Tensor of shape (C,). C being the number of grid cells in the SO3Grid used by the initialization network. N not supported, since same training network (and hence distribution) is used independent of view.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <ul> <li>3D pose of SDF center in world frame, shape (1,3,)</li> </ul> <code>tuple</code> <ul> <li>Orientation as normalized quaternion, scalar-last convention, shape (1,4,)</li> </ul> <code>tuple</code> <ul> <li>Size of SDF as length of half-width, shape (1,)</li> </ul> <code>tuple</code> <ul> <li>Latent shape representation of the object, shape (1,latent_size,).</li> </ul> Source code in <code>sdfest/estimation/simple_setup.py</code> <pre><code>def __call__(\n    self,\n    depth_images: torch.Tensor,\n    masks: torch.Tensor,\n    color_images: torch.Tensor,\n    visualize: bool = False,\n    camera_positions: Optional[torch.Tensor] = None,\n    camera_orientations: Optional[torch.Tensor] = None,\n    log_path: Optional[str] = None,\n    shape_optimization: bool = True,\n    animation_path: Optional[str] = None,\n    point_constraint: Optional[Tuple[torch.Tensor]] = None,\n    prior_orientation_distribution: Optional[torch.Tensor] = None,\n    training_orientation_distribution: Optional[torch.Tensor] = None,\n) -&gt; tuple:\n    \"\"\"Infer pose, size and latent representation from depth and mask.\n\n    If multiple images are passed the cameras are assumed to be fixed.\n    All tensors should be on the same device as the pipeline.\n\n    Batch dimension N must be provided either for all or none of the arguments.\n\n    Args:\n        depth_images:\n            The depth map containing the distance along the camera's z-axis.\n            Does not have to be masked, necessary preprocessing is done by pipeline.\n            Will be masked and preprocessed in-place (pass copy if full depth is\n            used afterwards).\n            Shape (N, H, W) or (H, W) for a single depth image.\n        masks:\n            binary mask of the object to estimate, same shape as depth_images\n        color_images:\n            the color image (currently only used in visualization),\n            shape (N, H, W, 3) or (H, W, 3), RGB, float, 0-1.\n        visualize: Whether to visualize the intermediate steps and final result.\n        camera_positions:\n            position of camera in world coordinates for each image,\n            if None, (0,0,0) will be assumed for all images,\n            shape (N, 3) or (3,)\n        camera_orientations:\n            orientation of camera in world-frame as normalized quaternion,\n            quaternion is in scalar-last convention,\n            note, that this is the quaternion that transforms a point from camera\n            to world-frame\n            if None, (0,0,0,1) will be assumed for all images,\n            shape (N, 4) or (4,)\n        log_path:\n            file path to write timestamps and intermediate steps to,\n            no logging is performed if None\n        shape_optimization:\n            enable or disable shape optimization during iterative optimization\n        animation_path:\n            file path to write rendering and error visualizations to\n        point_constraint:\n            tuple of source point and rotated target point and weight\n            a loss will be added that penalizes\n                weight * || rotation @ source - target ||_2\n        prior_orientation_distribution:\n            Prior distribution of orientations used for initialization.\n            If None, distribution of initialization network will not be modified.\n            Only supported for initialization network with discretized orientation\n            representation.\n            Output distribution of initialization network will be adjusted by\n            multiplying with\n            prior_orientation_distribution / training_orientation_distribution\n            and renormalizing.\n            Tensor of shape (N,C,) or (C,) for single image.\n            C being the number of grid cells in the SO3Grid used by the\n            initialization network.\n        training_orientation_distribution:\n            Distribution of orientations used for training initialization network.\n            If None, equal probability for each cell will be assumed.\n            Note this is only approximately the same as a uniform distribution.\n            Only used if prior_orientation_distribution is provided.\n            Tensor of shape (C,). C being the number of grid cells in the SO3Grid\n            used by the initialization network. N not supported, since same\n            training network (and hence distribution) is used independent of view.\n\n    Returns:\n        - 3D pose of SDF center in world frame, shape (1,3,)\n        - Orientation as normalized quaternion, scalar-last convention, shape (1,4,)\n        - Size of SDF as length of half-width, shape (1,)\n        - Latent shape representation of the object, shape (1,latent_size,).\n    \"\"\"\n    # initialize optimization\n    self._best_inlier_ratio = None\n    self._point_constraint = point_constraint\n\n    if animation_path is not None:\n        self._create_animation_folders(animation_path)\n\n    start_time = time.time()  # for logging\n\n    # Add batch dimension if necessary\n    if depth_images.dim() == 2:\n        depth_images = depth_images.unsqueeze(0)\n        masks = masks.unsqueeze(0)\n        color_images = color_images.unsqueeze(0)\n        if camera_positions is not None:\n            camera_positions = camera_positions.unsqueeze(0)\n        if camera_orientations is not None:\n            camera_orientations = camera_orientations.unsqueeze(0)\n        if prior_orientation_distribution is not None:\n            prior_orientation_distribution = (\n                prior_orientation_distribution.unsqueeze(0)\n            )\n\n    # TODO assert all tensors have expected dimension\n\n    if animation_path is not None:\n        self._save_inputs(animation_path, depth_images, color_images, masks)\n\n    n_imgs = depth_images.shape[0]\n\n    if camera_positions is None:\n        camera_positions = torch.zeros(n_imgs, 3, device=self.device)\n    if camera_orientations is None:\n        camera_orientations = torch.zeros(n_imgs, 4, device=self.device)\n        camera_orientations[:, 3] = 1.0\n\n    with torch.no_grad():\n        self._preprocess_depth(depth_images, masks)\n\n    # store pointcloud without reconstruction\n    if log_path is not None:\n        torch.cuda.synchronize()\n        self._log_data(\n            {\n                \"timestamp\": time.time() - start_time,\n                \"depth_images\": depth_images,\n                \"color_images\": color_images,\n                \"masks\": masks,\n                \"color_images\": color_images,\n                \"camera_positions\": camera_positions,\n                \"camera_orientations\": camera_orientations,\n            },\n        )\n\n    # Initialization\n    with torch.no_grad():\n        latent_shape, position, scale, orientation = self._nn_init(\n            depth_images,\n            camera_positions,\n            camera_orientations,\n            prior_orientation_distribution,\n            training_orientation_distribution,\n        )\n\n    if log_path is not None:\n        torch.cuda.synchronize()\n        self._log_data(\n            {\n                \"timestamp\": time.time() - start_time,\n                \"camera_positions\": camera_positions,\n                \"camera_orientations\": camera_orientations,\n                \"latent_shape\": latent_shape,\n                \"position\": position,\n                \"scale_inv\": 1 / scale,\n                \"orientation\": orientation,\n            },\n        )\n\n    if animation_path is not None:\n        self._save_preprocessed_inputs(animation_path, depth_images)\n\n    # Iterative optimization\n    self._current_iteration = 1\n\n    position.requires_grad_()\n    scale.requires_grad_()\n    orientation.requires_grad_()\n    latent_shape.requires_grad_()\n\n    if visualize:\n        fig_vis, axes = plt.subplots(\n            2, 3, sharex=True, sharey=True, figsize=(12, 8)\n        )\n        fig_loss, (loss_ax, inlier_ax) = plt.subplots(1, 2)\n        vmin, vmax = None, None\n\n    depth_losses = []\n    pointcloud_losses = []\n    nn_losses = []\n    point_constraint_losses = []\n    inlier_ratios = []\n    total_losses = []\n\n    opt_vars = [\n        {\"params\": position, \"lr\": 1e-3},\n        {\"params\": orientation, \"lr\": 1e-2},\n        {\"params\": scale, \"lr\": 1e-3},\n        {\"params\": latent_shape, \"lr\": 1e-2},\n    ]\n    optimizer = torch.optim.Adam(opt_vars)\n\n    while self._current_iteration &lt;= self.config[\"max_iterations\"]:\n        optimizer.zero_grad()\n\n        norm_orientation = orientation / torch.sqrt(torch.sum(orientation**2))\n\n        with torch.set_grad_enabled(shape_optimization):\n            sdf = self.vae.decode(latent_shape)\n\n        loss_depth = torch.tensor(0.0, device=self.device, requires_grad=True)\n        loss_pc = torch.tensor(0.0, device=self.device, requires_grad=True)\n        loss_nn = torch.tensor(0.0, device=self.device, requires_grad=True)\n\n        for depth_image, camera_position, camera_orientation in zip(\n            depth_images, camera_positions, camera_orientations\n        ):\n            # transform object to camera frame\n            q_w2c = quaternion_utils.quaternion_invert(camera_orientation)\n            position_c = quaternion_utils.quaternion_apply(\n                q_w2c, position - camera_position\n            )\n            orientation_c = quaternion_utils.quaternion_multiply(\n                q_w2c, norm_orientation\n            )\n\n            depth_estimate = self.render(\n                sdf[0, 0], position_c[0], orientation_c[0], 1 / scale[0]\n            )\n\n            view_loss_depth, view_loss_pc, view_loss_nn = self._compute_view_losses(\n                depth_image,\n                depth_estimate,\n                position_c[0],\n                orientation_c[0],\n                scale[0],\n                sdf[0, 0],\n            )\n            loss_depth = loss_depth + view_loss_depth\n            loss_pc = loss_pc + view_loss_pc\n            loss_nn = loss_nn + view_loss_nn\n\n        loss_point_constraint = self._compute_point_constraint_loss(orientation)\n        loss = (\n            self.config[\"depth_weight\"] * loss_depth\n            + self.config[\"pc_weight\"] * loss_pc\n            + self.config[\"nn_weight\"] * loss_nn\n            + loss_point_constraint\n        )\n\n        self._compute_gradients(loss)\n\n        optimizer.step()\n        optimizer.zero_grad()\n\n        with torch.no_grad():\n            orientation /= torch.sqrt(torch.sum(orientation**2))\n            inlier_ratio = self._update_best_estimate(\n                depth_image,\n                depth_estimate,\n                position,\n                orientation,\n                scale,\n                latent_shape,\n            )\n\n        if visualize:\n            depth_losses.append(loss_depth.item())\n            pointcloud_losses.append(loss_pc.item())\n            nn_losses.append(loss_nn.item())\n            point_constraint_losses.append(loss_point_constraint.item())\n            inlier_ratios.append(inlier_ratio.item())\n            total_losses.append(loss.item())\n\n        if log_path is not None:\n            torch.cuda.synchronize()\n            self._log_data(\n                {\n                    \"timestamp\": time.time() - start_time,\n                    \"latent_shape\": latent_shape,\n                    \"position\": position,\n                    \"scale_inv\": 1 / scale,\n                    \"orientation\": orientation,\n                },\n            )\n\n        with torch.no_grad():\n            if animation_path is not None:\n                self._save_current_state(\n                    depth_images,\n                    animation_path,\n                    camera_positions,\n                    camera_orientations,\n                    position,\n                    orientation,\n                    1 / scale,\n                    sdf,\n                )\n\n            if visualize and (\n                self._current_iteration % 10 == 1\n                or self._current_iteration == self.config[\"max_iterations\"]\n            ):\n                q_w2c = quaternion_utils.quaternion_invert(camera_orientations[0])\n                position_c = quaternion_utils.quaternion_apply(\n                    q_w2c, position - camera_positions[0]\n                )\n                orientation_c = quaternion_utils.quaternion_multiply(\n                    q_w2c, orientation\n                )\n\n                current_depth = self.render(\n                    sdf[0, 0], position_c, orientation_c, 1 / scale\n                )\n\n                depth_image = depth_images[0]\n                color_image = color_images[0]\n\n                if self._current_iteration == 1:\n                    vmin = depth_image[depth_image != 0].min() * 0.9\n                    vmax = depth_image[depth_image != 0].max()\n                    # show input image\n                    axes[0, 0].clear()\n                    axes[0, 0].imshow(depth_image.cpu(), vmin=vmin, vmax=vmax)\n                    axes[0, 1].imshow(color_image.cpu())\n\n                    # show initial estimate\n                    axes[1, 0].clear()\n                    axes[1, 0].imshow(\n                        current_depth.detach().cpu(), vmin=vmin, vmax=vmax\n                    )\n                    axes[1, 0].set_title(f\"loss {loss.item()}\")\n\n                # update iterative estimate\n                # axes[0, 2].clear()\n                # axes[0, 2].imshow(rendered_error.detach().cpu())\n                # axes[0, 2].set_title(\"depth_loss\")\n                # axes[1, 2].clear()\n                # axes[1, 2].imshow(error_pc.detach().cpu())\n                # axes[1, 2].set_title(\"pointcloud_loss\")\n\n                loss_ax.clear()\n                loss_ax.plot(depth_losses, label=\"Depth\")\n                loss_ax.plot(pointcloud_losses, label=\"Pointcloud\")\n                loss_ax.plot(nn_losses, label=\"Nearest Neighbor\")\n                if self._point_constraint is not None:\n                    loss_ax.plot(point_constraint_losses, label=\"Point constraint\")\n                loss_ax.plot(total_losses, label=\"Total\")\n                loss_ax.set_yscale(\"log\")\n                loss_ax.legend()\n\n                inlier_ax.clear()\n                inlier_ax.plot(inlier_ratios, label=\"Inlier Ratio\")\n                inlier_ax.legend()\n\n                axes[1, 1].clear()\n                axes[1, 1].imshow(\n                    current_depth.detach().cpu(), vmin=vmin, vmax=vmax\n                )\n                axes[1, 1].set_title(f\"loss {loss.item()}\")\n                fig_loss.canvas.draw()\n                fig_vis.canvas.draw()\n                plt.pause(0.1)\n\n        self._current_iteration += 1\n\n    if visualize:\n        plt.show()\n        plt.close(fig_loss)\n        plt.close(fig_vis)\n\n    if log_path is not None:\n        self._write_log_data(log_path)\n\n    if animation_path is not None:\n        self._create_animations(animation_path)\n\n    if self.result_selection_strategy == \"last_iteration\":\n        return position, orientation, scale, latent_shape\n    elif self.result_selection_strategy == \"best_inlier_ratio\":\n        return (\n            self._best_position,\n            self._best_orientation,\n            self._best_scale,\n            self._best_latent_shape,\n        )\n    else:\n        raise ValueError(\n            f\"Result selection strategy {self.result_selection_strategy} is not\"\n            \"supported.\"\n        )\n</code></pre>"},{"location":"reference/estimation/simple_setup/#sdfest.estimation.simple_setup.SDFPipeline.__init__","title":"<code>__init__(config)</code>","text":"<p>Load and initialize the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration dictionary.</p> required Source code in <code>sdfest/estimation/simple_setup.py</code> <pre><code>def __init__(self, config: dict) -&gt; None:\n    \"\"\"Load and initialize the pipeline.\n\n    Args:\n        config: Configuration dictionary.\n    \"\"\"\n    # improve runtime for conv3d backward\n    # https://github.com/pytorch/pytorch/issues/32370\n    torch.backends.cudnn.enabled = False\n\n    self._parse_config(config)\n\n    self.init_network = SDFPoseNet(\n        INIT_MODULE_DICT[self.init_config[\"backbone_type\"]](\n            **self.init_config[\"backbone\"]\n        ),\n        INIT_MODULE_DICT[self.init_config[\"head_type\"]](\n            shape_dimension=self.vae_config[\"latent_size\"],\n            **self.init_config[\"head\"],\n        ),\n    ).to(self.device)\n    load_model_weights(\n        self.init_config[\"model\"],\n        self.init_network,\n        self.device,\n        self.init_config.get(\"model_url\"),\n    )\n    self.init_network.eval()\n\n    self.resolution = 64\n    self.vae = SDFVAE(\n        sdf_size=64,\n        latent_size=self.vae_config[\"latent_size\"],\n        encoder_dict=self.vae_config[\"encoder\"],\n        decoder_dict=self.vae_config[\"decoder\"],\n        device=self.device,\n    ).to(self.device)\n    load_model_weights(\n        self.vae_config[\"model\"],\n        self.vae,\n        self.device,\n        self.vae_config.get(\"model_url\"),\n    )\n    self.vae.eval()\n\n    self.cam = Camera(**self.camera_config)\n    self.render = lambda sdf, pos, quat, i_s: render_depth_gpu(\n        sdf, pos, quat, i_s, None, None, None, config[\"threshold\"], self.cam\n    )\n    self.config = config\n\n    self.log_data = []\n</code></pre>"},{"location":"reference/estimation/simple_setup/#sdfest.estimation.simple_setup.SDFPipeline.generate_depth","title":"<code>generate_depth(position, orientation, scale, latent)</code>","text":"<p>Generate depth image representing positioned object.</p> Source code in <code>sdfest/estimation/simple_setup.py</code> <pre><code>def generate_depth(\n    self,\n    position: torch.Tensor,\n    orientation: torch.Tensor,\n    scale: torch.Tensor,\n    latent: torch.Tensor,\n) -&gt; torch.Tensor:\n    \"\"\"Generate depth image representing positioned object.\"\"\"\n    sdf = self.vae.decode(latent)\n    depth = self.render(sdf[0, 0], position, orientation, 1 / scale)\n    return depth\n</code></pre>"},{"location":"reference/estimation/simple_setup/#sdfest.estimation.simple_setup.SDFPipeline.generate_mesh","title":"<code>generate_mesh(latent, scale, complete_mesh=False)</code>","text":"<p>Generate mesh without pose.</p> <p>Currently only supports batch size 1.</p> <p>Parameters:</p> Name Type Description Default <code>latent</code> <code>tensor</code> <p>Latent shape descriptor, shape (1,L).</p> required <code>scale</code> <code>tensor</code> <p>Relative scale of the signed distance field, (i.e., half-width), shape (1,).</p> required <code>complete_mesh</code> <code>bool</code> <p>If True, the SDF will be padded with positive values prior to converting it to a mesh. This ensures a watertight mesh is created.</p> <code>False</code> <p>Returns:</p> Type Description <code>Mesh</code> <p>Generate mesh by decoding latent shape descriptor and scaling it.</p> Source code in <code>sdfest/estimation/simple_setup.py</code> <pre><code>def generate_mesh(\n    self, latent: torch.tensor, scale: torch.tensor, complete_mesh: bool = False\n) -&gt; synthetic.Mesh:\n    \"\"\"Generate mesh without pose.\n\n    Currently only supports batch size 1.\n\n    Args:\n        latent: Latent shape descriptor, shape (1,L).\n        scale:\n            Relative scale of the signed distance field, (i.e., half-width),\n            shape (1,).\n        complete_mesh:\n            If True, the SDF will be padded with positive values prior to converting\n            it to a mesh. This ensures a watertight mesh is created.\n\n    Returns:\n        Generate mesh by decoding latent shape descriptor and scaling it.\n    \"\"\"\n    with torch.no_grad():\n        sdf = self.vae.decode(latent)\n        if complete_mesh:\n            inc = 2\n            sdf = torch.nn.functional.pad(sdf, (1, 1, 1, 1, 1, 1), value=1.0)\n        else:\n            inc = 0\n        try:\n            sdf = sdf.cpu().numpy()\n            s = 2.0 / (self.resolution - 1)\n            vertices, faces, _, _ = marching_cubes(\n                sdf[0, 0],\n                spacing=(\n                    s,\n                    s,\n                    s,\n                ),\n                level=self.config[\"iso_threshold\"],\n            )\n\n            c = s * (self.resolution + inc - 1) / 2.0  # move origin to center\n            vertices -= np.array([[c, c, c]])\n\n            mesh = o3d.geometry.TriangleMesh(\n                vertices=o3d.utility.Vector3dVector(vertices),\n                triangles=o3d.utility.Vector3iVector(faces),\n            )\n        except KeyError:\n            return None\n        return synthetic.Mesh(mesh=mesh, scale=scale.item(), rel_scale=True)\n</code></pre>"},{"location":"reference/estimation/synthetic/","title":"synthetic","text":"<p>Module for synthetic data generation.</p>"},{"location":"reference/estimation/synthetic/#sdfest.estimation.synthetic.Mesh","title":"<code>Mesh</code>","text":"<p>               Bases: <code>Object</code></p> <p>Object with associated mesh.</p> <p>This class maintains two meshes, the original mesh and the scaled mesh.</p> <p>Updating the scale will always be relative to the original mesh. I.e., two times setting the relative scale by 0.1 will not yield a final scale of 0.01 as it will always be relative to the original mesh.</p> Source code in <code>sdfest/estimation/synthetic.py</code> <pre><code>class Mesh(Object):\n    \"\"\"Object with associated mesh.\n\n    This class maintains two meshes, the original mesh and the scaled mesh.\n\n    Updating the scale will always be relative to the original mesh. I.e., two times\n    setting the relative scale by 0.1 will not yield a final scale of 0.01 as it will\n    always be relative to the original mesh.\n    \"\"\"\n\n    def __init__(\n        self,\n        mesh: Optional[o3d.geometry.TriangleMesh] = None,\n        path: Optional[str] = None,\n        scale: float = 1,\n        rel_scale: bool = False,\n        center: bool = False,\n        position: Optional[np.array] = None,\n        orientation: Optional[np.array] = None,\n    ):\n        \"\"\"Initialize mesh.\n\n        Must provide either mesh or path. If path is given, mesh will be loaded from\n        specified file. If mesh is given, it will be used as the original mesh.\n\n        Args:\n            mesh: The original (i.e., unscaled mesh).\n            path: The path of the mesh to load.\n            scale: See Mesh.update_scale.\n            rel_scale: See Mesh.update_scale.\n            center: whether to center the mesh upon loading.\n            position: See Object.__init__.\n            orientation:  See Object.__init__.\n        \"\"\"\n        super().__init__(position=position, orientation=orientation)\n        if mesh is not None and path is not None:\n            raise ValueError(\"Only one of mesh or path can be specified\")\n        if mesh is not None:\n            self._original_mesh = mesh\n        if path is not None:\n            self._original_mesh = o3d.io.read_triangle_mesh(path)\n        if center:\n            self._original_mesh.translate([0, 0, 0], relative=False)\n\n        self.update_scale(scale, rel_scale)\n\n    def load_mesh_from_file(\n        self, path: str, scale: float = 1, rel_scale: bool = False\n    ) -&gt; None:\n        \"\"\"Load mesh from file.\n\n        Args:\n            path: Path of the obj file.\n            scale: See Mesh.update_scale.\n            rel_scale: See Mesh.update_scale.\n        \"\"\"\n        self._original_mesh = o3d.io.read_triangle_mesh(path)\n        self.update_scale(scale, rel_scale)\n\n    def update_scale(self, scale: float = 1, rel_scale: bool = False) -&gt; None:\n        \"\"\"Update relative or absolute scale of mesh.\n\n        Absolute scale represents half the largest extent in x, y, or z direction.\n        Relative scale represents the scale factor from original mesh.\n\n        Args:\n            scale: The desired absolute or relative scale of the object.\n            rel_scale:\n                If true, scale will be relative to original mesh.\n                Otherwise, scale will be the resulting absolute scale.\n        \"\"\"\n        # self._scale will always be absolute scale\n        if rel_scale:\n            # copy construct mesh\n            self._scaled_mesh = o3d.geometry.TriangleMesh(self._original_mesh)\n\n            original_scale = self._get_original_scale()\n            self._scaled_mesh.scale(scale, [0, 0, 0])\n\n            self._scale = original_scale * scale\n        else:\n            # copy construct mesh\n            self._scaled_mesh = o3d.geometry.TriangleMesh(self._original_mesh)\n\n            # scale original mesh s.t. output has the provided scale\n            original_scale = self._get_original_scale()\n            scale_factor = scale / original_scale\n            self._scaled_mesh.scale(scale_factor, [0, 0, 0])\n\n            self._scale = scale\n\n    def _get_original_scale(self) -&gt; float:\n        \"\"\"Compute current scale of original mesh.\n\n        Scale is largest x/y/z extent over 2.\n        \"\"\"\n        mins = np.amin(self._original_mesh.vertices, axis=0)\n        maxs = np.amax(self._original_mesh.vertices, axis=0)\n        ranges = maxs - mins\n        return np.max(ranges) / 2\n\n    def get_transformed_o3d_geometry(self) -&gt; o3d.geometry.TriangleMesh:\n        \"\"\"Get o3d mesh at current pose.\"\"\"\n        transformed_mesh = o3d.geometry.TriangleMesh(self._scaled_mesh)\n        R = Rotation.from_quat(self.orientation).as_matrix()\n        transformed_mesh.rotate(R, center=np.array([0, 0, 0]))\n        transformed_mesh.translate(self.position)\n        transformed_mesh.compute_vertex_normals()\n        return transformed_mesh\n</code></pre>"},{"location":"reference/estimation/synthetic/#sdfest.estimation.synthetic.Mesh.__init__","title":"<code>__init__(mesh=None, path=None, scale=1, rel_scale=False, center=False, position=None, orientation=None)</code>","text":"<p>Initialize mesh.</p> <p>Must provide either mesh or path. If path is given, mesh will be loaded from specified file. If mesh is given, it will be used as the original mesh.</p> <p>Parameters:</p> Name Type Description Default <code>mesh</code> <code>Optional[TriangleMesh]</code> <p>The original (i.e., unscaled mesh).</p> <code>None</code> <code>path</code> <code>Optional[str]</code> <p>The path of the mesh to load.</p> <code>None</code> <code>scale</code> <code>float</code> <p>See Mesh.update_scale.</p> <code>1</code> <code>rel_scale</code> <code>bool</code> <p>See Mesh.update_scale.</p> <code>False</code> <code>center</code> <code>bool</code> <p>whether to center the mesh upon loading.</p> <code>False</code> <code>position</code> <code>Optional[array]</code> <p>See Object.init.</p> <code>None</code> <code>orientation</code> <code>Optional[array]</code> <p>See Object.init.</p> <code>None</code> Source code in <code>sdfest/estimation/synthetic.py</code> <pre><code>def __init__(\n    self,\n    mesh: Optional[o3d.geometry.TriangleMesh] = None,\n    path: Optional[str] = None,\n    scale: float = 1,\n    rel_scale: bool = False,\n    center: bool = False,\n    position: Optional[np.array] = None,\n    orientation: Optional[np.array] = None,\n):\n    \"\"\"Initialize mesh.\n\n    Must provide either mesh or path. If path is given, mesh will be loaded from\n    specified file. If mesh is given, it will be used as the original mesh.\n\n    Args:\n        mesh: The original (i.e., unscaled mesh).\n        path: The path of the mesh to load.\n        scale: See Mesh.update_scale.\n        rel_scale: See Mesh.update_scale.\n        center: whether to center the mesh upon loading.\n        position: See Object.__init__.\n        orientation:  See Object.__init__.\n    \"\"\"\n    super().__init__(position=position, orientation=orientation)\n    if mesh is not None and path is not None:\n        raise ValueError(\"Only one of mesh or path can be specified\")\n    if mesh is not None:\n        self._original_mesh = mesh\n    if path is not None:\n        self._original_mesh = o3d.io.read_triangle_mesh(path)\n    if center:\n        self._original_mesh.translate([0, 0, 0], relative=False)\n\n    self.update_scale(scale, rel_scale)\n</code></pre>"},{"location":"reference/estimation/synthetic/#sdfest.estimation.synthetic.Mesh.get_transformed_o3d_geometry","title":"<code>get_transformed_o3d_geometry()</code>","text":"<p>Get o3d mesh at current pose.</p> Source code in <code>sdfest/estimation/synthetic.py</code> <pre><code>def get_transformed_o3d_geometry(self) -&gt; o3d.geometry.TriangleMesh:\n    \"\"\"Get o3d mesh at current pose.\"\"\"\n    transformed_mesh = o3d.geometry.TriangleMesh(self._scaled_mesh)\n    R = Rotation.from_quat(self.orientation).as_matrix()\n    transformed_mesh.rotate(R, center=np.array([0, 0, 0]))\n    transformed_mesh.translate(self.position)\n    transformed_mesh.compute_vertex_normals()\n    return transformed_mesh\n</code></pre>"},{"location":"reference/estimation/synthetic/#sdfest.estimation.synthetic.Mesh.load_mesh_from_file","title":"<code>load_mesh_from_file(path, scale=1, rel_scale=False)</code>","text":"<p>Load mesh from file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path of the obj file.</p> required <code>scale</code> <code>float</code> <p>See Mesh.update_scale.</p> <code>1</code> <code>rel_scale</code> <code>bool</code> <p>See Mesh.update_scale.</p> <code>False</code> Source code in <code>sdfest/estimation/synthetic.py</code> <pre><code>def load_mesh_from_file(\n    self, path: str, scale: float = 1, rel_scale: bool = False\n) -&gt; None:\n    \"\"\"Load mesh from file.\n\n    Args:\n        path: Path of the obj file.\n        scale: See Mesh.update_scale.\n        rel_scale: See Mesh.update_scale.\n    \"\"\"\n    self._original_mesh = o3d.io.read_triangle_mesh(path)\n    self.update_scale(scale, rel_scale)\n</code></pre>"},{"location":"reference/estimation/synthetic/#sdfest.estimation.synthetic.Mesh.update_scale","title":"<code>update_scale(scale=1, rel_scale=False)</code>","text":"<p>Update relative or absolute scale of mesh.</p> <p>Absolute scale represents half the largest extent in x, y, or z direction. Relative scale represents the scale factor from original mesh.</p> <p>Parameters:</p> Name Type Description Default <code>scale</code> <code>float</code> <p>The desired absolute or relative scale of the object.</p> <code>1</code> <code>rel_scale</code> <code>bool</code> <p>If true, scale will be relative to original mesh. Otherwise, scale will be the resulting absolute scale.</p> <code>False</code> Source code in <code>sdfest/estimation/synthetic.py</code> <pre><code>def update_scale(self, scale: float = 1, rel_scale: bool = False) -&gt; None:\n    \"\"\"Update relative or absolute scale of mesh.\n\n    Absolute scale represents half the largest extent in x, y, or z direction.\n    Relative scale represents the scale factor from original mesh.\n\n    Args:\n        scale: The desired absolute or relative scale of the object.\n        rel_scale:\n            If true, scale will be relative to original mesh.\n            Otherwise, scale will be the resulting absolute scale.\n    \"\"\"\n    # self._scale will always be absolute scale\n    if rel_scale:\n        # copy construct mesh\n        self._scaled_mesh = o3d.geometry.TriangleMesh(self._original_mesh)\n\n        original_scale = self._get_original_scale()\n        self._scaled_mesh.scale(scale, [0, 0, 0])\n\n        self._scale = original_scale * scale\n    else:\n        # copy construct mesh\n        self._scaled_mesh = o3d.geometry.TriangleMesh(self._original_mesh)\n\n        # scale original mesh s.t. output has the provided scale\n        original_scale = self._get_original_scale()\n        scale_factor = scale / original_scale\n        self._scaled_mesh.scale(scale_factor, [0, 0, 0])\n\n        self._scale = scale\n</code></pre>"},{"location":"reference/estimation/synthetic/#sdfest.estimation.synthetic.Object","title":"<code>Object</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Generic positioned object representation.</p> <p>Each object has a 6-DOF pose, stored as a 3D translation vector and a normalized quaternion representing its orientation.</p> Source code in <code>sdfest/estimation/synthetic.py</code> <pre><code>class Object(ABC):\n    \"\"\"Generic positioned object representation.\n\n    Each object has a 6-DOF pose, stored as a 3D translation vector and\n    a normalized quaternion representing its orientation.\n    \"\"\"\n\n    def __init__(self, position=None, orientation=None):\n        \"\"\"Initialize object position and orientation.\"\"\"\n        if position is None:\n            position = np.array([0, 0, 0])\n        if orientation is None:\n            orientation = np.array([0, 0, 0, 1])\n        self.position = position\n        self.orientation = orientation\n</code></pre>"},{"location":"reference/estimation/synthetic/#sdfest.estimation.synthetic.Object.__init__","title":"<code>__init__(position=None, orientation=None)</code>","text":"<p>Initialize object position and orientation.</p> Source code in <code>sdfest/estimation/synthetic.py</code> <pre><code>def __init__(self, position=None, orientation=None):\n    \"\"\"Initialize object position and orientation.\"\"\"\n    if position is None:\n        position = np.array([0, 0, 0])\n    if orientation is None:\n        orientation = np.array([0, 0, 0, 1])\n    self.position = position\n    self.orientation = orientation\n</code></pre>"},{"location":"reference/estimation/synthetic/#sdfest.estimation.synthetic.draw_depth_geometry","title":"<code>draw_depth_geometry(obj, camera)</code>","text":"<p>Render an object given a camera.</p> Source code in <code>sdfest/estimation/synthetic.py</code> <pre><code>def draw_depth_geometry(obj: Object, camera: Camera):\n    \"\"\"Render an object given a camera.\"\"\"\n    # see http://www.open3d.org/docs/latest/tutorial/visualization/customized_visualization.html\n    # rend = o3d.visualization.rendering.OffscreenRenderer()\n    # img = rend.render_to_image()\n\n    # Create visualizer\n    vis = o3d.visualization.Visualizer()\n    vis.create_window(width=camera.width, height=camera.height, visible=False)\n\n    # Add mesh in correct position\n    vis.add_geometry(obj.get_transformed_o3d_geometry(), True)\n\n    options = vis.get_render_option()\n    options.mesh_show_back_face = True\n\n    # Set camera at fixed position (i.e., at 0,0,0, looking along z axis)\n    view_control = vis.get_view_control()\n    o3d_cam = camera.get_o3d_pinhole_camera_parameters()\n    o3d_cam.extrinsic = np.array(\n        [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]\n    )\n    view_control.convert_from_pinhole_camera_parameters(o3d_cam, True)\n\n    # Generate the depth image\n    vis.poll_events()\n    vis.update_renderer()\n    depth = np.asarray(vis.capture_depth_float_buffer(do_render=True))\n\n    return depth\n</code></pre>"},{"location":"reference/estimation/scripts/play_log/","title":"play_log","text":"<p>Script to play back log file and generate animation.</p> Usage <p>python play_log.py --log_file filename.pkl</p> <p>Log files in the required format is generated by render_evaluation.py.</p>"},{"location":"reference/estimation/scripts/play_log/#sdfest.estimation.scripts.play_log.main","title":"<code>main()</code>","text":"<p>Entry point of the evaluation program.</p> Source code in <code>sdfest/estimation/scripts/play_log.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Entry point of the evaluation program.\"\"\"\n    global reset\n    global animation_queued\n    parser = argparse.ArgumentParser(\n        description=\"Play log file and generate animation.\"\n    )\n    parser.add_argument(\"--log_file\", required=True)\n    args = parser.parse_args()\n\n    with open(args.log_file, \"rb\") as f:\n        data = pickle.load(f)\n        config = data[\"config\"]\n        log_entries = data[\"log\"]\n\n    pipeline = SDFPipeline(config)\n\n    # precompute meshes for each timestep\n    for log_entry in tqdm(log_entries):\n        # print(log_entry)\n        if \"latent_shape\" in log_entry:\n            out_mesh = pipeline.generate_mesh(\n                log_entry[\"latent_shape\"], 1 / log_entry[\"scale_inv\"], True\n            )\n            out_mesh.position = log_entry[\"position\"][0].detach().cpu().numpy()\n            out_mesh.orientation = log_entry[\"orientation\"][0].detach().cpu().numpy()\n            log_entry[\"mesh\"] = out_mesh.get_transformed_o3d_geometry()\n            log_entry[\"mesh\"].paint_uniform_color([0.2, 0.4, 0.7])\n            log_entry[\"pointcloud\"] = log_entry[\"mesh\"].sample_points_uniformly(20000)\n\n    # visualize log entries\n    cam_meshes = []\n    pointclouds = []\n    KEY_ESCAPE = 256\n    vis = o3d.visualization.VisualizerWithKeyCallback()\n    vis.register_key_callback(key=ord(\"A\"), callback_func=reset_bounding_box)\n    vis.register_key_callback(key=ord(\"R\"), callback_func=toggle_realtime)\n    vis.register_key_callback(key=ord(\"S\"), callback_func=switch_reconstruction_type)\n    vis.register_key_callback(key=ord(\"N\"), callback_func=queue_animation)\n    vis.register_key_callback(key=ord(\"C\"), callback_func=toggle_color)\n    vis.register_key_callback(key=ord(\"F\"), callback_func=toggle_camera_frames)\n    vis.register_key_callback(key=ord(\" \"), callback_func=toggle_pause)\n    vis.register_key_callback(key=KEY_ESCAPE, callback_func=quit_program)\n    vis.create_window(width=640, height=480)\n    print(\n        \"Controls\\n\\ta: reset view point &amp; bounding box\\n\"\n        \"\\tr: toggle realtime\\n\"\n        \"\\ts: switch reconstruction_type\\n\",\n        \"\\tn: queue animation\\n\",\n        \"\\tc: toggle color\\n\",\n        \"\\tf: toggle camera frames\\n\",\n        \"\\tspace: pause loop\\n\",\n    )\n    first = True\n    while True:\n        vis.clear_geometries()\n        animation_folder = None\n        animation_files = []\n        animation_timestamps = []\n        if animation_queued:\n            animation_queued = False\n            animation_folder = (\n                f\"animation_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n            )\n            os.makedirs(animation_folder, exist_ok=True)\n        start_time = time.time()\n        for log_entry in log_entries:\n            vis.poll_events()\n            vis.update_renderer()\n            if realtime and animation_folder is None:\n                while log_entry[\"timestamp\"] &gt; time.time() - start_time:\n                    vis.poll_events()\n                    vis.update_renderer()\n\n            if \"camera_positions\" in log_entry:\n                cam_meshes = []\n                for t_c2w, quat_c2w in zip(\n                    log_entry[\"camera_positions\"], log_entry[\"camera_orientations\"]\n                ):\n                    frame_mesh = synthetic.Mesh(\n                        mesh=o3d.geometry.TriangleMesh.create_coordinate_frame(\n                            size=0.1, origin=[0, 0, 0]\n                        ),\n                        rel_scale=True,\n                    )\n                    frame_mesh.position = t_c2w.cpu().numpy()\n                    frame_mesh.orientation = quaternion_utils.quaternion_multiply(\n                        quat_c2w.cpu(), torch.tensor([1.0, 0, 0, 0])\n                    ).numpy()\n                    cam_meshes.append(frame_mesh.get_transformed_o3d_geometry())\n\n            if \"depth_images\" in log_entry:\n                pointclouds = []\n                for depth_image, color_image, t_c2w, quat_c2w in zip(\n                    log_entry[\"depth_images\"],\n                    log_entry[\"color_images\"],\n                    log_entry[\"camera_positions\"],\n                    log_entry[\"camera_orientations\"],\n                ):\n                    points_c = pointset_utils.depth_to_pointcloud(\n                        depth_image, pipeline.cam, normalize=False\n                    )\n                    pointcloud_torch = (\n                        quaternion_utils.quaternion_apply(quat_c2w, points_c) + t_c2w\n                    )\n                    pointcloud_numpy = pointcloud_torch.cpu().numpy()\n                    pointcloud_o3d = o3d.geometry.PointCloud(\n                        o3d.utility.Vector3dVector(pointcloud_numpy)\n                    )\n                    if color:\n                        pointcloud_colors_torch = color_image[depth_image != 0]\n                        pointcloud_colors_numpy = pointcloud_colors_torch.cpu().numpy()\n                        pointcloud_o3d.colors = o3d.utility.Vector3dVector(\n                            pointcloud_colors_numpy\n                        )\n                    else:\n                        pointcloud_o3d.colors = o3d.utility.Vector3dVector(\n                            np.ones_like(pointcloud_numpy) * np.array([1.0, 0.2, 0.2])\n                        )\n\n                    pointclouds.append(pointcloud_o3d)\n\n            while True:\n                geometries = [] + pointclouds\n                if camera_frames:\n                    geometries += cam_meshes\n                if \"mesh\" in log_entry and reconstruction_type != \"none\":\n                    geometries.append(log_entry[reconstruction_type])\n                vis.clear_geometries()\n                for i, geometry in enumerate(geometries):\n                    vis.add_geometry(geometry, reset_bounding_box=first or reset)\n                    if i == len(geometries) - 1:\n                        reset = first = False\n                if not pause:\n                    break\n                else:\n                    vis.poll_events()\n                    vis.update_renderer()\n\n            if animation_folder is not None:\n                vis.poll_events()\n                vis.update_renderer()\n                timestamp = log_entry[\"timestamp\"]\n                filename = f\"{timestamp}.png\"\n                vis.capture_screen_image(os.path.join(animation_folder, filename))\n                animation_files.append(filename)\n                animation_timestamps.append(timestamp)\n\n        # create constant framerate video\n        if animation_folder is not None:\n            fps = 30\n            frame_folder = os.path.join(animation_folder, \"constant_framerate\")\n            os.makedirs(frame_folder, exist_ok=True)\n            video_name = f\"{animation_folder}.mp4\"\n            current_frame = animation_files.pop(0)\n            current_time = animation_timestamps.pop(0)\n            frame_number = 0\n            while animation_files:\n                if animation_timestamps[0] &lt;= current_time:\n                    current_frame = animation_files.pop(0)\n                    animation_timestamps.pop(0)\n                copyfile(\n                    os.path.join(animation_folder, current_frame),\n                    os.path.join(frame_folder, f\"{frame_number:06d}.png\"),\n                )\n\n                current_time += 1 / fps\n                frame_number += 1\n\n            ffmpeg.input(\n                os.path.join(frame_folder, \"*.png\"), pattern_type=\"glob\", framerate=fps\n            ).output(video_name).run()\n</code></pre>"},{"location":"reference/estimation/scripts/play_log/#sdfest.estimation.scripts.play_log.queue_animation","title":"<code>queue_animation(_)</code>","text":"<p>Switch between mesh and pointcloud reconstruction.</p> Source code in <code>sdfest/estimation/scripts/play_log.py</code> <pre><code>def queue_animation(_: o3d.visualization.VisualizerWithKeyCallback) -&gt; bool:\n    \"\"\"Switch between mesh and pointcloud reconstruction.\"\"\"\n    global animation_queued\n    animation_queued = True\n    print(\"Saving animation for next loop.\")\n    return False\n</code></pre>"},{"location":"reference/estimation/scripts/play_log/#sdfest.estimation.scripts.play_log.quit_program","title":"<code>quit_program(_)</code>","text":"<p>Quit program.</p> Source code in <code>sdfest/estimation/scripts/play_log.py</code> <pre><code>def quit_program(_: o3d.visualization.VisualizerWithKeyCallback) -&gt; bool:\n    \"\"\"Quit program.\"\"\"\n    exit()\n</code></pre>"},{"location":"reference/estimation/scripts/play_log/#sdfest.estimation.scripts.play_log.reset_bounding_box","title":"<code>reset_bounding_box(_)</code>","text":"<p>Schedule resetting of bounding box.</p> Source code in <code>sdfest/estimation/scripts/play_log.py</code> <pre><code>def reset_bounding_box(_: o3d.visualization.VisualizerWithKeyCallback) -&gt; bool:\n    \"\"\"Schedule resetting of bounding box.\"\"\"\n    global reset\n    print(\"Reset bounding box.\")\n    reset = True\n    return False\n</code></pre>"},{"location":"reference/estimation/scripts/play_log/#sdfest.estimation.scripts.play_log.switch_reconstruction_type","title":"<code>switch_reconstruction_type(_)</code>","text":"<p>Switch between mesh and pointcloud reconstruction.</p> Source code in <code>sdfest/estimation/scripts/play_log.py</code> <pre><code>def switch_reconstruction_type(_: o3d.visualization.VisualizerWithKeyCallback) -&gt; bool:\n    \"\"\"Switch between mesh and pointcloud reconstruction.\"\"\"\n    global reconstruction_type\n    cur_id = reconstruction_types.index(reconstruction_type)\n    cur_id = (cur_id + 1) % len(reconstruction_types)\n    reconstruction_type = reconstruction_types[cur_id]\n    print(f\"Reconstruction: {reconstruction_type}\")\n    return False\n</code></pre>"},{"location":"reference/estimation/scripts/play_log/#sdfest.estimation.scripts.play_log.toggle_camera_frames","title":"<code>toggle_camera_frames(_)</code>","text":"<p>Toggle camera frame visualization.</p> Source code in <code>sdfest/estimation/scripts/play_log.py</code> <pre><code>def toggle_camera_frames(_: o3d.visualization.VisualizerWithKeyCallback) -&gt; bool:\n    \"\"\"Toggle camera frame visualization.\"\"\"\n    global camera_frames\n    camera_frames = not camera_frames\n    print(f\"Camera frames: {camera_frames}\")\n    return False\n</code></pre>"},{"location":"reference/estimation/scripts/play_log/#sdfest.estimation.scripts.play_log.toggle_color","title":"<code>toggle_color(_)</code>","text":"<p>Toggle color pointcloud.</p> Source code in <code>sdfest/estimation/scripts/play_log.py</code> <pre><code>def toggle_color(_: o3d.visualization.VisualizerWithKeyCallback) -&gt; bool:\n    \"\"\"Toggle color pointcloud.\"\"\"\n    global color\n    color = not color\n    print(f\"Color: {color}\")\n    return False\n</code></pre>"},{"location":"reference/estimation/scripts/play_log/#sdfest.estimation.scripts.play_log.toggle_pause","title":"<code>toggle_pause(_)</code>","text":"<p>Toggle pause.</p> Source code in <code>sdfest/estimation/scripts/play_log.py</code> <pre><code>def toggle_pause(_: o3d.visualization.VisualizerWithKeyCallback) -&gt; bool:\n    \"\"\"Toggle pause.\"\"\"\n    global pause\n    pause = not pause\n    print(f\"Pause: {pause}\")\n    return False\n</code></pre>"},{"location":"reference/estimation/scripts/play_log/#sdfest.estimation.scripts.play_log.toggle_realtime","title":"<code>toggle_realtime(_)</code>","text":"<p>Toggle realtime playback.</p> Source code in <code>sdfest/estimation/scripts/play_log.py</code> <pre><code>def toggle_realtime(_: o3d.visualization.VisualizerWithKeyCallback) -&gt; bool:\n    \"\"\"Toggle realtime playback.\"\"\"\n    global realtime\n    realtime = not realtime\n    print(f\"Realtime: {realtime}\")\n    return False\n</code></pre>"},{"location":"reference/estimation/scripts/real_data/","title":"real_data","text":"<p>Simple script to run inference on real data.</p> <p>Usage (evaluation on random RGB-D images from folder):     python -m sdfest.estimation.scripts.real_data         --config estimation/configs/rgbd_objects_uw.yaml estimation/configs/mug.yaml         --folder data/rgbd_objects_uw/coffee_mug/</p> <p>Usage (evaluation on single RGB image from Redwood or RGB-D objects dataset):     python -m sdfest.estimation.scripts.real_data         --config configs/rgbd_objects_uw.yaml configs/mug.yaml         --input rgbd_objects_uw/coffee_mug/coffee_mug_1/coffee_mug_1_1_103.png</p> Specific parameters <p>measure_runtime:     if True, a breakdown of the runtime will be generated     only supported for single input out_folder:     if provided and measure_runtime is true, the runtime results are written to file visualize_optimization: whether to visualize optimization while at it visualize_input: whether to visualize the input create_animation:     If true, three animations will be created. One for depth optimization, depth     error, and mesh.</p>"},{"location":"reference/estimation/scripts/real_data/#sdfest.estimation.scripts.real_data.load_real275_rgbd","title":"<code>load_real275_rgbd(rgb_path)</code>","text":"<p>Load RGB-D image from RGB path.</p> <p>Parameters:</p> Name Type Description Default <code>rgb_path</code> <code>str</code> <p>path to RGB image</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray, str, str]</code> <p>Tuple containing: - The color image, float32, RGB, 0-1, shape (H,W,C). - The depth image, float32, in meters, shape (H,W). - The color path. - The depth path.</p> Source code in <code>sdfest/estimation/scripts/real_data.py</code> <pre><code>def load_real275_rgbd(rgb_path: str) -&gt; Tuple[np.ndarray, np.ndarray, str, str]:\n    \"\"\"Load RGB-D image from RGB path.\n\n    Args:\n        rgb_path: path to RGB image\n\n    Returns:\n        Tuple containing:\n            - The color image, float32, RGB, 0-1, shape (H,W,C).\n            - The depth image, float32, in meters, shape (H,W).\n            - The color path.\n            - The depth path.\n    \"\"\"\n    depth_path = rgb_path[:-10] + \"_depth.png\"\n    color_img = np.asarray(o3d.io.read_image(rgb_path), dtype=np.float32) / 255\n    depth_img = (\n        np.asarray(\n            o3d.io.read_image(depth_path),\n            dtype=np.float32,\n        )\n        * 0.001\n    )\n    return color_img, depth_img, rgb_path, depth_path\n</code></pre>"},{"location":"reference/estimation/scripts/real_data/#sdfest.estimation.scripts.real_data.load_real275_sample","title":"<code>load_real275_sample(folder)</code>","text":"<p>Load a sample from RGBD Object dataset.</p> <p>https://rgbd-dataset.cs.washington.edu/dataset/</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>The root folder of the dataset.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray, str, str]</code> <p>See load_real275_rgbd.</p> Source code in <code>sdfest/estimation/scripts/real_data.py</code> <pre><code>def load_real275_sample(folder: str) -&gt; Tuple[np.ndarray, np.ndarray, str, str]:\n    \"\"\"Load a sample from RGBD Object dataset.\n\n    https://rgbd-dataset.cs.washington.edu/dataset/\n\n    Args:\n        folder: The root folder of the dataset.\n\n    Returns:\n        See load_real275_rgbd.\n    \"\"\"\n    files = glob.glob(folder + \"/**/*color.png\", recursive=True)\n    rgb_path = random.choice(files)\n    return load_real275_rgbd(rgb_path)\n</code></pre>"},{"location":"reference/estimation/scripts/real_data/#sdfest.estimation.scripts.real_data.load_redwood_rgbd","title":"<code>load_redwood_rgbd(rgb_path)</code>","text":"<p>Load RGB-D image from RGB path of Redwood dataset.</p> <p>Parameters:</p> Name Type Description Default <code>rgb_path</code> <code>str</code> <p>path to RGB image</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray, str, str]</code> <p>Tuple containing: - The color image, float32, RGB, 0-1, shape (H,W,C). - The depth image, float32, in meters, shape (H,W). - The color path. - The depth path.</p> Source code in <code>sdfest/estimation/scripts/real_data.py</code> <pre><code>def load_redwood_rgbd(rgb_path: str) -&gt; Tuple[np.ndarray, np.ndarray, str, str]:\n    \"\"\"Load RGB-D image from RGB path of Redwood dataset.\n\n    Args:\n        rgb_path: path to RGB image\n\n    Returns:\n        Tuple containing:\n            - The color image, float32, RGB, 0-1, shape (H,W,C).\n            - The depth image, float32, in meters, shape (H,W).\n            - The color path.\n            - The depth path.\n    \"\"\"\n    depth_dir = os.path.join(os.path.dirname(rgb_path), \"..\", \"depth\")\n\n    rgb_timestamp = int(rgb_path[-16:-4])\n\n    # find closest depth image in time\n    depth_paths = glob.glob(depth_dir + \"/*.png\")\n    depth_timestamps = np.array([int(p[-16:-4]) for p in depth_paths])\n    ind = np.argmin(np.abs(depth_timestamps - rgb_timestamp))\n    depth_path = depth_paths[ind]\n\n    color_img = np.asarray(o3d.io.read_image(rgb_path), dtype=np.float32) / 255\n\n    depth_img = (\n        np.asarray(\n            o3d.io.read_image(depth_path),\n            dtype=np.float32,\n        )\n        * 0.001\n    )\n    return color_img, depth_img, rgb_path, depth_path\n</code></pre>"},{"location":"reference/estimation/scripts/real_data/#sdfest.estimation.scripts.real_data.load_redwood_sample","title":"<code>load_redwood_sample(folder)</code>","text":"<p>Load a sample from Redwood dataset.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>The root folder of the dataset.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray, str, str]</code> <p>Tuple containing: - The color image, float32, RGB, 0-1, shape (H,W,C). - The depth image, float32, in meters, shape (H,W). - The color path. - The depth path.</p> Source code in <code>sdfest/estimation/scripts/real_data.py</code> <pre><code>def load_redwood_sample(folder: str) -&gt; Tuple[np.ndarray, np.ndarray, str, str]:\n    \"\"\"Load a sample from Redwood dataset.\n\n    Args:\n        folder: The root folder of the dataset.\n\n    Returns:\n        Tuple containing:\n            - The color image, float32, RGB, 0-1, shape (H,W,C).\n            - The depth image, float32, in meters, shape (H,W).\n            - The color path.\n            - The depth path.\n    \"\"\"\n    sequence_paths = glob.glob(folder + \"/*\")\n    sequence_path = random.choice(sequence_paths)\n\n    rgb_dir = os.path.join(sequence_path, \"rgb\")\n    rgb_paths = glob.glob(rgb_dir + \"/*.jpg\")\n    rgb_path = random.choice(rgb_paths)\n\n    return load_redwood_rgbd(rgb_path)\n</code></pre>"},{"location":"reference/estimation/scripts/real_data/#sdfest.estimation.scripts.real_data.load_rgbd","title":"<code>load_rgbd(config)</code>","text":"<p>Load a single RGB-D image from path and dataset specified in config.</p> <p>See the dataset specific load functions for more details of the expected folder structure.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration dictionary that must contain the following keys: \"dataset\": one of \"redwood\" | \"rgbd_object_uw\" \"input\": the path to the RGB image</p> required <p>Returns:     Tuple containing:         - The color image, float32, RGB, 0-1, shape (H,W,C).         - The depth image, float32, in meters, shape (H,W).         - The color path.         - The depth path.</p> Source code in <code>sdfest/estimation/scripts/real_data.py</code> <pre><code>def load_rgbd(config: dict) -&gt; Tuple[np.ndarray, np.ndarray, str, str]:\n    \"\"\"Load a single RGB-D image from path and dataset specified in config.\n\n    See the dataset specific load functions for more details of the expected folder\n    structure.\n\n    Params:\n        config: Configuration dictionary that must contain the following keys:\n            \"dataset\": one of \"redwood\" | \"rgbd_object_uw\"\n            \"input\": the path to the RGB image\n    Returns:\n        Tuple containing:\n            - The color image, float32, RGB, 0-1, shape (H,W,C).\n            - The depth image, float32, in meters, shape (H,W).\n            - The color path.\n            - The depth path.\n    \"\"\"\n    if config[\"dataset\"] == \"redwood\":\n        return load_redwood_rgbd(config[\"input\"])\n    elif config[\"dataset\"] == \"rgbd_object_uw\":\n        return load_rgbd_object_uw_rgbd(config[\"input\"])\n    elif config[\"dataset\"] == \"real275\":\n        return load_real275_rgbd(config[\"input\"])\n    else:\n        raise NotImplementedError(f\"Dataset {config['dataset']} is not supported\")\n</code></pre>"},{"location":"reference/estimation/scripts/real_data/#sdfest.estimation.scripts.real_data.load_rgbd_object_uw_rgbd","title":"<code>load_rgbd_object_uw_rgbd(rgb_path)</code>","text":"<p>Load RGB-D image from RGB path.</p> <p>Parameters:</p> Name Type Description Default <code>rgb_path</code> <code>str</code> <p>path to RGB image</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray, str, str]</code> <p>Tuple containing: - The color image, float32, RGB, 0-1, shape (H,W,C). - The depth image, float32, in meters, shape (H,W). - The color path. - The depth path.</p> Source code in <code>sdfest/estimation/scripts/real_data.py</code> <pre><code>def load_rgbd_object_uw_rgbd(rgb_path: str) -&gt; Tuple[np.ndarray, np.ndarray, str, str]:\n    \"\"\"Load RGB-D image from RGB path.\n\n    Args:\n        rgb_path: path to RGB image\n\n    Returns:\n        Tuple containing:\n            - The color image, float32, RGB, 0-1, shape (H,W,C).\n            - The depth image, float32, in meters, shape (H,W).\n            - The color path.\n            - The depth path.\n    \"\"\"\n    depth_path = rgb_path[:-4] + \"_depth\" + rgb_path[-4:]\n    color_img = np.asarray(o3d.io.read_image(rgb_path), dtype=np.float32) / 255\n    depth_img = (\n        np.asarray(\n            o3d.io.read_image(depth_path),\n            dtype=np.float32,\n        )\n        * 0.001\n    )\n    return color_img, depth_img, rgb_path, depth_path\n</code></pre>"},{"location":"reference/estimation/scripts/real_data/#sdfest.estimation.scripts.real_data.load_rgbd_object_uw_sample","title":"<code>load_rgbd_object_uw_sample(folder)</code>","text":"<p>Load a sample from RGBD Object dataset.</p> <p>https://rgbd-dataset.cs.washington.edu/dataset/</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>The root folder of the dataset.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray, str, str]</code> <p>See load_rgbd_object_uw_rgbd.</p> Source code in <code>sdfest/estimation/scripts/real_data.py</code> <pre><code>def load_rgbd_object_uw_sample(folder: str) -&gt; Tuple[np.ndarray, np.ndarray, str, str]:\n    \"\"\"Load a sample from RGBD Object dataset.\n\n    https://rgbd-dataset.cs.washington.edu/dataset/\n\n    Args:\n        folder: The root folder of the dataset.\n\n    Returns:\n        See load_rgbd_object_uw_rgbd.\n    \"\"\"\n    files = glob.glob(folder + \"/**/*[0-9].png\", recursive=True)\n    rgb_path = random.choice(files)\n    return load_rgbd_object_uw_rgbd(rgb_path)\n</code></pre>"},{"location":"reference/estimation/scripts/real_data/#sdfest.estimation.scripts.real_data.load_sample_from_folder","title":"<code>load_sample_from_folder(config)</code>","text":"<p>Load a sample from dataset specified in config.</p> <p>See the dataset specific load functions for more details of the expected folder structure.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration dictionary that must contain the following keys: \"dataset\": one of \"redwood\" | \"rgbd_object_uw\" \"folder\": the root folder of the dataset</p> required <p>Returns:     Tuple containing:         - The color image, float32, RGB, 0-1, shape (H,W,C).         - The depth image, float32, in meters, shape (H,W).</p> Source code in <code>sdfest/estimation/scripts/real_data.py</code> <pre><code>def load_sample_from_folder(config: dict) -&gt; Tuple[np.ndarray, np.ndarray, str, str]:\n    \"\"\"Load a sample from dataset specified in config.\n\n    See the dataset specific load functions for more details of the expected folder\n    structure.\n\n    Params:\n        config: Configuration dictionary that must contain the following keys:\n            \"dataset\": one of \"redwood\" | \"rgbd_object_uw\"\n            \"folder\": the root folder of the dataset\n    Returns:\n        Tuple containing:\n            - The color image, float32, RGB, 0-1, shape (H,W,C).\n            - The depth image, float32, in meters, shape (H,W).\n    \"\"\"\n    if config[\"dataset\"] == \"redwood\":\n        return load_redwood_sample(config[\"folder\"])\n    elif config[\"dataset\"] == \"rgbd_object_uw\":\n        return load_rgbd_object_uw_sample(config[\"folder\"])\n    elif config[\"dataset\"] == \"real275\":\n        return load_real275_sample(config[\"folder\"])\n    else:\n        raise NotImplementedError(f\"Dataset {config['dataset']} is not supported\")\n</code></pre>"},{"location":"reference/estimation/scripts/real_data/#sdfest.estimation.scripts.real_data.main","title":"<code>main()</code>","text":"<p>Entry point of the program.</p> Source code in <code>sdfest/estimation/scripts/real_data.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Entry point of the program.\"\"\"\n    # define the arguments\n    parser = argparse.ArgumentParser(description=\"SDF pose estimation in real data\")\n\n    # parse arguments\n    parser.add_argument(\"--device\")\n    parser.add_argument(\"--input\")\n    parser.add_argument(\"--folder\")\n    parser.add_argument(\"--measure_runtime\", type=str_to_bool, default=False)\n    parser.add_argument(\"--visualize_optimization\", type=str_to_bool, default=False)\n    parser.add_argument(\"--visualize_input\", type=str_to_bool, default=False)\n    parser.add_argument(\"--cached_segmentation\", action=\"store_true\")\n    parser.add_argument(\"--segmentation_dir\", default=\"./cached_segmentations/\")\n    parser.add_argument(\"--config\", default=\"configs/default.yaml\", nargs=\"+\")\n\n    config = yoco.load_config_from_args(\n        parser, search_paths=[\".\", \"~/.sdfest/\", sdfest.__path__[0]]\n    )\n\n    if \"input\" in config and \"folder\" in config:\n        print(\"Only one of input and folder can be specified.\")\n        exit()\n\n    if config[\"measure_runtime\"] and config[\"visualize_optimization\"]:\n        print(\"Visualization not supported while measuring runtime.\")\n        exit()\n\n    pipeline = SDFPipeline(config)\n    create_animation = (\n        config[\"create_animation\"] if \"create_animation\" in config else False\n    )\n\n    timing_dict = None\n    timing_dicts = []\n    if config[\"measure_runtime\"]:\n        timing_dict = add_timing_decorators(pipeline)\n\n    # Segmentation using detectron2\n    print(\"Loading segmentation model...\")\n    cfg = detectron2.config.get_cfg()\n    cfg.merge_from_file(\n        model_zoo.get_config_file(\n            \"COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml\"\n        )\n    )\n    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\n        \"COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml\"\n    )\n\n    predictor = detectron2.engine.DefaultPredictor(cfg)\n    print(\"Segmentation model loaded.\")\n\n    completed_runs = 0\n    shape_optimization = True\n\n    while True:\n        if timing_dict is not None:\n            timing_dict.clear()\n        if \"folder\" in config:\n            color_img, depth_img, color_path, _ = load_sample_from_folder(config)\n        elif \"input\" in config:\n            color_img, depth_img, color_path, _ = load_rgbd(config)\n        else:\n            print(\"No folder or input file specified.\")\n            exit()\n\n        if timing_dict is not None:\n            timing_dict[\"pipeline\"].append([time.time(), None])\n            timing_dict[\"segmentation\"].append([time.time(), None])\n\n        if config[\"cached_segmentation\"]:\n            # check if segmentation exists\n            color_name, _ = os.path.splitext(color_path)\n            color_dir = os.path.dirname(color_name)\n            segmentation_dir = os.path.join(config[\"segmentation_dir\"], color_dir)\n            segmentation_path = (\n                os.path.join(config[\"segmentation_dir\"], color_name) + \".pickle\"\n            )\n            os.makedirs(segmentation_dir, exist_ok=True)\n\n            if os.path.isfile(segmentation_path):\n                with open(segmentation_path, \"rb\") as f:\n                    outputs = pickle.load(f)\n            else:\n                # compute segmentation and save\n                # detectron expects (H,C,W), BGR, 0-255 as input\n                detectron_color_img = color_img[:, :, ::-1] * 255\n                outputs = predictor(detectron_color_img)\n                with open(segmentation_path, \"wb\") as f:\n                    pickle.dump(outputs, f)\n        else:\n            # detectron expects (H,C,W), BGR, 0-255 as input\n            detectron_color_img = color_img[:, :, ::-1] * 255\n            outputs = predictor(detectron_color_img)\n\n        if timing_dict is not None:\n            torch.cuda.synchronize()\n            timing_dict[\"segmentation\"][-1][1] = time.time()\n\n        category_id = MetadataCatalog.get(cfg.DATASETS.TRAIN[0]).thing_classes.index(\n            config[\"category\"]\n        )\n        matching_instances = []\n        for i in range(len(outputs[\"instances\"])):\n            instance = outputs[\"instances\"][i]\n            if instance.pred_classes != category_id:\n                continue\n            matching_instances.append(instance)\n\n        matching_instances.sort(key=lambda k: k.pred_masks.sum())\n\n        if not matching_instances:\n            print(\"Warning: category not detected in input\")\n        else:\n            print(\"Category detected\")\n\n        for instance in matching_instances:\n            if create_animation:\n                animation_path = os.path.join(\n                    os.getcwd(),\n                    f\"animation_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\",\n                )\n            else:\n                animation_path = None\n\n            if config[\"visualize_input\"]:\n                v = Visualizer(\n                    color_img * 255,\n                    MetadataCatalog.get(cfg.DATASETS.TRAIN[0]),\n                    scale=1.2,\n                )\n                out = v.draw_instance_predictions(instance.to(\"cpu\"))\n                plt.imshow(out.get_image())\n                plt.show()\n                plt.imshow(depth_img)\n                plt.show()\n            depth_tensor = torch.from_numpy(depth_img).to(config[\"device\"])\n            instance_mask = instance.pred_masks.cuda()[0]\n            color_img_tensor = torch.from_numpy(color_img).to(config[\"device\"])\n\n            try:\n                position, orientation, scale, shape = pipeline(\n                    depth_tensor,\n                    instance_mask,\n                    color_img_tensor,\n                    visualize=config[\"visualize_optimization\"],\n                    animation_path=animation_path,\n                    shape_optimization=shape_optimization,\n                )\n            except NoDepthError:\n                print(\"No depth data, skipping\")\n\n            break  # comment to evaluate all instances, instead of largest only\n\n        if timing_dict is not None:\n            torch.cuda.synchronize()\n            timing_dict[\"pipeline\"][-1][1] = time.time()\n\n            if completed_runs != 0 or not config[\"skip_first_run\"]:\n                timing_dicts.append(copy.deepcopy(timing_dict))\n                timing_dicts[-1][\"shape_optimization\"] = shape_optimization\n\n            print(f\"\\r{completed_runs+1}/{config['runs']}\", end=\"\")\n\n        completed_runs += 1\n\n        # only run single evaluation for single file\n        if \"input\" in config and not config[\"measure_runtime\"]:\n            break\n        elif config[\"measure_runtime\"] and completed_runs == config[\"runs\"]:\n            if shape_optimization:\n                shape_optimization = False\n                completed_runs = 0\n                print(\"\")\n            else:\n                print(\"\")\n                break\n\n    if config[\"measure_runtime\"]:\n        generate_runtime_overview(config, timing_dicts)\n</code></pre>"},{"location":"reference/estimation/scripts/real_data/#sdfest.estimation.scripts.real_data.str_to_bool","title":"<code>str_to_bool(v)</code>","text":"<p>Try to convert string to boolean.</p> <p>From: https://stackoverflow.com/a/43357954</p> Source code in <code>sdfest/estimation/scripts/real_data.py</code> <pre><code>def str_to_bool(v: str) -&gt; bool:\n    \"\"\"Try to convert string to boolean.\n\n    From: https://stackoverflow.com/a/43357954\n    \"\"\"\n    if isinstance(v, bool):\n        return v\n    if v.lower() in (\"yes\", \"true\", \"t\", \"y\", \"1\"):\n        return True\n    elif v.lower() in (\"no\", \"false\", \"f\", \"n\", \"0\"):\n        return False\n    else:\n        raise argparse.ArgumentTypeError(\"Boolean value expected.\")\n</code></pre>"},{"location":"reference/estimation/scripts/rendering_evaluation/","title":"rendering_evaluation","text":"<p>Script to run randomized rendering evaluation on synthetic data.</p> Usage <p>python rendering_evaluation.py --config configs/config.yaml --data_path ./data/ --out_folder ./results</p> <p>See configs/rendering_evaluation.yaml for all supported arguments. See simple_setup for pipeline parameters.</p> Specific parameters <p>log_folder:     if passed each optimization step is logged and can be played back with     play_log.py visualize_optimization: whether to visualize optimization while at it visualize_points: whether to show result pointclouds after optimization visualize_meshes: whether to show result mesh after optimization camera_distance: mesh distance from the camera num_views: list of number of views to evaluate for each mesh mesh_scale: the applied scale, see rel_scale rel_scale:     if True, the original mesh will be scaled by mesh_scale, if False the original     mesh will be scaled such that its largest extent is mesh_scale * 2 samples: number of evaluation samples ablation_configs:     used to specify specific settings for ablation study     dictionary, in which each key maps to a config dictionary which will be applied     on existing settings metrics:     dictionary of metrics to evaluate     each key, is interpreted as the name of the metric, each value has to be a dict     containing f and kwargs, where f is fully qualified name of the function to     evaluate and kwargs is a dictionary of keyword arguments if applicable     f gets ground truth points as first, and estimated points as second     parameter seed:     seed used for view generation and sampling of points</p>"},{"location":"reference/estimation/scripts/rendering_evaluation/#sdfest.estimation.scripts.rendering_evaluation.Evaluator","title":"<code>Evaluator</code>","text":"<p>Class to evaluate SDF pipeline on synthetic data.</p> Source code in <code>sdfest/estimation/scripts/rendering_evaluation.py</code> <pre><code>class Evaluator:\n    \"\"\"Class to evaluate SDF pipeline on synthetic data.\"\"\"\n\n    def __init__(self, config: dict) -&gt; None:\n        \"\"\"Construct evaluator and initialize pipeline.\"\"\"\n        self.base_config = config\n        self.cam = Camera(**self.base_config[\"camera\"])\n\n    def run(self) -&gt; None:\n        \"\"\"Run the evaluation.\"\"\"\n        o3d.utility.set_verbosity_level(o3d.utility.VerbosityLevel.Warning)\n        if self.base_config[\"ablation_configs\"]:\n            ablation_results_dict = {}\n            for name, ablation_config in self.base_config[\"ablation_configs\"].items():\n                config = yoco.load_config(\n                    ablation_config, copy.deepcopy(self.base_config)\n                )\n                self._set_seed(config[\"seed\"])\n                ablation_results_dict[name] = self._evaluate_config(config)\n            self._save_and_print_results(ablation_results_dict)\n        else:\n            self._set_seed(self.base_config[\"seed\"])\n            results_dict = self._evaluate_config(self.base_config)\n            self._save_and_print_results(results_dict)\n\n    @staticmethod\n    def _set_seed(seed: int = 0) -&gt; None:\n        random.seed(seed)\n\n    def _evaluate_config(self, config: dict) -&gt; dict:\n        results_dict = {}\n        self.pipeline = SDFPipeline(config)\n        for views in config[\"num_views\"]:\n            metrics_list = []\n            files = glob_exts(config[\"data_path\"], [\".obj\", \".off\"])\n            files.sort()\n            for file in tqdm(files):\n                metrics = self._evaluate_file(file, views, config)\n                metrics_list.append(metrics)\n\n            results_dict[views] = Evaluator._compute_metric_statistics(metrics_list)\n        return results_dict\n\n    def _save_and_print_results(self, results_dict: Dict) -&gt; None:\n        \"\"\"Save results and config to yaml file and print results as table.\n\n        Args:\n            results_dict:\n                dictionary containing the results that should be saved\n        \"\"\"\n        os.makedirs(self.base_config[\"out_folder\"], exist_ok=True)\n        run_name = self.base_config[\"run_name\"]\n        filename = (\n            f\"rend_eval_{run_name}_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.yaml\"\n        )\n        out_path = os.path.join(self.base_config[\"out_folder\"], filename)\n        combined_dict = {**self.base_config, \"results\": results_dict}\n        yoco.save_config_to_file(out_path, combined_dict)\n        print(f\"Results saved to: {out_path}\")\n\n    @staticmethod\n    def _compute_metric_statistics(metrics_list: List) -&gt; Dict:\n        \"\"\"Compute mean and standard deviation for each metric.\n\n        Args:\n            metrics_list: metric dictionaries as returned by _evaluate_file.\n\n        Returns:\n            Statistic for each metric in the provided metrics dictionaries.\n            The returned dictionary keys will be the name of the metrics. Each value\n            will be another dictionary containing the keys mean, var, and std.\n        \"\"\"\n        metric_stats = defaultdict(lambda: {\"mean\": 0, \"var\": 0})\n        for metrics in metrics_list:\n            for name, val in metrics.items():\n                metric_stats[name][\"mean\"] += val\n\n        for _, stats in metric_stats.items():\n            stats[\"mean\"] /= len(metrics_list)\n\n        for metrics in metrics_list:\n            for name, val in metrics.items():\n                metric_stats[name][\"var\"] += (val - metric_stats[name][\"mean\"]) ** 2\n\n        for _, stats in metric_stats.items():\n            stats[\"var\"] /= len(metrics_list)\n            stats[\"std\"] = math.sqrt(stats[\"var\"])\n\n        metric_stats = dict(metric_stats)\n        return metric_stats\n\n    def _generate_views(self, mesh: synthetic.Mesh, num_views: int) -&gt; Dict:\n        \"\"\"Generate random views around mesh.\n\n        Args:\n            mesh:\n                mesh to generate views of,\n                position and orientation will be assumed to be in world coordinates\n            num_views: number of views to generate\n\n        Returns:\n            Dictionary containing the following keys.\n\n            depth_images:\n                the depth map containing the distance along the camera's z-axis,\n                shape (num_views, H, W)\n            masks:\n                binary mask of the object to estimate, same shape as depth_images\n            color_images:\n                color images of objects to estimate, shape (num_views, H, W, 3)\n                note that this is currently just zero\n            camera_positions:\n                position of camera in world coordinates for each image,\n                shape (num_views, 3)\n            camera_orientations:\n                orientation of camera in world-frame as normalized quaternion,\n                quaternion is in scalar-last convention,\n                this is the quaternion that transforms a point from camera to world,\n                shape (num_views, 4)\n        \"\"\"\n        views_dict = defaultdict(lambda: list())\n\n        mesh.position = np.array([0.0, 0.0, 0.0], dtype=np.float32)\n        mesh_position = torch.tensor(mesh.position)\n        mesh_orientation = torch.tensor(mesh.orientation)\n\n        for _ in range(num_views):\n            while True:\n                # OpenGL convention camera\n                camera_orientation = generate_uniform_quaternion()  # ogl to world\n                camera_position = mesh_position - quaternion_utils.quaternion_apply(\n                    camera_orientation,\n                    torch.tensor([0, 0, -self.base_config[\"camera_distance\"]]),\n                )  # transform camera position s.t. object lies on principal axis\n\n                # Transform mesh into camera frame, now with Open3D convention camera\n                camera_orientation_o3d = quaternion_utils.quaternion_multiply(\n                    camera_orientation,  # ogl to world\n                    torch.tensor([1.0, 0, 0, 0]),  # o3d to ogl\n                )  # quaternion: o3d to world\n                mesh_orientation_cam = quaternion_utils.quaternion_multiply(\n                    quaternion_utils.quaternion_invert(\n                        camera_orientation_o3d\n                    ),  # world to o3d\n                    mesh_orientation,  # obj to world\n                )  # quaternion: obj to o3d\n                mesh.position = np.array([0, 0, self.base_config[\"camera_distance\"]])\n                mesh.orientation = mesh_orientation_cam.numpy()\n                depth_np = synthetic.draw_depth_geometry(mesh, self.cam)\n                depth = torch.tensor(depth_np)\n\n                if (depth != 0).any():\n                    views_dict[\"depth_images\"].append(depth)\n                    views_dict[\"masks\"].append(depth != 0)\n                    views_dict[\"color_images\"].append(torch.zeros(depth.shape + (3,)))\n                    views_dict[\"camera_positions\"].append(camera_position)\n                    views_dict[\"camera_orientations\"].append(camera_orientation)\n                    break\n\n                print(\"Warning: invalid depth generated, skipping this sample\")\n\n        mesh.position = mesh_position.numpy()\n        mesh.orientation = mesh_orientation.numpy()\n\n        return {\n            k: torch.stack(v).to(self.base_config[\"device\"])\n            for k, v in views_dict.items()\n        }\n\n    def _evaluate_file(self, path: str, num_views: int, config: dict) -&gt; dict:\n        \"\"\"Evaluate a single mesh.\n\n        This will generate depth images from a few views with the mesh centered, at\n        fixed distance,  to the camera.\n\n        Args:\n            path: The path of the obj file.\n            num_views: The number of views to generate and use in the optimization.\n            config: Configuration dictionary.\n\n        Returns:\n            Evaluation metrics as specified in config.\n        \"\"\"\n        gt_mesh = synthetic.Mesh(\n            path=path,\n            scale=self.base_config[\"mesh_scale\"],\n            rel_scale=self.base_config[\"rel_scale\"],\n            center=True,\n        )\n        inputs = self._generate_views(gt_mesh, num_views)\n\n        log_path = self._get_log_path()\n\n        position, orientation, scale, shape = self.pipeline(\n            **inputs,\n            visualize=self.base_config[\"visualize_optimization\"],\n            log_path=log_path,\n            shape_optimization=config[\"shape_optimization\"],\n        )\n        out_mesh = self.pipeline.generate_mesh(shape, scale, True)\n\n        # Output and ground truth are in world frame\n        out_mesh.position = position[0].detach().cpu().numpy()\n        out_mesh.orientation = orientation[0].detach().cpu().numpy()\n        gt_mesh = gt_mesh.get_transformed_o3d_geometry()\n        out_mesh = out_mesh.get_transformed_o3d_geometry()\n        gt_mesh.paint_uniform_color([0.7, 0.4, 0.2])\n        out_mesh.paint_uniform_color([0.2, 0.4, 0.7])\n        gt_pts = gt_mesh.sample_points_uniformly(\n            number_of_points=self.base_config[\"samples\"], seed=self.base_config[\"seed\"]\n        )\n        out_pts = out_mesh.sample_points_uniformly(\n            number_of_points=self.base_config[\"samples\"], seed=self.base_config[\"seed\"]\n        )\n        gt_pts_np = np.asarray(gt_pts.points)\n        out_pts_np = np.asarray(out_pts.points)\n\n        metric_dict = {}\n        for metric_name, m in self.base_config[\"metrics\"].items():\n            metric_dict[metric_name] = float(\n                locate(m[\"f\"])(gt_pts_np, out_pts_np, **m[\"kwargs\"])\n            )\n\n        self.visualize_result(gt_mesh, out_mesh, gt_pts, out_pts, inputs)\n\n        return metric_dict\n\n    def visualize_result(\n        self,\n        mesh_1: o3d.geometry.TriangleMesh,\n        mesh_2: o3d.geometry.TriangleMesh,\n        pts_1: o3d.geometry.PointCloud,\n        pts_2: o3d.geometry.PointCloud,\n        inputs: Optional[dict] = None,\n    ) -&gt; None:\n        \"\"\"Visualize result of a single evaluation.\n\n        Args:\n            mesh_1: the first mesh to visualize\n            mesh_2: the second mesh to visualize\n            pts_1: the first pointcloud to visualize\n            pts_2: the second pointcloud to visualize\n            inputs: the input dictionary as producted by Evaluator._generate_view\n        \"\"\"\n        # generate coordinate frames of cameras\n        cam_meshes = []\n        if inputs is not None:\n            # visualize OpenGL convention camera\n            for t_c2w, quat_c2w in zip(\n                inputs[\"camera_positions\"], inputs[\"camera_orientations\"]\n            ):\n                frame_mesh = synthetic.Mesh(\n                    mesh=o3d.geometry.TriangleMesh.create_coordinate_frame(\n                        size=0.1, origin=[0, 0, 0]\n                    ),\n                    rel_scale=True,\n                )\n                frame_mesh.position = t_c2w.cpu().numpy()\n                frame_mesh.orientation = quat_c2w.cpu().numpy()\n                cam_meshes.append(frame_mesh.get_transformed_o3d_geometry())\n\n        if self.base_config[\"visualize_meshes\"]:\n            # Visualize result\n            o3d.visualization.draw_geometries(\n                [\n                    mesh_1,\n                    mesh_2,\n                ]\n                + cam_meshes\n            )\n            time.sleep(0.1)\n\n        if self.base_config[\"visualize_points\"]:\n            o3d.visualization.draw_geometries([pts_1, pts_2] + cam_meshes)\n            time.sleep(0.1)\n\n    def _get_log_path(self) -&gt; Optional[str]:\n        \"\"\"Return unique filename in log folder.\n\n        If log path is None, None will be returned.\n        \"\"\"\n        log_path = None\n        if self.base_config[\"log_folder\"] is not None:\n            os.makedirs(self.base_config[\"log_folder\"], exist_ok=True)\n            filename = f\"log_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.pkl\"\n            log_path = os.path.join(self.base_config[\"log_folder\"], filename)\n        return log_path\n</code></pre>"},{"location":"reference/estimation/scripts/rendering_evaluation/#sdfest.estimation.scripts.rendering_evaluation.Evaluator.__init__","title":"<code>__init__(config)</code>","text":"<p>Construct evaluator and initialize pipeline.</p> Source code in <code>sdfest/estimation/scripts/rendering_evaluation.py</code> <pre><code>def __init__(self, config: dict) -&gt; None:\n    \"\"\"Construct evaluator and initialize pipeline.\"\"\"\n    self.base_config = config\n    self.cam = Camera(**self.base_config[\"camera\"])\n</code></pre>"},{"location":"reference/estimation/scripts/rendering_evaluation/#sdfest.estimation.scripts.rendering_evaluation.Evaluator.run","title":"<code>run()</code>","text":"<p>Run the evaluation.</p> Source code in <code>sdfest/estimation/scripts/rendering_evaluation.py</code> <pre><code>def run(self) -&gt; None:\n    \"\"\"Run the evaluation.\"\"\"\n    o3d.utility.set_verbosity_level(o3d.utility.VerbosityLevel.Warning)\n    if self.base_config[\"ablation_configs\"]:\n        ablation_results_dict = {}\n        for name, ablation_config in self.base_config[\"ablation_configs\"].items():\n            config = yoco.load_config(\n                ablation_config, copy.deepcopy(self.base_config)\n            )\n            self._set_seed(config[\"seed\"])\n            ablation_results_dict[name] = self._evaluate_config(config)\n        self._save_and_print_results(ablation_results_dict)\n    else:\n        self._set_seed(self.base_config[\"seed\"])\n        results_dict = self._evaluate_config(self.base_config)\n        self._save_and_print_results(results_dict)\n</code></pre>"},{"location":"reference/estimation/scripts/rendering_evaluation/#sdfest.estimation.scripts.rendering_evaluation.Evaluator.visualize_result","title":"<code>visualize_result(mesh_1, mesh_2, pts_1, pts_2, inputs=None)</code>","text":"<p>Visualize result of a single evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>mesh_1</code> <code>TriangleMesh</code> <p>the first mesh to visualize</p> required <code>mesh_2</code> <code>TriangleMesh</code> <p>the second mesh to visualize</p> required <code>pts_1</code> <code>PointCloud</code> <p>the first pointcloud to visualize</p> required <code>pts_2</code> <code>PointCloud</code> <p>the second pointcloud to visualize</p> required <code>inputs</code> <code>Optional[dict]</code> <p>the input dictionary as producted by Evaluator._generate_view</p> <code>None</code> Source code in <code>sdfest/estimation/scripts/rendering_evaluation.py</code> <pre><code>def visualize_result(\n    self,\n    mesh_1: o3d.geometry.TriangleMesh,\n    mesh_2: o3d.geometry.TriangleMesh,\n    pts_1: o3d.geometry.PointCloud,\n    pts_2: o3d.geometry.PointCloud,\n    inputs: Optional[dict] = None,\n) -&gt; None:\n    \"\"\"Visualize result of a single evaluation.\n\n    Args:\n        mesh_1: the first mesh to visualize\n        mesh_2: the second mesh to visualize\n        pts_1: the first pointcloud to visualize\n        pts_2: the second pointcloud to visualize\n        inputs: the input dictionary as producted by Evaluator._generate_view\n    \"\"\"\n    # generate coordinate frames of cameras\n    cam_meshes = []\n    if inputs is not None:\n        # visualize OpenGL convention camera\n        for t_c2w, quat_c2w in zip(\n            inputs[\"camera_positions\"], inputs[\"camera_orientations\"]\n        ):\n            frame_mesh = synthetic.Mesh(\n                mesh=o3d.geometry.TriangleMesh.create_coordinate_frame(\n                    size=0.1, origin=[0, 0, 0]\n                ),\n                rel_scale=True,\n            )\n            frame_mesh.position = t_c2w.cpu().numpy()\n            frame_mesh.orientation = quat_c2w.cpu().numpy()\n            cam_meshes.append(frame_mesh.get_transformed_o3d_geometry())\n\n    if self.base_config[\"visualize_meshes\"]:\n        # Visualize result\n        o3d.visualization.draw_geometries(\n            [\n                mesh_1,\n                mesh_2,\n            ]\n            + cam_meshes\n        )\n        time.sleep(0.1)\n\n    if self.base_config[\"visualize_points\"]:\n        o3d.visualization.draw_geometries([pts_1, pts_2] + cam_meshes)\n        time.sleep(0.1)\n</code></pre>"},{"location":"reference/estimation/scripts/rendering_evaluation/#sdfest.estimation.scripts.rendering_evaluation.generate_uniform_quaternion","title":"<code>generate_uniform_quaternion()</code>","text":"<p>Generate a normalized uniform quaternion.</p> <p>Following the method from K. Shoemake, Uniform Random Rotations, 1992.</p> <p>See: http://planning.cs.uiuc.edu/node198.html</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Uniformly distributed unit quaternion on the estimator's device.</p> Source code in <code>sdfest/estimation/scripts/rendering_evaluation.py</code> <pre><code>def generate_uniform_quaternion() -&gt; torch.Tensor:\n    \"\"\"Generate a normalized uniform quaternion.\n\n    Following the method from K. Shoemake, Uniform Random Rotations, 1992.\n\n    See: http://planning.cs.uiuc.edu/node198.html\n\n    Returns:\n        Uniformly distributed unit quaternion on the estimator's device.\n    \"\"\"\n    u1, u2, u3 = random.random(), random.random(), random.random()\n    return torch.tensor(\n        [\n            math.sqrt(1 - u1) * math.sin(2 * math.pi * u2),\n            math.sqrt(1 - u1) * math.cos(2 * math.pi * u2),\n            math.sqrt(u1) * math.sin(2 * math.pi * u3),\n            math.sqrt(u1) * math.cos(2 * math.pi * u3),\n        ]\n    )\n</code></pre>"},{"location":"reference/estimation/scripts/rendering_evaluation/#sdfest.estimation.scripts.rendering_evaluation.glob_exts","title":"<code>glob_exts(path, exts)</code>","text":"<p>Return all files in a nested directory with extensions matching.</p> <p>Directory is scanned recursively.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>root path to search</p> required <code>exts</code> <code>List[str]</code> <p>extensions that will be checked, must include separator (e.g., \".obj\")</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of paths in the directory with matching extension.</p> Source code in <code>sdfest/estimation/scripts/rendering_evaluation.py</code> <pre><code>def glob_exts(path: str, exts: List[str]) -&gt; List[str]:\n    \"\"\"Return all files in a nested directory with extensions matching.\n\n    Directory is scanned recursively.\n\n    Args:\n        path: root path to search\n        exts: extensions that will be checked, must include separator (e.g., \".obj\")\n\n    Returns:\n        List of paths in the directory with matching extension.\n    \"\"\"\n    files = []\n    for ext in exts:\n        files.extend(glob.glob(os.path.join(path, f\"**/*{ext}\"), recursive=True))\n\n    return files\n</code></pre>"},{"location":"reference/estimation/scripts/rendering_evaluation/#sdfest.estimation.scripts.rendering_evaluation.main","title":"<code>main()</code>","text":"<p>Entry point of the evaluation program.</p> Source code in <code>sdfest/estimation/scripts/rendering_evaluation.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Entry point of the evaluation program.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"SDF shape and pose estimation evaluation on synthetic data\"\n    )\n    parser.add_argument(\n        \"--config\", default=\"configs/rendering_evaluation.yaml\", nargs=\"+\"\n    )\n    parser.add_argument(\"--data_path\", required=True)\n    parser.add_argument(\"--out_folder\", required=True)\n    config = yoco.load_config_from_args(\n        parser, search_paths=[\".\", \"~/.sdfest/\", sdfest.__path__[0]]\n    )\n\n    evaluator = Evaluator(config)\n    evaluator.run()\n</code></pre>"},{"location":"reference/initialization/pointnet/","title":"pointnet","text":"<p>Parametrized PointNet.</p>"},{"location":"reference/initialization/pointnet/#sdfest.initialization.pointnet.GeneralizedIterativePointNet","title":"<code>GeneralizedIterativePointNet</code>","text":"<p>               Bases: <code>Module</code></p> <p>Generalized Iterative PointNet composed of multiple IterativePointNet instances.</p> <p>This is a sequence of iterative pointnets, where the initial input will be concatenated to each input, e.g.,     out = IterativePointNet1(in)     out = IterativePointNet2(concat(out, in))     out = IterativePointNet3(concat(out, in))     ...</p> Source code in <code>sdfest/initialization/pointnet.py</code> <pre><code>class GeneralizedIterativePointNet(nn.Module):\n    \"\"\"Generalized Iterative PointNet composed of multiple IterativePointNet instances.\n\n    This is a sequence of iterative pointnets, where the initial input will be\n    concatenated to each input, e.g.,\n        out = IterativePointNet1(in)\n        out = IterativePointNet2(concat(out, in))\n        out = IterativePointNet3(concat(out, in))\n        ...\n    \"\"\"\n\n    def __init__(\n        self, list_concat: list, in_size: int, list_mlp_out_sizes: list, batchnorm: bool\n    ) -&gt; None:\n        \"\"\"Initialize GeneralizedIterativePointnet module.\n\n        Args:\n            list_concat:\n                List of concatenations for each MLP.\n            in_size: Dimension of the input points.\n            list_mlp_out_sizes:\n                List of Output sizes of each linear layer.\n                It is a List of Lists.\n            batchnorm: Whether to use batchnorm or not.\n        \"\"\"\n        super().__init__()\n\n        init_in_size = in_size\n        self.iterative_pointnet_list = torch.nn.ModuleList([])\n        temp_iterative_pointnet = IterativePointNet(\n            list_concat[0], in_size, list_mlp_out_sizes[0], batchnorm\n        )\n        self.iterative_pointnet_list.append(temp_iterative_pointnet)\n        for iterative_pointnet_num in range(1, len(list_mlp_out_sizes)):\n            # the input size to new MLP should be the output size of the previous MLP\n            # plus previous input size\n            in_size = list_mlp_out_sizes[iterative_pointnet_num - 1][-1] + init_in_size\n            temp_iterative_pointnet = IterativePointNet(\n                list_concat[iterative_pointnet_num],\n                in_size,\n                list_mlp_out_sizes[iterative_pointnet_num],\n                batchnorm,\n            )\n            self.iterative_pointnet_list.append(temp_iterative_pointnet)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass.\n\n        Input has dimension NxMxC, where N is the batch size, M the number of points\n        per set, and C the number of channels per point.\n\n        Args:\n            x: batch of point sets\n        \"\"\"\n        set_size = x.shape[1]\n        init_x = x\n        for iterative_pointnet in self.iterative_pointnet_list:\n            out = iterative_pointnet(x)  # shape (batch_size, num_outputs)\n            # repeat output vector across 2nd dimension\n            x = out.unsqueeze(1).repeat(1, set_size, 1)\n            x = torch.cat((x, init_x), 2)\n        return out\n</code></pre>"},{"location":"reference/initialization/pointnet/#sdfest.initialization.pointnet.GeneralizedIterativePointNet.__init__","title":"<code>__init__(list_concat, in_size, list_mlp_out_sizes, batchnorm)</code>","text":"<p>Initialize GeneralizedIterativePointnet module.</p> <p>Parameters:</p> Name Type Description Default <code>list_concat</code> <code>list</code> <p>List of concatenations for each MLP.</p> required <code>in_size</code> <code>int</code> <p>Dimension of the input points.</p> required <code>list_mlp_out_sizes</code> <code>list</code> <p>List of Output sizes of each linear layer. It is a List of Lists.</p> required <code>batchnorm</code> <code>bool</code> <p>Whether to use batchnorm or not.</p> required Source code in <code>sdfest/initialization/pointnet.py</code> <pre><code>def __init__(\n    self, list_concat: list, in_size: int, list_mlp_out_sizes: list, batchnorm: bool\n) -&gt; None:\n    \"\"\"Initialize GeneralizedIterativePointnet module.\n\n    Args:\n        list_concat:\n            List of concatenations for each MLP.\n        in_size: Dimension of the input points.\n        list_mlp_out_sizes:\n            List of Output sizes of each linear layer.\n            It is a List of Lists.\n        batchnorm: Whether to use batchnorm or not.\n    \"\"\"\n    super().__init__()\n\n    init_in_size = in_size\n    self.iterative_pointnet_list = torch.nn.ModuleList([])\n    temp_iterative_pointnet = IterativePointNet(\n        list_concat[0], in_size, list_mlp_out_sizes[0], batchnorm\n    )\n    self.iterative_pointnet_list.append(temp_iterative_pointnet)\n    for iterative_pointnet_num in range(1, len(list_mlp_out_sizes)):\n        # the input size to new MLP should be the output size of the previous MLP\n        # plus previous input size\n        in_size = list_mlp_out_sizes[iterative_pointnet_num - 1][-1] + init_in_size\n        temp_iterative_pointnet = IterativePointNet(\n            list_concat[iterative_pointnet_num],\n            in_size,\n            list_mlp_out_sizes[iterative_pointnet_num],\n            batchnorm,\n        )\n        self.iterative_pointnet_list.append(temp_iterative_pointnet)\n</code></pre>"},{"location":"reference/initialization/pointnet/#sdfest.initialization.pointnet.GeneralizedIterativePointNet.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass.</p> <p>Input has dimension NxMxC, where N is the batch size, M the number of points per set, and C the number of channels per point.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>batch of point sets</p> required Source code in <code>sdfest/initialization/pointnet.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass.\n\n    Input has dimension NxMxC, where N is the batch size, M the number of points\n    per set, and C the number of channels per point.\n\n    Args:\n        x: batch of point sets\n    \"\"\"\n    set_size = x.shape[1]\n    init_x = x\n    for iterative_pointnet in self.iterative_pointnet_list:\n        out = iterative_pointnet(x)  # shape (batch_size, num_outputs)\n        # repeat output vector across 2nd dimension\n        x = out.unsqueeze(1).repeat(1, set_size, 1)\n        x = torch.cat((x, init_x), 2)\n    return out\n</code></pre>"},{"location":"reference/initialization/pointnet/#sdfest.initialization.pointnet.IterativePointNet","title":"<code>IterativePointNet</code>","text":"<p>               Bases: <code>Module</code></p> <p>Iterative PointNet which concatenates input.</p> <p>This is composed of 2 PointNets, where the first PointNet is applied once, the second PointNet a number of times, i.e.,     out = PointNet1(in)     for i in range(num_concat):         out = PointNet2( concat( out, in ) )</p> Source code in <code>sdfest/initialization/pointnet.py</code> <pre><code>class IterativePointNet(nn.Module):\n    \"\"\"Iterative PointNet which concatenates input.\n\n    This is composed of 2 PointNets, where the first PointNet is applied once, the\n    second PointNet a number of times, i.e.,\n        out = PointNet1(in)\n        for i in range(num_concat):\n            out = PointNet2( concat( out, in ) )\n    \"\"\"\n\n    def __init__(\n        self, num_concat: int, in_size: int, mlp_out_sizes: List, batchnorm: bool\n    ) -&gt; None:\n        \"\"\"Initialize the IterativePointNet module.\n\n        Args:\n            num_concat:\n                Number of concatenations of input and previous iteration.\n                If 0 this module is the same as VanillaPointNet.\n            in_size: Dimension of the input points.\n            mlp_out_sizes: Output sizes of each linear layer.\n            batchnorm: Whether to use batchnorm or not.\n        \"\"\"\n        super().__init__()\n        self.num_concat = num_concat\n        # create 1st pointnet for taking points of channel = in_size\n        self.pointnet_1 = VanillaPointNet(in_size, mlp_out_sizes, batchnorm)\n        # create 2nd pointnet for taking points of channel = size of concatenated vector\n        self.pointnet_2 = VanillaPointNet(\n            in_size + mlp_out_sizes[-1], mlp_out_sizes, batchnorm\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass.\n\n        Input has dimension NxMxC, where N is the batch size, M the number of points\n        per set, and C the number of channels per point.\n\n        Args:\n            x: batch of point sets\n        \"\"\"\n        # apply 1st pointnet to input\n        out = self.pointnet_1(x)  # shape (batch_size, num_outputs)\n        set_size = x.shape[1]\n        for _ in range(self.num_concat):\n            # repeat output vector across 2nd dimension\n            repeated_out = out.unsqueeze(1).repeat(1, set_size, 1)\n            # concatenate input vector and repeated_out\n            modified_x = torch.cat((repeated_out, x), 2)\n            out = self.pointnet_2(modified_x)\n        return out\n</code></pre>"},{"location":"reference/initialization/pointnet/#sdfest.initialization.pointnet.IterativePointNet.__init__","title":"<code>__init__(num_concat, in_size, mlp_out_sizes, batchnorm)</code>","text":"<p>Initialize the IterativePointNet module.</p> <p>Parameters:</p> Name Type Description Default <code>num_concat</code> <code>int</code> <p>Number of concatenations of input and previous iteration. If 0 this module is the same as VanillaPointNet.</p> required <code>in_size</code> <code>int</code> <p>Dimension of the input points.</p> required <code>mlp_out_sizes</code> <code>List</code> <p>Output sizes of each linear layer.</p> required <code>batchnorm</code> <code>bool</code> <p>Whether to use batchnorm or not.</p> required Source code in <code>sdfest/initialization/pointnet.py</code> <pre><code>def __init__(\n    self, num_concat: int, in_size: int, mlp_out_sizes: List, batchnorm: bool\n) -&gt; None:\n    \"\"\"Initialize the IterativePointNet module.\n\n    Args:\n        num_concat:\n            Number of concatenations of input and previous iteration.\n            If 0 this module is the same as VanillaPointNet.\n        in_size: Dimension of the input points.\n        mlp_out_sizes: Output sizes of each linear layer.\n        batchnorm: Whether to use batchnorm or not.\n    \"\"\"\n    super().__init__()\n    self.num_concat = num_concat\n    # create 1st pointnet for taking points of channel = in_size\n    self.pointnet_1 = VanillaPointNet(in_size, mlp_out_sizes, batchnorm)\n    # create 2nd pointnet for taking points of channel = size of concatenated vector\n    self.pointnet_2 = VanillaPointNet(\n        in_size + mlp_out_sizes[-1], mlp_out_sizes, batchnorm\n    )\n</code></pre>"},{"location":"reference/initialization/pointnet/#sdfest.initialization.pointnet.IterativePointNet.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass.</p> <p>Input has dimension NxMxC, where N is the batch size, M the number of points per set, and C the number of channels per point.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>batch of point sets</p> required Source code in <code>sdfest/initialization/pointnet.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass.\n\n    Input has dimension NxMxC, where N is the batch size, M the number of points\n    per set, and C the number of channels per point.\n\n    Args:\n        x: batch of point sets\n    \"\"\"\n    # apply 1st pointnet to input\n    out = self.pointnet_1(x)  # shape (batch_size, num_outputs)\n    set_size = x.shape[1]\n    for _ in range(self.num_concat):\n        # repeat output vector across 2nd dimension\n        repeated_out = out.unsqueeze(1).repeat(1, set_size, 1)\n        # concatenate input vector and repeated_out\n        modified_x = torch.cat((repeated_out, x), 2)\n        out = self.pointnet_2(modified_x)\n    return out\n</code></pre>"},{"location":"reference/initialization/pointnet/#sdfest.initialization.pointnet.VanillaPointNet","title":"<code>VanillaPointNet</code>","text":"<p>               Bases: <code>Module</code></p> <p>Parametrized PointNet without transformation layers (no T-nets).</p> Generally following <p>PointNet Deep Learning on Point Sets for 3D Classification and Segmentation Qi, 2017</p> Source code in <code>sdfest/initialization/pointnet.py</code> <pre><code>class VanillaPointNet(nn.Module):\n    \"\"\"Parametrized PointNet without transformation layers (no T-nets).\n\n    Generally following:\n        PointNet Deep Learning on Point Sets for 3D Classification and Segmentation\n        Qi, 2017\n    \"\"\"\n\n    def __init__(\n        self,\n        in_size: int,\n        mlp_out_sizes: List,\n        batchnorm: bool,\n        residual: bool = False,\n        dense: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize the VanillaPointNet module.\n\n        This module will only implements the MLP + MaxPooling part of the pointnet.\n\n        It still requires a task specific head.\n\n        Args:\n            in_size:        dimension of the input points\n            mlp_out_sizes:  output sizes of each linear layer\n            batchnorm:      whether to use batchnorm or not\n        \"\"\"\n        super().__init__()\n\n        self._in_size = in_size\n        self._mlp_out_sizes = mlp_out_sizes\n        self._batchnorm = batchnorm\n        self._residual = residual\n        self._dense = dense\n\n        # define layers\n        self._linear_layers = torch.nn.ModuleList([])\n        for i, out_size in enumerate(mlp_out_sizes):\n            if i == 0:\n                self._linear_layers.append(nn.Linear(self._in_size, out_size))\n            else:\n                if dense:\n                    self._linear_layers.append(\n                        nn.Linear(2 * mlp_out_sizes[i - 1], out_size)\n                    )\n                else:\n                    self._linear_layers.append(\n                        nn.Linear(mlp_out_sizes[i - 1], out_size)\n                    )\n\n        self._bn_layers = torch.nn.ModuleList([])\n        if self._batchnorm:\n            for out_size in mlp_out_sizes:\n                self._bn_layers.append(nn.BatchNorm1d(out_size))\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass of the module.\n\n        Input has dimension NxMxC, where N is the batch size, M the number of points per\n        set, and C the number of channels per point.\n\n        Args:\n            x: batch of point sets\n        \"\"\"\n        set_size = x.shape[1]\n        out = prev_out = x\n        for i, linear_layer in enumerate(self._linear_layers):\n            out = linear_layer(out)\n            if self._batchnorm:\n                # BN over channels across all points and sets\n                pts_per_set = out.shape[1]\n                out_view = out.view(-1, self._mlp_out_sizes[i])\n                out = self._bn_layers[i](out_view)\n                out = out.view(-1, pts_per_set, self._mlp_out_sizes[i])\n            out = nn.functional.relu(out)\n\n            if self._dense:\n                out_max, _ = torch.max(out, 1, keepdim=True)\n                if i != len(self._linear_layers) - 1:\n                    out = torch.cat((out, out_max.expand(-1, set_size, -1)), dim=2)\n\n            if self._residual:\n                if prev_out.shape == out.shape:\n                    out = prev_out + out\n            prev_out = out\n\n        # Maximum over points in same set\n        out, _ = torch.max(out, 1)\n\n        return out\n</code></pre>"},{"location":"reference/initialization/pointnet/#sdfest.initialization.pointnet.VanillaPointNet.__init__","title":"<code>__init__(in_size, mlp_out_sizes, batchnorm, residual=False, dense=False)</code>","text":"<p>Initialize the VanillaPointNet module.</p> <p>This module will only implements the MLP + MaxPooling part of the pointnet.</p> <p>It still requires a task specific head.</p> <p>Parameters:</p> Name Type Description Default <code>in_size</code> <code>int</code> <p>dimension of the input points</p> required <code>mlp_out_sizes</code> <code>List</code> <p>output sizes of each linear layer</p> required <code>batchnorm</code> <code>bool</code> <p>whether to use batchnorm or not</p> required Source code in <code>sdfest/initialization/pointnet.py</code> <pre><code>def __init__(\n    self,\n    in_size: int,\n    mlp_out_sizes: List,\n    batchnorm: bool,\n    residual: bool = False,\n    dense: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the VanillaPointNet module.\n\n    This module will only implements the MLP + MaxPooling part of the pointnet.\n\n    It still requires a task specific head.\n\n    Args:\n        in_size:        dimension of the input points\n        mlp_out_sizes:  output sizes of each linear layer\n        batchnorm:      whether to use batchnorm or not\n    \"\"\"\n    super().__init__()\n\n    self._in_size = in_size\n    self._mlp_out_sizes = mlp_out_sizes\n    self._batchnorm = batchnorm\n    self._residual = residual\n    self._dense = dense\n\n    # define layers\n    self._linear_layers = torch.nn.ModuleList([])\n    for i, out_size in enumerate(mlp_out_sizes):\n        if i == 0:\n            self._linear_layers.append(nn.Linear(self._in_size, out_size))\n        else:\n            if dense:\n                self._linear_layers.append(\n                    nn.Linear(2 * mlp_out_sizes[i - 1], out_size)\n                )\n            else:\n                self._linear_layers.append(\n                    nn.Linear(mlp_out_sizes[i - 1], out_size)\n                )\n\n    self._bn_layers = torch.nn.ModuleList([])\n    if self._batchnorm:\n        for out_size in mlp_out_sizes:\n            self._bn_layers.append(nn.BatchNorm1d(out_size))\n</code></pre>"},{"location":"reference/initialization/pointnet/#sdfest.initialization.pointnet.VanillaPointNet.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the module.</p> <p>Input has dimension NxMxC, where N is the batch size, M the number of points per set, and C the number of channels per point.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>batch of point sets</p> required Source code in <code>sdfest/initialization/pointnet.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass of the module.\n\n    Input has dimension NxMxC, where N is the batch size, M the number of points per\n    set, and C the number of channels per point.\n\n    Args:\n        x: batch of point sets\n    \"\"\"\n    set_size = x.shape[1]\n    out = prev_out = x\n    for i, linear_layer in enumerate(self._linear_layers):\n        out = linear_layer(out)\n        if self._batchnorm:\n            # BN over channels across all points and sets\n            pts_per_set = out.shape[1]\n            out_view = out.view(-1, self._mlp_out_sizes[i])\n            out = self._bn_layers[i](out_view)\n            out = out.view(-1, pts_per_set, self._mlp_out_sizes[i])\n        out = nn.functional.relu(out)\n\n        if self._dense:\n            out_max, _ = torch.max(out, 1, keepdim=True)\n            if i != len(self._linear_layers) - 1:\n                out = torch.cat((out, out_max.expand(-1, set_size, -1)), dim=2)\n\n        if self._residual:\n            if prev_out.shape == out.shape:\n                out = prev_out + out\n        prev_out = out\n\n    # Maximum over points in same set\n    out, _ = torch.max(out, 1)\n\n    return out\n</code></pre>"},{"location":"reference/initialization/pointset_utils/","title":"pointset_utils","text":"<p>Utility functions to handle pointsets.</p>"},{"location":"reference/initialization/pointset_utils/#sdfest.initialization.pointset_utils.change_orientation_camera_convention","title":"<code>change_orientation_camera_convention(in_orientation_q, in_convention, out_convention)</code>","text":"<p>Change the camera convention for an orientation in a camera frame.</p> <p>Orientation is represented as a quaternion, that rotates points from a coordinate frame A to a camera frame (if those frames had the same origin).</p> <p>Parameters:</p> Name Type Description Default <code>in_orientation_q</code> <code>Tensor</code> <p>Quaternion(s) which transforms from coordinate frame A to in_convention camera frame. Scalar-last convention. Shape (...,4).</p> required <code>in_convention</code> <code>str</code> <p>Camera convention for the in_transform. One of \"opengl\", \"opencv\".</p> required <code>out_convention</code> <code>str</code> <p>Camera convention for the returned transform. One of \"opengl\", \"opencv\".</p> required <p>Returns:     Quaternion(s) which transforms from coordinate frame A to in_convention camera     frame. Scalar-last convention. Same shape as in_orientation_q.</p> Source code in <code>sdfest/initialization/pointset_utils.py</code> <pre><code>def change_orientation_camera_convention(\n    in_orientation_q: torch.Tensor,\n    in_convention: str,\n    out_convention: str,\n) -&gt; tuple:\n    \"\"\"Change the camera convention for an orientation in a camera frame.\n\n    Orientation is represented as a quaternion, that rotates points from a\n    coordinate frame A to a camera frame (if those frames had the same origin).\n\n    Args:\n        in_orientation_q:\n            Quaternion(s) which transforms from coordinate frame A to in_convention\n            camera frame. Scalar-last convention. Shape (...,4).\n        in_convention:\n            Camera convention for the in_transform. One of \"opengl\", \"opencv\".\n        out_convention:\n            Camera convention for the returned transform. One of \"opengl\", \"opencv\".\n    Returns:\n        Quaternion(s) which transforms from coordinate frame A to in_convention camera\n        frame. Scalar-last convention. Same shape as in_orientation_q.\n    \"\"\"\n    # check whether valild convention was provided\n    if in_convention not in [\"opengl\", \"opencv\"]:\n        raise ValueError(f\"In camera convention {in_convention} not supported.\")\n    if out_convention not in [\"opengl\", \"opencv\"]:\n        raise ValueError(f\"Out camera convention {in_convention} not supported.\")\n\n    if in_convention == out_convention:\n        return in_orientation_q\n    else:\n        # rotate 180deg around x direction\n        gl2cv_q = in_orientation_q.new_tensor([1.0, 0, 0, 0])  # == cv2gl\n        return quaternion_utils.quaternion_multiply(gl2cv_q, in_orientation_q)\n</code></pre>"},{"location":"reference/initialization/pointset_utils/#sdfest.initialization.pointset_utils.change_position_camera_convention","title":"<code>change_position_camera_convention(in_position, in_convention, out_convention)</code>","text":"<p>Change the camera convention for a position in a camera frame.</p> <p>Parameters:</p> Name Type Description Default <code>in_position</code> <code>Tensor</code> <p>Position(s) of coordinate frame A in in_convention camera frame. Shape (...,3).</p> required <code>in_convention</code> <code>str</code> <p>Camera convention for the in_position. One of \"opengl\", \"opencv\".</p> required <code>out_convention</code> <code>str</code> <p>Camera convention for the returned transform. One of \"opengl\", \"opencv\".</p> required <p>Returns:     Position(s) of coordinate frame A in out_convention camera frame. Shape (...,3).</p> Source code in <code>sdfest/initialization/pointset_utils.py</code> <pre><code>def change_position_camera_convention(\n    in_position: torch.Tensor,\n    in_convention: str,\n    out_convention: str,\n) -&gt; tuple:\n    \"\"\"Change the camera convention for a position in a camera frame.\n\n    Args:\n        in_position:\n            Position(s) of coordinate frame A in in_convention camera frame.\n            Shape (...,3).\n        in_convention:\n            Camera convention for the in_position. One of \"opengl\", \"opencv\".\n        out_convention:\n            Camera convention for the returned transform. One of \"opengl\", \"opencv\".\n    Returns:\n        Position(s) of coordinate frame A in out_convention camera frame. Shape (...,3).\n    \"\"\"\n    # check whether valild convention was provided\n    if in_convention not in [\"opengl\", \"opencv\"]:\n        raise ValueError(f\"In camera convention {in_convention} not supported.\")\n    if out_convention not in [\"opengl\", \"opencv\"]:\n        raise ValueError(f\"Out camera convention {in_convention} not supported.\")\n\n    if in_convention == out_convention:\n        return in_position\n    else:\n        gl2cv = in_position.new_tensor([1.0, -1.0, -1.0])  # == cv2gl\n        return gl2cv * in_position\n</code></pre>"},{"location":"reference/initialization/pointset_utils/#sdfest.initialization.pointset_utils.change_transform_camera_convention","title":"<code>change_transform_camera_convention(in_transform, in_convention, out_convention)</code>","text":"<p>Change the camera convention for a frame A -&gt; camera frame transform.</p> <p>Parameters:</p> Name Type Description Default <code>in_transform</code> <code>Tensor</code> <p>Transformtion matrix(es) from coordinate frame A to in_convention camera frame.  Shape (...,4,4).</p> required <code>in_convention</code> <code>str</code> <p>Camera convention for the in_transform. One of \"opengl\", \"opencv\".</p> required <code>out_convention</code> <code>str</code> <p>Camera convention for the returned transform. One of \"opengl\", \"opencv\".</p> required <p>Returns:     Transformtion matrix(es) from coordinate frame A to out_convention camera frame.     Same shape as in_transform.</p> Source code in <code>sdfest/initialization/pointset_utils.py</code> <pre><code>def change_transform_camera_convention(\n    in_transform: torch.Tensor, in_convention: str, out_convention: str\n) -&gt; torch.Tensor:\n    \"\"\"Change the camera convention for a frame A -&gt; camera frame transform.\n\n    Args:\n        in_transform:\n            Transformtion matrix(es) from coordinate frame A to in_convention camera\n            frame.  Shape (...,4,4).\n        in_convention:\n            Camera convention for the in_transform. One of \"opengl\", \"opencv\".\n        out_convention:\n            Camera convention for the returned transform. One of \"opengl\", \"opencv\".\n    Returns:\n        Transformtion matrix(es) from coordinate frame A to out_convention camera frame.\n        Same shape as in_transform.\n    \"\"\"\n    # check whether valild convention was provided\n    if in_convention not in [\"opengl\", \"opencv\"]:\n        raise ValueError(f\"In camera convention {in_convention} not supported.\")\n    if out_convention not in [\"opengl\", \"opencv\"]:\n        raise ValueError(f\"Out camera convention {in_convention} not supported.\")\n\n    if in_convention == out_convention:\n        return in_transform\n    else:\n        gl2cv_transform = torch.diag(\n            in_transform.new_tensor([1.0, -1.0, -1.0, 1.0])\n        )  # == cv2gl_transform\n        return gl2cv_transform @ in_transform\n</code></pre>"},{"location":"reference/initialization/pointset_utils/#sdfest.initialization.pointset_utils.depth_to_pointcloud","title":"<code>depth_to_pointcloud(depth_image, camera, normalize=False, mask=None, convention='opengl')</code>","text":"<p>Convert depth image to pointcloud.</p> <p>Parameters:</p> Name Type Description Default <code>depth_image</code> <code>Tensor</code> <p>The depth image to convert to pointcloud, shape (H,W).</p> required <code>camera</code> <code>Camera</code> <p>The camera used to lift the points.</p> required <code>normalize</code> <code>bool</code> <p>Whether to normalize the pointcloud with 0 centroid.</p> <code>False</code> <code>mask</code> <code>Optional[Tensor]</code> <p>Only points with mask != 0 will be added to pointcloud. No masking will be performed if None.</p> <code>None</code> <code>convention</code> <code>str</code> <p>The camera frame convention to use. One of:     \"opengl\": x right, y up, z back     \"opencv\": x right, y down, z forward</p> <code>'opengl'</code> <p>Returns:     The pointcloud in the camera frame, in OpenGL convention, shape (N,3).</p> Source code in <code>sdfest/initialization/pointset_utils.py</code> <pre><code>def depth_to_pointcloud(\n    depth_image: torch.Tensor,\n    camera: Camera,\n    normalize: bool = False,\n    mask: Optional[torch.Tensor] = None,\n    convention: str = \"opengl\",\n) -&gt; torch.Tensor:\n    \"\"\"Convert depth image to pointcloud.\n\n    Args:\n        depth_image: The depth image to convert to pointcloud, shape (H,W).\n        camera: The camera used to lift the points.\n        normalize: Whether to normalize the pointcloud with 0 centroid.\n        mask:\n            Only points with mask != 0 will be added to pointcloud.\n            No masking will be performed if None.\n        convention:\n            The camera frame convention to use. One of:\n                \"opengl\": x right, y up, z back\n                \"opencv\": x right, y down, z forward\n    Returns:\n        The pointcloud in the camera frame, in OpenGL convention, shape (N,3).\n    \"\"\"\n    fx, fy, cx, cy, _ = camera.get_pinhole_camera_parameters(0.0)\n\n    if mask is None:\n        indices = torch.nonzero(depth_image, as_tuple=True)\n    else:\n        indices = torch.nonzero(depth_image * mask, as_tuple=True)\n    depth_values = depth_image[indices]\n    points = torch.cat(\n        (\n            indices[1][:, None].float(),\n            indices[0][:, None].float(),\n            depth_values[:, None],\n        ),\n        dim=1,\n    )\n\n    if convention == \"opengl\":\n        final_points = torch.empty_like(points)\n        final_points[:, 0] = (points[:, 0] - cx) * points[:, 2] / fx\n        final_points[:, 1] = -(points[:, 1] - cy) * points[:, 2] / fy\n        final_points[:, 2] = -points[:, 2]\n    elif convention == \"opencv\":\n        final_points = torch.empty_like(points)\n        final_points[:, 0] = (points[:, 0] - cx) * points[:, 2] / fx\n        final_points[:, 1] = (points[:, 1] - cy) * points[:, 2] / fy\n        final_points[:, 2] = points[:, 2]\n    else:\n        raise ValueError(f\"Unsupported camera convention {convention}.\")\n\n    if normalize:\n        final_points, _ = normalize_points(final_points)\n\n    return final_points\n</code></pre>"},{"location":"reference/initialization/pointset_utils/#sdfest.initialization.pointset_utils.normalize_points","title":"<code>normalize_points(points)</code>","text":"<p>Normalize pointset to have zero mean.</p> <p>Normalization will be performed along second last dimension.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>Tensor</code> <p>The pointsets which will be normalized, shape (N, M, D) or shape (M, D), N pointsets with M points of dimension D.</p> required Return <p>normalized_points:     The normalized pointset, same shape as points. centroids:     The means of the pointclouds used to normalize points.     Shape (N, D) or (D,), for (N, M, D) and (M, D) inputs, respectively.</p> Source code in <code>sdfest/initialization/pointset_utils.py</code> <pre><code>def normalize_points(points: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Normalize pointset to have zero mean.\n\n    Normalization will be performed along second last dimension.\n\n    Args:\n        points:\n            The pointsets which will be normalized,\n            shape (N, M, D) or shape (M, D), N pointsets with M points of dimension D.\n\n    Return:\n        normalized_points:\n            The normalized pointset, same shape as points.\n        centroids:\n            The means of the pointclouds used to normalize points.\n            Shape (N, D) or (D,), for (N, M, D) and (M, D) inputs, respectively.\n    \"\"\"\n    centroids = torch.mean(points, dim=-2, keepdim=True)\n    normalized_points = points - centroids\n    return normalized_points, centroids.squeeze()\n</code></pre>"},{"location":"reference/initialization/pointset_utils/#sdfest.initialization.pointset_utils.visualize_pointset","title":"<code>visualize_pointset(pointset, max_points=1000)</code>","text":"<p>Visualize pointset as 3D scatter plot.</p> <p>Parameters:</p> Name Type Description Default <code>pointset</code> <code>Tensor</code> <p>The pointset to visualize. Either shape (N,3), xyz, or shape (N,6), xyzrgb.</p> required <code>max_points</code> <code>int</code> <p>Maximum number of points. If N&gt;max_points only a random subset will be shown.</p> <code>1000</code> Source code in <code>sdfest/initialization/pointset_utils.py</code> <pre><code>def visualize_pointset(pointset: torch.Tensor, max_points: int = 1000) -&gt; None:\n    \"\"\"Visualize pointset as 3D scatter plot.\n\n    Args:\n        pointset:\n            The pointset to visualize. Either shape (N,3), xyz, or shape (N,6), xyzrgb.\n        max_points:\n            Maximum number of points.\n            If N&gt;max_points only a random subset will be shown.\n    \"\"\"\n    pointset_np = pointset.cpu().detach().numpy()\n    fig = plt.figure()\n    ax = fig.add_subplot(projection=\"3d\")\n    ax.set_box_aspect((1, 1, 1))\n\n    if len(pointset_np) &gt; max_points:\n        indices = np.random.choice(len(pointset_np), replace=False, size=max_points)\n        pointset_np = pointset_np[indices]\n\n    if pointset_np.shape[1] == 6:\n        colors = pointset_np[:, 3:]\n    else:\n        colors = None\n\n    ax.scatter(pointset_np[:, 0], pointset_np[:, 1], pointset_np[:, 2], c=colors)\n\n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"y\")\n    ax.set_zlabel(\"z\")\n\n    utils.set_axes_equal(ax)\n    plt.show()\n</code></pre>"},{"location":"reference/initialization/quaternion_utils/","title":"quaternion_utils","text":"<p>Functions to handle transformations with quaternions.</p> <p>Inspired by PyTorch3D, but using scalar-last convention and not enforcing scalar &gt; 0. https://github.com/facebookresearch/pytorch3d</p>"},{"location":"reference/initialization/quaternion_utils/#sdfest.initialization.quaternion_utils.generate_uniform_quaternion","title":"<code>generate_uniform_quaternion()</code>","text":"<p>Generate a normalized uniform quaternion.</p> <p>Following the method from K. Shoemake, Uniform Random Rotations, 1992.</p> <p>See: http://planning.cs.uiuc.edu/node198.html</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Uniformly distributed unit quaternion on the estimator's device.</p> Source code in <code>sdfest/initialization/quaternion_utils.py</code> <pre><code>def generate_uniform_quaternion() -&gt; torch.Tensor:\n    \"\"\"Generate a normalized uniform quaternion.\n\n    Following the method from K. Shoemake, Uniform Random Rotations, 1992.\n\n    See: http://planning.cs.uiuc.edu/node198.html\n\n    Returns:\n        Uniformly distributed unit quaternion on the estimator's device.\n    \"\"\"\n    u1, u2, u3 = random.random(), random.random(), random.random()\n    return torch.tensor(\n        [\n            math.sqrt(1 - u1) * math.sin(2 * math.pi * u2),\n            math.sqrt(1 - u1) * math.cos(2 * math.pi * u2),\n            math.sqrt(u1) * math.sin(2 * math.pi * u3),\n            math.sqrt(u1) * math.cos(2 * math.pi * u3),\n        ]\n    )\n</code></pre>"},{"location":"reference/initialization/quaternion_utils/#sdfest.initialization.quaternion_utils.geodesic_distance","title":"<code>geodesic_distance(q1, q2)</code>","text":"<p>Compute geodesic distances between quaternions.</p> <p>Parameters:</p> Name Type Description Default <code>q1</code> <code>Tensor</code> <p>First set of quaterions, shape (N,4).</p> required <code>q2</code> <code>Tensor</code> <p>Second set of quaternions, shape (N,4).</p> required <p>Returns:     Mean distance between the quaternions, scalar.</p> Source code in <code>sdfest/initialization/quaternion_utils.py</code> <pre><code>def geodesic_distance(q1: torch.Tensor, q2: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute geodesic distances between quaternions.\n\n    Args:\n        q1: First set of quaterions, shape (N,4).\n        q2: Second set of quaternions, shape (N,4).\n    Returns:\n        Mean distance between the quaternions, scalar.\n    \"\"\"\n    abs_q1q2 = torch.clip(torch.abs(torch.sum(q1 * q2, dim=1)), 0, 1)\n    geodesic_distances = 2 * torch.acos(abs_q1q2)\n    return geodesic_distances\n</code></pre>"},{"location":"reference/initialization/quaternion_utils/#sdfest.initialization.quaternion_utils.quaternion_apply","title":"<code>quaternion_apply(quaternions, points)</code>","text":"<p>Rotate points by quaternions representing rotations.</p> <p>Normal broadcasting rules apply.</p> <p>Parameters:</p> Name Type Description Default <code>quaternions</code> <code>Tensor</code> <p>normalized quaternions of shape (..., 4), scalar-last convention</p> required <code>points</code> <code>Tensor</code> <p>points of shape (..., 3)</p> required <p>Returns:     Points rotated by the rotations representing quaternions.</p> Source code in <code>sdfest/initialization/quaternion_utils.py</code> <pre><code>def quaternion_apply(quaternions: torch.Tensor, points: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Rotate points by quaternions representing rotations.\n\n    Normal broadcasting rules apply.\n\n    Args:\n        quaternions:\n            normalized quaternions of shape (..., 4), scalar-last convention\n        points:\n            points of shape (..., 3)\n    Returns:\n        Points rotated by the rotations representing quaternions.\n    \"\"\"\n    points_as_quaternions = points.new_zeros(points.shape[:-1] + (4,))\n    points_as_quaternions[..., :-1] = points\n    return quaternion_multiply(\n        quaternion_multiply(quaternions, points_as_quaternions),\n        quaternion_invert(quaternions),\n    )[..., :-1]\n</code></pre>"},{"location":"reference/initialization/quaternion_utils/#sdfest.initialization.quaternion_utils.quaternion_invert","title":"<code>quaternion_invert(quaternions)</code>","text":"<p>Invert quaternions representing orientations.</p> <p>Parameters:</p> Name Type Description Default <code>quaternions</code> <code>Tensor</code> <p>The quaternions to invert, shape (..., 4), scalar-last convention.</p> required <p>Returns:     Inverted quaternions, same shape as quaternions.</p> Source code in <code>sdfest/initialization/quaternion_utils.py</code> <pre><code>def quaternion_invert(quaternions: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Invert quaternions representing orientations.\n\n    Args:\n        quaternions:\n            The quaternions to invert, shape (..., 4), scalar-last convention.\n    Returns:\n        Inverted quaternions, same shape as quaternions.\n    \"\"\"\n    return quaternions * quaternions.new_tensor([-1, -1, -1, 1])\n</code></pre>"},{"location":"reference/initialization/quaternion_utils/#sdfest.initialization.quaternion_utils.quaternion_multiply","title":"<code>quaternion_multiply(quaternions_1, quaternions_2)</code>","text":"<p>Multiply two quaternions representing rotations.</p> <p>Normal broadcasting rules apply.</p> <p>Parameters:</p> Name Type Description Default <code>quaternions_1</code> <code>Tensor</code> <p>normalized quaternions of shape (..., 4), scalar-last convention</p> required <code>quaternions_2</code> <code>Tensor</code> <p>normalized quaternions of shape (..., 4), scalar-last convention</p> required <p>Returns:     Composition of passed quaternions.</p> Source code in <code>sdfest/initialization/quaternion_utils.py</code> <pre><code>def quaternion_multiply(\n    quaternions_1: torch.Tensor, quaternions_2: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"Multiply two quaternions representing rotations.\n\n    Normal broadcasting rules apply.\n\n    Args:\n        quaternions_1:\n            normalized quaternions of shape (..., 4), scalar-last convention\n        quaternions_2:\n            normalized quaternions of shape (..., 4), scalar-last convention\n    Returns:\n        Composition of passed quaternions.\n    \"\"\"\n    ax, ay, az, aw = torch.unbind(quaternions_1, -1)\n    bx, by, bz, bw = torch.unbind(quaternions_2, -1)\n    ox = aw * bx + ax * bw + ay * bz - az * by\n    oy = aw * by - ax * bz + ay * bw + az * bx\n    oz = aw * bz + ax * by - ay * bx + az * bw\n    ow = aw * bw - ax * bx - ay * by - az * bz\n    return torch.stack((ox, oy, oz, ow), -1)\n</code></pre>"},{"location":"reference/initialization/quaternion_utils/#sdfest.initialization.quaternion_utils.simple_quaternion_loss","title":"<code>simple_quaternion_loss(q1, q2)</code>","text":"<p>Compute distance measure between quaternions not involving trig functions.</p> From <p>https://math.stackexchange.com/a/90098</p> <p>Parameters:</p> Name Type Description Default <code>q1</code> <code>Tensor</code> <p>First set of quaterions, shape (N,4).</p> required <code>q2</code> <code>Tensor</code> <p>Second set of quaternions, shape (N,4).</p> required <p>Returns:     Mean distance between the quaternions, scalar.</p> Source code in <code>sdfest/initialization/quaternion_utils.py</code> <pre><code>def simple_quaternion_loss(q1: torch.Tensor, q2: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute distance measure between quaternions not involving trig functions.\n\n    From:\n        https://math.stackexchange.com/a/90098\n\n    Args:\n        q1: First set of quaterions, shape (N,4).\n        q2: Second set of quaternions, shape (N,4).\n    Returns:\n        Mean distance between the quaternions, scalar.\n    \"\"\"\n    return torch.mean(1 - torch.sum(q1 * q2, 1) ** 2)\n</code></pre>"},{"location":"reference/initialization/sdf_pose_network/","title":"sdf_pose_network","text":"<p>Parametrized networks for pose and shape estimation.</p>"},{"location":"reference/initialization/sdf_pose_network/#sdfest.initialization.sdf_pose_network.SDFPoseHead","title":"<code>SDFPoseHead</code>","text":"<p>               Bases: <code>Module</code></p> <p>Parametrized head to estimate pose and shape from feature vector.</p> Source code in <code>sdfest/initialization/sdf_pose_network.py</code> <pre><code>class SDFPoseHead(nn.Module):\n    \"\"\"Parametrized head to estimate pose and shape from feature vector.\"\"\"\n\n    def __init__(\n        self,\n        in_size: int,\n        mlp_out_sizes: List,\n        shape_dimension: int,\n        batchnorm: bool,\n        orientation_repr: Optional[str] = \"quaternion\",\n        orientation_grid_resolution: Optional[int] = None,\n    ):\n        \"\"\"Initialize the SDFPoseHead.\n\n        Args:\n            in_size:            number of input features\n            mlp_out_sizes:      output sizes of each linear layer\n            shape_dimension:    dimension of shape description\n            batchnorm:          whether to use batchnorm or not\n            orientation_repr:\n                The orientation represention. One of \"quaternion\"|\"discretized\".\n            orientation_grid_resolution:\n                The resolution of the SO3 grid.\n                Only used when orientation_repr == \"discretized\".\n        \"\"\"\n        super().__init__()\n\n        self._in_size = in_size\n        self._mlp_out_sizes = mlp_out_sizes\n        self._batchnorm = batchnorm\n        self._shape_dimension = shape_dimension\n        self._orientation_repr = orientation_repr\n\n        # define layers\n        self._linear_layers = torch.nn.ModuleList([])\n        for i, out_size in enumerate(mlp_out_sizes):\n            if i == 0:\n                self._linear_layers.append(nn.Linear(self._in_size, out_size))\n            else:\n                self._linear_layers.append(nn.Linear(mlp_out_sizes[i - 1], out_size))\n\n        self._bn_layers = torch.nn.ModuleList([])\n        if self._batchnorm:\n            for out_size in mlp_out_sizes:\n                self._bn_layers.append(nn.BatchNorm1d(out_size))\n\n        if orientation_repr == \"quaternion\":\n            self._grid = None\n            self._final_layer = nn.Linear(mlp_out_sizes[-1], self._shape_dimension + 8)\n        elif orientation_repr == \"discretized\":\n            self._grid = SO3Grid(orientation_grid_resolution)\n            self._final_layer = nn.Linear(\n                mlp_out_sizes[-1], self._shape_dimension + 4 + self._grid.num_cells()\n            )\n        else:\n            raise NotImplementedError(\n                f\"orientation_repr {orientation_repr} is not supported.\"\n            )\n\n    def forward(self, x):\n        \"\"\"Forward pass of the module.\n\n        Input represents set of input features used to compute pose.\n\n        Args:\n            x: batch of input vectors\n        Returns:\n            Tuple with the following entries:\n                The predicted shape vector.\n                The predicted pose.\n                The predicted scale.\n                The predicted orientation in the specified orientation representation.\n                    For \"quaternion\" this will be of shape (N,4) with each quaternion\n                    having the order (x, y, z, w), i.e., scalar-last, and normalized.\n                    For \"discretized\" this will be of shape (N,M) based on the grid\n                    resolution. No activation function is applied. I.e., softmax has\n                    to be used to get probabilities, and cross_entropy_loss should be\n                    used during training.\n        \"\"\"\n        out = x\n        for i, linear_layer in enumerate(self._linear_layers):\n            out = linear_layer(out)\n            if self._batchnorm:\n                out = self._bn_layers[i](out)\n            out = nn.functional.relu(out)\n\n        # Normalize quaternion\n        if self._orientation_repr == \"quaternion\":\n            out = self._final_layer(out)\n            orientation = out[:, self._shape_dimension + 4 :]\n            orientation = orientation / torch.sqrt(\n                torch.sum(orientation ** 2, 1, keepdim=True)\n            )\n        elif self._orientation_repr == \"discretized\":\n            out = self._final_layer(out)\n            orientation = out[:, self._shape_dimension + 4 :]\n        else:\n            raise NotImplementedError(\n                f\"orientation_repr {self.orientation_repr} is not supported.\"\n            )\n\n        return (\n            out[:, 0 : self._shape_dimension],\n            out[:, self._shape_dimension : self._shape_dimension + 3],\n            out[:, self._shape_dimension + 3],\n            orientation,\n        )\n</code></pre>"},{"location":"reference/initialization/sdf_pose_network/#sdfest.initialization.sdf_pose_network.SDFPoseHead.__init__","title":"<code>__init__(in_size, mlp_out_sizes, shape_dimension, batchnorm, orientation_repr='quaternion', orientation_grid_resolution=None)</code>","text":"<p>Initialize the SDFPoseHead.</p> <p>Parameters:</p> Name Type Description Default <code>in_size</code> <code>int</code> <p>number of input features</p> required <code>mlp_out_sizes</code> <code>List</code> <p>output sizes of each linear layer</p> required <code>shape_dimension</code> <code>int</code> <p>dimension of shape description</p> required <code>batchnorm</code> <code>bool</code> <p>whether to use batchnorm or not</p> required <code>orientation_repr</code> <code>Optional[str]</code> <p>The orientation represention. One of \"quaternion\"|\"discretized\".</p> <code>'quaternion'</code> <code>orientation_grid_resolution</code> <code>Optional[int]</code> <p>The resolution of the SO3 grid. Only used when orientation_repr == \"discretized\".</p> <code>None</code> Source code in <code>sdfest/initialization/sdf_pose_network.py</code> <pre><code>def __init__(\n    self,\n    in_size: int,\n    mlp_out_sizes: List,\n    shape_dimension: int,\n    batchnorm: bool,\n    orientation_repr: Optional[str] = \"quaternion\",\n    orientation_grid_resolution: Optional[int] = None,\n):\n    \"\"\"Initialize the SDFPoseHead.\n\n    Args:\n        in_size:            number of input features\n        mlp_out_sizes:      output sizes of each linear layer\n        shape_dimension:    dimension of shape description\n        batchnorm:          whether to use batchnorm or not\n        orientation_repr:\n            The orientation represention. One of \"quaternion\"|\"discretized\".\n        orientation_grid_resolution:\n            The resolution of the SO3 grid.\n            Only used when orientation_repr == \"discretized\".\n    \"\"\"\n    super().__init__()\n\n    self._in_size = in_size\n    self._mlp_out_sizes = mlp_out_sizes\n    self._batchnorm = batchnorm\n    self._shape_dimension = shape_dimension\n    self._orientation_repr = orientation_repr\n\n    # define layers\n    self._linear_layers = torch.nn.ModuleList([])\n    for i, out_size in enumerate(mlp_out_sizes):\n        if i == 0:\n            self._linear_layers.append(nn.Linear(self._in_size, out_size))\n        else:\n            self._linear_layers.append(nn.Linear(mlp_out_sizes[i - 1], out_size))\n\n    self._bn_layers = torch.nn.ModuleList([])\n    if self._batchnorm:\n        for out_size in mlp_out_sizes:\n            self._bn_layers.append(nn.BatchNorm1d(out_size))\n\n    if orientation_repr == \"quaternion\":\n        self._grid = None\n        self._final_layer = nn.Linear(mlp_out_sizes[-1], self._shape_dimension + 8)\n    elif orientation_repr == \"discretized\":\n        self._grid = SO3Grid(orientation_grid_resolution)\n        self._final_layer = nn.Linear(\n            mlp_out_sizes[-1], self._shape_dimension + 4 + self._grid.num_cells()\n        )\n    else:\n        raise NotImplementedError(\n            f\"orientation_repr {orientation_repr} is not supported.\"\n        )\n</code></pre>"},{"location":"reference/initialization/sdf_pose_network/#sdfest.initialization.sdf_pose_network.SDFPoseHead.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the module.</p> <p>Input represents set of input features used to compute pose.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p>batch of input vectors</p> required <p>Returns:     Tuple with the following entries:         The predicted shape vector.         The predicted pose.         The predicted scale.         The predicted orientation in the specified orientation representation.             For \"quaternion\" this will be of shape (N,4) with each quaternion             having the order (x, y, z, w), i.e., scalar-last, and normalized.             For \"discretized\" this will be of shape (N,M) based on the grid             resolution. No activation function is applied. I.e., softmax has             to be used to get probabilities, and cross_entropy_loss should be             used during training.</p> Source code in <code>sdfest/initialization/sdf_pose_network.py</code> <pre><code>def forward(self, x):\n    \"\"\"Forward pass of the module.\n\n    Input represents set of input features used to compute pose.\n\n    Args:\n        x: batch of input vectors\n    Returns:\n        Tuple with the following entries:\n            The predicted shape vector.\n            The predicted pose.\n            The predicted scale.\n            The predicted orientation in the specified orientation representation.\n                For \"quaternion\" this will be of shape (N,4) with each quaternion\n                having the order (x, y, z, w), i.e., scalar-last, and normalized.\n                For \"discretized\" this will be of shape (N,M) based on the grid\n                resolution. No activation function is applied. I.e., softmax has\n                to be used to get probabilities, and cross_entropy_loss should be\n                used during training.\n    \"\"\"\n    out = x\n    for i, linear_layer in enumerate(self._linear_layers):\n        out = linear_layer(out)\n        if self._batchnorm:\n            out = self._bn_layers[i](out)\n        out = nn.functional.relu(out)\n\n    # Normalize quaternion\n    if self._orientation_repr == \"quaternion\":\n        out = self._final_layer(out)\n        orientation = out[:, self._shape_dimension + 4 :]\n        orientation = orientation / torch.sqrt(\n            torch.sum(orientation ** 2, 1, keepdim=True)\n        )\n    elif self._orientation_repr == \"discretized\":\n        out = self._final_layer(out)\n        orientation = out[:, self._shape_dimension + 4 :]\n    else:\n        raise NotImplementedError(\n            f\"orientation_repr {self.orientation_repr} is not supported.\"\n        )\n\n    return (\n        out[:, 0 : self._shape_dimension],\n        out[:, self._shape_dimension : self._shape_dimension + 3],\n        out[:, self._shape_dimension + 3],\n        orientation,\n    )\n</code></pre>"},{"location":"reference/initialization/sdf_pose_network/#sdfest.initialization.sdf_pose_network.SDFPoseNet","title":"<code>SDFPoseNet</code>","text":"<p>               Bases: <code>Module</code></p> <p>Pose and shape estimation from sensor data.</p> <p>Composed of feature extraction backbone and shape/pose head.</p> Source code in <code>sdfest/initialization/sdf_pose_network.py</code> <pre><code>class SDFPoseNet(nn.Module):\n    \"\"\"Pose and shape estimation from sensor data.\n\n    Composed of feature extraction backbone and shape/pose head.\n    \"\"\"\n\n    def __init__(self, backbone: nn.Module, head: nn.Module):\n        \"\"\"Construct SDF pose and shape network.\n\n        Args:\n            backbone:       function or class representing the backbone\n            backbone_dict:  parameters passed to backbone on construction\n            head:           function or class representing the head\n            head_dict:      parameters passed to head on construction\n        \"\"\"\n        super().__init__()\n        self._backbone = backbone\n        self._head = head\n\n    def forward(self, x):\n        \"\"\"Forward pass.\n\n        Args:\n            x: input compatible with backbone.\n        Returns:\n            output from head\n        \"\"\"\n        out = self._backbone(x)\n        out = self._head(out)\n        return out\n</code></pre>"},{"location":"reference/initialization/sdf_pose_network/#sdfest.initialization.sdf_pose_network.SDFPoseNet.__init__","title":"<code>__init__(backbone, head)</code>","text":"<p>Construct SDF pose and shape network.</p> <p>Parameters:</p> Name Type Description Default <code>backbone</code> <code>Module</code> <p>function or class representing the backbone</p> required <code>backbone_dict</code> <p>parameters passed to backbone on construction</p> required <code>head</code> <code>Module</code> <p>function or class representing the head</p> required <code>head_dict</code> <p>parameters passed to head on construction</p> required Source code in <code>sdfest/initialization/sdf_pose_network.py</code> <pre><code>def __init__(self, backbone: nn.Module, head: nn.Module):\n    \"\"\"Construct SDF pose and shape network.\n\n    Args:\n        backbone:       function or class representing the backbone\n        backbone_dict:  parameters passed to backbone on construction\n        head:           function or class representing the head\n        head_dict:      parameters passed to head on construction\n    \"\"\"\n    super().__init__()\n    self._backbone = backbone\n    self._head = head\n</code></pre>"},{"location":"reference/initialization/sdf_pose_network/#sdfest.initialization.sdf_pose_network.SDFPoseNet.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p>input compatible with backbone.</p> required <p>Returns:     output from head</p> Source code in <code>sdfest/initialization/sdf_pose_network.py</code> <pre><code>def forward(self, x):\n    \"\"\"Forward pass.\n\n    Args:\n        x: input compatible with backbone.\n    Returns:\n        output from head\n    \"\"\"\n    out = self._backbone(x)\n    out = self._head(out)\n    return out\n</code></pre>"},{"location":"reference/initialization/sdf_utils/","title":"sdf_utils","text":"<p>Utility functions to handle voxel-based signed distance fields.</p>"},{"location":"reference/initialization/sdf_utils/#sdfest.initialization.sdf_utils.sdf_to_pointcloud","title":"<code>sdf_to_pointcloud(sdf, position, orientation, scale, max_points=None, threshold=0)</code>","text":"<p>Convert SDF to pointcloud.</p> <p>Puts a point onto each cell vertex with a value &lt; threshold.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>array</code> <p>The values of the voxelized SDF. Shape (D, D, D).</p> required <code>position</code> <code>array</code> <p>The position of the SDF center. Shape (3,).</p> required <code>orientation</code> <code>array</code> <p>The orientation of the SDF as a normalized quaternion. This is the quaternion that will be applied to each point. Scalar-last convention, shape (4,).</p> required <code>scale</code> <code>float</code> <p>The half-length of the SDF.</p> required <code>max_points</code> <code>Optional[int]</code> <p>Maximum number of points in the pointcloud.</p> <code>None</code> <code>threshold</code> <code>float</code> <p>The threshold below which a voxel will be included in the pointcloud.</p> <code>0</code> <p>Returns:</p> Type Description <p>The pointcloud as a Nx3 array.</p> Source code in <code>sdfest/initialization/sdf_utils.py</code> <pre><code>def sdf_to_pointcloud(\n    sdf: np.array,\n    position: np.array,\n    orientation: np.array,\n    scale: float,\n    max_points: Optional[int] = None,\n    threshold: float = 0,\n):\n    \"\"\"Convert SDF to pointcloud.\n\n    Puts a point onto each cell vertex with a value &lt; threshold.\n\n    Args:\n        sdf: The values of the voxelized SDF. Shape (D, D, D).\n        position: The position of the SDF center. Shape (3,).\n        orientation:\n            The orientation of the SDF as a normalized quaternion.\n            This is the quaternion that will be applied to each point.\n            Scalar-last convention, shape (4,).\n        scale: The half-length of the SDF.\n        max_points: Maximum number of points in the pointcloud.\n        threshold: The threshold below which a voxel will be included in the pointcloud.\n\n    Returns:\n        The pointcloud as a Nx3 array.\n    \"\"\"\n    grid_size = 2.0 / (sdf.shape[0] - 1.0)\n    indices = np.argwhere(sdf &lt;= threshold)\n    points = (indices * grid_size - 1.0) * scale\n    rot_matrix = Rotation.from_quat(orientation).as_matrix()\n    points = (rot_matrix @ points.T).T\n    points += position\n    if max_points is not None and max_points &lt; points.shape[0]:\n        points = points[np.random.choice(points.shape[0], 2, replace=False), :]\n    return points\n</code></pre>"},{"location":"reference/initialization/so3grid/","title":"so3grid","text":"<p>This module provides a deterministic low-dispersion grid on SO3.</p>"},{"location":"reference/initialization/so3grid/#sdfest.initialization.so3grid.SO3Grid","title":"<code>SO3Grid</code>","text":"<p>Low-dispersion SO3 grid.</p> <p>This approach was introduced by Generating Uniform Incremental Grids on SO3 Using the Hopf Fibration, Yershova, 2010. We only generate the base grid (i.e., up to and including Section 5.2 of the paper), since we only need a fixed size set.</p> <p>Implementation roughly based on https://github.com/zhonge/cryodrgn</p> Source code in <code>sdfest/initialization/so3grid.py</code> <pre><code>class SO3Grid:\n    \"\"\"Low-dispersion SO3 grid.\n\n    This approach was introduced by Generating Uniform Incremental Grids on SO3 Using\n    the Hopf Fibration, Yershova, 2010. We only generate the base grid (i.e., up to and\n    including Section 5.2 of the paper), since we only need a fixed size set.\n\n    Implementation roughly based on https://github.com/zhonge/cryodrgn\n    \"\"\"\n\n    def __init__(self, resol: int):\n        \"\"\"Construct the SO3 grid.\n\n        Args:\n            resol: The resolution of the grid. Coarsest possible grid for 0.\n        \"\"\"\n        self._resol = resol\n        self._s1 = self._grid_s1(resol)\n        self._s2_theta, self._s2_phi = self._grid_s2(resol)\n\n    def num_cells(self) -&gt; int:\n        \"\"\"Return the number of points in the grid.\"\"\"\n        return len(self._s1) * len(self._s2_theta)\n\n    def hopf_to_index(self, psi, theta, phi):\n        \"\"\"Convert hopf coordinate to index.\n\n        Args:\n            phi: [0, 2pi)\n            theta: [0, pi]\n            psi: [0, 2pi)\n        Returns:\n            Grid index of closest point in grid.\n        \"\"\"\n        s1_index = int(psi // (2 * np.pi / len(self._s1)))\n        s2_index = hp.ang2pix(2 ** self._resol, theta, phi, nest=True)\n        return s1_index * len(self._s2_theta) + s2_index\n\n    def index_to_hopf(self, index: int) -&gt; Tuple[float, float, float]:\n        \"\"\"Convert index to hopf coordinates.\n\n        Psi: [0,2*pi)\n        Theta: [0, pi]\n        Phi: [0, 2*pi)\n\n        Args:\n            index: The index of the grid point.\n        Returns:\n            Tuple of psi, theta, phi.\n        \"\"\"\n        s1_index = index // len(self._s2_theta)\n        s2_index = index % len(self._s2_theta)\n        psi = self._s1[s1_index]\n        theta = self._s2_theta[s2_index]\n        phi = self._s2_phi[s2_index]\n        return psi, theta, phi\n\n    def quat_to_index(self, quaternion: np.array) -&gt; int:\n        \"\"\"Convert quaternion to index.\n\n        Will convert quaternion to Hopf coordinates and look up closest Hopf coordinate.\n        Closest means, closest in Hopf coordinates.\n\n        Args:\n            quaternion:\n                Array of shape (4,), containing a normalized quaternion.\n                The order of the quaternion is (x, y, z, w).\n        Returns:\n            The index of the closest (in Hopf coordinates) point.\n        \"\"\"\n        hopf = SO3Grid._quat_to_hopf(quaternion)\n        return self.hopf_to_index(*hopf)\n\n    def index_to_quat(self, index: int) -&gt; np.array:\n        \"\"\"Convert index to quaternion.\n\n        Returns:\n            Array of shape (4,), containing the normalized quaternion corresponding\n            to the index.\n        \"\"\"\n        hopf = self.index_to_hopf(index)\n        return SO3Grid._hopf_to_quat(*hopf)\n\n    @staticmethod\n    def _quat_to_hopf(quaternion: np.array) -&gt; Tuple[float, float, float]:\n        \"\"\"Convert quaternion to hopf coordinates.\n\n        Args:\n            quaternion:\n                Array of shape (4,), containing a normalized quaternion.\n                The order of the quaternion is (x, y, z, w).\n        Returns:\n            Tuple of psi, theta, phi.\n\n            With psi, theta, phi in [0,2pi), [0,pi], [0,2pi) respectively.\n        \"\"\"\n        x, y, z, w = quaternion\n        psi = 2 * np.arctan2(x, w)\n        theta = 2 * np.arctan2(np.sqrt(z ** 2 + y ** 2), np.sqrt(w ** 2 + x ** 2))\n        phi = np.arctan2(z * w - x * y, y * w + x * z)\n\n        # Note for the following correction use while instead of if, to support\n        # float32, because atan2 range for float32 ([-np.float32(np.pi),\n        # np.float32(np.pi)]) is larger than for float64 ([-np.pi,np.pi]).\n\n        # Psi must be [0, 2pi) and wraps around at 4*pi, so this correction changes the\n        # the half-sphere\n        while psi &lt; 0:\n            psi += 2 * np.pi\n        while psi &gt;= 2 * np.pi:\n            psi -= 2 * np.pi\n\n        # Phi must be [0, 2pi) and wraps around at 2*pi, so this correction just makes\n        # sure the angle is in the expected range\n        while phi &lt; 0:\n            phi += 2 * np.pi\n        while phi &gt;= 2 * np.pi:\n            phi -= 2 * np.pi\n        return psi, theta, phi\n\n    @staticmethod\n    def _hopf_to_quat(psi, theta, phi):\n        \"\"\"Convert quaternion to hopf coordinates.\n\n        Args:\n            phi: [0, 2pi)\n            theta: [0, pi]\n            psi: [0, 2pi)\n        Returns:\n            Array of shape (4,), containing the normalized quaternion corresponding\n            to the index.\n        \"\"\"\n        quaternion = np.array(\n            [\n                np.cos(theta / 2) * np.sin(psi / 2),  # x\n                np.sin(theta / 2) * np.cos(phi + psi / 2),  # y\n                np.sin(theta / 2) * np.sin(phi + psi / 2),  # z\n                np.cos(theta / 2) * np.cos(psi / 2),  # w\n            ]\n        )\n        if quaternion[0] &lt; 0:\n            quaternion *= -1\n        return quaternion\n\n    @staticmethod\n    def _grid_s1(resol):\n        \"\"\"Compute equidistant grid on 1-sphere.\n\n        Args:\n            resol: Resolution of grid.\n        Returns:\n            Center points of the grid cells.\"\"\"\n        points = 6 * 2 ** resol\n        grid = np.linspace(0, 2 * np.pi, points, endpoint=False) + np.pi / points\n        return grid\n\n    @staticmethod\n    def _grid_s2(resol):\n        \"\"\"Compute HEALpix coordinates of 2-sphere.\n\n        Args:\n            resol: Resolution of grid.\n        Returns:\n            Center points of the grid cells.\"\"\"\n        points_per_side = 2 ** resol\n        points = 12 * points_per_side * points_per_side\n        theta, phi = hp.pix2ang(points_per_side, np.arange(points), nest=True)\n        return theta, phi\n</code></pre>"},{"location":"reference/initialization/so3grid/#sdfest.initialization.so3grid.SO3Grid.__init__","title":"<code>__init__(resol)</code>","text":"<p>Construct the SO3 grid.</p> <p>Parameters:</p> Name Type Description Default <code>resol</code> <code>int</code> <p>The resolution of the grid. Coarsest possible grid for 0.</p> required Source code in <code>sdfest/initialization/so3grid.py</code> <pre><code>def __init__(self, resol: int):\n    \"\"\"Construct the SO3 grid.\n\n    Args:\n        resol: The resolution of the grid. Coarsest possible grid for 0.\n    \"\"\"\n    self._resol = resol\n    self._s1 = self._grid_s1(resol)\n    self._s2_theta, self._s2_phi = self._grid_s2(resol)\n</code></pre>"},{"location":"reference/initialization/so3grid/#sdfest.initialization.so3grid.SO3Grid.hopf_to_index","title":"<code>hopf_to_index(psi, theta, phi)</code>","text":"<p>Convert hopf coordinate to index.</p> <p>Parameters:</p> Name Type Description Default <code>phi</code> <p>[0, 2pi)</p> required <code>theta</code> <p>[0, pi]</p> required <code>psi</code> <p>[0, 2pi)</p> required <p>Returns:     Grid index of closest point in grid.</p> Source code in <code>sdfest/initialization/so3grid.py</code> <pre><code>def hopf_to_index(self, psi, theta, phi):\n    \"\"\"Convert hopf coordinate to index.\n\n    Args:\n        phi: [0, 2pi)\n        theta: [0, pi]\n        psi: [0, 2pi)\n    Returns:\n        Grid index of closest point in grid.\n    \"\"\"\n    s1_index = int(psi // (2 * np.pi / len(self._s1)))\n    s2_index = hp.ang2pix(2 ** self._resol, theta, phi, nest=True)\n    return s1_index * len(self._s2_theta) + s2_index\n</code></pre>"},{"location":"reference/initialization/so3grid/#sdfest.initialization.so3grid.SO3Grid.index_to_hopf","title":"<code>index_to_hopf(index)</code>","text":"<p>Convert index to hopf coordinates.</p> <p>Psi: [0,2pi) Theta: [0, pi] Phi: [0, 2pi)</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index of the grid point.</p> required <p>Returns:     Tuple of psi, theta, phi.</p> Source code in <code>sdfest/initialization/so3grid.py</code> <pre><code>def index_to_hopf(self, index: int) -&gt; Tuple[float, float, float]:\n    \"\"\"Convert index to hopf coordinates.\n\n    Psi: [0,2*pi)\n    Theta: [0, pi]\n    Phi: [0, 2*pi)\n\n    Args:\n        index: The index of the grid point.\n    Returns:\n        Tuple of psi, theta, phi.\n    \"\"\"\n    s1_index = index // len(self._s2_theta)\n    s2_index = index % len(self._s2_theta)\n    psi = self._s1[s1_index]\n    theta = self._s2_theta[s2_index]\n    phi = self._s2_phi[s2_index]\n    return psi, theta, phi\n</code></pre>"},{"location":"reference/initialization/so3grid/#sdfest.initialization.so3grid.SO3Grid.index_to_quat","title":"<code>index_to_quat(index)</code>","text":"<p>Convert index to quaternion.</p> <p>Returns:</p> Type Description <code>array</code> <p>Array of shape (4,), containing the normalized quaternion corresponding</p> <code>array</code> <p>to the index.</p> Source code in <code>sdfest/initialization/so3grid.py</code> <pre><code>def index_to_quat(self, index: int) -&gt; np.array:\n    \"\"\"Convert index to quaternion.\n\n    Returns:\n        Array of shape (4,), containing the normalized quaternion corresponding\n        to the index.\n    \"\"\"\n    hopf = self.index_to_hopf(index)\n    return SO3Grid._hopf_to_quat(*hopf)\n</code></pre>"},{"location":"reference/initialization/so3grid/#sdfest.initialization.so3grid.SO3Grid.num_cells","title":"<code>num_cells()</code>","text":"<p>Return the number of points in the grid.</p> Source code in <code>sdfest/initialization/so3grid.py</code> <pre><code>def num_cells(self) -&gt; int:\n    \"\"\"Return the number of points in the grid.\"\"\"\n    return len(self._s1) * len(self._s2_theta)\n</code></pre>"},{"location":"reference/initialization/so3grid/#sdfest.initialization.so3grid.SO3Grid.quat_to_index","title":"<code>quat_to_index(quaternion)</code>","text":"<p>Convert quaternion to index.</p> <p>Will convert quaternion to Hopf coordinates and look up closest Hopf coordinate. Closest means, closest in Hopf coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>quaternion</code> <code>array</code> <p>Array of shape (4,), containing a normalized quaternion. The order of the quaternion is (x, y, z, w).</p> required <p>Returns:     The index of the closest (in Hopf coordinates) point.</p> Source code in <code>sdfest/initialization/so3grid.py</code> <pre><code>def quat_to_index(self, quaternion: np.array) -&gt; int:\n    \"\"\"Convert quaternion to index.\n\n    Will convert quaternion to Hopf coordinates and look up closest Hopf coordinate.\n    Closest means, closest in Hopf coordinates.\n\n    Args:\n        quaternion:\n            Array of shape (4,), containing a normalized quaternion.\n            The order of the quaternion is (x, y, z, w).\n    Returns:\n        The index of the closest (in Hopf coordinates) point.\n    \"\"\"\n    hopf = SO3Grid._quat_to_hopf(quaternion)\n    return self.hopf_to_index(*hopf)\n</code></pre>"},{"location":"reference/initialization/utils/","title":"utils","text":"<p>General functions for experiments and pytorch.</p>"},{"location":"reference/initialization/utils/#sdfest.initialization.utils.dict_to","title":"<code>dict_to(data_dict, device)</code>","text":"<p>Move values in dictionary of type torch.Tensor to a specfied device.</p> <p>Parameters:</p> Name Type Description Default <code>data_dict</code> <code>dict</code> <p>Dictionary to be iterated over.</p> required <code>device</code> <code>device</code> <p>Device to move objects of type torch.Tensor to.</p> required <p>Returns:     Dictionary containing the same keys and values as data_dict, but with all     objects of type torch.Tensor moved to the specified device.</p> Source code in <code>sdfest/initialization/utils.py</code> <pre><code>def dict_to(data_dict: dict, device: torch.device) -&gt; dict:\n    \"\"\"Move values in dictionary of type torch.Tensor to a specfied device.\n\n    Args:\n        data_dict: Dictionary to be iterated over.\n        device: Device to move objects of type torch.Tensor to.\n    Returns:\n        Dictionary containing the same keys and values as data_dict, but with all\n        objects of type torch.Tensor moved to the specified device.\n    \"\"\"\n    new_data_dict = {}\n    for k, v in data_dict.items():\n        if isinstance(v, torch.Tensor):\n            new_data_dict[k] = v.to(device)\n        else:\n            new_data_dict[k] = v\n    return new_data_dict\n</code></pre>"},{"location":"reference/initialization/utils/#sdfest.initialization.utils.set_axes_equal","title":"<code>set_axes_equal(ax)</code>","text":"<p>Make axes of 3D plot have equal scale.</p> <p>This ensures that spheres appear as spheres, cubes as cubes, ... This is needed since Matplotlib's ax.set_aspect('equal') and and ax.axis('equal') are not supported for 3D.</p> <p>From: https://stackoverflow.com/a/31364297</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <p>A Matplotlib axis, e.g., as output from plt.gca().</p> required Source code in <code>sdfest/initialization/utils.py</code> <pre><code>def set_axes_equal(ax) -&gt; None:\n    \"\"\"Make axes of 3D plot have equal scale.\n\n    This ensures that spheres appear as spheres, cubes as cubes, ...\n    This is needed since Matplotlib's ax.set_aspect('equal') and\n    and ax.axis('equal') are not supported for 3D.\n\n    From: https://stackoverflow.com/a/31364297\n\n    Args:\n      ax: A Matplotlib axis, e.g., as output from plt.gca().\n    \"\"\"\n    x_limits = ax.get_xlim3d()\n    y_limits = ax.get_ylim3d()\n    z_limits = ax.get_zlim3d()\n\n    x_range = abs(x_limits[1] - x_limits[0])\n    x_middle = np.mean(x_limits)\n    y_range = abs(y_limits[1] - y_limits[0])\n    y_middle = np.mean(y_limits)\n    z_range = abs(z_limits[1] - z_limits[0])\n    z_middle = np.mean(z_limits)\n\n    # The plot bounding box is a sphere in the sense of the infinity\n    # norm, hence I call half the max range the plot radius.\n    plot_radius = 0.5 * max([x_range, y_range, z_range])\n\n    ax.set_xlim3d([x_middle - plot_radius, x_middle + plot_radius])\n    ax.set_ylim3d([y_middle - plot_radius, y_middle + plot_radius])\n    ax.set_zlim3d([z_middle - plot_radius, z_middle + plot_radius])\n</code></pre>"},{"location":"reference/initialization/utils/#sdfest.initialization.utils.str_to_object","title":"<code>str_to_object(name)</code>","text":"<p>Try to find object with a given name.</p> <p>First scope of calling function is checked for the name, then current environment (in which case name has to be a fully qualified name). In the second case, the object is imported if found.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the object to resolve.</p> required <p>Returns:     The object which the provided name refers to. None if no object was found.</p> Source code in <code>sdfest/initialization/utils.py</code> <pre><code>def str_to_object(name: str) -&gt; Any:\n    \"\"\"Try to find object with a given name.\n\n    First scope of calling function is checked for the name, then current environment\n    (in which case name has to be a fully qualified name). In the second case, the\n    object is imported if found.\n\n    Args:\n        name: Name of the object to resolve.\n    Returns:\n        The object which the provided name refers to. None if no object was found.\n    \"\"\"\n    # check callers local variables\n    caller_locals = inspect.currentframe().f_back.f_locals\n    if name in caller_locals:\n        return caller_locals[name]\n\n    # check callers global variables (i.e., imported modules etc.)\n    caller_globals = inspect.currentframe().f_back.f_globals\n    if name in caller_globals:\n        return caller_globals[name]\n\n    # check environment\n    return locate(name)\n</code></pre>"},{"location":"reference/initialization/utils/#sdfest.initialization.utils.visualize_sample","title":"<code>visualize_sample(sample=None, prediction=None)</code>","text":"<p>Visualize sample and prediction.</p> <p>Assumes the following conventions and keys     \"scale\": Half maximum side length of bounding box.     \"quaternion: Scalar-last orientation of object.</p> Source code in <code>sdfest/initialization/utils.py</code> <pre><code>def visualize_sample(sample: Optional[dict] = None, prediction: Optional[dict] = None):\n    \"\"\"Visualize sample and prediction.\n\n    Assumes the following conventions and keys\n        \"scale\": Half maximum side length of bounding box.\n        \"quaternion: Scalar-last orientation of object.\n    \"\"\"\n    pointset = sample[\"pointset\"].cpu().numpy()\n    plt.imshow(sample[\"mask\"].cpu().numpy())\n    plt.show()\n    plt.imshow(sample[\"depth\"].cpu().numpy())\n    plt.show()\n    fig = plt.figure()\n    ax = fig.add_subplot(projection=\"3d\")\n    ax.set_box_aspect((1, 1, 1))\n    max_points = 500\n    if len(pointset) &gt; max_points:\n        indices = np.random.choice(len(pointset), replace=False, size=max_points)\n        ax.scatter(pointset[indices, 0], pointset[indices, 1], pointset[indices, 2])\n    else:\n        ax.scatter(pointset[:, 0], pointset[:, 1], pointset[:, 2])\n\n    _plot_coordinate_frame(ax, sample)\n\n    _plot_bounding_box(ax, sample)\n\n    set_axes_equal(ax)\n\n    plt.show()\n</code></pre>"},{"location":"reference/initialization/datasets/dataset_utils/","title":"dataset_utils","text":"<p>Utility functions to handle various datasets.</p>"},{"location":"reference/initialization/datasets/dataset_utils/#sdfest.initialization.datasets.dataset_utils.MultiDataLoader","title":"<code>MultiDataLoader</code>","text":"<p>Wrapper for multiple dataloaders.</p> Source code in <code>sdfest/initialization/datasets/dataset_utils.py</code> <pre><code>class MultiDataLoader:\n    \"\"\"Wrapper for multiple dataloaders.\"\"\"\n\n    def __init__(\n        self,\n        data_loaders: List[torch.utils.data.DataLoader],\n        probabilities: List[float],\n    ) -&gt; None:\n        \"\"\"Initialize the class.\"\"\"\n        self._data_loaders = data_loaders\n        self._data_loader_iterators = [iter(dl) for dl in self._data_loaders]\n        self._probabilities = probabilities\n        assert len(self._data_loaders) == len(self._probabilities)\n\n    def __iter__(self) -&gt; Iterator:\n        \"\"\"Return infinite iterator which returns samples from sampled data_loader.\"\"\"\n        while True:\n            i = np.random.choice(\n                np.arange(len(self._probabilities)), p=self._probabilities\n            )\n            try:\n                yield next(self._data_loader_iterators[i])\n            except StopIteration:\n                self._data_loader_iterators[i] = iter(self._data_loaders[i])\n</code></pre>"},{"location":"reference/initialization/datasets/dataset_utils/#sdfest.initialization.datasets.dataset_utils.MultiDataLoader.__init__","title":"<code>__init__(data_loaders, probabilities)</code>","text":"<p>Initialize the class.</p> Source code in <code>sdfest/initialization/datasets/dataset_utils.py</code> <pre><code>def __init__(\n    self,\n    data_loaders: List[torch.utils.data.DataLoader],\n    probabilities: List[float],\n) -&gt; None:\n    \"\"\"Initialize the class.\"\"\"\n    self._data_loaders = data_loaders\n    self._data_loader_iterators = [iter(dl) for dl in self._data_loaders]\n    self._probabilities = probabilities\n    assert len(self._data_loaders) == len(self._probabilities)\n</code></pre>"},{"location":"reference/initialization/datasets/dataset_utils/#sdfest.initialization.datasets.dataset_utils.MultiDataLoader.__iter__","title":"<code>__iter__()</code>","text":"<p>Return infinite iterator which returns samples from sampled data_loader.</p> Source code in <code>sdfest/initialization/datasets/dataset_utils.py</code> <pre><code>def __iter__(self) -&gt; Iterator:\n    \"\"\"Return infinite iterator which returns samples from sampled data_loader.\"\"\"\n    while True:\n        i = np.random.choice(\n            np.arange(len(self._probabilities)), p=self._probabilities\n        )\n        try:\n            yield next(self._data_loader_iterators[i])\n        except StopIteration:\n            self._data_loader_iterators[i] = iter(self._data_loaders[i])\n</code></pre>"},{"location":"reference/initialization/datasets/dataset_utils/#sdfest.initialization.datasets.dataset_utils.collate_samples","title":"<code>collate_samples(samples)</code>","text":"<p>Collate sample dictionaries.</p> <p>Performs standard batching and additionally batches pointsets by taking subset of points. Also supports non-tensor types, which will be returned as standard lists.</p> <p>Reduces all pointsets to a common size based on the smallest set.</p> <p>Parameters:</p> Name Type Description Default <code>samples</code> <code>List[dict]</code> <p>Dictionary containing various types of data. All keys except \"pointset\" will use standard batching. All samples are expected to contain the same keys.</p> required <p>Returns:     Dictionary containing same keys as each sample.     For \"pointset\" key:         Tensor of size (N, M_min, D) where N is the batch size, M_min the number of         points in the smallest pointset and D the number of channels per point.</p> Source code in <code>sdfest/initialization/datasets/dataset_utils.py</code> <pre><code>def collate_samples(samples: List[dict]) -&gt; dict:\n    \"\"\"Collate sample dictionaries.\n\n    Performs standard batching and additionally batches pointsets by taking subset of\n    points.\n    Also supports non-tensor types, which will be returned as standard lists.\n\n    Reduces all pointsets to a common size based on the smallest set.\n\n    Args:\n        samples:\n            Dictionary containing various types of data.\n            All keys except \"pointset\" will use standard batching.\n            All samples are expected to contain the same keys.\n    Returns:\n        Dictionary containing same keys as each sample.\n        For \"pointset\" key:\n            Tensor of size (N, M_min, D) where N is the batch size, M_min the number of\n            points in the smallest pointset and D the number of channels per point.\n    \"\"\"\n    batch = {}\n\n    for key in samples[0].keys():\n        if key == \"pointset\":\n            batch_size = len(samples)\n\n            smallest_set = min(s[\"pointset\"].shape[0] for s in samples)\n            # limit number of points to limit memory usage\n            smallest_set = min(smallest_set, 2500)\n\n            sample_pointset = samples[0][\"pointset\"]\n\n            channels = sample_pointset.shape[-1]\n            device = sample_pointset.device\n            batch[\"pointset\"] = torch.empty(\n                batch_size, smallest_set, channels, device=device\n            )\n            for i, sample in enumerate(samples):\n                num_points = sample[\"pointset\"].shape[0]\n                point_indices = random.sample(range(0, num_points), smallest_set)\n                batch[\"pointset\"][i] = sample[\"pointset\"][point_indices]\n        elif isinstance(samples[0][key], torch.Tensor):\n            # standard batching for torch tensors\n            batch[key] = torch.stack([s[key] for s in samples])\n        else:\n            # standard list for other data types\n            batch[key] = [s[key] for s in samples]\n\n    return batch\n</code></pre>"},{"location":"reference/initialization/datasets/generated_dataset/","title":"generated_dataset","text":"<p>Module which provides SDFDataset class.</p>"},{"location":"reference/initialization/datasets/generated_dataset/#sdfest.initialization.datasets.generated_dataset.SDFVAEViewDataset","title":"<code>SDFVAEViewDataset</code>","text":"<p>               Bases: <code>IterableDataset</code></p> <p>Dataset of SDF views generated by VAE and renderer from a random view.</p> Source code in <code>sdfest/initialization/datasets/generated_dataset.py</code> <pre><code>class SDFVAEViewDataset(torch.utils.data.IterableDataset):\n    \"\"\"Dataset of SDF views generated by VAE and renderer from a random view.\"\"\"\n    class Config(TypedDict, total=False):\n        \"\"\"Configuration dictionary for SDFVAEViewDataset.\n\n        Attributes:\n            width: The width of the generated images in px.\n            height: The height of the generated images in px.\n            fov_deg: The horizontal fov in deg.\n            z_min:\n                Minimum z value (i.e., distance from camera) for the SDF.\n                Note that positive z means in front of the camera, hence z_sampler\n                should in most cases return positive values.\n            z_max:\n                Maximum z value (i.e., distance from camera) for the SDF.\n            extent_mean:\n                Mean extent of the SDF.\n                Extent is the total side length of an SDF.\n            extent_std:\n                Standard deviation of the SDF scale.\n            pointcloud: Whether to generate pointcloud or depth image.\n            normalize_pose:\n                Whether to center the augmented pointcloud at 0,0,0.\n                Ignored if pointcloud=False\n            orientation_repr:\n                Which orientation representation is used. One of:\n                    \"quaternion\"\n                    \"discretized\"\n            orientation_grid_resolution:\n                Resolution of the orientation grid.\n                Only used if orientation_repr is \"discretized\".\n            mask_noise:\n                Whether the mask should be perturbed to simulate noisy segmentation.\n                If True a random, small, affine transform will be applied to the correct\n                mask. The outliers will be filled with a random value sampled between\n                mask_noise_min, and mask_noise_max.\n            mask_noise_min:\n                Minimum value to fill in for noisy mask.\n                Only used if mask_noise is True.\n            mask_noise_max:\n                Maximum value to fill in for noisy mask.\n                Only used if mask_noise is True.\n            gaussian_noise_probability:\n                Probability to apply gaussian noise filter on depth image.\n            gaussian_noise_kernel_size:\n                Size of the Gaussian kernel.\n                Only used if Gaussian noise probability &gt; 0.0.\n            gausian_noise_kernel_std:\n                Standard deviation of the Gaussian kernel.\n                Only used if Gaussian noise probability &gt; 0.0.\n        \"\"\"\n\n        width: int\n        height: int\n        fov_deg: float\n        z_min: float\n        z_max: float\n        extent_mean: float\n        extent_std: float\n        pointcloud: bool\n        normalize_pose: Optional[bool]\n        render_threshold: float\n        orientation_repr: str\n        orientation_grid_resolution: Optional[int]\n        mask_noise: bool\n        mask_noise_min: Optional[float]\n        mask_noise_max: Optional[float]\n        norm_noise: bool\n        norm_noise_min: Optional[float]\n        norm_noise_max: Optional[float]\n        scale_to_unit_ball: bool\n        gaussian_noise_probability: float\n        gaussian_noise_kernel_size: Optional[int]\n        gausian_noise_kernel_std: Optional[float]\n\n    default_config: Config = {\n        \"device\": \"cuda\",\n        \"width\": 640,\n        \"height\": 480,\n        \"fov_deg\": 90,\n        \"render_threshold\": 0.004,\n        \"normalize_pose\": None,\n        \"orientation_repr\": \"quaternion\",\n        \"orientation_grid_resolution\": None,\n        \"mask_noise\": False,\n        \"mask_noise_min\": 0.1,\n        \"mask_noise_max\": 2.0,\n        \"norm_noise\": False,\n        \"norm_noise_min\": -0.2,\n        \"norm_noise_max\": 0.2,\n        \"scale_to_unit_ball\": False,\n        \"gaussian_noise_probability\": 0.0,\n        \"gaussian_noise_kernel_size\": 5,\n        \"gaussian_noise_kernel_std\": 1,\n    }\n\n    def __init__(\n        self,\n        config: dict,\n        vae: SDFVAE,\n    ) -&gt; None:\n        \"\"\"Initialize the dataset.\n\n        Args:\n            config:\n                Configuration dictionary of dataset. Provided dictionary will be merged\n                with default_dict. See SDFVAEViewDataset.Config for supported keys.\n            vae: The variational autoencoder used to create training samples.\n        \"\"\"\n        config = yoco.load_config(config, current_dict=SDFVAEViewDataset.default_config)\n        self._vae = vae\n        self._vae.eval()\n        self._device = next(self._vae.parameters()).device\n        f = config[\"width\"] / math.tan(config[\"fov_deg\"] * math.pi / 180.0 / 2.0) / 2\n        self._camera = Camera(\n            width=config[\"width\"],\n            height=config[\"height\"],\n            fx=f,\n            fy=f,\n            cx=config[\"width\"] / 2,\n            cy=config[\"height\"] / 2,\n            pixel_center=0.5,\n        )\n        self._fov_deg = config[\"fov_deg\"]\n        self._z_min = config[\"z_min\"]\n        self._z_max = config[\"z_max\"]\n        self._z_sampler = lambda: random.uniform(self._z_min, self._z_max)\n        self._extent_mean = config[\"extent_mean\"]\n        self._extent_std = config[\"extent_std\"]\n        self._scale_sampler = (\n            lambda: random.gauss(self._extent_mean, self._extent_std) / 2.0\n        )\n        self._mask_noise = config[\"mask_noise\"]\n        self._mask_noise_min = config[\"mask_noise_min\"]\n        self._mask_noise_max = config[\"mask_noise_max\"]\n        self._mask_noise_sampler = lambda: random.uniform(\n            config[\"mask_noise_min\"], config[\"mask_noise_max\"]\n        )\n        self._norm_noise = config[\"norm_noise\"]\n        self._norm_noise_min = config[\"norm_noise_min\"]\n        self._norm_noise_max = config[\"norm_noise_max\"]\n        self._norm_noise_sampler = lambda: random.uniform(\n            config[\"norm_noise_min\"], config[\"norm_noise_max\"]\n        )\n        self._scale_to_unit_ball = config[\"scale_to_unit_ball\"]\n        self._pointcloud = config[\"pointcloud\"]\n        self._normalize_pose = config[\"normalize_pose\"]\n        self._render_threshold = config[\"render_threshold\"]\n        self._orientation_repr = config[\"orientation_repr\"]\n        if self._orientation_repr == \"discretized\":\n            self._orientation_grid = so3grid.SO3Grid(\n                config[\"orientation_grid_resolution\"]\n            )\n        self._gaussian_noise_probability = config[\"gaussian_noise_probability\"]\n        self._create_gaussian_kernel(\n            config[\"gaussian_noise_kernel_std\"], config[\"gaussian_noise_kernel_size\"]\n        )\n\n    def __iter__(self) -&gt; Iterator:\n        \"\"\"Return SDF volume at a specific index.\n\n        Returns:\n            Infinite iterator, generating sample dictionaries.\n            See SDFVAEViewDataset._generate_sample for more details about returned\n            dictionaries.\n        \"\"\"\n        # this is an infinite iterator as the sentinel False will never be returned\n        while True:\n            yield self._generate_valid_sample()\n\n    def _generate_uniform_quaternion(self) -&gt; torch.Tensor:\n        \"\"\"Generate a uniform quaternion.\n\n        Following the method from K. Shoemake, Uniform Random Rotations, 1992.\n\n        See: http://planning.cs.uiuc.edu/node198.html\n\n        Returns:\n            Uniformly distributed unit quaternion on the dataset's device.\n        \"\"\"\n        u1, u2, u3 = random.random(), random.random(), random.random()\n        return torch.tensor(\n            [\n                math.sqrt(1 - u1) * math.sin(2 * math.pi * u2),\n                math.sqrt(1 - u1) * math.cos(2 * math.pi * u2),\n                math.sqrt(u1) * math.sin(2 * math.pi * u3),\n                math.sqrt(u1) * math.cos(2 * math.pi * u3),\n            ]\n        ).to(self._device)\n\n    def _is_valid(self, sample: dict) -&gt; bool:\n        \"\"\"Check whether a generated sample is valid.\n\n        A valid sample contains at least one valid point in the depth image and hence\n        in the pointcloud.\n        \"\"\"\n        if sample[\"depth\"].max() == 0:\n            return False\n        return True\n\n    def _generate_valid_sample(self) -&gt; dict:\n        \"\"\"Generate a single non-zero sample.\n\n        Returns:\n            See _generate_sample.\n        \"\"\"\n        sample = self._generate_sample()\n        while not self._is_valid(sample):\n            sample = self._generate_sample()\n            print(\"Warning: invalid sample, this should only happen very infrequently.\")\n            # if this happens often, either the SDF does not have any zero crossings\n            # or the object pose is completely outside the frustum\n        return sample\n\n    def _perturb_mask(self, mask: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Perturb mask by applying small random affine transform to it.\n\n        Args:\n            mask: The mask to perturb.\n        Returns:\n            The perturbed mask. Same shape as mask.\n        \"\"\"\n        affine_transfomer = T.RandomAffine(\n            degrees=(0, 1), translate=(0.00, 0.01), scale=(0.999, 1.001)\n        )\n        return affine_transfomer(mask.unsqueeze(0))[0]\n\n    def _generate_sample(self) -&gt; Tuple:\n        \"\"\"Generate a single sample. Possibly (albeit very unlikely) zero / empty.\n\n        Return:\n            Sample containing the following items:\n                \"depth\"\n                \"pointset\"\n                \"latent_shape\"\n                \"position\"\n                \"orientation\"\n                \"quaternion\"\n                \"scale\"\n        \"\"\"\n        sample = {}\n\n        latent = self._vae.sample()  # will be 1xlatent_size (batch size 1 here)\n        with torch.no_grad():\n            sdf = self._vae.decode(latent)\n        # generate x, y, z s.t. center is inside frustum\n        z = self._z_sampler()\n        x_pix = random.uniform(-self._camera.width / 2, self._camera.height / 2)\n        x = x_pix / self._camera.fx * z\n        y_pix = random.uniform(-self._camera.height / 2, self._camera.height / 2)\n        y = y_pix / self._camera.fy * z\n        position = torch.tensor([x, y, -z]).to(self._device)\n        quaternion = self._generate_uniform_quaternion()\n        scale = torch.tensor(self._scale_sampler()).to(self._device)\n        inv_scale = 1.0 / scale\n        orientation = self._quat_to_orientation_repr(quaternion)\n\n        depth = render_depth_gpu(\n            sdf[0, 0],\n            position,\n            quaternion,\n            inv_scale,\n            threshold=self._render_threshold,\n            camera=self._camera,\n        )\n\n        exact_mask = depth != 0\n        if self._mask_noise:\n            final_mask = self._perturb_mask(exact_mask)\n            depth[~exact_mask] = self._mask_noise_sampler()\n        else:\n            final_mask = exact_mask\n\n        if self._gaussian_noise_probability &gt; 0.0:\n            if random.random() &lt; self._gaussian_noise_probability:\n                invalid_depth_mask = depth == 0\n                depth[invalid_depth_mask] = torch.nan\n                depth_filtered = torch.nn.functional.conv2d(\n                    depth[None, None], self._gaussian_kernel, padding=\"same\"\n                )[0, 0]\n                # nan might become inf be preserved\n                # https://github.com/pytorch/pytorch/issues/12484\n                mask = torch.logical_or(depth_filtered.isnan(), depth_filtered.isinf())\n                depth[~mask] = depth_filtered[~mask]\n                depth[depth.isnan()] = 0.0\n\n        depth[~final_mask] = 0\n\n        if self._pointcloud:\n            pointset = pointset_utils.depth_to_pointcloud(\n                depth, self._camera, convention=\"opengl\"\n            )\n\n            if self._normalize_pose:\n                pointset, centroid = pointset_utils.normalize_points(pointset)\n                position -= centroid  # adjust target\n\n                if self._norm_noise:\n                    noise = position.new_tensor(\n                        [\n                            self._norm_noise_sampler(),\n                            self._norm_noise_sampler(),\n                            self._norm_noise_sampler(),\n                        ]\n                    )\n                    position += noise\n                    pointset += noise\n\n                if self._scale_to_unit_ball:\n                    max_distance = torch.max(torch.linalg.norm(pointset))\n                    pointset /= max_distance\n                    scale /= max_distance\n\n            sample[\"pointset\"] = pointset\n\n        sample[\"depth\"] = depth\n        sample[\"latent_shape\"] = latent.squeeze()\n        sample[\"position\"] = position\n        sample[\"orientation\"] = orientation\n        sample[\"quaternion\"] = quaternion\n        sample[\"scale\"] = scale\n\n        return sample\n\n        # TODO: add augmentation\n\n    def _quat_to_orientation_repr(self, quaternion: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Convert quaternion to selected orientation representation.\n\n        Args:\n            quaternion:\n                The quaternion to convert, scalar-last, shape (4,).\n        Returns:\n            The same orientation as represented by the quaternion in the chosen\n            orientation representation.\n        \"\"\"\n        if self._orientation_repr == \"quaternion\":\n            return quaternion\n        elif self._orientation_repr == \"discretized\":\n            index = self._orientation_grid.quat_to_index(quaternion.cpu().numpy())\n            return torch.tensor(\n                index,\n                device=self._device,\n                dtype=torch.long,\n            )\n        else:\n            raise NotImplementedError(\n                f\"Orientation representation {self._orientation_repr} is not supported.\"\n            )\n\n    def _create_gaussian_kernel(self, std: float, kernel_size: int) -&gt; None:\n        \"\"\"Create and set Gaussian noise kernel used for smoothing the depth image.\"\"\"\n        if kernel_size % 2 != 1:\n            raise ValueError(\"Kernel size should be odd.\")\n        impulse = np.zeros((kernel_size, kernel_size))\n        impulse[kernel_size // 2, kernel_size // 2] = 1\n        kernel = gaussian_filter(impulse, std)\n        self._gaussian_kernel = torch.Tensor(kernel[None, None]).to(self._device)\n</code></pre>"},{"location":"reference/initialization/datasets/generated_dataset/#sdfest.initialization.datasets.generated_dataset.SDFVAEViewDataset.Config","title":"<code>Config</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Configuration dictionary for SDFVAEViewDataset.</p> <p>Attributes:</p> Name Type Description <code>width</code> <code>int</code> <p>The width of the generated images in px.</p> <code>height</code> <code>int</code> <p>The height of the generated images in px.</p> <code>fov_deg</code> <code>float</code> <p>The horizontal fov in deg.</p> <code>z_min</code> <code>float</code> <p>Minimum z value (i.e., distance from camera) for the SDF. Note that positive z means in front of the camera, hence z_sampler should in most cases return positive values.</p> <code>z_max</code> <code>float</code> <p>Maximum z value (i.e., distance from camera) for the SDF.</p> <code>extent_mean</code> <code>float</code> <p>Mean extent of the SDF. Extent is the total side length of an SDF.</p> <code>extent_std</code> <code>float</code> <p>Standard deviation of the SDF scale.</p> <code>pointcloud</code> <code>bool</code> <p>Whether to generate pointcloud or depth image.</p> <code>normalize_pose</code> <code>Optional[bool]</code> <p>Whether to center the augmented pointcloud at 0,0,0. Ignored if pointcloud=False</p> <code>orientation_repr</code> <code>str</code> <p>Which orientation representation is used. One of:     \"quaternion\"     \"discretized\"</p> <code>orientation_grid_resolution</code> <code>Optional[int]</code> <p>Resolution of the orientation grid. Only used if orientation_repr is \"discretized\".</p> <code>mask_noise</code> <code>bool</code> <p>Whether the mask should be perturbed to simulate noisy segmentation. If True a random, small, affine transform will be applied to the correct mask. The outliers will be filled with a random value sampled between mask_noise_min, and mask_noise_max.</p> <code>mask_noise_min</code> <code>Optional[float]</code> <p>Minimum value to fill in for noisy mask. Only used if mask_noise is True.</p> <code>mask_noise_max</code> <code>Optional[float]</code> <p>Maximum value to fill in for noisy mask. Only used if mask_noise is True.</p> <code>gaussian_noise_probability</code> <code>float</code> <p>Probability to apply gaussian noise filter on depth image.</p> <code>gaussian_noise_kernel_size</code> <code>Optional[int]</code> <p>Size of the Gaussian kernel. Only used if Gaussian noise probability &gt; 0.0.</p> <code>gausian_noise_kernel_std</code> <code>Optional[float]</code> <p>Standard deviation of the Gaussian kernel. Only used if Gaussian noise probability &gt; 0.0.</p> Source code in <code>sdfest/initialization/datasets/generated_dataset.py</code> <pre><code>class Config(TypedDict, total=False):\n    \"\"\"Configuration dictionary for SDFVAEViewDataset.\n\n    Attributes:\n        width: The width of the generated images in px.\n        height: The height of the generated images in px.\n        fov_deg: The horizontal fov in deg.\n        z_min:\n            Minimum z value (i.e., distance from camera) for the SDF.\n            Note that positive z means in front of the camera, hence z_sampler\n            should in most cases return positive values.\n        z_max:\n            Maximum z value (i.e., distance from camera) for the SDF.\n        extent_mean:\n            Mean extent of the SDF.\n            Extent is the total side length of an SDF.\n        extent_std:\n            Standard deviation of the SDF scale.\n        pointcloud: Whether to generate pointcloud or depth image.\n        normalize_pose:\n            Whether to center the augmented pointcloud at 0,0,0.\n            Ignored if pointcloud=False\n        orientation_repr:\n            Which orientation representation is used. One of:\n                \"quaternion\"\n                \"discretized\"\n        orientation_grid_resolution:\n            Resolution of the orientation grid.\n            Only used if orientation_repr is \"discretized\".\n        mask_noise:\n            Whether the mask should be perturbed to simulate noisy segmentation.\n            If True a random, small, affine transform will be applied to the correct\n            mask. The outliers will be filled with a random value sampled between\n            mask_noise_min, and mask_noise_max.\n        mask_noise_min:\n            Minimum value to fill in for noisy mask.\n            Only used if mask_noise is True.\n        mask_noise_max:\n            Maximum value to fill in for noisy mask.\n            Only used if mask_noise is True.\n        gaussian_noise_probability:\n            Probability to apply gaussian noise filter on depth image.\n        gaussian_noise_kernel_size:\n            Size of the Gaussian kernel.\n            Only used if Gaussian noise probability &gt; 0.0.\n        gausian_noise_kernel_std:\n            Standard deviation of the Gaussian kernel.\n            Only used if Gaussian noise probability &gt; 0.0.\n    \"\"\"\n\n    width: int\n    height: int\n    fov_deg: float\n    z_min: float\n    z_max: float\n    extent_mean: float\n    extent_std: float\n    pointcloud: bool\n    normalize_pose: Optional[bool]\n    render_threshold: float\n    orientation_repr: str\n    orientation_grid_resolution: Optional[int]\n    mask_noise: bool\n    mask_noise_min: Optional[float]\n    mask_noise_max: Optional[float]\n    norm_noise: bool\n    norm_noise_min: Optional[float]\n    norm_noise_max: Optional[float]\n    scale_to_unit_ball: bool\n    gaussian_noise_probability: float\n    gaussian_noise_kernel_size: Optional[int]\n    gausian_noise_kernel_std: Optional[float]\n</code></pre>"},{"location":"reference/initialization/datasets/generated_dataset/#sdfest.initialization.datasets.generated_dataset.SDFVAEViewDataset.__init__","title":"<code>__init__(config, vae)</code>","text":"<p>Initialize the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration dictionary of dataset. Provided dictionary will be merged with default_dict. See SDFVAEViewDataset.Config for supported keys.</p> required <code>vae</code> <code>SDFVAE</code> <p>The variational autoencoder used to create training samples.</p> required Source code in <code>sdfest/initialization/datasets/generated_dataset.py</code> <pre><code>def __init__(\n    self,\n    config: dict,\n    vae: SDFVAE,\n) -&gt; None:\n    \"\"\"Initialize the dataset.\n\n    Args:\n        config:\n            Configuration dictionary of dataset. Provided dictionary will be merged\n            with default_dict. See SDFVAEViewDataset.Config for supported keys.\n        vae: The variational autoencoder used to create training samples.\n    \"\"\"\n    config = yoco.load_config(config, current_dict=SDFVAEViewDataset.default_config)\n    self._vae = vae\n    self._vae.eval()\n    self._device = next(self._vae.parameters()).device\n    f = config[\"width\"] / math.tan(config[\"fov_deg\"] * math.pi / 180.0 / 2.0) / 2\n    self._camera = Camera(\n        width=config[\"width\"],\n        height=config[\"height\"],\n        fx=f,\n        fy=f,\n        cx=config[\"width\"] / 2,\n        cy=config[\"height\"] / 2,\n        pixel_center=0.5,\n    )\n    self._fov_deg = config[\"fov_deg\"]\n    self._z_min = config[\"z_min\"]\n    self._z_max = config[\"z_max\"]\n    self._z_sampler = lambda: random.uniform(self._z_min, self._z_max)\n    self._extent_mean = config[\"extent_mean\"]\n    self._extent_std = config[\"extent_std\"]\n    self._scale_sampler = (\n        lambda: random.gauss(self._extent_mean, self._extent_std) / 2.0\n    )\n    self._mask_noise = config[\"mask_noise\"]\n    self._mask_noise_min = config[\"mask_noise_min\"]\n    self._mask_noise_max = config[\"mask_noise_max\"]\n    self._mask_noise_sampler = lambda: random.uniform(\n        config[\"mask_noise_min\"], config[\"mask_noise_max\"]\n    )\n    self._norm_noise = config[\"norm_noise\"]\n    self._norm_noise_min = config[\"norm_noise_min\"]\n    self._norm_noise_max = config[\"norm_noise_max\"]\n    self._norm_noise_sampler = lambda: random.uniform(\n        config[\"norm_noise_min\"], config[\"norm_noise_max\"]\n    )\n    self._scale_to_unit_ball = config[\"scale_to_unit_ball\"]\n    self._pointcloud = config[\"pointcloud\"]\n    self._normalize_pose = config[\"normalize_pose\"]\n    self._render_threshold = config[\"render_threshold\"]\n    self._orientation_repr = config[\"orientation_repr\"]\n    if self._orientation_repr == \"discretized\":\n        self._orientation_grid = so3grid.SO3Grid(\n            config[\"orientation_grid_resolution\"]\n        )\n    self._gaussian_noise_probability = config[\"gaussian_noise_probability\"]\n    self._create_gaussian_kernel(\n        config[\"gaussian_noise_kernel_std\"], config[\"gaussian_noise_kernel_size\"]\n    )\n</code></pre>"},{"location":"reference/initialization/datasets/generated_dataset/#sdfest.initialization.datasets.generated_dataset.SDFVAEViewDataset.__iter__","title":"<code>__iter__()</code>","text":"<p>Return SDF volume at a specific index.</p> <p>Returns:</p> Type Description <code>Iterator</code> <p>Infinite iterator, generating sample dictionaries.</p> <code>Iterator</code> <p>See SDFVAEViewDataset._generate_sample for more details about returned</p> <code>Iterator</code> <p>dictionaries.</p> Source code in <code>sdfest/initialization/datasets/generated_dataset.py</code> <pre><code>def __iter__(self) -&gt; Iterator:\n    \"\"\"Return SDF volume at a specific index.\n\n    Returns:\n        Infinite iterator, generating sample dictionaries.\n        See SDFVAEViewDataset._generate_sample for more details about returned\n        dictionaries.\n    \"\"\"\n    # this is an infinite iterator as the sentinel False will never be returned\n    while True:\n        yield self._generate_valid_sample()\n</code></pre>"},{"location":"reference/initialization/datasets/nocs_dataset/","title":"nocs_dataset","text":"<p>Module providing dataset class for NOCS datasets (CAMERA / REAL).</p>"},{"location":"reference/initialization/datasets/nocs_dataset/#sdfest.initialization.datasets.nocs_dataset.NOCSDataset","title":"<code>NOCSDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset class for NOCS dataset.</p> <p>CAMERA and REAL are training sets. CAMERA25 and REAL275 are test data. Some papers use CAMERA25 as validation when benchmarking REAL275.</p> <p>Datasets can be found here: https://github.com/hughw19/NOCS_CVPR2019/tree/master</p> Expected directory format <p>{root_dir}/real_train/... {root_dir}/real_test/... {root_dir}/gts/... {root_dir}/obj_models/... {root_dir}/camera_composed_depth/... {root_dir}/val/... {root_dir}/train/...</p> <p>Which is easily obtained by downloading all the provided files and extracting them into the same directory.</p> <p>Necessary preprocessing of this data is performed during first initialization per and is saved to     {root_dir}/sdfest_pre/...</p> Source code in <code>sdfest/initialization/datasets/nocs_dataset.py</code> <pre><code>class NOCSDataset(torch.utils.data.Dataset):\n    \"\"\"Dataset class for NOCS dataset.\n\n    CAMERA* and REAL* are training sets.\n    CAMERA25 and REAL275 are test data.\n    Some papers use CAMERA25 as validation when benchmarking REAL275.\n\n    Datasets can be found here:\n    https://github.com/hughw19/NOCS_CVPR2019/tree/master\n\n    Expected directory format:\n        {root_dir}/real_train/...\n        {root_dir}/real_test/...\n        {root_dir}/gts/...\n        {root_dir}/obj_models/...\n        {root_dir}/camera_composed_depth/...\n        {root_dir}/val/...\n        {root_dir}/train/...\n    Which is easily obtained by downloading all the provided files and extracting them\n    into the same directory.\n\n    Necessary preprocessing of this data is performed during first initialization per\n    and is saved to\n        {root_dir}/sdfest_pre/...\n    \"\"\"\n\n    num_categories = 7\n    category_id_to_str = {\n        0: \"unknown\",\n        1: \"bottle\",\n        2: \"bowl\",\n        3: \"camera\",\n        4: \"can\",\n        5: \"laptop\",\n        6: \"mug\",\n    }\n    category_str_to_id = {v: k for k, v in category_id_to_str.items()}\n\n    class Config(TypedDict, total=False):\n        \"\"\"Configuration dictionary for NOCSDataset.\n\n        Attributes:\n            root_dir: See NOCSDataset docstring.\n            split:\n                The dataset split. The following strings are supported:\n                    \"camera_train\": 275000 images, synthetic objects + real background\n                    \"camera_val\": 25000 images, synthetic objects + real background\n                    \"real_train\": 4300 images in 7 scenes, real\n                    \"real_test\": 2750 images in 6 scenes, real\n            mask_pointcloud: Whether the returned pointcloud will be masked.\n            normalize_pointcloud:\n                Whether the returned pointcloud and position will be normalized, such\n                that pointcloud centroid is at the origin.\n            scale_convention:\n                Which scale is returned. The following strings are supported:\n                    \"diagonal\":\n                        Length of bounding box' diagonal. This is what NOCS uses.\n                    \"max\": Maximum side length of bounding box.\n                    \"half_max\": Half maximum side length of bounding box.\n                    \"full\": Bounding box side lengths. Shape (3,).\n            camera_convention:\n                Which camera convention is used for position and orientation. One of:\n                    \"opengl\": x right, y up, z back\n                    \"opencv\": x right, y down, z forward\n                Note that this does not influence how the dataset is processed, only the\n                returned position and quaternion.\n            orientation_repr:\n                Which orientation representation is used. One of:\n                    \"quaternion\"\n                    \"discretized\"\n            orientation_grid_resolution:\n                Resolution of the orientation grid.\n                Only used if orientation_repr is \"discretized\".\n            remap_y_axis:\n                If not None, the NOCS y-axis will be mapped to the provided axis.\n                Resulting coordinate system will always be right-handed.\n                This is typically the up-axis.\n                Note that NOCS object models are NOT aligned the same as ShapeNetV2.\n                To get ShapeNetV2 alignment: y\n                One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\"\n            remap_x_axis:\n                If not None, the original x-axis will be mapped to the provided axis.\n                Resulting coordinate system will always be right-handed.\n                Note that NOCS object models are NOT aligned the same as ShapeNetV2.\n                To get ShapeNetV2 alignment: -z\n                One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\"\n            category_str:\n                If not None, only samples from the matching category will be returned.\n                See NOCSDataset.category_id_to_str for admissible category strings.\n        \"\"\"\n\n        root_dir: str\n        split: str\n        mask_pointcloud: bool\n        normalize_pointcloud: bool\n        scale_convention: str\n        camera_convention: str\n        orientation_repr: str\n        orientation_grid_resolution: int\n        remap_y_axis: Optional[str]\n        remap_x_axis: Optional[str]\n        category_str: Optional[str]\n\n    # TODO symmetry\n    # TODO unify dataset code, adapt generated_dataset\n\n    default_config: Config = {\n        \"root_dir\": None,\n        \"split\": None,\n        \"mask_pointcloud\": False,\n        \"normalize_pointcloud\": False,\n        \"camera_convention\": \"opengl\",\n        \"scale_convention\": \"half_max\",\n        \"orientation_repr\": \"quaternion\",\n        \"orientation_grid_resolution\": None,\n        \"category_str\": None,\n        \"remap_y_axis\": None,\n        \"remap_x_axis\": None,\n    }\n\n    def __init__(\n        self,\n        config: Config,\n    ) -&gt; None:\n        \"\"\"Initialize the dataset.\n\n        Args:\n            config:\n                Configuration dictionary of dataset. Provided dictionary will be merged\n                with default_dict. See NOCSDataset.Config for supported keys.\n        \"\"\"\n        config = yoco.load_config(config, current_dict=NOCSDataset.default_config)\n        self._root_dir = config[\"root_dir\"]\n        self._split = config[\"split\"]\n        self._camera_convention = config[\"camera_convention\"]\n        self._camera = self._get_split_camera()\n        self._preprocess_path = os.path.join(self._root_dir, \"sdfest_pre\", self._split)\n        if not os.path.isdir(self._preprocess_path):\n            self._preprocess_dataset()\n        self._mask_pointcloud = config[\"mask_pointcloud\"]\n        self._normalize_pointcloud = config[\"normalize_pointcloud\"]\n        self._scale_convention = config[\"scale_convention\"]\n        self._sample_files = self._get_sample_files(config[\"category_str\"])\n        self._remap_y_axis = config[\"remap_y_axis\"]\n        self._remap_x_axis = config[\"remap_x_axis\"]\n        self._orientation_repr = config[\"orientation_repr\"]\n        if self._orientation_repr == \"discretized\":\n            self._orientation_grid = so3grid.SO3Grid(\n                config[\"orientation_grid_resolution\"]\n            )\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return number of sample in dataset.\"\"\"\n        return len(self._sample_files)\n\n    def __getitem__(self, idx: int) -&gt; dict:\n        \"\"\"Return a sample of the dataset.\n\n        Args:\n            idx: Index of the instance.\n        Returns:\n            Sample containing the following items:\n                \"color\"\n                \"depth\"\n                \"mask\"\n                \"pointset\"\n                \"position\"\n                \"orientation\"\n                \"quaternion\"\n                \"scale\"\n                \"color_path\"\n        \"\"\"\n        sample_file = self._sample_files[idx]\n        sample_data = pickle.load(open(sample_file, \"rb\"))\n        sample = self._sample_from_sample_data(sample_data)\n        return sample\n\n    def _preprocess_dataset(self) -&gt; None:\n        \"\"\"Create preprocessing files for the current split.\n\n        One file per sample, which currently means per valid object mask will be\n        created.\n\n        Preprocessing will be stored on disk to {root_dir}/sdfest_pre/...\n        This function will not store the preprocessing, so it still has to be loaded\n        afterwards.\n        \"\"\"\n        os.makedirs(self._preprocess_path)\n\n        self._fix_obj_models()\n\n        self._start_time = time.time()\n        self._color_paths = self._get_color_files()\n\n        Parallel(n_jobs=-1)(\n            (\n                delayed(self._preprocess_color_path)(i, color_path)\n                for i, color_path in enumerate(self._color_paths)\n            )\n        )\n\n        # store dictionary to map category to files\n        sample_files = self._get_sample_files()\n        category_str_to_files = {\n            category_str: [] for category_str in NOCSDataset.category_id_to_str.values()\n        }\n        for sample_file in tqdm(sample_files):\n            sample_data = pickle.load(open(sample_file, \"rb\"))\n            category_id = sample_data[\"category_id\"]\n            category_str = NOCSDataset.category_id_to_str[category_id]\n            _, file_name = os.path.split(sample_file)\n            category_str_to_files[category_str].append(file_name)\n\n        category_json_path = os.path.join(self._preprocess_path, \"categories.json\")\n        with open(category_json_path, \"w\") as f:\n            json.dump(dict(category_str_to_files), f)\n\n        print(f\"Finished preprocessing for {self._split}.\", end=\"\\033[K\\n\")\n\n    def _fix_obj_models(self) -&gt; None:\n        \"\"\"Fix issues with fileextensions.\n\n        Some png files have jpg extension. This function fixes these models.\n        \"\"\"\n        glob_pattern = os.path.join(self._root_dir, \"**\", \"*.jpg\")\n        files = glob(glob_pattern, recursive=True)\n        for filepath in files:\n            what = imghdr.what(filepath)\n            if what == \"png\":\n                print(\"Fixing: \", filepath)\n                folder, problematic_filename = os.path.split(filepath)\n                name, _ = problematic_filename.split(\".\")\n                fixed_filename = f\"fixed_{name}.png\"\n                fixed_filepath = os.path.join(folder, fixed_filename)\n\n                mtl_filepath = os.path.join(folder, \"model.mtl\")\n                bu_mtl_filepath = os.path.join(folder, \"model.mtl.old\")\n                copyfile(mtl_filepath, bu_mtl_filepath)\n\n                copyfile(filepath, fixed_filepath)\n\n    def _update_preprocess_progress(self, image_id: int) -&gt; None:\n        current_time = time.time()\n        duration = current_time - self._start_time\n        imgs_per_sec = image_id / duration\n        if image_id &gt; 10:\n            remaining_imgs = len(self._color_paths) - image_id\n            remaining_secs = remaining_imgs / imgs_per_sec\n            remaining_time_str = str(datetime.timedelta(seconds=round(remaining_secs)))\n        else:\n            remaining_time_str = \"N/A\"\n        print(\n            f\"Preprocessing image: {image_id:&gt;10} / {len(self._color_paths)}\"\n            f\" {image_id / len(self._color_paths) * 100:&gt;6.2f}%\"  # progress percentage\n            f\" Remaining time: {remaining_time_str}\"  # remaining time\n            \"\\033[K\",  # clear until end of line\n            end=\"\\r\",  # overwrite previous\n        )\n\n    def _preprocess_color_path(self, image_id: int, color_path: str) -&gt; None:\n        counter = 0\n        self._update_preprocess_progress(image_id)\n\n        depth_path = self._depth_path_from_color_path(color_path)\n        if not os.path.isfile(depth_path):\n            print(f\"Missing depth file {depth_path}. Skipping.\", end=\"\\033[K\\n\")\n            return\n\n        mask_path = self._mask_path_from_color_path(color_path)\n        meta_path = self._meta_path_from_color_path(color_path)\n        meta_data = pd.read_csv(\n            meta_path, sep=\" \", header=None, converters={2: lambda x: str(x)}\n        )\n        instances_mask = self._load_mask(mask_path)\n        mask_ids = np.unique(instances_mask).tolist()\n        gt_id = 0  # GT only contains valid objects of interests and is 0-indexed\n        for mask_id in mask_ids:\n\n            if mask_id == 255:  # 255 is background\n                continue\n            match = meta_data[meta_data.iloc[:, 0] == mask_id]\n            if match.empty:\n                print(\n                    f\"Warning: mask {mask_id} not found in {meta_path}\", end=\"\\033[K\\n\"\n                )\n            elif match.shape[0] != 1:\n                print(\n                    f\"Warning: mask {mask_id} not unique in {meta_path}\", end=\"\\033[K\\n\"\n                )\n\n            meta_row = match.iloc[0]\n            category_id = meta_row.iloc[1]\n            if category_id == 0:  # unknown / distractor object\n                continue\n\n            try:\n                (\n                    position,\n                    orientation_q,\n                    extents,\n                    nocs_transform,\n                ) = self._get_pose_and_scale(color_path, mask_id, gt_id, meta_row)\n            except nocs_utils.PoseEstimationError:\n                print(\n                    \"Insufficient data for pose estimation. \"\n                    f\"Skipping {color_path}:{mask_id}.\",\n                    end=\"\\033[K\\n\",\n                )\n                continue\n            except ObjectError:\n                print(\n                    \"Insufficient object mesh for pose estimation. \"\n                    f\"Skipping {color_path}:{mask_id}.\",\n                    end=\"\\033[K\\n\",\n                )\n                continue\n\n            obj_path = self._get_obj_path(meta_row)\n            sample_info = {\n                \"color_path\": color_path,\n                \"depth_path\": depth_path,\n                \"mask_path\": mask_path,\n                \"mask_id\": mask_id,\n                \"category_id\": category_id,\n                \"obj_path\": obj_path,\n                \"nocs_transform\": nocs_transform,\n                \"position\": position,\n                \"orientation_q\": orientation_q,\n                \"extents\": extents,\n                \"nocs_scale\": torch.linalg.norm(extents),\n                \"max_extent\": torch.max(extents),\n            }\n            out_file = os.path.join(\n                self._preprocess_path, f\"{image_id:08}_{counter}.pkl\"\n            )\n            pickle.dump(sample_info, open(out_file, \"wb\"))\n            counter += 1\n            gt_id += 1\n\n    def _get_color_files(self) -&gt; list:\n        \"\"\"Return list of paths of color images of the selected split.\"\"\"\n        if self._split == \"camera_train\":\n            glob_pattern = os.path.join(self._root_dir, \"train\", \"**\", \"*_color.png\")\n            return sorted(glob(glob_pattern, recursive=True))\n        elif self._split == \"camera_val\":\n            glob_pattern = os.path.join(self._root_dir, \"val\", \"**\", \"*_color.png\")\n            return sorted(glob(glob_pattern, recursive=True))\n        elif self._split == \"real_train\":\n            glob_pattern = os.path.join(\n                self._root_dir, \"real_train\", \"**\", \"*_color.png\"\n            )\n            return sorted(glob(glob_pattern, recursive=True))\n        elif self._split == \"real_test\":\n            glob_pattern = os.path.join(\n                self._root_dir, \"real_test\", \"**\", \"*_color.png\"\n            )\n            return sorted(glob(glob_pattern, recursive=True))\n        else:\n            raise ValueError(f\"Specified split {self._split} is not supported.\")\n\n    def _get_sample_files(self, category_str: Optional[str] = None) -&gt; list:\n        \"\"\"Return sorted list of sample file paths.\n\n        Sample files are generated by NOCSDataset._preprocess_dataset.\n\n        Args:\n            category_str:\n                If not None, only instances of the provided category will be returned.\n        Returns:\n            List of sample_data files.\n        \"\"\"\n        glob_pattern = os.path.join(self._preprocess_path, \"*.pkl\")\n        sample_files = glob(glob_pattern)\n        sample_files.sort()\n        if category_str is None:\n            return sample_files\n        if category_str not in NOCSDataset.category_str_to_id:\n            raise ValueError(f\"Unsupported category_str {category_str}.\")\n\n        category_json_path = os.path.join(self._preprocess_path, \"categories.json\")\n        with open(category_json_path, \"r\") as f:\n            category_str_to_filenames = json.load(f)\n        filtered_sample_files = [\n            os.path.join(self._preprocess_path, fn)\n            for fn in category_str_to_filenames[category_str]\n        ]\n        return filtered_sample_files\n\n    def _get_split_camera(self) -&gt; None:\n        \"\"\"Return camera information for selected split.\"\"\"\n        # from: https://github.com/hughw19/NOCS_CVPR2019/blob/master/detect_eval.py\n        if self._split in [\"real_train\", \"real_test\"]:\n            return Camera(\n                width=640,\n                height=480,\n                fx=591.0125,\n                fy=590.16775,\n                cx=322.525,\n                cy=244.11084,\n                pixel_center=0.0,\n            )\n        elif self._split in [\"camera_train\", \"camera_val\"]:\n            return Camera(\n                width=640,\n                height=480,\n                fx=577.5,\n                fy=577.5,\n                cx=319.5,\n                cy=239.5,\n                pixel_center=0.0,\n            )\n        else:\n            raise ValueError(f\"Specified split {self._split} is not supported.\")\n\n    def _sample_from_sample_data(self, sample_data: dict) -&gt; dict:\n        \"\"\"Create dictionary containing a single sample.\"\"\"\n        color = torch.from_numpy(\n            np.asarray(Image.open(sample_data[\"color_path\"]), dtype=np.float32) / 255\n        )\n        depth = self._load_depth(sample_data[\"depth_path\"])\n        instances_mask = self._load_mask(sample_data[\"mask_path\"])\n        instance_mask = instances_mask == sample_data[\"mask_id\"]\n\n        pointcloud_mask = instance_mask if self._mask_pointcloud else None\n        pointcloud = pointset_utils.depth_to_pointcloud(\n            depth,\n            self._camera,\n            mask=pointcloud_mask,\n            convention=self._camera_convention,\n        )\n\n        # adjust camera convention for position, orientation and scale\n        position = pointset_utils.change_position_camera_convention(\n            sample_data[\"position\"], \"opencv\", self._camera_convention\n        )\n\n        # orientation / scale\n        orientation_q, extents = self._change_axis_convention(\n            sample_data[\"orientation_q\"], sample_data[\"extents\"]\n        )\n        orientation_q = pointset_utils.change_orientation_camera_convention(\n            orientation_q, \"opencv\", self._camera_convention\n        )\n        orientation = self._quat_to_orientation_repr(orientation_q)\n        scale = self._get_scale(sample_data, extents)\n\n        # normalize pointcloud &amp; position\n        if self._normalize_pointcloud:\n            pointcloud, centroid = pointset_utils.normalize_points(pointcloud)\n            position = position - centroid\n\n        sample = {\n            \"color\": color,\n            \"depth\": depth,\n            \"pointset\": pointcloud,\n            \"mask\": instance_mask,\n            \"position\": position,\n            \"orientation\": orientation,\n            \"quaternion\": orientation_q,\n            \"scale\": scale,\n            \"color_path\": sample_data[\"color_path\"],\n            \"obj_path\": sample_data[\"obj_path\"],\n            \"category_id\": sample_data[\"category_id\"],\n            \"category_str\": NOCSDataset.category_id_to_str[sample_data[\"category_id\"]],\n        }\n        return sample\n\n    def _depth_path_from_color_path(self, color_path: str) -&gt; str:\n        \"\"\"Return path to depth file from color filepath.\"\"\"\n        if self._split in [\"real_train\", \"real_test\"]:\n            depth_path = color_path.replace(\"color\", \"depth\")\n        elif self._split in [\"camera_train\"]:\n            depth_path = color_path.replace(\"color\", \"composed\")\n            depth_path = depth_path.replace(\"/train/\", \"/camera_full_depths/train/\")\n        elif self._split in [\"camera_val\"]:\n            depth_path = color_path.replace(\"color\", \"composed\")\n            depth_path = depth_path.replace(\"/val/\", \"/camera_full_depths/val/\")\n        else:\n            raise ValueError(f\"Specified split {self._split} is not supported.\")\n        return depth_path\n\n    def _mask_path_from_color_path(self, color_path: str) -&gt; str:\n        \"\"\"Return path to mask file from color filepath.\"\"\"\n        mask_path = color_path.replace(\"color\", \"mask\")\n        return mask_path\n\n    def _meta_path_from_color_path(self, color_path: str) -&gt; str:\n        \"\"\"Return path to meta file from color filepath.\"\"\"\n        meta_path = color_path.replace(\"color.png\", \"meta.txt\")\n        return meta_path\n\n    def _nocs_map_path_from_color_path(self, color_path: str) -&gt; str:\n        \"\"\"Return NOCS map filepath from color filepath.\"\"\"\n        nocs_map_path = color_path.replace(\"color.png\", \"coord.png\")\n        return nocs_map_path\n\n    def _get_pose_and_scale(\n        self, color_path: str, mask_id: int, gt_id: int, meta_row: pd.Series\n    ) -&gt; tuple:\n        \"\"\"Return position, orientation, scale and NOCS transform.\n\n        All of those follow OpenCV (x right, y down, z forward) convention.\n\n        Args:\n            color_path: Path to the color file.\n            mask_id: Instance id in the instances mask.\n            gt_id:\n                Ground truth id. This is 0-indexed id of valid instances in meta file.\n            meta_row:\n                Matching row of meta file. Contains necessary information about mesh.\n\n        Returns:\n            position (torch.Tensor):\n                Position of object center in camera frame. Shape (3,).\n            quaternion (torch.Tensor):\n                Orientation of object in camera frame.\n                Scalar-last quaternion, shape (4,).\n            extents (torch.Tensor):\n                Bounding box side lengths.\n            nocs_transformation (torch.Tensor):\n                Transformation from centered [-0.5,0.5]^3 NOCS coordinates to camera.\n        \"\"\"\n        gts_path = self._get_gts_path(color_path)\n        obj_path = self._get_obj_path(meta_row)\n        if self._split == \"real_test\":\n            # only use gt for real test data, since there are errors in camera val\n            gts_data = pickle.load(open(gts_path, \"rb\"))\n            nocs_transform = gts_data[\"gt_RTs\"][gt_id]\n            position = nocs_transform[0:3, 3]\n            rot_scale = nocs_transform[0:3, 0:3]\n            nocs_scales = np.sqrt(np.sum(rot_scale ** 2, axis=0))\n            rotation_matrix = rot_scale / nocs_scales[:, None]\n            nocs_scale = nocs_scales[0]\n        else:  # camera_train, camera_val, real_train\n            # use ground truth NOCS mask to perform alignment\n            (\n                position,\n                rotation_matrix,\n                nocs_scale,\n                nocs_transform,\n            ) = self._estimate_object(color_path, mask_id)\n\n        orientation_q = Rotation.from_matrix(rotation_matrix).as_quat()\n        mesh_extents = self._get_mesh_extents_from_obj(obj_path)\n\n        if \"camera\" in self._split:\n            # CAMERA / ShapeNet meshes are normalized s.t. diagonal == 1\n            # get metric extents by scaling with the diagonal\n            extents = nocs_scale * mesh_extents\n        elif \"real\" in self._split:\n            # REAL object meshes are not normalized\n            extents = mesh_extents\n        else:\n            raise ValueError(f\"Specified split {self._split} is not supported.\")\n\n        position = torch.Tensor(position)\n        orientation_q = torch.Tensor(orientation_q)\n        extents = torch.Tensor(extents)\n        nocs_transform = torch.Tensor(nocs_transform)\n        return position, orientation_q, extents, nocs_transform\n\n    def _get_gts_path(self, color_path: str) -&gt; Optional[str]:\n        \"\"\"Return path to gts file from color filepath.\n\n        Return None if split does not have ground truth information.\n        \"\"\"\n        if self._split == \"real_test\":\n            gts_folder = os.path.join(self._root_dir, \"gts\", \"real_test\")\n        elif self._split == \"camera_val\":\n            gts_folder = os.path.join(self._root_dir, \"gts\", \"val\")\n        else:\n            return None\n\n        path = os.path.normpath(color_path)\n        split_path = path.split(os.sep)\n        number = path[-14:-10]\n        gts_filename = f\"results_{split_path[-3]}_{split_path[-2]}_{number}.pkl\"\n        gts_path = os.path.join(gts_folder, gts_filename)\n        return gts_path\n\n    def _get_obj_path(self, meta_row: pd.Series) -&gt; str:\n        \"\"\"Return path to object file from meta data row.\"\"\"\n        if \"camera\" in self._split:  # ShapeNet mesh\n            synset_id = meta_row.iloc[2]\n            object_id = meta_row.iloc[3]\n            obj_path = os.path.join(\n                self._root_dir,\n                \"obj_models\",\n                self._split.replace(\"camera_\", \"\"),\n                synset_id,\n                object_id,\n                \"model.obj\",\n            )\n        elif \"real\" in self._split:  # REAL mesh\n            object_id = meta_row.iloc[2]\n            obj_path = os.path.join(\n                self._root_dir, \"obj_models\", self._split, object_id + \".obj\"\n            )\n        else:\n            raise ValueError(f\"Specified split {self._split} is not supported.\")\n        return obj_path\n\n    def _get_mesh_extents_from_obj(self, obj_path: str) -&gt; torch.Tensor:\n        \"\"\"Return maximum extent of bounding box from obj filepath.\n\n        Note that this is normalized extent (with diagonal == 1) in the case of CAMERA\n        dataset, and unnormalized (i.e., metric) extent in the case of REAL dataset.\n        \"\"\"\n        mesh = o3d.io.read_triangle_mesh(obj_path)\n        vertices = np.asarray(mesh.vertices)\n        if len(vertices) == 0:\n            raise ObjectError()\n        extents = np.max(vertices, axis=0) - np.min(vertices, axis=0)\n        return torch.Tensor(extents)\n\n    def _load_mask(self, mask_path: str) -&gt; torch.Tensor:\n        \"\"\"Load mask from mask filepath.\"\"\"\n        mask_img = np.asarray(Image.open(mask_path), dtype=np.uint8)\n        if mask_img.ndim == 3 and mask_img.shape[2] == 4:  # CAMERA masks are RGBA\n            instances_mask = mask_img[:, :, 0]  # use first channel only\n        else:  # REAL masks are grayscale\n            instances_mask = mask_img\n        return torch.from_numpy(instances_mask)\n\n    def _load_depth(self, depth_path: str) -&gt; torch.Tensor:\n        \"\"\"Load depth from depth filepath.\"\"\"\n        depth = torch.from_numpy(\n            np.asarray(Image.open(depth_path), dtype=np.float32) * 0.001\n        )\n        return depth\n\n    def _load_nocs_map(self, nocs_map_path: str) -&gt; torch.Tensor:\n        \"\"\"Load NOCS map from NOCS map filepath.\n\n        Returns:\n            NOCS map where each channel corresponds to one dimension in NOCS.\n            Coordinates are normalized to [0,1], shape (H,W,3).\n        \"\"\"\n        nocs_map = torch.from_numpy(\n            np.asarray(Image.open(nocs_map_path), dtype=np.float32) / 255\n        )\n        # z-coordinate has to be flipped\n        # see https://github.com/hughw19/NOCS_CVPR2019/blob/14dbce775c3c7c45bb7b19269bd53d68efb8f73f/dataset.py#L327 # noqa: E501\n        nocs_map[:, :, 2] = 1 - nocs_map[:, :, 2]\n        return nocs_map[:, :, :3]\n\n    def _estimate_object(self, color_path: str, mask_id: int) -&gt; tuple:\n        \"\"\"Estimate pose and scale through ground truth NOCS map.\"\"\"\n        position = rotation_matrix = scale = out_transform = None\n        depth_path = self._depth_path_from_color_path(color_path)\n        depth = self._load_depth(depth_path)\n        mask_path = self._mask_path_from_color_path(color_path)\n        instances_mask = self._load_mask(mask_path)\n        instance_mask = instances_mask == mask_id\n        nocs_map_path = self._nocs_map_path_from_color_path(color_path)\n        nocs_map = self._load_nocs_map(nocs_map_path)\n        valid_instance_mask = instance_mask * depth != 0\n        nocs_map[~valid_instance_mask] = 0\n        centered_nocs_points = nocs_map[valid_instance_mask] - 0.5\n\n        measured_points = pointset_utils.depth_to_pointcloud(\n            depth, self._camera, mask=valid_instance_mask, convention=\"opencv\"\n        )\n\n        # require at least 30 point correspondences to prevent outliers\n        if len(measured_points) &lt; 30:\n            raise nocs_utils.PoseEstimationError()\n\n        # skip object if it cointains errorneous depth\n        if torch.max(depth[valid_instance_mask]) &gt; 32.0:\n            print(\"Erroneous depth detected.\", end=\"\\033[K\\n\")\n            raise nocs_utils.PoseEstimationError()\n\n        (\n            position,\n            rotation_matrix,\n            scale,\n            out_transform,\n        ) = nocs_utils.estimate_similarity_transform(\n            centered_nocs_points, measured_points, verbose=False\n        )\n\n        if position is None:\n            raise nocs_utils.PoseEstimationError()\n\n        return position, rotation_matrix, scale, out_transform\n\n    def _get_scale(self, sample_data: dict, extents: torch.Tensor) -&gt; float:\n        \"\"\"Return scale from stored sample data and extents.\"\"\"\n        if self._scale_convention == \"diagonal\":\n            return sample_data[\"nocs_scale\"]\n        elif self._scale_convention == \"max\":\n            return sample_data[\"max_extent\"]\n        elif self._scale_convention == \"half_max\":\n            return 0.5 * sample_data[\"max_extent\"]\n        elif self._scale_convention == \"full\":\n            return extents\n        else:\n            raise ValueError(\n                f\"Specified scale convention {self._scale_convnetion} not supported.\"\n            )\n\n    def _change_axis_convention(\n        self, orientation_q: torch.Tensor, extents: torch.Tensor\n    ) -&gt; tuple:\n        \"\"\"Adjust up-axis for orientation and extents.\n\n        Returns:\n            Tuple of position, orienation_q and extents, with specified up-axis.\n        \"\"\"\n        if self._remap_y_axis is None and self._remap_x_axis is None:\n            return orientation_q, extents\n        elif self._remap_y_axis is None or self._remap_x_axis is None:\n            raise ValueError(\"Either both or none of remap_{y,x}_axis have to be None.\")\n\n        rotation_o2n = self._get_o2n_object_rotation_matrix()\n        remapped_extents = torch.abs(torch.Tensor(rotation_o2n) @ extents)\n\n        # quaternion so far: original -&gt; camera\n        # we want a quaternion: new -&gt; camera\n        rotation_n2o = rotation_o2n.T\n\n        quaternion_n2o = torch.from_numpy(Rotation.from_matrix(rotation_n2o).as_quat())\n\n        remapped_orientation_q = quaternion_utils.quaternion_multiply(\n            orientation_q, quaternion_n2o\n        )  # new -&gt; original -&gt; camera\n\n        return remapped_orientation_q, remapped_extents\n\n    def _get_o2n_object_rotation_matrix(self) -&gt; np.ndarray:\n        \"\"\"Compute rotation matrix which rotates original to new object coordinates.\"\"\"\n        rotation_o2n = np.zeros((3, 3))  # original to new object convention\n        if self._remap_y_axis == \"x\":\n            rotation_o2n[0, 1] = 1\n        elif self._remap_y_axis == \"-x\":\n            rotation_o2n[0, 1] = -1\n        elif self._remap_y_axis == \"y\":\n            rotation_o2n[1, 1] = 1\n        elif self._remap_y_axis == \"-y\":\n            rotation_o2n[1, 1] = -1\n        elif self._remap_y_axis == \"z\":\n            rotation_o2n[2, 1] = 1\n        elif self._remap_y_axis == \"-z\":\n            rotation_o2n[2, 1] = -1\n        else:\n            raise ValueError(\"Unsupported remap_y_axis {self.remap_y}\")\n\n        if self._remap_x_axis == \"x\":\n            rotation_o2n[0, 0] = 1\n        elif self._remap_x_axis == \"-x\":\n            rotation_o2n[0, 0] = -1\n        elif self._remap_x_axis == \"y\":\n            rotation_o2n[1, 0] = 1\n        elif self._remap_x_axis == \"-y\":\n            rotation_o2n[1, 0] = -1\n        elif self._remap_x_axis == \"z\":\n            rotation_o2n[2, 0] = 1\n        elif self._remap_x_axis == \"-z\":\n            rotation_o2n[2, 0] = -1\n        else:\n            raise ValueError(\"Unsupported remap_x_axis {self.remap_y}\")\n\n        # infer last column\n        rotation_o2n[:, 2] = 1 - np.abs(np.sum(rotation_o2n, 1))  # rows must sum to +-1\n        rotation_o2n[:, 2] *= np.linalg.det(rotation_o2n)  # make special orthogonal\n        if np.linalg.det(rotation_o2n) != 1.0:  # check if special orthogonal\n            raise ValueError(\"Unsupported combination of remap_{y,x}_axis. det != 1\")\n        return rotation_o2n\n\n    def _quat_to_orientation_repr(self, quaternion: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Convert quaternion to selected orientation representation.\n\n        Args:\n            quaternion:\n                The quaternion to convert, scalar-last, shape (4,).\n        Returns:\n            The same orientation as represented by the quaternion in the chosen\n            orientation representation.\n        \"\"\"\n        if self._orientation_repr == \"quaternion\":\n            return quaternion\n        elif self._orientation_repr == \"discretized\":\n            index = self._orientation_grid.quat_to_index(quaternion.numpy())\n            return torch.tensor(\n                index,\n                dtype=torch.long,\n            )\n        else:\n            raise NotImplementedError(\n                f\"Orientation representation {self._orientation_repr} is not supported.\"\n            )\n\n    def load_mesh(self, object_path: str) -&gt; o3d.geometry.TriangleMesh:\n        \"\"\"Load an object mesh and adjust its object frame convention.\"\"\"\n        mesh = o3d.io.read_triangle_mesh(object_path)\n        if self._remap_y_axis is None and self._remap_x_axis is None:\n            return mesh\n        elif self._remap_y_axis is None or self._remap_x_axis is None:\n            raise ValueError(\"Either both or none of remap_{y,x}_axis have to be None.\")\n\n        rotation_o2n = self._get_o2n_object_rotation_matrix()\n        mesh.rotate(\n            rotation_o2n,\n            center=np.array([0.0, 0.0, 0.0])[:, None],\n        )\n        return mesh\n</code></pre>"},{"location":"reference/initialization/datasets/nocs_dataset/#sdfest.initialization.datasets.nocs_dataset.NOCSDataset.Config","title":"<code>Config</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Configuration dictionary for NOCSDataset.</p> <p>Attributes:</p> Name Type Description <code>root_dir</code> <code>str</code> <p>See NOCSDataset docstring.</p> <code>split</code> <code>str</code> <p>The dataset split. The following strings are supported:     \"camera_train\": 275000 images, synthetic objects + real background     \"camera_val\": 25000 images, synthetic objects + real background     \"real_train\": 4300 images in 7 scenes, real     \"real_test\": 2750 images in 6 scenes, real</p> <code>mask_pointcloud</code> <code>bool</code> <p>Whether the returned pointcloud will be masked.</p> <code>normalize_pointcloud</code> <code>bool</code> <p>Whether the returned pointcloud and position will be normalized, such that pointcloud centroid is at the origin.</p> <code>scale_convention</code> <code>str</code> <p>Which scale is returned. The following strings are supported:     \"diagonal\":         Length of bounding box' diagonal. This is what NOCS uses.     \"max\": Maximum side length of bounding box.     \"half_max\": Half maximum side length of bounding box.     \"full\": Bounding box side lengths. Shape (3,).</p> <code>camera_convention</code> <code>str</code> <p>Which camera convention is used for position and orientation. One of:     \"opengl\": x right, y up, z back     \"opencv\": x right, y down, z forward Note that this does not influence how the dataset is processed, only the returned position and quaternion.</p> <code>orientation_repr</code> <code>str</code> <p>Which orientation representation is used. One of:     \"quaternion\"     \"discretized\"</p> <code>orientation_grid_resolution</code> <code>int</code> <p>Resolution of the orientation grid. Only used if orientation_repr is \"discretized\".</p> <code>remap_y_axis</code> <code>Optional[str]</code> <p>If not None, the NOCS y-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. This is typically the up-axis. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: y One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\"</p> <code>remap_x_axis</code> <code>Optional[str]</code> <p>If not None, the original x-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: -z One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\"</p> <code>category_str</code> <code>Optional[str]</code> <p>If not None, only samples from the matching category will be returned. See NOCSDataset.category_id_to_str for admissible category strings.</p> Source code in <code>sdfest/initialization/datasets/nocs_dataset.py</code> <pre><code>class Config(TypedDict, total=False):\n    \"\"\"Configuration dictionary for NOCSDataset.\n\n    Attributes:\n        root_dir: See NOCSDataset docstring.\n        split:\n            The dataset split. The following strings are supported:\n                \"camera_train\": 275000 images, synthetic objects + real background\n                \"camera_val\": 25000 images, synthetic objects + real background\n                \"real_train\": 4300 images in 7 scenes, real\n                \"real_test\": 2750 images in 6 scenes, real\n        mask_pointcloud: Whether the returned pointcloud will be masked.\n        normalize_pointcloud:\n            Whether the returned pointcloud and position will be normalized, such\n            that pointcloud centroid is at the origin.\n        scale_convention:\n            Which scale is returned. The following strings are supported:\n                \"diagonal\":\n                    Length of bounding box' diagonal. This is what NOCS uses.\n                \"max\": Maximum side length of bounding box.\n                \"half_max\": Half maximum side length of bounding box.\n                \"full\": Bounding box side lengths. Shape (3,).\n        camera_convention:\n            Which camera convention is used for position and orientation. One of:\n                \"opengl\": x right, y up, z back\n                \"opencv\": x right, y down, z forward\n            Note that this does not influence how the dataset is processed, only the\n            returned position and quaternion.\n        orientation_repr:\n            Which orientation representation is used. One of:\n                \"quaternion\"\n                \"discretized\"\n        orientation_grid_resolution:\n            Resolution of the orientation grid.\n            Only used if orientation_repr is \"discretized\".\n        remap_y_axis:\n            If not None, the NOCS y-axis will be mapped to the provided axis.\n            Resulting coordinate system will always be right-handed.\n            This is typically the up-axis.\n            Note that NOCS object models are NOT aligned the same as ShapeNetV2.\n            To get ShapeNetV2 alignment: y\n            One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\"\n        remap_x_axis:\n            If not None, the original x-axis will be mapped to the provided axis.\n            Resulting coordinate system will always be right-handed.\n            Note that NOCS object models are NOT aligned the same as ShapeNetV2.\n            To get ShapeNetV2 alignment: -z\n            One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\"\n        category_str:\n            If not None, only samples from the matching category will be returned.\n            See NOCSDataset.category_id_to_str for admissible category strings.\n    \"\"\"\n\n    root_dir: str\n    split: str\n    mask_pointcloud: bool\n    normalize_pointcloud: bool\n    scale_convention: str\n    camera_convention: str\n    orientation_repr: str\n    orientation_grid_resolution: int\n    remap_y_axis: Optional[str]\n    remap_x_axis: Optional[str]\n    category_str: Optional[str]\n</code></pre>"},{"location":"reference/initialization/datasets/nocs_dataset/#sdfest.initialization.datasets.nocs_dataset.NOCSDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Return a sample of the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the instance.</p> required <p>Returns:     Sample containing the following items:         \"color\"         \"depth\"         \"mask\"         \"pointset\"         \"position\"         \"orientation\"         \"quaternion\"         \"scale\"         \"color_path\"</p> Source code in <code>sdfest/initialization/datasets/nocs_dataset.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; dict:\n    \"\"\"Return a sample of the dataset.\n\n    Args:\n        idx: Index of the instance.\n    Returns:\n        Sample containing the following items:\n            \"color\"\n            \"depth\"\n            \"mask\"\n            \"pointset\"\n            \"position\"\n            \"orientation\"\n            \"quaternion\"\n            \"scale\"\n            \"color_path\"\n    \"\"\"\n    sample_file = self._sample_files[idx]\n    sample_data = pickle.load(open(sample_file, \"rb\"))\n    sample = self._sample_from_sample_data(sample_data)\n    return sample\n</code></pre>"},{"location":"reference/initialization/datasets/nocs_dataset/#sdfest.initialization.datasets.nocs_dataset.NOCSDataset.__init__","title":"<code>__init__(config)</code>","text":"<p>Initialize the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>Configuration dictionary of dataset. Provided dictionary will be merged with default_dict. See NOCSDataset.Config for supported keys.</p> required Source code in <code>sdfest/initialization/datasets/nocs_dataset.py</code> <pre><code>def __init__(\n    self,\n    config: Config,\n) -&gt; None:\n    \"\"\"Initialize the dataset.\n\n    Args:\n        config:\n            Configuration dictionary of dataset. Provided dictionary will be merged\n            with default_dict. See NOCSDataset.Config for supported keys.\n    \"\"\"\n    config = yoco.load_config(config, current_dict=NOCSDataset.default_config)\n    self._root_dir = config[\"root_dir\"]\n    self._split = config[\"split\"]\n    self._camera_convention = config[\"camera_convention\"]\n    self._camera = self._get_split_camera()\n    self._preprocess_path = os.path.join(self._root_dir, \"sdfest_pre\", self._split)\n    if not os.path.isdir(self._preprocess_path):\n        self._preprocess_dataset()\n    self._mask_pointcloud = config[\"mask_pointcloud\"]\n    self._normalize_pointcloud = config[\"normalize_pointcloud\"]\n    self._scale_convention = config[\"scale_convention\"]\n    self._sample_files = self._get_sample_files(config[\"category_str\"])\n    self._remap_y_axis = config[\"remap_y_axis\"]\n    self._remap_x_axis = config[\"remap_x_axis\"]\n    self._orientation_repr = config[\"orientation_repr\"]\n    if self._orientation_repr == \"discretized\":\n        self._orientation_grid = so3grid.SO3Grid(\n            config[\"orientation_grid_resolution\"]\n        )\n</code></pre>"},{"location":"reference/initialization/datasets/nocs_dataset/#sdfest.initialization.datasets.nocs_dataset.NOCSDataset.__len__","title":"<code>__len__()</code>","text":"<p>Return number of sample in dataset.</p> Source code in <code>sdfest/initialization/datasets/nocs_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return number of sample in dataset.\"\"\"\n    return len(self._sample_files)\n</code></pre>"},{"location":"reference/initialization/datasets/nocs_dataset/#sdfest.initialization.datasets.nocs_dataset.NOCSDataset.load_mesh","title":"<code>load_mesh(object_path)</code>","text":"<p>Load an object mesh and adjust its object frame convention.</p> Source code in <code>sdfest/initialization/datasets/nocs_dataset.py</code> <pre><code>def load_mesh(self, object_path: str) -&gt; o3d.geometry.TriangleMesh:\n    \"\"\"Load an object mesh and adjust its object frame convention.\"\"\"\n    mesh = o3d.io.read_triangle_mesh(object_path)\n    if self._remap_y_axis is None and self._remap_x_axis is None:\n        return mesh\n    elif self._remap_y_axis is None or self._remap_x_axis is None:\n        raise ValueError(\"Either both or none of remap_{y,x}_axis have to be None.\")\n\n    rotation_o2n = self._get_o2n_object_rotation_matrix()\n    mesh.rotate(\n        rotation_o2n,\n        center=np.array([0.0, 0.0, 0.0])[:, None],\n    )\n    return mesh\n</code></pre>"},{"location":"reference/initialization/datasets/nocs_dataset/#sdfest.initialization.datasets.nocs_dataset.ObjectError","title":"<code>ObjectError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Error if something with the mesh is wrong.</p> Source code in <code>sdfest/initialization/datasets/nocs_dataset.py</code> <pre><code>class ObjectError(Exception):\n    \"\"\"Error if something with the mesh is wrong.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/initialization/datasets/nocs_utils/","title":"nocs_utils","text":"<p>Module for utility function related to NOCS dataset.</p> <p>This module contains functions to find similarity transform from NOCS maps and evaluation function for typical metrics on the NOCS datasets.</p> Aligning code by Srinath Sridhar <p>https://raw.githubusercontent.com/hughw19/NOCS_CVPR2019/master/aligning.py</p> <p>Evaluation code by ... TODO</p>"},{"location":"reference/initialization/datasets/nocs_utils/#sdfest.initialization.datasets.nocs_utils.PoseEstimationError","title":"<code>PoseEstimationError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Error if pose estimation encountered an error.</p> Source code in <code>sdfest/initialization/datasets/nocs_utils.py</code> <pre><code>class PoseEstimationError(Exception):\n    \"\"\"Error if pose estimation encountered an error.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/initialization/datasets/nocs_utils/#sdfest.initialization.datasets.nocs_utils.estimate_similarity_transform","title":"<code>estimate_similarity_transform(source, target, verbose=False)</code>","text":"<p>Estimate similarity transform from source to target from point correspondences.</p> <p>Source and target are pairwise correponding pointsets, i.e., they include same number of points and the first point of source corresponds to the first point of target. RANSAC is used for outlier-robust estimation.</p> <p>A similarity transform is estimated (i.e., isotropic scale, rotation and translation) that transforms source points onto the target points.</p> <p>Note that the returned values fulfill the following equations     transform @ source_points = scale * rotation_matrix @ source_points + position when ignoring homogeneous coordinate for left-hand side.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>ndarray</code> <p>Source points that will be transformed, shape (N,3).</p> required <code>target</code> <code>ndarray</code> <p>Target points to which source will be aligned to, shape (N,3).</p> required <code>verbose</code> <code>bool</code> <p>If true additional information will be printed.</p> <code>False</code> <p>Returns:     position (np.ndarray): Translation to translate source to target, shape (3,).     rotation_matrix (np.ndarray): Rotation to rotate source to target, shape (3,3).     scale (float):         Scaling factor along each axis, to scale source to target.     transform (np.ndarray): Homogeneous transformation matrix, shape (4,4).</p> Source code in <code>sdfest/initialization/datasets/nocs_utils.py</code> <pre><code>def estimate_similarity_transform(\n    source: np.ndarray, target: np.ndarray, verbose: bool = False\n) -&gt; tuple:\n    \"\"\"Estimate similarity transform from source to target from point correspondences.\n\n    Source and target are pairwise correponding pointsets, i.e., they include same\n    number of points and the first point of source corresponds to the first point of\n    target. RANSAC is used for outlier-robust estimation.\n\n    A similarity transform is estimated (i.e., isotropic scale, rotation and\n    translation) that transforms source points onto the target points.\n\n    Note that the returned values fulfill the following equations\n        transform @ source_points = scale * rotation_matrix @ source_points + position\n    when ignoring homogeneous coordinate for left-hand side.\n\n    Args:\n        source: Source points that will be transformed, shape (N,3).\n        target: Target points to which source will be aligned to, shape (N,3).\n        verbose: If true additional information will be printed.\n    Returns:\n        position (np.ndarray): Translation to translate source to target, shape (3,).\n        rotation_matrix (np.ndarray): Rotation to rotate source to target, shape (3,3).\n        scale (float):\n            Scaling factor along each axis, to scale source to target.\n        transform (np.ndarray): Homogeneous transformation matrix, shape (4,4).\n    \"\"\"\n    if len(source) &lt; 5 or len(target) &lt; 5:\n        print(\"Pose estimation failed. Not enough point correspondences: \", len(source))\n        return None, None, None, None\n\n    # make points homogeneous\n    source_hom = np.transpose(np.hstack([source, np.ones([source.shape[0], 1])]))  # 4,N\n    target_hom = np.transpose(np.hstack([target, np.ones([source.shape[0], 1])]))  # 4,N\n\n    # Auto-parameter selection based on source-target heuristics\n    target_norm = np.mean(np.linalg.norm(target, axis=1))  # mean distance from origin\n    source_norm = np.mean(np.linalg.norm(source, axis=1))\n    ratio_ts = target_norm / source_norm\n    ratio_st = source_norm / target_norm\n    pass_t = ratio_st if (ratio_st &gt; ratio_ts) else ratio_ts\n    pass_t *= 0.01  # tighter bound\n    stop_t = pass_t / 100\n    n_iter = 100\n    if verbose:\n        print(\"Pass threshold: \", pass_t)\n        print(\"Stop threshold: \", stop_t)\n        print(\"Number of iterations: \", n_iter)\n\n    source_inliers_hom, target_inliers_hom, best_inlier_ratio = _get_ransac_inliers(\n        source_hom,\n        target_hom,\n        max_iterations=n_iter,\n        pass_threshold=pass_t,\n        stop_threshold=stop_t,\n    )\n\n    if best_inlier_ratio &lt; 0.1:\n        print(\"Pose estimation failed. Small inlier ratio: \", best_inlier_ratio)\n        return None, None, None, None\n\n    scales, rotation_matrix, position, out_transform = _estimate_similarity_umeyama(\n        source_inliers_hom, target_inliers_hom\n    )\n    scale = scales[0]\n\n    if verbose:\n        print(\"BestInlierRatio:\", best_inlier_ratio)\n        print(\"Rotation:\\n\", rotation_matrix)\n        print(\"Position:\\n\", position)\n        print(\"Scales:\", scales)\n\n    return position, rotation_matrix, scale, out_transform\n</code></pre>"},{"location":"reference/initialization/datasets/redwood_dataset/","title":"redwood_dataset","text":"<p>Module providing dataset class for annotated Redwood dataset.</p>"},{"location":"reference/initialization/datasets/redwood_dataset/#sdfest.initialization.datasets.redwood_dataset.AnnotatedRedwoodDataset","title":"<code>AnnotatedRedwoodDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset class for annotated Redwood dataset.</p> <p>Data can be found here: http://redwood-data.org/3dscan/index.html</p> <p>Annotations are part SDFEst repo.</p> Expected directory format <p>{root_dir}/{category_str}/rgbd/{sequence_id}/... {ann_dir}/{sequence_id}.obj {ann_dir}/annotations.json</p> Source code in <code>sdfest/initialization/datasets/redwood_dataset.py</code> <pre><code>class AnnotatedRedwoodDataset(torch.utils.data.Dataset):\n    \"\"\"Dataset class for annotated Redwood dataset.\n\n    Data can be found here:\n    http://redwood-data.org/3dscan/index.html\n\n    Annotations are part SDFEst repo.\n\n    Expected directory format:\n        {root_dir}/{category_str}/rgbd/{sequence_id}/...\n        {ann_dir}/{sequence_id}.obj\n        {ann_dir}/annotations.json\n    \"\"\"\n\n    num_categories = 3\n    category_id_to_str = {\n        0: \"bottle\",\n        1: \"bowl\",\n        2: \"mug\",\n    }\n    category_str_to_id = {v: k for k, v in category_id_to_str.items()}\n\n    class Config(TypedDict, total=False):\n        \"\"\"Configuration dictionary for annoated Redwood dataset.\n\n        Attributes:\n            root_dir: See AnnotatedRedwoodDataset docstring.\n            ann_dir: See AnnotatedRedwoodDataset docstring.\n            mask_pointcloud: Whether the returned pointcloud will be masked.\n            normalize_pointcloud:\n                Whether the returned pointcloud and position will be normalized, such\n                that pointcloud centroid is at the origin.\n            scale_convention:\n                Which scale is returned. The following strings are supported:\n                    \"diagonal\":\n                        Length of bounding box' diagonal. This is what NOCS uses.\n                    \"max\": Maximum side length of bounding box.\n                    \"half_max\": Half maximum side length of bounding box.\n                    \"full\": Bounding box side lengths. Shape (3,).\n            camera_convention:\n                Which camera convention is used for position and orientation. One of:\n                    \"opengl\": x right, y up, z back\n                    \"opencv\": x right, y down, z forward\n                Note that this does not influence how the dataset is processed, only the\n                returned position and quaternion.\n            orientation_repr:\n                Which orientation representation is used. One of:\n                    \"quaternion\"\n                    \"discretized\"\n            orientation_grid_resolution:\n                Resolution of the orientation grid.\n                Only used if orientation_repr is \"discretized\".\n            remap_y_axis:\n                If not None, the Redwood y-axis will be mapped to the provided axis.\n                Resulting coordinate system will always be right-handed.\n                This is typically the up-axis.\n                Note that NOCS object models are NOT aligned the same as ShapeNetV2.\n                To get ShapeNetV2 alignment: -y\n                One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\"\n            remap_x_axis:\n                If not None, the original x-axis will be mapped to the provided axis.\n                Resulting coordinate system will always be right-handed.\n                Note that NOCS object models are NOT aligned the same as ShapeNetV2.\n                To get ShapeNetV2 alignment: z\n                One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\"\n            category_str:\n                If not None, only samples from the matching category will be returned.\n                See AnnotatedRedwoodDataset.category_id_to_str for admissible category\n                strings.\n        \"\"\"\n\n        root_dir: str\n        ann_dir: str\n        split: str\n        mask_pointcloud: bool\n        normalize_pointcloud: bool\n        scale_convention: str\n        camera_convention: str\n        orientation_repr: str\n        orientation_grid_resolution: int\n        remap_y_axis: Optional[str]\n        remap_x_axis: Optional[str]\n        category_str: Optional[str]\n\n    default_config: Config = {\n        \"root_dir\": None,\n        \"ann_dir\": None,\n        \"mask_pointcloud\": False,\n        \"normalize_pointcloud\": False,\n        \"camera_convention\": \"opengl\",\n        \"scale_convention\": \"half_max\",\n        \"orientation_repr\": \"quaternion\",\n        \"orientation_grid_resolution\": None,\n        \"category_str\": None,\n        \"remap_y_axis\": None,\n        \"remap_x_axis\": None,\n    }\n\n    def __init__(\n        self,\n        config: Config,\n    ) -&gt; None:\n        \"\"\"Initialize the dataset.\n\n        Args:\n            config:\n                Configuration dictionary of dataset. Provided dictionary will be merged\n                with default_dict. See AnnotatedRedwoodDataset.Config for keys.\n        \"\"\"\n        config = yoco.load_config(\n            config, current_dict=AnnotatedRedwoodDataset.default_config\n        )\n        self._root_dir = config[\"root_dir\"]\n        self._ann_dir = config[\"ann_dir\"]\n        self._camera_convention = config[\"camera_convention\"]\n        self._mask_pointcloud = config[\"mask_pointcloud\"]\n        self._normalize_pointcloud = config[\"normalize_pointcloud\"]\n        self._scale_convention = config[\"scale_convention\"]\n        self._remap_y_axis = config[\"remap_y_axis\"]\n        self._remap_x_axis = config[\"remap_x_axis\"]\n        self._orientation_repr = config[\"orientation_repr\"]\n        if self._orientation_repr == \"discretized\":\n            self._orientation_grid = so3grid.SO3Grid(\n                config[\"orientation_grid_resolution\"]\n            )\n        self._load_annotations()\n        self._camera = Camera(width=640, height=480, fx=525, fy=525, cx=319.5, cy=239.5)\n\n    def _load_annotations(self) -&gt; None:\n        \"\"\"Load annotations into memory.\"\"\"\n        ann_json = os.path.join(self._ann_dir, \"annotations.json\")\n        with open(ann_json, \"r\") as f:\n            anns_dict = json.load(f)\n        self._raw_samples = []\n        for seq_id, seq_anns in anns_dict.items():\n            for pose_ann in seq_anns[\"pose_anns\"]:\n                self._raw_samples.append(\n                    self._create_raw_sample(seq_id, seq_anns, pose_ann)\n                )\n\n    def _create_raw_sample(\n        self, seq_id: str, sequence_dict: dict, annotation_dict: dict\n    ) -&gt; dict:\n        \"\"\"Create raw sample from information in annotations file.\"\"\"\n        position = torch.tensor(annotation_dict[\"position\"])\n        orientation_q = torch.tensor(annotation_dict[\"orientation\"])\n        rgb_filename = annotation_dict[\"rgb_file\"]\n        depth_filename = annotation_dict[\"depth_file\"]\n        mesh_filename = sequence_dict[\"mesh\"]\n        mesh_path = os.path.join(self._ann_dir, mesh_filename)\n        category_str = sequence_dict[\"category\"]\n        color_path = os.path.join(\n            self._root_dir, category_str, \"rgbd\", seq_id, \"rgb\", rgb_filename\n        )\n        depth_path = os.path.join(\n            self._root_dir, category_str, \"rgbd\", seq_id, \"depth\", depth_filename\n        )\n        extents = torch.tensor(sequence_dict[\"scale\"]) * 2\n        return {\n            \"position\": position,\n            \"orientation_q\": orientation_q,\n            \"extents\": extents,\n            \"color_path\": color_path,\n            \"depth_path\": depth_path,\n            \"mesh_path\": mesh_path,\n            \"category_str\": category_str,\n        }\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return number of sample in dataset.\"\"\"\n        return len(self._raw_samples)\n\n    def __getitem__(self, idx: int) -&gt; dict:\n        \"\"\"Return a sample of the dataset.\n\n        Args:\n            idx: Index of the instance.\n        Returns:\n            Sample containing the following keys:\n                \"color\"\n                \"depth\"\n                \"mask\"\n                \"pointset\"\n                \"position\"\n                \"orientation\"\n                \"quaternion\"\n                \"scale\"\n                \"color_path\"\n                \"obj_path\"\n                \"category_id\"\n                \"category_str\"\n        \"\"\"\n        raw_sample = self._raw_samples[idx]\n        color = torch.from_numpy(\n            np.asarray(Image.open(raw_sample[\"color_path\"]), dtype=np.float32) / 255\n        )\n        depth = self._load_depth(raw_sample[\"depth_path\"])\n        instance_mask = self._compute_mask(depth, raw_sample)\n\n        pointcloud_mask = instance_mask if self._mask_pointcloud else None\n        pointcloud = pointset_utils.depth_to_pointcloud(\n            depth,\n            self._camera,\n            mask=pointcloud_mask,\n            convention=self._camera_convention,\n        )\n\n        # adjust camera convention for position, orientation and scale\n        position = pointset_utils.change_position_camera_convention(\n            raw_sample[\"position\"], \"opencv\", self._camera_convention\n        )\n\n        # orientation / scale\n        orientation_q, extents = self._change_axis_convention(\n            raw_sample[\"orientation_q\"], raw_sample[\"extents\"]\n        )\n        orientation_q = pointset_utils.change_orientation_camera_convention(\n            orientation_q, \"opencv\", self._camera_convention\n        )\n        orientation = self._quat_to_orientation_repr(orientation_q)\n        scale = self._get_scale(extents)\n\n        # normalize pointcloud &amp; position\n        if self._normalize_pointcloud:\n            pointcloud, centroid = pointset_utils.normalize_points(pointcloud)\n            position = position - centroid\n\n        category_str = raw_sample[\"category_str\"]\n        sample = {\n            \"color\": color,\n            \"depth\": depth,\n            \"pointset\": pointcloud,\n            \"mask\": instance_mask,\n            \"position\": position,\n            \"orientation\": orientation,\n            \"quaternion\": orientation_q,\n            \"scale\": scale,\n            \"color_path\": raw_sample[\"color_path\"],\n            \"obj_path\": raw_sample[\"mesh_path\"],\n            \"category_id\": self.category_str_to_id[category_str],\n            \"category_str\": category_str,\n        }\n        return sample\n\n    def _compute_mask(self, depth: torch.Tensor, raw_sample: dict) -&gt; torch.Tensor:\n        mesh = synthetic.Mesh(\n            path=raw_sample[\"mesh_path\"],\n            scale=1.0,  # do not resize mesh, as it is already at right size\n            rel_scale=True,\n            center=False,\n        )\n        mesh.position = raw_sample[\"position\"]\n        mesh.orientation = raw_sample[\"orientation_q\"]\n        gt_depth = torch.from_numpy(synthetic.draw_depth_geometry(mesh, self._camera))\n        mask = gt_depth != 0\n        # exclude occluded parts from mask\n        mask[(depth != 0) * (depth &lt; gt_depth - 0.01)] = 0\n        return mask\n\n    def _load_depth(self, depth_path: str) -&gt; torch.Tensor:\n        \"\"\"Load depth from depth filepath.\"\"\"\n        depth = torch.from_numpy(\n            np.asarray(Image.open(depth_path), dtype=np.float32) * 0.001\n        )\n        return depth\n\n    def _get_scale(self, extents: torch.Tensor) -&gt; float:\n        \"\"\"Return scale from stored sample data and extents.\"\"\"\n        if self._scale_convention == \"diagonal\":\n            return torch.linalg.norm(extents)\n        elif self._scale_convention == \"max\":\n            return extents.max()\n        elif self._scale_convention == \"half_max\":\n            return 0.5 * extents.max()\n        elif self._scale_convention == \"full\":\n            return extents\n        else:\n            raise ValueError(\n                f\"Specified scale convention {self._scale_convention} not supported.\"\n            )\n\n    def _change_axis_convention(\n        self, orientation_q: torch.Tensor, extents: torch.Tensor\n    ) -&gt; tuple:\n        \"\"\"Adjust up-axis for orientation and extents.\n\n        Returns:\n            Tuple of position, orienation_q and extents, with specified up-axis.\n        \"\"\"\n        if self._remap_y_axis is None and self._remap_x_axis is None:\n            return orientation_q, extents\n        elif self._remap_y_axis is None or self._remap_x_axis is None:\n            raise ValueError(\"Either both or none of remap_{y,x}_axis have to be None.\")\n\n        rotation_o2n = self._get_o2n_object_rotation_matrix()\n        remapped_extents = torch.abs(torch.Tensor(rotation_o2n) @ extents)\n\n        # quaternion so far: original -&gt; camera\n        # we want a quaternion: new -&gt; camera\n        rotation_n2o = rotation_o2n.T\n\n        quaternion_n2o = torch.from_numpy(Rotation.from_matrix(rotation_n2o).as_quat())\n\n        remapped_orientation_q = quaternion_utils.quaternion_multiply(\n            orientation_q, quaternion_n2o\n        )  # new -&gt; original -&gt; camera\n\n        return remapped_orientation_q, remapped_extents\n\n    def _get_o2n_object_rotation_matrix(self) -&gt; np.ndarray:\n        \"\"\"Compute rotation matrix which rotates original to new object coordinates.\"\"\"\n        rotation_o2n = np.zeros((3, 3))  # original to new object convention\n        if self._remap_y_axis == \"x\":\n            rotation_o2n[0, 1] = 1\n        elif self._remap_y_axis == \"-x\":\n            rotation_o2n[0, 1] = -1\n        elif self._remap_y_axis == \"y\":\n            rotation_o2n[1, 1] = 1\n        elif self._remap_y_axis == \"-y\":\n            rotation_o2n[1, 1] = -1\n        elif self._remap_y_axis == \"z\":\n            rotation_o2n[2, 1] = 1\n        elif self._remap_y_axis == \"-z\":\n            rotation_o2n[2, 1] = -1\n        else:\n            raise ValueError(\"Unsupported remap_y_axis {self.remap_y}\")\n\n        if self._remap_x_axis == \"x\":\n            rotation_o2n[0, 0] = 1\n        elif self._remap_x_axis == \"-x\":\n            rotation_o2n[0, 0] = -1\n        elif self._remap_x_axis == \"y\":\n            rotation_o2n[1, 0] = 1\n        elif self._remap_x_axis == \"-y\":\n            rotation_o2n[1, 0] = -1\n        elif self._remap_x_axis == \"z\":\n            rotation_o2n[2, 0] = 1\n        elif self._remap_x_axis == \"-z\":\n            rotation_o2n[2, 0] = -1\n        else:\n            raise ValueError(\"Unsupported remap_x_axis {self.remap_y}\")\n\n        # infer last column\n        rotation_o2n[:, 2] = 1 - np.abs(np.sum(rotation_o2n, 1))  # rows must sum to +-1\n        rotation_o2n[:, 2] *= np.linalg.det(rotation_o2n)  # make special orthogonal\n        if np.linalg.det(rotation_o2n) != 1.0:  # check if special orthogonal\n            raise ValueError(\"Unsupported combination of remap_{y,x}_axis. det != 1\")\n        return rotation_o2n\n\n    def _quat_to_orientation_repr(self, quaternion: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Convert quaternion to selected orientation representation.\n\n        Args:\n            quaternion:\n                The quaternion to convert, scalar-last, shape (4,).\n        Returns:\n            The same orientation as represented by the quaternion in the chosen\n            orientation representation.\n        \"\"\"\n        if self._orientation_repr == \"quaternion\":\n            return quaternion\n        elif self._orientation_repr == \"discretized\":\n            index = self._orientation_grid.quat_to_index(quaternion.numpy())\n            return torch.tensor(\n                index,\n                dtype=torch.long,\n            )\n        else:\n            raise NotImplementedError(\n                f\"Orientation representation {self._orientation_repr} is not supported.\"\n            )\n\n    def load_mesh(self, object_path: str) -&gt; o3d.geometry.TriangleMesh:\n        \"\"\"Load an object mesh and adjust its object frame convention.\"\"\"\n        mesh = o3d.io.read_triangle_mesh(object_path)\n        if self._remap_y_axis is None and self._remap_x_axis is None:\n            return mesh\n        elif self._remap_y_axis is None or self._remap_x_axis is None:\n            raise ValueError(\"Either both or none of remap_{y,x}_axis have to be None.\")\n\n        rotation_o2n = self._get_o2n_object_rotation_matrix()\n        mesh.rotate(\n            rotation_o2n,\n            center=np.array([0.0, 0.0, 0.0])[:, None],\n        )\n        return mesh\n</code></pre>"},{"location":"reference/initialization/datasets/redwood_dataset/#sdfest.initialization.datasets.redwood_dataset.AnnotatedRedwoodDataset.Config","title":"<code>Config</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Configuration dictionary for annoated Redwood dataset.</p> <p>Attributes:</p> Name Type Description <code>root_dir</code> <code>str</code> <p>See AnnotatedRedwoodDataset docstring.</p> <code>ann_dir</code> <code>str</code> <p>See AnnotatedRedwoodDataset docstring.</p> <code>mask_pointcloud</code> <code>bool</code> <p>Whether the returned pointcloud will be masked.</p> <code>normalize_pointcloud</code> <code>bool</code> <p>Whether the returned pointcloud and position will be normalized, such that pointcloud centroid is at the origin.</p> <code>scale_convention</code> <code>str</code> <p>Which scale is returned. The following strings are supported:     \"diagonal\":         Length of bounding box' diagonal. This is what NOCS uses.     \"max\": Maximum side length of bounding box.     \"half_max\": Half maximum side length of bounding box.     \"full\": Bounding box side lengths. Shape (3,).</p> <code>camera_convention</code> <code>str</code> <p>Which camera convention is used for position and orientation. One of:     \"opengl\": x right, y up, z back     \"opencv\": x right, y down, z forward Note that this does not influence how the dataset is processed, only the returned position and quaternion.</p> <code>orientation_repr</code> <code>str</code> <p>Which orientation representation is used. One of:     \"quaternion\"     \"discretized\"</p> <code>orientation_grid_resolution</code> <code>int</code> <p>Resolution of the orientation grid. Only used if orientation_repr is \"discretized\".</p> <code>remap_y_axis</code> <code>Optional[str]</code> <p>If not None, the Redwood y-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. This is typically the up-axis. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: -y One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\"</p> <code>remap_x_axis</code> <code>Optional[str]</code> <p>If not None, the original x-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: z One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\"</p> <code>category_str</code> <code>Optional[str]</code> <p>If not None, only samples from the matching category will be returned. See AnnotatedRedwoodDataset.category_id_to_str for admissible category strings.</p> Source code in <code>sdfest/initialization/datasets/redwood_dataset.py</code> <pre><code>class Config(TypedDict, total=False):\n    \"\"\"Configuration dictionary for annoated Redwood dataset.\n\n    Attributes:\n        root_dir: See AnnotatedRedwoodDataset docstring.\n        ann_dir: See AnnotatedRedwoodDataset docstring.\n        mask_pointcloud: Whether the returned pointcloud will be masked.\n        normalize_pointcloud:\n            Whether the returned pointcloud and position will be normalized, such\n            that pointcloud centroid is at the origin.\n        scale_convention:\n            Which scale is returned. The following strings are supported:\n                \"diagonal\":\n                    Length of bounding box' diagonal. This is what NOCS uses.\n                \"max\": Maximum side length of bounding box.\n                \"half_max\": Half maximum side length of bounding box.\n                \"full\": Bounding box side lengths. Shape (3,).\n        camera_convention:\n            Which camera convention is used for position and orientation. One of:\n                \"opengl\": x right, y up, z back\n                \"opencv\": x right, y down, z forward\n            Note that this does not influence how the dataset is processed, only the\n            returned position and quaternion.\n        orientation_repr:\n            Which orientation representation is used. One of:\n                \"quaternion\"\n                \"discretized\"\n        orientation_grid_resolution:\n            Resolution of the orientation grid.\n            Only used if orientation_repr is \"discretized\".\n        remap_y_axis:\n            If not None, the Redwood y-axis will be mapped to the provided axis.\n            Resulting coordinate system will always be right-handed.\n            This is typically the up-axis.\n            Note that NOCS object models are NOT aligned the same as ShapeNetV2.\n            To get ShapeNetV2 alignment: -y\n            One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\"\n        remap_x_axis:\n            If not None, the original x-axis will be mapped to the provided axis.\n            Resulting coordinate system will always be right-handed.\n            Note that NOCS object models are NOT aligned the same as ShapeNetV2.\n            To get ShapeNetV2 alignment: z\n            One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\"\n        category_str:\n            If not None, only samples from the matching category will be returned.\n            See AnnotatedRedwoodDataset.category_id_to_str for admissible category\n            strings.\n    \"\"\"\n\n    root_dir: str\n    ann_dir: str\n    split: str\n    mask_pointcloud: bool\n    normalize_pointcloud: bool\n    scale_convention: str\n    camera_convention: str\n    orientation_repr: str\n    orientation_grid_resolution: int\n    remap_y_axis: Optional[str]\n    remap_x_axis: Optional[str]\n    category_str: Optional[str]\n</code></pre>"},{"location":"reference/initialization/datasets/redwood_dataset/#sdfest.initialization.datasets.redwood_dataset.AnnotatedRedwoodDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Return a sample of the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the instance.</p> required <p>Returns:     Sample containing the following keys:         \"color\"         \"depth\"         \"mask\"         \"pointset\"         \"position\"         \"orientation\"         \"quaternion\"         \"scale\"         \"color_path\"         \"obj_path\"         \"category_id\"         \"category_str\"</p> Source code in <code>sdfest/initialization/datasets/redwood_dataset.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; dict:\n    \"\"\"Return a sample of the dataset.\n\n    Args:\n        idx: Index of the instance.\n    Returns:\n        Sample containing the following keys:\n            \"color\"\n            \"depth\"\n            \"mask\"\n            \"pointset\"\n            \"position\"\n            \"orientation\"\n            \"quaternion\"\n            \"scale\"\n            \"color_path\"\n            \"obj_path\"\n            \"category_id\"\n            \"category_str\"\n    \"\"\"\n    raw_sample = self._raw_samples[idx]\n    color = torch.from_numpy(\n        np.asarray(Image.open(raw_sample[\"color_path\"]), dtype=np.float32) / 255\n    )\n    depth = self._load_depth(raw_sample[\"depth_path\"])\n    instance_mask = self._compute_mask(depth, raw_sample)\n\n    pointcloud_mask = instance_mask if self._mask_pointcloud else None\n    pointcloud = pointset_utils.depth_to_pointcloud(\n        depth,\n        self._camera,\n        mask=pointcloud_mask,\n        convention=self._camera_convention,\n    )\n\n    # adjust camera convention for position, orientation and scale\n    position = pointset_utils.change_position_camera_convention(\n        raw_sample[\"position\"], \"opencv\", self._camera_convention\n    )\n\n    # orientation / scale\n    orientation_q, extents = self._change_axis_convention(\n        raw_sample[\"orientation_q\"], raw_sample[\"extents\"]\n    )\n    orientation_q = pointset_utils.change_orientation_camera_convention(\n        orientation_q, \"opencv\", self._camera_convention\n    )\n    orientation = self._quat_to_orientation_repr(orientation_q)\n    scale = self._get_scale(extents)\n\n    # normalize pointcloud &amp; position\n    if self._normalize_pointcloud:\n        pointcloud, centroid = pointset_utils.normalize_points(pointcloud)\n        position = position - centroid\n\n    category_str = raw_sample[\"category_str\"]\n    sample = {\n        \"color\": color,\n        \"depth\": depth,\n        \"pointset\": pointcloud,\n        \"mask\": instance_mask,\n        \"position\": position,\n        \"orientation\": orientation,\n        \"quaternion\": orientation_q,\n        \"scale\": scale,\n        \"color_path\": raw_sample[\"color_path\"],\n        \"obj_path\": raw_sample[\"mesh_path\"],\n        \"category_id\": self.category_str_to_id[category_str],\n        \"category_str\": category_str,\n    }\n    return sample\n</code></pre>"},{"location":"reference/initialization/datasets/redwood_dataset/#sdfest.initialization.datasets.redwood_dataset.AnnotatedRedwoodDataset.__init__","title":"<code>__init__(config)</code>","text":"<p>Initialize the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>Configuration dictionary of dataset. Provided dictionary will be merged with default_dict. See AnnotatedRedwoodDataset.Config for keys.</p> required Source code in <code>sdfest/initialization/datasets/redwood_dataset.py</code> <pre><code>def __init__(\n    self,\n    config: Config,\n) -&gt; None:\n    \"\"\"Initialize the dataset.\n\n    Args:\n        config:\n            Configuration dictionary of dataset. Provided dictionary will be merged\n            with default_dict. See AnnotatedRedwoodDataset.Config for keys.\n    \"\"\"\n    config = yoco.load_config(\n        config, current_dict=AnnotatedRedwoodDataset.default_config\n    )\n    self._root_dir = config[\"root_dir\"]\n    self._ann_dir = config[\"ann_dir\"]\n    self._camera_convention = config[\"camera_convention\"]\n    self._mask_pointcloud = config[\"mask_pointcloud\"]\n    self._normalize_pointcloud = config[\"normalize_pointcloud\"]\n    self._scale_convention = config[\"scale_convention\"]\n    self._remap_y_axis = config[\"remap_y_axis\"]\n    self._remap_x_axis = config[\"remap_x_axis\"]\n    self._orientation_repr = config[\"orientation_repr\"]\n    if self._orientation_repr == \"discretized\":\n        self._orientation_grid = so3grid.SO3Grid(\n            config[\"orientation_grid_resolution\"]\n        )\n    self._load_annotations()\n    self._camera = Camera(width=640, height=480, fx=525, fy=525, cx=319.5, cy=239.5)\n</code></pre>"},{"location":"reference/initialization/datasets/redwood_dataset/#sdfest.initialization.datasets.redwood_dataset.AnnotatedRedwoodDataset.__len__","title":"<code>__len__()</code>","text":"<p>Return number of sample in dataset.</p> Source code in <code>sdfest/initialization/datasets/redwood_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return number of sample in dataset.\"\"\"\n    return len(self._raw_samples)\n</code></pre>"},{"location":"reference/initialization/datasets/redwood_dataset/#sdfest.initialization.datasets.redwood_dataset.AnnotatedRedwoodDataset.load_mesh","title":"<code>load_mesh(object_path)</code>","text":"<p>Load an object mesh and adjust its object frame convention.</p> Source code in <code>sdfest/initialization/datasets/redwood_dataset.py</code> <pre><code>def load_mesh(self, object_path: str) -&gt; o3d.geometry.TriangleMesh:\n    \"\"\"Load an object mesh and adjust its object frame convention.\"\"\"\n    mesh = o3d.io.read_triangle_mesh(object_path)\n    if self._remap_y_axis is None and self._remap_x_axis is None:\n        return mesh\n    elif self._remap_y_axis is None or self._remap_x_axis is None:\n        raise ValueError(\"Either both or none of remap_{y,x}_axis have to be None.\")\n\n    rotation_o2n = self._get_o2n_object_rotation_matrix()\n    mesh.rotate(\n        rotation_o2n,\n        center=np.array([0.0, 0.0, 0.0])[:, None],\n    )\n    return mesh\n</code></pre>"},{"location":"reference/initialization/datasets/redwood_dataset/#sdfest.initialization.datasets.redwood_dataset.ObjectError","title":"<code>ObjectError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Error if something with the mesh is wrong.</p> Source code in <code>sdfest/initialization/datasets/redwood_dataset.py</code> <pre><code>class ObjectError(Exception):\n    \"\"\"Error if something with the mesh is wrong.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/initialization/scripts/train/","title":"train","text":"<p>Script to train initialization model.</p>"},{"location":"reference/initialization/scripts/train/#sdfest.initialization.scripts.train.Trainer","title":"<code>Trainer</code>","text":"<p>Trainer for single shot pose and shape estimation network.</p> Source code in <code>sdfest/initialization/scripts/train.py</code> <pre><code>class Trainer:\n    \"\"\"Trainer for single shot pose and shape estimation network.\"\"\"\n\n    def __init__(self, config: dict) -&gt; None:\n        \"\"\"Construct trainer.\n\n        Args:\n            config: The configuration for model and training.\n        \"\"\"\n        self._read_config(config)\n\n    def _read_config(self, config: dict) -&gt; None:\n        self._config = config\n        self._validation_iteration = config[\"validation_iteration\"]\n        self._visualization_iteration = config[\"visualization_iteration\"]\n        self._checkpoint_iteration = config[\"checkpoint_iteration\"]\n        self._iterations = config[\"iterations\"]\n        self._init_weights_path = (\n            config[\"init_weights\"] if \"init_weights\" in config else None\n        )\n\n        # propagate orientation representation and category to datasets\n        datasets = list(self._config[\"datasets\"].values()) + list(\n            self._config[\"validation_datasets\"].values()\n        )\n        for dataset in datasets:\n            dataset[\"config_dict\"][\"orientation_repr\"] = config[\"orientation_repr\"]\n            if \"orientation_grid_resolution\" in config:\n                dataset[\"config_dict\"][\"orientation_grid_resolution\"] = config[\n                    \"orientation_grid_resolution\"\n                ]\n            if \"category_str\" in config:\n                dataset[\"config_dict\"][\"category_str\"] = config[\"category_str\"]\n\n        # propagate orientation representation to init head\n        self._config[\"head\"][\"orientation_repr\"] = config[\"orientation_repr\"]\n        if \"orientation_grid_resolution\" in config:\n            self._config[\"head\"][\"orientation_grid_resolution\"] = config[\n                \"orientation_grid_resolution\"\n            ]\n\n    def run(self) -&gt; None:\n        \"\"\"Train the model.\"\"\"\n        wandb.init(project=\"sdfest.initialization\", config=self._config)\n\n        self._device = self.get_device()\n\n        # init dataset and dataloader\n        self.vae = self.create_sdfvae()\n\n        # init model to train\n        self._sdf_pose_net = SDFPoseNet(\n            backbone=MODULE_DICT[self._config[\"backbone_type\"]](\n                **self._config[\"backbone\"]\n            ),\n            head=MODULE_DICT[self._config[\"head_type\"]](\n                shape_dimension=self._config[\"vae\"][\"latent_size\"],\n                **self._config[\"head\"],\n            ),\n        ).to(self._device)\n        self._sdf_pose_net.train()\n\n        # deterministic samples (needs to be done after model initialization, as it\n        # can have varying number of parameters)\n        torch.manual_seed(0)\n        random.seed(torch.initial_seed())  # to get deterministic examples\n\n        # print network summary\n        torchinfo.summary(self._sdf_pose_net, (1, 500, 3), device=self._device)\n\n        # init optimizer\n        self._optimizer = torch.optim.Adam(\n            self._sdf_pose_net.parameters(), lr=self._config[\"learning_rate\"]\n        )\n\n        # load weights if provided\n        if self._init_weights_path is not None:\n            state_dict = torch.load(self._init_weights_path, map_location=self._device)\n            self._sdf_pose_net.load_state_dict(state_dict)\n\n        self._current_iteration = 0\n        self._run_name = (\n            f\"sdfest.initialization_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S-%f')}\"\n        )\n\n        wandb.config.run_name = (\n            self._run_name  # to allow association of pt files with wandb runs\n        )\n\n        self._model_base_path = os.path.join(os.getcwd(), \"models\", self._run_name)\n\n        self._multi_data_loader = self._create_multi_data_loader()\n        self._validation_data_loader_dict = self._create_validation_data_loader_dict()\n\n        # backup config to model directory\n        os.makedirs(self._model_base_path, exist_ok=True)\n        config_path = os.path.join(self._model_base_path, \"config.yaml\")\n        yoco.save_config_to_file(config_path, self._config)\n\n        self._start_time = time.time()\n        for samples in self._multi_data_loader:\n            self._current_iteration += 1\n            print(f\"Current iteration: {self._current_iteration}\\033[K\", end=\"\\r\")\n            self._update_progress()\n\n            samples = utils.dict_to(samples, self._device)\n\n            latent_shape, position, scale, orientation = self._sdf_pose_net(\n                samples[\"pointset\"]\n            )\n            predictions = {\n                \"latent_shape\": latent_shape,\n                \"position\": position,\n                \"scale\": scale,\n                \"orientation\": orientation,\n            }\n\n            loss = self._compute_loss(samples, predictions)\n            self._optimizer.zero_grad()\n            loss.backward()\n            self._optimizer.step()\n\n            with torch.no_grad():\n                # samples_dict = defaultdict(lambda: dict())\n                # for k,vs in samples.items():\n                #     for i, v in enumerate(vs):\n                #         samples_dict[i][k] = v\n                # for sample in samples_dict.values():\n                #     utils.visualize_sample(sample, None)\n\n                self._compute_metrics(samples, predictions)\n\n                if self._current_iteration % self._visualization_iteration == 0:\n                    self._generate_visualizations()\n\n                if self._current_iteration % self._validation_iteration == 0:\n                    self._compute_validation_metrics()\n\n            if self._current_iteration % self._checkpoint_iteration == 0:\n                self._save_checkpoint()\n\n            if self._current_iteration &gt;= self._iterations:\n                break\n\n        now = time.time()\n        print(f\"Training finished after {now-self._start_time} seconds.\")\n\n        # save the final model\n        torch.save(\n            self._sdf_pose_net.state_dict(),\n            os.path.join(wandb.run.dir, f\"{wandb.run.name}.pt\"),\n        )\n        config_path = os.path.join(wandb.run.dir, f\"{wandb.run.name}.yaml\")\n        self._config[\"model\"] = os.path.join(\".\", f\"{wandb.run.name}.pt\")\n        yoco.save_config_to_file(config_path, self._config)\n\n    def get_device(self) -&gt; torch.device:\n        \"\"\"Create device based on config.\"\"\"\n        if \"device\" not in self._config or self._config[\"device\"] is None:\n            return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        return torch.device(self._config[\"device\"])\n\n    def create_sdfvae(self) -&gt; SDFVAE:\n        \"\"\"Create SDFVAE based on config.\n\n        Returns:\n            The SDFVAE on the specified device, with weights from specified model.\n        \"\"\"\n        model_url = self._config[\"vae\"].get(\"model_url\")\n        device = self.get_device()\n        vae = SDFVAE(\n            sdf_size=64,\n            latent_size=self._config[\"vae\"][\"latent_size\"],\n            encoder_dict=self._config[\"vae\"][\"encoder\"],\n            decoder_dict=self._config[\"vae\"][\"decoder\"],\n            device=device,\n        ).to(device)\n        load_model_weights(self._config[\"vae\"][\"model\"], vae, device, model_url)\n        vae.eval()\n        return vae\n\n    def _compute_loss(\n        self,\n        samples: dict,\n        predictions: dict,\n    ) -&gt; torch.Tensor:\n        \"\"\"Compute total loss.\n\n        Args:\n            samples:\n                Samples dictionary containing a subset of the following keys:\n                    \"latent_shape\": Shape (N,D).\n                    \"position\": Shape (N,3).\n                    \"scale\": Shape (N,).\n                    \"orientation\":\n                        Shape (N,4) for quaternion representation.\n                        Shape (N,) for discretized representation.\n            predictions: Dictionary containing the following keys:\n                \"latent_shape\": Shape (N,D).\n                \"position\": Shape (N,3).\n                \"scale\": Shape (N,).\n                \"orientation\":\n                    Shape (N,4) for quaternion representation.\n                    Shape (N,R) for discretized representation.\n        Returns:\n            The combined loss. Scalar.\n        \"\"\"\n        log_dict = {}\n\n        loss = 0\n\n        if \"latent_shape\" in samples:\n            loss_latent_l2 = torch.nn.functional.mse_loss(\n                predictions[\"latent_shape\"], samples[\"latent_shape\"]\n            )\n            log_dict[\"loss latent\"] = loss_latent_l2.item()\n            loss = loss + self._config[\"latent_weight\"] * loss_latent_l2\n\n        if \"position\" in samples:\n            loss_position_l2 = torch.nn.functional.mse_loss(\n                predictions[\"position\"], samples[\"position\"]\n            )\n            log_dict[\"loss position\"] = loss_position_l2.item()\n            loss = loss + self._config[\"position_weight\"] * loss_position_l2\n\n        if \"scale\" in samples:\n            loss_scale_l2 = torch.nn.functional.mse_loss(\n                predictions[\"scale\"], samples[\"scale\"]\n            )\n            log_dict[\"loss scale\"] = loss_scale_l2.item()\n            loss = loss + self._config[\"scale_weight\"] * loss_scale_l2\n\n        if \"orientation\" in samples:\n            if self._config[\"head\"][\"orientation_repr\"] == \"quaternion\":\n                loss_orientation = quaternion_utils.simple_quaternion_loss(\n                    predictions[\"orientation\"], samples[\"orientation\"]\n                )\n            elif self._config[\"head\"][\"orientation_repr\"] == \"discretized\":\n                loss_orientation = torch.nn.functional.cross_entropy(\n                    predictions[\"orientation\"], samples[\"orientation\"]\n                )\n            else:\n                raise NotImplementedError(\n                    \"Orientation repr \"\n                    f\"{self._config['head']['orientation_repr']}\"\n                    \" not supported.\"\n                )\n            log_dict[\"loss orientation\"] = loss_orientation.item()\n            loss = loss + self._config[\"orientation_weight\"] * loss_orientation\n\n        log_dict[\"total loss\"] = loss.item()\n\n        wandb.log(\n            log_dict,\n            step=self._current_iteration,\n        )\n\n        return loss\n\n    def _create_multi_data_loader(self) -&gt; dataset_utils.MultiDataLoader:\n        data_loaders = []\n        probabilities = []\n        for dataset_dict in self._config[\"datasets\"].values():\n            if dataset_dict[\"probability\"] == 0.0:\n                continue\n            dataset = self._create_dataset(\n                dataset_dict[\"type\"], dataset_dict[\"config_dict\"]\n            )\n            num_workers = 8 if dataset_dict[\"type\"] != \"SDFVAEViewDataset\" else 0\n            shuffle = (\n                False if isinstance(dataset, torch.utils.data.IterableDataset) else True\n            )\n            probabilities.append(dataset_dict[\"probability\"])\n            data_loader = torch.utils.data.DataLoader(\n                dataset=dataset,\n                batch_size=self._config[\"batch_size\"],\n                collate_fn=dataset_utils.collate_samples,\n                drop_last=True,\n                shuffle=shuffle,\n                num_workers=num_workers,\n            )\n            data_loaders.append(data_loader)\n        return dataset_utils.MultiDataLoader(data_loaders, probabilities)\n\n    def _create_validation_data_loader_dict(self) -&gt; dict:\n        data_loader_dict = {}\n        for dataset_name, dataset_dict in self._config[\"validation_datasets\"].items():\n            dataset = self._create_dataset(\n                dataset_dict[\"type\"], dataset_dict[\"config_dict\"]\n            )\n            data_loader = torch.utils.data.DataLoader(\n                dataset=dataset,\n                batch_size=self._config[\"batch_size\"],\n                collate_fn=dataset_utils.collate_samples,\n                num_workers=12,\n            )\n            data_loader_dict[dataset_name] = data_loader\n        return data_loader_dict\n\n    def _create_dataset(\n        self, type_str: str, config_dict: dict\n    ) -&gt; torch.utils.data.Dataset:\n        dataset_type = utils.str_to_object(type_str)\n        if dataset_type == SDFVAEViewDataset:\n            dataset = dataset_type(\n                config=config_dict,\n                vae=self.vae,\n            )\n        elif dataset_type is not None:\n            dataset = dataset_type(config=config_dict)\n        else:\n            raise NotImplementedError(f\"Dataset type {type_str} not supported.\")\n        return dataset\n\n    def _mean_geodesic_distance(self, samples: dict, predictions: dict) -&gt; torch.Tensor:\n        target_quaternions = samples[\"quaternion\"]\n        if self._config[\"head\"][\"orientation_repr\"] == \"quaternion\":\n            predicted_quaternions = predictions[\"orientation\"]\n        elif self._config[\"head\"][\"orientation_repr\"] == \"discretized\":\n            predicted_quaternions = torch.empty_like(target_quaternions)\n            for i, v in enumerate(predictions[\"orientation\"]):\n                index = v.argmax().item()\n                quat = self._sdf_pose_net._head._grid.index_to_quat(index)\n                predicted_quaternions[i, :] = torch.tensor(quat)\n        else:\n            raise NotImplementedError(\n                \"Orientation representation \"\n                f\"{self._config['head']['orientation_repr']}\"\n                \" is not supported\"\n            )\n        geodesic_distances = quaternion_utils.geodesic_distance(\n            target_quaternions, predicted_quaternions\n        )\n        return torch.mean(geodesic_distances)\n\n    def _compute_metrics(self, samples: dict, predictions: dict) -&gt; None:\n        # compute metrics / i.e., loss and representation independent metrics\n        # extract quaternion from orientation representation\n        geodesic_distance = self._mean_geodesic_distance(samples, predictions)\n        wandb.log(\n            {\n                \"metric geodesic distance\": geodesic_distance.item(),\n            },\n            step=self._current_iteration,\n        )\n\n    def _generate_visualizations(self) -&gt; None:\n        # generate visualizations\n        if self._current_iteration % self._visualization_iteration == 0:\n            # generate unseen input and target\n            samples = next(iter(self._multi_data_loader))\n            samples = utils.dict_to(samples, self._device)\n            predictions = self._sdf_pose_net(samples[\"pointset\"])\n            input_pointcloud = samples[\"pointset\"][0].detach().cpu().numpy()\n            input_pointcloud = np.hstack(\n                (\n                    input_pointcloud,\n                    np.full((input_pointcloud.shape[0], 1), 0),\n                )\n            )\n            output_sdfs = self.vae.decode(predictions[0])\n            output_sdf = output_sdfs[0][0].detach().cpu().numpy()\n            output_position = predictions[1][0].detach().cpu().numpy()\n            output_scale = predictions[2][0].detach().cpu().numpy()\n            if self._config[\"head\"][\"orientation_repr\"] == \"quaternion\":\n                output_quaternion = predictions[3][0].detach().cpu().numpy()\n            elif self._config[\"head\"][\"orientation_repr\"] == \"discretized\":\n                index = predictions[3][0].argmax().item()\n                output_quaternion = self._sdf_pose_net._head._grid.index_to_quat(index)\n            else:\n                raise NotImplementedError(\n                    \"Orientation representation \"\n                    f\"{self._config['head']['orientation_repr']}\"\n                    \" is not supported\"\n                )\n            output_pointcloud = sdf_utils.sdf_to_pointcloud(\n                output_sdf, output_position, output_quaternion, output_scale\n            )\n            output_pointcloud = np.hstack(\n                (\n                    output_pointcloud,\n                    np.full((output_pointcloud.shape[0], 1), 1),\n                )\n            )\n            pointcloud = np.vstack((input_pointcloud, output_pointcloud))\n\n            wandb.log(\n                {\"point_cloud\": wandb.Object3D(pointcloud)},\n                step=self._current_iteration,\n            )\n\n            output_pointcloud = sdf_utils.sdf_to_pointcloud(\n                output_sdf,\n                samples[\"position\"][0].detach().cpu().numpy(),\n                samples[\"quaternion\"][0].detach().cpu().numpy(),\n                samples[\"scale\"][0].detach().cpu().numpy(),\n            )\n            output_pointcloud = np.hstack(\n                (\n                    output_pointcloud,\n                    np.full((output_pointcloud.shape[0], 1), 1),\n                )\n            )\n            pointcloud = np.vstack((input_pointcloud, output_pointcloud))\n            wandb.log(\n                {\"point_cloud gt pose\": wandb.Object3D(pointcloud)},\n                step=self._current_iteration,\n            )\n\n    def _compute_validation_metrics(self) -&gt; None:\n        self._sdf_pose_net.eval()\n        for name, data_loader in self._validation_data_loader_dict.items():\n            metrics_dict = defaultdict(lambda: 0)\n            sample_count = 0\n            for samples in tqdm(data_loader, desc=\"Validation\"):\n                batch_size = samples[\"position\"].shape[0]\n                samples = utils.dict_to(samples, self._device)\n                latent_shape, position, scale, orientation = self._sdf_pose_net(\n                    samples[\"pointset\"]\n                )\n                predictions = {\n                    \"latent_shape\": latent_shape,\n                    \"position\": position,\n                    \"scale\": scale,\n                    \"orientation\": orientation,\n                }\n                euclidean_distance = torch.linalg.norm(\n                    predictions[\"position\"] - samples[\"position\"], dim=1\n                )\n                metrics_dict[f\"{name} validation mean position error / m\"] += torch.sum(\n                    euclidean_distance\n                ).item()\n                metrics_dict[f\"{name} validation mean scale error / m\"] += torch.sum(\n                    torch.abs(predictions[\"scale\"] - samples[\"scale\"])\n                ).item()\n                metrics_dict[f\"{name} validation mean geodesic_distance / rad\"] += (\n                    self._mean_geodesic_distance(samples, predictions).item()\n                    * batch_size\n                )\n                sample_count += batch_size\n                if self._config[\"head\"][\"orientation_repr\"] == \"discretized\":\n                    metrics_dict[\n                        f\"{name} validation orientation mean NLL\"\n                    ] += torch.nn.functional.cross_entropy(\n                        predictions[\"orientation\"],\n                        samples[\"orientation\"],\n                        reduction=\"sum\",\n                    )\n            for metric_name in metrics_dict:\n                metrics_dict[metric_name] /= sample_count\n            wandb.log(metrics_dict, step=self._current_iteration)\n        self._sdf_pose_net.train()\n\n    def _save_checkpoint(self) -&gt; None:\n        checkpoint_path = os.path.join(\n            self._model_base_path, f\"{self._current_iteration}.pt\"\n        )\n        torch.save(\n            self._sdf_pose_net.state_dict(),\n            checkpoint_path,\n        )\n\n    def _update_progress(self) -&gt; None:\n        current_time = time.time()\n        duration = current_time - self._start_time\n        iterations_per_sec = self._current_iteration / duration\n        if self._current_iteration &gt; 10:\n            remaining_iterations = self._iterations - self._current_iteration\n            remaining_secs = remaining_iterations / iterations_per_sec\n            remaining_time_str = str(timedelta(seconds=round(remaining_secs)))\n        else:\n            remaining_time_str = \"N/A\"\n        print(\n            f\"Current iteration: {self._current_iteration:&gt;10} / {self._iterations}\"\n            f\" {self._current_iteration / self._iterations * 100:&gt;6.2f}%\"\n            f\" Remaining time: {remaining_time_str}\"  # remaining time\n            \"\\033[K\",  # clear until end of line\n            end=\"\\r\",  # overwrite previous\n        )\n</code></pre>"},{"location":"reference/initialization/scripts/train/#sdfest.initialization.scripts.train.Trainer.__init__","title":"<code>__init__(config)</code>","text":"<p>Construct trainer.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>The configuration for model and training.</p> required Source code in <code>sdfest/initialization/scripts/train.py</code> <pre><code>def __init__(self, config: dict) -&gt; None:\n    \"\"\"Construct trainer.\n\n    Args:\n        config: The configuration for model and training.\n    \"\"\"\n    self._read_config(config)\n</code></pre>"},{"location":"reference/initialization/scripts/train/#sdfest.initialization.scripts.train.Trainer.create_sdfvae","title":"<code>create_sdfvae()</code>","text":"<p>Create SDFVAE based on config.</p> <p>Returns:</p> Type Description <code>SDFVAE</code> <p>The SDFVAE on the specified device, with weights from specified model.</p> Source code in <code>sdfest/initialization/scripts/train.py</code> <pre><code>def create_sdfvae(self) -&gt; SDFVAE:\n    \"\"\"Create SDFVAE based on config.\n\n    Returns:\n        The SDFVAE on the specified device, with weights from specified model.\n    \"\"\"\n    model_url = self._config[\"vae\"].get(\"model_url\")\n    device = self.get_device()\n    vae = SDFVAE(\n        sdf_size=64,\n        latent_size=self._config[\"vae\"][\"latent_size\"],\n        encoder_dict=self._config[\"vae\"][\"encoder\"],\n        decoder_dict=self._config[\"vae\"][\"decoder\"],\n        device=device,\n    ).to(device)\n    load_model_weights(self._config[\"vae\"][\"model\"], vae, device, model_url)\n    vae.eval()\n    return vae\n</code></pre>"},{"location":"reference/initialization/scripts/train/#sdfest.initialization.scripts.train.Trainer.get_device","title":"<code>get_device()</code>","text":"<p>Create device based on config.</p> Source code in <code>sdfest/initialization/scripts/train.py</code> <pre><code>def get_device(self) -&gt; torch.device:\n    \"\"\"Create device based on config.\"\"\"\n    if \"device\" not in self._config or self._config[\"device\"] is None:\n        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    return torch.device(self._config[\"device\"])\n</code></pre>"},{"location":"reference/initialization/scripts/train/#sdfest.initialization.scripts.train.Trainer.run","title":"<code>run()</code>","text":"<p>Train the model.</p> Source code in <code>sdfest/initialization/scripts/train.py</code> <pre><code>def run(self) -&gt; None:\n    \"\"\"Train the model.\"\"\"\n    wandb.init(project=\"sdfest.initialization\", config=self._config)\n\n    self._device = self.get_device()\n\n    # init dataset and dataloader\n    self.vae = self.create_sdfvae()\n\n    # init model to train\n    self._sdf_pose_net = SDFPoseNet(\n        backbone=MODULE_DICT[self._config[\"backbone_type\"]](\n            **self._config[\"backbone\"]\n        ),\n        head=MODULE_DICT[self._config[\"head_type\"]](\n            shape_dimension=self._config[\"vae\"][\"latent_size\"],\n            **self._config[\"head\"],\n        ),\n    ).to(self._device)\n    self._sdf_pose_net.train()\n\n    # deterministic samples (needs to be done after model initialization, as it\n    # can have varying number of parameters)\n    torch.manual_seed(0)\n    random.seed(torch.initial_seed())  # to get deterministic examples\n\n    # print network summary\n    torchinfo.summary(self._sdf_pose_net, (1, 500, 3), device=self._device)\n\n    # init optimizer\n    self._optimizer = torch.optim.Adam(\n        self._sdf_pose_net.parameters(), lr=self._config[\"learning_rate\"]\n    )\n\n    # load weights if provided\n    if self._init_weights_path is not None:\n        state_dict = torch.load(self._init_weights_path, map_location=self._device)\n        self._sdf_pose_net.load_state_dict(state_dict)\n\n    self._current_iteration = 0\n    self._run_name = (\n        f\"sdfest.initialization_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S-%f')}\"\n    )\n\n    wandb.config.run_name = (\n        self._run_name  # to allow association of pt files with wandb runs\n    )\n\n    self._model_base_path = os.path.join(os.getcwd(), \"models\", self._run_name)\n\n    self._multi_data_loader = self._create_multi_data_loader()\n    self._validation_data_loader_dict = self._create_validation_data_loader_dict()\n\n    # backup config to model directory\n    os.makedirs(self._model_base_path, exist_ok=True)\n    config_path = os.path.join(self._model_base_path, \"config.yaml\")\n    yoco.save_config_to_file(config_path, self._config)\n\n    self._start_time = time.time()\n    for samples in self._multi_data_loader:\n        self._current_iteration += 1\n        print(f\"Current iteration: {self._current_iteration}\\033[K\", end=\"\\r\")\n        self._update_progress()\n\n        samples = utils.dict_to(samples, self._device)\n\n        latent_shape, position, scale, orientation = self._sdf_pose_net(\n            samples[\"pointset\"]\n        )\n        predictions = {\n            \"latent_shape\": latent_shape,\n            \"position\": position,\n            \"scale\": scale,\n            \"orientation\": orientation,\n        }\n\n        loss = self._compute_loss(samples, predictions)\n        self._optimizer.zero_grad()\n        loss.backward()\n        self._optimizer.step()\n\n        with torch.no_grad():\n            # samples_dict = defaultdict(lambda: dict())\n            # for k,vs in samples.items():\n            #     for i, v in enumerate(vs):\n            #         samples_dict[i][k] = v\n            # for sample in samples_dict.values():\n            #     utils.visualize_sample(sample, None)\n\n            self._compute_metrics(samples, predictions)\n\n            if self._current_iteration % self._visualization_iteration == 0:\n                self._generate_visualizations()\n\n            if self._current_iteration % self._validation_iteration == 0:\n                self._compute_validation_metrics()\n\n        if self._current_iteration % self._checkpoint_iteration == 0:\n            self._save_checkpoint()\n\n        if self._current_iteration &gt;= self._iterations:\n            break\n\n    now = time.time()\n    print(f\"Training finished after {now-self._start_time} seconds.\")\n\n    # save the final model\n    torch.save(\n        self._sdf_pose_net.state_dict(),\n        os.path.join(wandb.run.dir, f\"{wandb.run.name}.pt\"),\n    )\n    config_path = os.path.join(wandb.run.dir, f\"{wandb.run.name}.yaml\")\n    self._config[\"model\"] = os.path.join(\".\", f\"{wandb.run.name}.pt\")\n    yoco.save_config_to_file(config_path, self._config)\n</code></pre>"},{"location":"reference/initialization/scripts/train/#sdfest.initialization.scripts.train.main","title":"<code>main()</code>","text":"<p>Entry point of the program.</p> Source code in <code>sdfest/initialization/scripts/train.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Entry point of the program.\"\"\"\n    # define the arguments\n    parser = argparse.ArgumentParser(description=\"Training script for init network.\")\n    parser.add_argument(\"--config\", default=\"configs/default.yaml\", nargs=\"+\")\n    config = yoco.load_config_from_args(\n        parser, search_paths=[\".\", \"~/.sdfest/\", sdfest.__path__[0]]\n    )\n    trainer = Trainer(config)\n    trainer.run()\n</code></pre>"},{"location":"reference/vae/sdf_dataset/","title":"sdf_dataset","text":"<p>Module which provides SDFDataset class.</p>"},{"location":"reference/vae/sdf_dataset/#sdfest.vae.sdf_dataset.SDFDataset","title":"<code>SDFDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset of SDF volumes stored in .npy format.</p> Expected dataset format <p>{sdf_folder}/00000.npy {sdf_folder}/00001.npy ...</p> Source code in <code>sdfest/vae/sdf_dataset.py</code> <pre><code>class SDFDataset(torch.utils.data.Dataset):\n    \"\"\"Dataset of SDF volumes stored in .npy format.\n\n    Expected dataset format:\n        {sdf_folder}/00000.npy\n        {sdf_folder}/00001.npy\n        ...\n    \"\"\"\n\n    def __init__(self, sdf_folder: str):\n        \"\"\"Construct the dataset.\n\n        Args:\n            sdf_folder: The folder containing the npy files.\n        \"\"\"\n        self.path = sdf_folder\n\n        self.size = len([f for f in os.listdir(sdf_folder) if f.endswith(\".npy\")])\n\n    def __len__(self):\n        \"\"\"Return the number of images in the dataset.\"\"\"\n        return self.size\n\n    def __getitem__(self, idx: int) -&gt; torch.Tensor:\n        \"\"\"Return SDF volume at a specific index.\n\n        Args:\n            idx: The index of the sdf file to retrieve.\n        Returns:\n            The loaded SDF volume.\n        \"\"\"\n        sdf_path = os.path.join(self.path, f\"{idx:05}.npy\")\n        sdf_np = np.load(sdf_path)\n        return torch.as_tensor(sdf_np).unsqueeze(0)\n</code></pre>"},{"location":"reference/vae/sdf_dataset/#sdfest.vae.sdf_dataset.SDFDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Return SDF volume at a specific index.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>The index of the sdf file to retrieve.</p> required <p>Returns:     The loaded SDF volume.</p> Source code in <code>sdfest/vae/sdf_dataset.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; torch.Tensor:\n    \"\"\"Return SDF volume at a specific index.\n\n    Args:\n        idx: The index of the sdf file to retrieve.\n    Returns:\n        The loaded SDF volume.\n    \"\"\"\n    sdf_path = os.path.join(self.path, f\"{idx:05}.npy\")\n    sdf_np = np.load(sdf_path)\n    return torch.as_tensor(sdf_np).unsqueeze(0)\n</code></pre>"},{"location":"reference/vae/sdf_dataset/#sdfest.vae.sdf_dataset.SDFDataset.__init__","title":"<code>__init__(sdf_folder)</code>","text":"<p>Construct the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sdf_folder</code> <code>str</code> <p>The folder containing the npy files.</p> required Source code in <code>sdfest/vae/sdf_dataset.py</code> <pre><code>def __init__(self, sdf_folder: str):\n    \"\"\"Construct the dataset.\n\n    Args:\n        sdf_folder: The folder containing the npy files.\n    \"\"\"\n    self.path = sdf_folder\n\n    self.size = len([f for f in os.listdir(sdf_folder) if f.endswith(\".npy\")])\n</code></pre>"},{"location":"reference/vae/sdf_dataset/#sdfest.vae.sdf_dataset.SDFDataset.__len__","title":"<code>__len__()</code>","text":"<p>Return the number of images in the dataset.</p> Source code in <code>sdfest/vae/sdf_dataset.py</code> <pre><code>def __len__(self):\n    \"\"\"Return the number of images in the dataset.\"\"\"\n    return self.size\n</code></pre>"},{"location":"reference/vae/sdf_utils/","title":"sdf_utils","text":"<p>This module provides utility functions for working with SDF volumes.</p>"},{"location":"reference/vae/sdf_utils/#sdfest.vae.sdf_utils.mesh_from_sdf","title":"<code>mesh_from_sdf(sdf_volume, level=0, complete_mesh=False)</code>","text":"<p>Compute mesh from sdf using marching cubes algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>sdf_volume</code> <code>array</code> <p>the SDF volume to convert, shape (M, M, M)</p> required <code>level</code> <code>Optional[float]</code> <p>the isosurface level to extract the mesh for</p> <code>0</code> <code>complete_mesh</code> <code>bool</code> <p>if True, the SDF will be padded with positive values prior to converting it to a mesh. This ensures a watertight mesh is created.</p> <code>False</code> <p>Returns:     The resulting mesh.</p> Source code in <code>sdfest/vae/sdf_utils.py</code> <pre><code>def mesh_from_sdf(\n    sdf_volume: np.array, level: Optional[float] = 0, complete_mesh: bool = False\n) -&gt; Trimesh:\n    \"\"\"Compute mesh from sdf using marching cubes algorithm.\n\n    Args:\n        sdf_volume: the SDF volume to convert, shape (M, M, M)\n        level: the isosurface level to extract the mesh for\n        complete_mesh:\n            if True, the SDF will be padded with positive values prior to converting it\n            to a mesh. This ensures a watertight mesh is created.\n    Returns:\n        The resulting mesh.\n    \"\"\"\n    try:\n        if complete_mesh:\n            sdf_volume = np.pad(sdf_volume, pad_width=1, constant_values=1.0)\n        sdf_volume.shape\n        vertices, faces, normals, _ = marching_cubes(\n            sdf_volume, spacing=2 / np.array(sdf_volume.shape), level=level\n        )\n        vertices -= 1\n    except ValueError:\n        return None\n    return Trimesh(\n        vertices,\n        faces,\n        vertex_normals=normals,\n        visual=trimesh.visual.TextureVisuals(material=SimpleMaterial()),\n    )\n</code></pre>"},{"location":"reference/vae/sdf_utils/#sdfest.vae.sdf_utils.mesh_to_sdf","title":"<code>mesh_to_sdf(mesh, cells_per_dim, padding=0)</code>","text":"<p>Convert mesh to discretized signed distance field.</p> <p>The mesh will be stretched, so that its longest extend fills out the unit cube leaving the specified padding empty.</p> <p>Parameters:</p> Name Type Description Default <code>mesh</code> <code>Trimesh</code> <p>The mesh to convert.</p> required <code>cells_per_dim</code> <code>int</code> <p>The number of cells along each dimension.</p> required <code>padding</code> <code>Optional[int]</code> <p>Number of empty space cells.</p> <code>0</code> <p>Returns:     The discretized signed distance field.</p> Source code in <code>sdfest/vae/sdf_utils.py</code> <pre><code>def mesh_to_sdf(mesh: Trimesh, cells_per_dim: int, padding: Optional[int] = 0):\n    \"\"\"Convert mesh to discretized signed distance field.\n\n    The mesh will be stretched, so that its longest extend fills out the unit cube\n    leaving the specified padding empty.\n\n    Args:\n        mesh: The mesh to convert.\n        cells_per_dim: The number of cells along each dimension.\n        padding: Number of empty space cells.\n    Returns:\n        The discretized signed distance field.\n    \"\"\"\n    surface_point_method = \"scan\"\n    sign_method = \"depth\"\n    scaled_mesh = mts.utils.scale_to_unit_cube(mesh)\n    scaled_mesh.vertices *= (cells_per_dim - 2 * padding) / cells_per_dim\n    surface_point_cloud = mts.get_surface_point_cloud(\n        scaled_mesh, surface_point_method, calculate_normals=sign_method == \"normal\"\n    )\n    try:\n        return surface_point_cloud.get_voxels(\n            cells_per_dim, check_result=True, use_depth_buffer=sign_method == \"depth\"\n        )\n    except mts.BadMeshException:\n        print(\"Bad mesh detected. Skipping.\")\n        return None\n</code></pre>"},{"location":"reference/vae/sdf_utils/#sdfest.vae.sdf_utils.plot_mesh","title":"<code>plot_mesh(mesh, polar_angle=np.pi / 4, azimuth=0, camera_distance=2.5, plot_object=None, transform=None)</code>","text":"<p>Render a mesh with camera pointing at its center.</p> <p>Note that in pyrender z-axis is up, x,y form the polar_angle=0 plane.</p> <p>Parameters:</p> Name Type Description Default <code>mesh</code> <code>Trimesh</code> <p>The mesh to render.</p> required <code>polar_angle</code> <p>Polar angle of the camera. For 0 the camera will look down the z-axis.</p> <code>pi / 4</code> <code>azimuth</code> <p>Azimuth of the camera. For 0, polar_anlge=pi/2 the camera will look down the x axis.</p> <code>0</code> <code>camera_distance</code> <p>Distance of camera to the origin.</p> <code>2.5</code> <code>plot_object</code> <code>Optional[Axes]</code> <p>Axis to plot the image. Will use plt if not provided.</p> <code>None</code> <code>transform</code> <code>Optional[array]</code> <p>Transform of the object. Identity by default.</p> <code>None</code> Source code in <code>sdfest/vae/sdf_utils.py</code> <pre><code>def plot_mesh(\n    mesh: Trimesh,\n    polar_angle=np.pi / 4,\n    azimuth=0,\n    camera_distance=2.5,\n    plot_object: Optional[Axes] = None,\n    transform: Optional[np.array] = None,\n):\n    \"\"\"Render a mesh with camera pointing at its center.\n\n    Note that in pyrender z-axis is up, x,y form the polar_angle=0 plane.\n\n    Args:\n        mesh: The mesh to render.\n        polar_angle:\n            Polar angle of the camera.\n            For 0 the camera will look down the z-axis.\n        azimuth:\n            Azimuth of the camera.\n            For 0, polar_anlge=pi/2 the camera will look down the x axis.\n        camera_distance:\n            Distance of camera to the origin.\n        plot_object:\n            Axis to plot the image. Will use plt if not provided.\n        transform: Transform of the object. Identity by default.\n    \"\"\"\n    if plot_object is None:\n        plot_object = plt\n    if transform is None:\n        transform = np.eye(4, 4)[None]\n    elif transform.ndim == 2:\n        transform = transform[None]\n    pyrender_mesh = pyrender.Mesh.from_trimesh(mesh, poses=transform, smooth=False)\n    scene = pyrender.Scene()\n    scene.add(pyrender_mesh)\n    camera = pyrender.PerspectiveCamera(yfov=np.pi / 3.0, aspectRatio=1.0)\n\n    # position camera on sphere centered and pointing at centroid\n    camera_unit_vector = np.array(\n        [\n            np.sin(polar_angle) * np.cos(azimuth),\n            np.sin(polar_angle) * np.sin(azimuth),\n            np.cos(polar_angle),\n        ]\n    )\n    camera_position = camera_distance * camera_unit_vector\n\n    camera_pose = np.array(\n        [\n            [\n                -np.sin(azimuth),\n                -np.cos(azimuth) * np.cos(polar_angle),\n                np.cos(azimuth) * np.sin(polar_angle),\n                camera_position[0],\n            ],\n            [\n                np.cos(azimuth),\n                -np.sin(azimuth) * np.cos(polar_angle),\n                np.sin(azimuth) * np.sin(polar_angle),\n                camera_position[1],\n            ],\n            [0, np.sin(polar_angle), np.cos(polar_angle), camera_position[2]],\n            [0.0, 0.0, 0.0, 1.0],\n        ]\n    )\n    scene.add(camera, pose=camera_pose)\n    light = pyrender.PointLight(color=[1.0, 1.0, 1.0], intensity=45.0)\n    light_pose = np.array(\n        [\n            [\n                np.cos(azimuth) * np.cos(polar_angle),\n                -np.sin(azimuth),\n                np.cos(azimuth) * np.sin(polar_angle),\n                camera_position[0],\n            ],\n            [\n                np.sin(azimuth) * np.cos(polar_angle),\n                np.cos(azimuth),\n                np.sin(azimuth) * np.sin(polar_angle),\n                camera_position[1],\n            ],\n            [-np.sin(polar_angle), 0, np.cos(polar_angle), camera_position[2]],\n            [0.0, 0.0, 0.0, 1.0],\n        ]\n    )\n    # scene.add(light, pose=camera_pose)\n    scene.add_node(pyrender.Node(light=light, matrix=light_pose))\n    flags = RenderFlags.RGBA | RenderFlags.ALL_SOLID\n    r = pyrender.OffscreenRenderer(1000, 1000, flags)\n    color, _ = r.render(scene)\n    plot_object.axis(\"off\")\n    plot_object.imshow(color, interpolation=\"none\")\n</code></pre>"},{"location":"reference/vae/sdf_utils/#sdfest.vae.sdf_utils.visualize_sdf_batch_columns","title":"<code>visualize_sdf_batch_columns(sdfs, show=False)</code>","text":"<p>Visualize batch of sdfs, with one per column (mesh + cross-views).</p> Source code in <code>sdfest/vae/sdf_utils.py</code> <pre><code>def visualize_sdf_batch_columns(sdfs: np.array, show: bool = False):\n    \"\"\"Visualize batch of sdfs, with one per column (mesh + cross-views).\"\"\"\n    fig = plt.figure()\n    num_sdfs = sdfs.shape[0]\n\n    center = np.array(sdfs.shape)[2] // 2\n\n    level = 1.0 / sdfs.shape[-1]\n\n    for c in range(num_sdfs):\n        min = np.min(sdfs[c])\n        max = np.max(sdfs[c])\n\n        plt.subplot(4, num_sdfs, 0 * num_sdfs + c + 1)\n        mesh = mesh_from_sdf(sdfs[c], level=level)\n        if mesh is not None:\n            plot_mesh(mesh)\n        plt.subplot(4, num_sdfs, 1 * num_sdfs + c + 1)\n        plt.imshow(sdfs[c, center, :, :].T, origin=\"lower\", vmin=min, vmax=max)\n        plt.xlabel(\"y\")\n        plt.ylabel(\"z\")\n        plt.subplot(4, num_sdfs, 2 * num_sdfs + c + 1)\n        plt.imshow(sdfs[c, :, center, :].T, origin=\"lower\", vmin=min, vmax=max)\n        plt.xlabel(\"x\")\n        plt.ylabel(\"z\")\n        plt.subplot(4, num_sdfs, 3 * num_sdfs + c + 1)\n        plt.imshow(sdfs[c, :, :, center].T, origin=\"lower\", vmin=min, vmax=max)\n        plt.xlabel(\"x\")\n        plt.ylabel(\"y\")\n\n    if show:\n        plt.show()\n\n    return fig\n</code></pre>"},{"location":"reference/vae/sdf_vae/","title":"sdf_vae","text":"<p>This module provides various PyTorch modules for working with SDFs.</p>"},{"location":"reference/vae/sdf_vae/#sdfest.vae.sdf_vae.SDFDecoder","title":"<code>SDFDecoder</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>sdfest/vae/sdf_vae.py</code> <pre><code>class SDFDecoder(nn.Module):\n    def __init__(\n        self,\n        volume_size: int,\n        latent_size: int,\n        fc_layers: list,\n        conv_layers: list,\n        tsdf: Optional[Union[bool, float]] = False,\n    ):\n        super().__init__()\n\n        # sanity checks\n        self.sanity_check(volume_size, fc_layers, conv_layers)\n\n        # create layers and store params\n        self._volume_size = volume_size\n\n        in_size = latent_size\n        self._fc_layers = torch.nn.ModuleList()\n        for fc_layer in fc_layers:\n            self._fc_layers.append(nn.Linear(in_size, fc_layer[\"out\"]))\n            in_size = fc_layer[\"out\"]\n        self._fc_info = fc_layers\n\n        self._conv_layers = torch.nn.ModuleList()\n        for conv_layer in conv_layers:\n            self._conv_layers.append(\n                nn.Conv3d(\n                    conv_layer[\"in_channels\"],\n                    conv_layer[\"out_channels\"],\n                    conv_layer[\"kernel_size\"],\n                )\n            )\n        self._conv_info = conv_layers\n\n        self._tsdf = tsdf\n\n    def sanity_check(self, volume_size, fc_dicts, conv_dicts):\n        assert fc_dicts[-1][\"out\"] == (\n            conv_dicts[0][\"in_channels\"] * conv_dicts[0][\"in_size\"] ** 3\n        )\n\n        for i, conv_dict in enumerate(conv_dicts[:-1]):\n            assert conv_dict[\"out_channels\"] == conv_dicts[i + 1][\"in_channels\"]\n\n        assert conv_dicts[-1][\"out_channels\"] == 1\n\n    def forward(self, z, enforce_tsdf=False):\n        \"\"\"Decode latent vectors to SDFs.\n\n        Args:\n            z: Batch of latent vectors. Expected shape of (N,latent_size).\n        \"\"\"\n        out = z\n        for fc_layer in self._fc_layers:\n            out = nn.functional.relu(fc_layer(out))\n\n        out = out.view(\n            -1,\n            self._conv_info[0][\"in_channels\"],\n            self._conv_info[0][\"in_size\"],\n            self._conv_info[0][\"in_size\"],\n            self._conv_info[0][\"in_size\"],\n        )\n\n        for info, layer in zip(self._conv_info, self._conv_layers):\n            # interpolate to match next input\n            if out.shape[2] != info[\"in_size\"]:\n                out = torch.nn.functional.interpolate(\n                    out,\n                    size=(info[\"in_size\"], info[\"in_size\"], info[\"in_size\"]),\n                    mode=\"trilinear\",\n                    align_corners=False,\n                )\n            out = layer(out)\n            if info[\"relu\"]:\n                out = nn.functional.relu(out)\n\n        if out.shape[2] != self._volume_size:\n            out = torch.nn.functional.interpolate(\n                out,\n                size=(self._volume_size, self._volume_size, self._volume_size),\n                mode=\"trilinear\",\n                align_corners=False,\n            )\n\n        if self._tsdf is not False and enforce_tsdf:\n            out = out.clamp(-self._tsdf, self._tsdf)\n\n        return out\n</code></pre>"},{"location":"reference/vae/sdf_vae/#sdfest.vae.sdf_vae.SDFDecoder.forward","title":"<code>forward(z, enforce_tsdf=False)</code>","text":"<p>Decode latent vectors to SDFs.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <p>Batch of latent vectors. Expected shape of (N,latent_size).</p> required Source code in <code>sdfest/vae/sdf_vae.py</code> <pre><code>def forward(self, z, enforce_tsdf=False):\n    \"\"\"Decode latent vectors to SDFs.\n\n    Args:\n        z: Batch of latent vectors. Expected shape of (N,latent_size).\n    \"\"\"\n    out = z\n    for fc_layer in self._fc_layers:\n        out = nn.functional.relu(fc_layer(out))\n\n    out = out.view(\n        -1,\n        self._conv_info[0][\"in_channels\"],\n        self._conv_info[0][\"in_size\"],\n        self._conv_info[0][\"in_size\"],\n        self._conv_info[0][\"in_size\"],\n    )\n\n    for info, layer in zip(self._conv_info, self._conv_layers):\n        # interpolate to match next input\n        if out.shape[2] != info[\"in_size\"]:\n            out = torch.nn.functional.interpolate(\n                out,\n                size=(info[\"in_size\"], info[\"in_size\"], info[\"in_size\"]),\n                mode=\"trilinear\",\n                align_corners=False,\n            )\n        out = layer(out)\n        if info[\"relu\"]:\n            out = nn.functional.relu(out)\n\n    if out.shape[2] != self._volume_size:\n        out = torch.nn.functional.interpolate(\n            out,\n            size=(self._volume_size, self._volume_size, self._volume_size),\n            mode=\"trilinear\",\n            align_corners=False,\n        )\n\n    if self._tsdf is not False and enforce_tsdf:\n        out = out.clamp(-self._tsdf, self._tsdf)\n\n    return out\n</code></pre>"},{"location":"reference/vae/sdf_vae/#sdfest.vae.sdf_vae.SDFEncoder","title":"<code>SDFEncoder</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>sdfest/vae/sdf_vae.py</code> <pre><code>class SDFEncoder(nn.Module):\n    def __init__(self, volume_size, latent_size, layer_infos, tsdf=False):\n        \"\"\"Create SDFEncoder, i.e., define trainable layers.\n\n        Args:\n            volume_size:\n                Input size D of the volume.\n                (i.e., input tensor is expected to be Nx1xDxDxD)\n            latent_size: Dimensionality of the latent representation.\n            layers:\n                Dictionaries defining the layers before the final output layers.\n                Required fields:\n                    - fully qualified type (str)\n                    - args (dict): params passed to init of type\n                These layers need to construct an 2D tensor (including batch dimension).\n        \"\"\"\n        super().__init__()\n\n        in_channels = 1\n\n        # define layers\n        layers = []\n        for layer_info in layer_infos:\n            t = locate(layer_info[\"type\"])\n            layers.append(t(**layer_info[\"args\"]))\n        self._features = torch.nn.Sequential(*layers)\n\n        with torch.no_grad():\n            output_size = self._features(\n                torch.zeros(1, in_channels, volume_size, volume_size, volume_size)\n            ).shape\n\n        self.linear_means = nn.Linear(output_size[1], latent_size)\n        self.linear_log_var = nn.Linear(output_size[1], latent_size)\n\n        self._tsdf = tsdf\n\n    def forward(self, x):\n        \"\"\"Forward pass of the module.\n\n        Args:\n\n        Returns:\n\n        \"\"\"\n        out = self._features(x)\n\n        means = self.linear_means(out)\n        log_vars = self.linear_log_var(out)\n\n        return means, log_vars\n\n    def prepare_input(self, sdfs: torch.Tensor) -&gt; None:\n        \"\"\"Convert batched SDFs to expected input format.\n\n        This will truncate the SDFs based on tsdf argument passed to constructor.\n\n        Will be done in place without gradients.\n\n        Args:\n            sdfs: Batched SDFs, expected shape (N,C,D,D,D).\n        \"\"\"\n        if self._tsdf is not False:\n            with torch.no_grad():\n                sdfs.clamp_(-self._tsdf, self._tsdf)\n</code></pre>"},{"location":"reference/vae/sdf_vae/#sdfest.vae.sdf_vae.SDFEncoder.__init__","title":"<code>__init__(volume_size, latent_size, layer_infos, tsdf=False)</code>","text":"<p>Create SDFEncoder, i.e., define trainable layers.</p> <p>Parameters:</p> Name Type Description Default <code>volume_size</code> <p>Input size D of the volume. (i.e., input tensor is expected to be Nx1xDxDxD)</p> required <code>latent_size</code> <p>Dimensionality of the latent representation.</p> required <code>layers</code> <p>Dictionaries defining the layers before the final output layers. Required fields:     - fully qualified type (str)     - args (dict): params passed to init of type These layers need to construct an 2D tensor (including batch dimension).</p> required Source code in <code>sdfest/vae/sdf_vae.py</code> <pre><code>def __init__(self, volume_size, latent_size, layer_infos, tsdf=False):\n    \"\"\"Create SDFEncoder, i.e., define trainable layers.\n\n    Args:\n        volume_size:\n            Input size D of the volume.\n            (i.e., input tensor is expected to be Nx1xDxDxD)\n        latent_size: Dimensionality of the latent representation.\n        layers:\n            Dictionaries defining the layers before the final output layers.\n            Required fields:\n                - fully qualified type (str)\n                - args (dict): params passed to init of type\n            These layers need to construct an 2D tensor (including batch dimension).\n    \"\"\"\n    super().__init__()\n\n    in_channels = 1\n\n    # define layers\n    layers = []\n    for layer_info in layer_infos:\n        t = locate(layer_info[\"type\"])\n        layers.append(t(**layer_info[\"args\"]))\n    self._features = torch.nn.Sequential(*layers)\n\n    with torch.no_grad():\n        output_size = self._features(\n            torch.zeros(1, in_channels, volume_size, volume_size, volume_size)\n        ).shape\n\n    self.linear_means = nn.Linear(output_size[1], latent_size)\n    self.linear_log_var = nn.Linear(output_size[1], latent_size)\n\n    self._tsdf = tsdf\n</code></pre>"},{"location":"reference/vae/sdf_vae/#sdfest.vae.sdf_vae.SDFEncoder.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the module.</p> <p>Args:</p> <p>Returns:</p> Source code in <code>sdfest/vae/sdf_vae.py</code> <pre><code>def forward(self, x):\n    \"\"\"Forward pass of the module.\n\n    Args:\n\n    Returns:\n\n    \"\"\"\n    out = self._features(x)\n\n    means = self.linear_means(out)\n    log_vars = self.linear_log_var(out)\n\n    return means, log_vars\n</code></pre>"},{"location":"reference/vae/sdf_vae/#sdfest.vae.sdf_vae.SDFEncoder.prepare_input","title":"<code>prepare_input(sdfs)</code>","text":"<p>Convert batched SDFs to expected input format.</p> <p>This will truncate the SDFs based on tsdf argument passed to constructor.</p> <p>Will be done in place without gradients.</p> <p>Parameters:</p> Name Type Description Default <code>sdfs</code> <code>Tensor</code> <p>Batched SDFs, expected shape (N,C,D,D,D).</p> required Source code in <code>sdfest/vae/sdf_vae.py</code> <pre><code>def prepare_input(self, sdfs: torch.Tensor) -&gt; None:\n    \"\"\"Convert batched SDFs to expected input format.\n\n    This will truncate the SDFs based on tsdf argument passed to constructor.\n\n    Will be done in place without gradients.\n\n    Args:\n        sdfs: Batched SDFs, expected shape (N,C,D,D,D).\n    \"\"\"\n    if self._tsdf is not False:\n        with torch.no_grad():\n            sdfs.clamp_(-self._tsdf, self._tsdf)\n</code></pre>"},{"location":"reference/vae/sdf_vae/#sdfest.vae.sdf_vae.SDFVAE","title":"<code>SDFVAE</code>","text":"<p>               Bases: <code>Module</code></p> <p>Variational Autoencoder for Signed Distance Fields.</p> Source code in <code>sdfest/vae/sdf_vae.py</code> <pre><code>class SDFVAE(nn.Module):\n    \"\"\"Variational Autoencoder for Signed Distance Fields.\"\"\"\n\n    def __init__(\n        self,\n        sdf_size: int,\n        latent_size: int,\n        encoder_dict: dict,\n        decoder_dict: dict,\n        device: torch.device,\n        tsdf: Optional[Union[bool, float]] = False,\n    ):\n        \"\"\"Initialize the SDFVAE module.\n\n        Args:\n            sdf_size:       depth/width/length of the sdf\n            latent_size:    dimensions of latent representation\n            encoder_dict:\n                Arguments passed to encoder constructor.\n                See SDFEncoder.__init__ for details.\n            decoder_dict:\n                Arguments passed to decoder constructor.\n                See SDFDecoder.__init__ for details.\n            device:\n                The device this model will work on.\n                This is used for tensors created inside this model for sampling.\n            tsdf:\n                Value to truncate the SDF at. False for untruncated SDF.\n                For the input this is only done in prepare_input, not in the forward\n                pass. The output is truncated in the forward pass.\n        \"\"\"\n        super().__init__()\n\n        self.latent_size = latent_size\n\n        self._device = device\n\n        self.encoder = SDFEncoder(sdf_size, latent_size, tsdf=tsdf, **encoder_dict)\n        self.decoder = SDFDecoder(sdf_size, latent_size, tsdf=tsdf, **decoder_dict)\n        self.sdf_size = sdf_size\n        self._tsdf = tsdf\n\n    def forward(self, x, enforce_tsdf=False):\n        z, means, log_var = self.encode(x)\n\n        recon_x = self.decoder(z, enforce_tsdf)\n\n        return recon_x, means, log_var, z\n\n    def sample(self, n=1):\n        z = torch.randn([n, self.latent_size]).to(self._device)\n        return z\n\n    def encode(self, x):\n        means, log_var = self.encoder(x)\n\n        std = torch.exp(0.5 * log_var)\n        eps = torch.randn([x.shape[0], self.latent_size]).to(self._device)\n        z = eps * std + means\n        return z, means, log_var\n\n    def inference(self, n=1, enforce_tsdf=False):\n        z = self.sample(n)\n\n        recon_x = self.decoder(z, enforce_tsdf)\n\n        return recon_x, z\n\n    def decode(self, z, enforce_tsdf=False):\n        \"\"\"Returns decoded SDF.\n\n        Args:\n            Batch of latent shapes. Shape (N, L).\n        Returns:\n            The decoded SDF. Shape (N, C, D, D, D).\n        \"\"\"\n        return self.decoder(z, enforce_tsdf)\n\n    def prepare_input(self, sdfs: torch.Tensor) -&gt; None:\n        \"\"\"Convert batched SDFs to expected input format.\n\n        This will transform inputs as defined by the decoder.\n        See SDFEncoder.prepare_input for details.\n\n        Will be done in place without gradients.\n\n        Args:\n            sdfs: Batched SDFs, expected shape (N,C,D,D,D).\n        \"\"\"\n        self.encoder.prepare_input(sdfs)\n</code></pre>"},{"location":"reference/vae/sdf_vae/#sdfest.vae.sdf_vae.SDFVAE.__init__","title":"<code>__init__(sdf_size, latent_size, encoder_dict, decoder_dict, device, tsdf=False)</code>","text":"<p>Initialize the SDFVAE module.</p> <p>Parameters:</p> Name Type Description Default <code>sdf_size</code> <code>int</code> <p>depth/width/length of the sdf</p> required <code>latent_size</code> <code>int</code> <p>dimensions of latent representation</p> required <code>encoder_dict</code> <code>dict</code> <p>Arguments passed to encoder constructor. See SDFEncoder.init for details.</p> required <code>decoder_dict</code> <code>dict</code> <p>Arguments passed to decoder constructor. See SDFDecoder.init for details.</p> required <code>device</code> <code>device</code> <p>The device this model will work on. This is used for tensors created inside this model for sampling.</p> required <code>tsdf</code> <code>Optional[Union[bool, float]]</code> <p>Value to truncate the SDF at. False for untruncated SDF. For the input this is only done in prepare_input, not in the forward pass. The output is truncated in the forward pass.</p> <code>False</code> Source code in <code>sdfest/vae/sdf_vae.py</code> <pre><code>def __init__(\n    self,\n    sdf_size: int,\n    latent_size: int,\n    encoder_dict: dict,\n    decoder_dict: dict,\n    device: torch.device,\n    tsdf: Optional[Union[bool, float]] = False,\n):\n    \"\"\"Initialize the SDFVAE module.\n\n    Args:\n        sdf_size:       depth/width/length of the sdf\n        latent_size:    dimensions of latent representation\n        encoder_dict:\n            Arguments passed to encoder constructor.\n            See SDFEncoder.__init__ for details.\n        decoder_dict:\n            Arguments passed to decoder constructor.\n            See SDFDecoder.__init__ for details.\n        device:\n            The device this model will work on.\n            This is used for tensors created inside this model for sampling.\n        tsdf:\n            Value to truncate the SDF at. False for untruncated SDF.\n            For the input this is only done in prepare_input, not in the forward\n            pass. The output is truncated in the forward pass.\n    \"\"\"\n    super().__init__()\n\n    self.latent_size = latent_size\n\n    self._device = device\n\n    self.encoder = SDFEncoder(sdf_size, latent_size, tsdf=tsdf, **encoder_dict)\n    self.decoder = SDFDecoder(sdf_size, latent_size, tsdf=tsdf, **decoder_dict)\n    self.sdf_size = sdf_size\n    self._tsdf = tsdf\n</code></pre>"},{"location":"reference/vae/sdf_vae/#sdfest.vae.sdf_vae.SDFVAE.decode","title":"<code>decode(z, enforce_tsdf=False)</code>","text":"<p>Returns decoded SDF.</p> <p>Returns:     The decoded SDF. Shape (N, C, D, D, D).</p> Source code in <code>sdfest/vae/sdf_vae.py</code> <pre><code>def decode(self, z, enforce_tsdf=False):\n    \"\"\"Returns decoded SDF.\n\n    Args:\n        Batch of latent shapes. Shape (N, L).\n    Returns:\n        The decoded SDF. Shape (N, C, D, D, D).\n    \"\"\"\n    return self.decoder(z, enforce_tsdf)\n</code></pre>"},{"location":"reference/vae/sdf_vae/#sdfest.vae.sdf_vae.SDFVAE.prepare_input","title":"<code>prepare_input(sdfs)</code>","text":"<p>Convert batched SDFs to expected input format.</p> <p>This will transform inputs as defined by the decoder. See SDFEncoder.prepare_input for details.</p> <p>Will be done in place without gradients.</p> <p>Parameters:</p> Name Type Description Default <code>sdfs</code> <code>Tensor</code> <p>Batched SDFs, expected shape (N,C,D,D,D).</p> required Source code in <code>sdfest/vae/sdf_vae.py</code> <pre><code>def prepare_input(self, sdfs: torch.Tensor) -&gt; None:\n    \"\"\"Convert batched SDFs to expected input format.\n\n    This will transform inputs as defined by the decoder.\n    See SDFEncoder.prepare_input for details.\n\n    Will be done in place without gradients.\n\n    Args:\n        sdfs: Batched SDFs, expected shape (N,C,D,D,D).\n    \"\"\"\n    self.encoder.prepare_input(sdfs)\n</code></pre>"},{"location":"reference/vae/torch_utils/","title":"torch_utils","text":"<p>Generally useful modules for pytorch.</p>"},{"location":"reference/vae/torch_utils/#sdfest.vae.torch_utils.View","title":"<code>View</code>","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper of torch's view method to use with nn.Sequential.</p> Source code in <code>sdfest/vae/torch_utils.py</code> <pre><code>class View(torch.nn.Module):\n    \"\"\"Wrapper of torch's view method to use with nn.Sequential.\"\"\"\n\n    def __init__(self, *shape):\n        \"\"\"Construct the module.\"\"\"\n        super(View, self).__init__()\n        self.shape = shape\n\n    def forward(self, x):\n        \"\"\"Reshape the tensor.\"\"\"\n        return x.view(*self.shape)\n</code></pre>"},{"location":"reference/vae/torch_utils/#sdfest.vae.torch_utils.View.__init__","title":"<code>__init__(*shape)</code>","text":"<p>Construct the module.</p> Source code in <code>sdfest/vae/torch_utils.py</code> <pre><code>def __init__(self, *shape):\n    \"\"\"Construct the module.\"\"\"\n    super(View, self).__init__()\n    self.shape = shape\n</code></pre>"},{"location":"reference/vae/torch_utils/#sdfest.vae.torch_utils.View.forward","title":"<code>forward(x)</code>","text":"<p>Reshape the tensor.</p> Source code in <code>sdfest/vae/torch_utils.py</code> <pre><code>def forward(self, x):\n    \"\"\"Reshape the tensor.\"\"\"\n    return x.view(*self.shape)\n</code></pre>"},{"location":"reference/vae/utils/","title":"utils","text":"<p>General functions for experiments and pytorch.</p>"},{"location":"reference/vae/utils/#sdfest.vae.utils.View","title":"<code>View</code>","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper of torch's view method to use with nn.Sequential.</p> Source code in <code>sdfest/vae/utils.py</code> <pre><code>class View(torch.nn.Module):\n    \"\"\"Wrapper of torch's view method to use with nn.Sequential.\"\"\"\n\n    def __init__(self, shape):\n        \"\"\"Construct the module.\"\"\"\n        super(View, self).__init__()\n        self.shape = shape\n\n    def forward(self, x):\n        \"\"\"Reshape the tensor.\"\"\"\n        return x.view(*self.shape)\n</code></pre>"},{"location":"reference/vae/utils/#sdfest.vae.utils.View.__init__","title":"<code>__init__(shape)</code>","text":"<p>Construct the module.</p> Source code in <code>sdfest/vae/utils.py</code> <pre><code>def __init__(self, shape):\n    \"\"\"Construct the module.\"\"\"\n    super(View, self).__init__()\n    self.shape = shape\n</code></pre>"},{"location":"reference/vae/utils/#sdfest.vae.utils.View.forward","title":"<code>forward(x)</code>","text":"<p>Reshape the tensor.</p> Source code in <code>sdfest/vae/utils.py</code> <pre><code>def forward(self, x):\n    \"\"\"Reshape the tensor.\"\"\"\n    return x.view(*self.shape)\n</code></pre>"},{"location":"reference/vae/utils/#sdfest.vae.utils.load_checkpoint","title":"<code>load_checkpoint(path, model, optimizer)</code>","text":"<p>Load a checkpoint during training.</p> <p>Args:</p> Source code in <code>sdfest/vae/utils.py</code> <pre><code>def load_checkpoint(path, model, optimizer):\n    \"\"\"Load a checkpoint during training.\n\n    Args:\n    \"\"\"\n    print(f\"Loading checkpoint at {path} ...\")\n    checkpoint = torch.load(path)\n    model.load_state_dict(checkpoint[\"model_state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n    iteration = checkpoint[\"iteration\"]\n    epoch = checkpoint[\"epoch\"]\n    run_name = checkpoint[\"run_name\"]\n\n    print(\"Checkpoint loaded\")\n\n    model.train()  # training mode\n\n    return model, optimizer, iteration, run_name, epoch\n</code></pre>"},{"location":"reference/vae/utils/#sdfest.vae.utils.save_checkpoint","title":"<code>save_checkpoint(path, model, optimizer, iteration, epoch, run_name)</code>","text":"<p>Save a checkpoint during training.</p> <p>Args:</p> Source code in <code>sdfest/vae/utils.py</code> <pre><code>def save_checkpoint(\n    path: str, model: torch.nn.Module, optimizer, iteration, epoch, run_name\n):\n    \"\"\"Save a checkpoint during training.\n\n    Args:\n\n    \"\"\"\n    torch.save(\n        {\n            \"iteration\": iteration,\n            \"epoch\": epoch,\n            \"model_state_dict\": model.state_dict(),\n            \"optimizer_state_dict\": optimizer.state_dict(),\n            \"run_name\": run_name,\n        },\n        path,\n    )\n</code></pre>"},{"location":"reference/vae/utils/#sdfest.vae.utils.str_to_tsdf","title":"<code>str_to_tsdf(x)</code>","text":"<p>Convert string to expected values for tsdf setting.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>str</code> <p>A string containing either some representation of False or a float.</p> required <p>Returns:     False or float.</p> Source code in <code>sdfest/vae/utils.py</code> <pre><code>def str_to_tsdf(x: str) -&gt; Union[bool, float]:\n    \"\"\"Convert string to expected values for tsdf setting.\n\n    Args:\n        x: A string containing either some representation of False or a float.\n    Returns:\n        False or float.\n    \"\"\"\n    if x.lower() in (\"no\", \"false\", \"f\", \"n\", \"0\"):\n        return False\n    return float(x)\n</code></pre>"},{"location":"reference/vae/scripts/benchmark/","title":"benchmark","text":""},{"location":"reference/vae/scripts/benchmark_vae/","title":"benchmark_vae","text":"<p>Script to benchmark VAE architecture in isolation.</p> Usage <p>python -m sdfest.vae.scripts.benchmark_vae            --config initialization/configs/vae_models/mug.yaml            --device cuda</p>"},{"location":"reference/vae/scripts/process_shapenet/","title":"process_shapenet","text":"<p>Script to preprocess shapenet like dataset.</p> <p>This script allows to interactively go through a ShapeNet dataset and decide whether to keep or remove a mesh. All textures are removed, only the obj file and a converted SDF volume are stored in the output folder.</p>"},{"location":"reference/vae/scripts/process_shapenet/#sdfest.vae.scripts.process_shapenet.Object3D","title":"<code>Object3D</code>","text":"<p>Representation of a 3D object.</p> Source code in <code>sdfest/vae/scripts/process_shapenet.py</code> <pre><code>class Object3D:\n    \"\"\"Representation of a 3D object.\"\"\"\n\n    def __init__(self, mesh_path: str):\n        \"\"\"Initialize the 3D object.\n\n        Args:\n            mesh_path: the full file path of the mesh\n        \"\"\"\n        self.mesh_path = mesh_path\n        self.simplified_mesh = None\n        self.sdf_volume = None\n        self.reconstructed_mesh = None\n\n    def convert_to_sdf(self, cells_per_dim, padding):\n        self.sdf_volume = sdf_utils.mesh_to_sdf(\n            self.simplified_mesh, cells_per_dim, padding\n        )\n        return self\n\n    def load_mesh(self):\n        loaded_obj = trimesh.load(self.mesh_path, process=False)\n        if isinstance(loaded_obj, Trimesh):\n            self.simplified_mesh = Trimesh(\n                loaded_obj.vertices,\n                loaded_obj.faces,\n                vertex_normals=loaded_obj.vertex_normals,\n                visual=trimesh.visual.TextureVisuals(material=SimpleMaterial()),\n            )\n        elif isinstance(loaded_obj, Scene):\n            mesh = trimesh.util.concatenate(\n                tuple(\n                    trimesh.Trimesh(\n                        vertices=g.vertices,\n                        faces=g.faces,\n                        vertex_normals=g.vertex_normals,\n                    )\n                    for g in loaded_obj.geometry.values()\n                )\n            )\n            self.simplified_mesh = Trimesh(\n                mesh.vertices,\n                mesh.faces,\n                vertex_normals=mesh.vertex_normals,\n                visual=trimesh.visual.TextureVisuals(material=SimpleMaterial()),\n            )\n        else:\n            print(f\"Not supported: {type(loaded_obj)}\")\n            return False\n        return True\n\n    def reconstruct_from_sdf(self):\n        level = 1.0 / self.sdf_volume.shape[-1]\n        self.reconstructed_mesh = sdf_utils.mesh_from_sdf(self.sdf_volume, level)\n        return self\n</code></pre>"},{"location":"reference/vae/scripts/process_shapenet/#sdfest.vae.scripts.process_shapenet.Object3D.__init__","title":"<code>__init__(mesh_path)</code>","text":"<p>Initialize the 3D object.</p> <p>Parameters:</p> Name Type Description Default <code>mesh_path</code> <code>str</code> <p>the full file path of the mesh</p> required Source code in <code>sdfest/vae/scripts/process_shapenet.py</code> <pre><code>def __init__(self, mesh_path: str):\n    \"\"\"Initialize the 3D object.\n\n    Args:\n        mesh_path: the full file path of the mesh\n    \"\"\"\n    self.mesh_path = mesh_path\n    self.simplified_mesh = None\n    self.sdf_volume = None\n    self.reconstructed_mesh = None\n</code></pre>"},{"location":"reference/vae/scripts/process_shapenet/#sdfest.vae.scripts.process_shapenet.trimesh_decision_viewer","title":"<code>trimesh_decision_viewer(mesh, return_dict)</code>","text":"<p>View a trimesh with pyrender adding support to press left and right.</p> Source code in <code>sdfest/vae/scripts/process_shapenet.py</code> <pre><code>def trimesh_decision_viewer(mesh, return_dict):\n    \"\"\"View a trimesh with pyrender adding support to press left and right.\"\"\"\n\n    def on_press(key):\n        nonlocal inp\n        try:\n            inp = key.char  # single-char keys\n        except AttributeError:\n            inp = key.name\n\n    pyrender_mesh = pyrender.Mesh.from_trimesh(mesh)\n    scene = pyrender.Scene(ambient_light=[0.3, 0.3, 0.3, 1.0])\n    scene.add(pyrender_mesh)\n    v = pyrender.Viewer(scene, run_in_thread=True, use_raymond_lighting=True)\n    inp = None\n    from pynput import keyboard\n\n    decision = None\n\n    listener = keyboard.Listener(on_press=on_press)\n    listener.start()  # start to listen on a separate thread\n    while True:\n        time.sleep(0.1)\n        if inp == \"left\":\n            decision = \"remove\"\n            break\n        elif inp == \"right\":\n            decision = \"keep\"\n            break\n        elif inp == \"down\":\n            decision = \"stop\"\n            break\n    # TODO: listener.stop() hangs randomly (especially under load)\n    v.close_external()\n    return_dict[\"decision\"] = decision\n    return decision\n</code></pre>"},{"location":"reference/vae/scripts/train/","title":"train","text":"<p>Script to train model.</p>"},{"location":"reference/vae/scripts/train/#sdfest.vae.scripts.train.main","title":"<code>main()</code>","text":"<p>Entry point of the program.</p> Source code in <code>sdfest/vae/scripts/train.py</code> <pre><code>def main():\n    \"\"\"Entry point of the program.\"\"\"\n    # define the arguments\n    parser = argparse.ArgumentParser(description=\"Training script for SDFVAE.\")\n\n    # parse arguments\n    parser.add_argument(\"--checkpoint\")\n    parser.add_argument(\"--dataset_path\", required=True)\n    parser.add_argument(\"--batch_size\", type=lambda x: int(float(x)))\n    parser.add_argument(\"--iterations\", type=lambda x: int(float(x)))\n    parser.add_argument(\"--latent_size\", type=lambda x: int(float(x)))\n    parser.add_argument(\"--tsdf\", type=utils.str_to_tsdf, default=False)\n    parser.add_argument(\"--kld_weight\", type=lambda x: float(x))\n    parser.add_argument(\"--l2_large_weight\", type=lambda x: float(x))\n    parser.add_argument(\"--l2_small_weight\", type=lambda x: float(x))\n    parser.add_argument(\"--l1_large_weight\", type=lambda x: float(x))\n    parser.add_argument(\"--l1_small_weight\", type=lambda x: float(x))\n    parser.add_argument(\"--pc_weight\", type=lambda x: float(x))\n    parser.add_argument(\"--sign_weight\", type=lambda x: float(x))\n    parser.add_argument(\"--bce_weight\", type=lambda x: float(x))\n    parser.add_argument(\"--learning_rate\", type=lambda x: float(x))\n    parser.add_argument(\"--device\")\n    parser.add_argument(\"--config\", default=\"configs/default.yaml\", nargs=\"+\")\n    config = yoco.load_config_from_args(\n        parser, search_paths=[\".\", \"~/.sdfest/\", sdfest.__path__[0]]\n    )\n    train(config)\n</code></pre>"},{"location":"reference/vae/scripts/train/#sdfest.vae.scripts.train.pc_loss","title":"<code>pc_loss(points, position, orientation, scale, sdf)</code>","text":"<p>Compute trilinerly interpolated SDF value at the points positions.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <p>Mx4 pointcloud in camera frame.</p> required <code>position</code> <p>Position of SDF center in camera frame.</p> required <code>orientation</code> <p>Quaternion representing orientation of SDF.</p> required <code>scale</code> <p>Half-width of SDF volume.</p> required <code>sdf</code> <p>Volumetric signed distance field.</p> required <p>Returns:     Trilinearly interpolated distance at the passed points     0 if outside of SDF volume.</p> Source code in <code>sdfest/vae/scripts/train.py</code> <pre><code>def pc_loss(points, position, orientation, scale, sdf):\n    \"\"\"Compute trilinerly interpolated SDF value at the points positions.\n\n    Args:\n        points:\n            Mx4 pointcloud in camera frame.\n        position:\n            Position of SDF center in camera frame.\n        orientation:\n            Quaternion representing orientation of SDF.\n        scale:\n            Half-width of SDF volume.\n        sdf:\n            Volumetric signed distance field.\n    Returns:\n        Trilinearly interpolated distance at the passed points\n        0 if outside of SDF volume.\n    \"\"\"\n    q = orientation / torch.norm(orientation)  # to get normalization gradients\n    obj_points = points - position.unsqueeze(0)\n\n    # Quaternion to rotation matrix\n    # Note that we use conjugate here since we want to transform points from\n    # world to object coordinates and the quaternion describes rotation of\n    # object coordinate system in world coordinates.\n    R = obj_points.new_zeros(3, 3)\n\n    R[0, 0] = 1 - 2 * (q[1] * q[1] + q[2] * q[2])\n    R[0, 1] = 2 * (q[0] * q[1] + q[2] * q[3])\n    R[0, 2] = 2 * (q[0] * q[2] - q[3] * q[1])\n\n    R[1, 0] = 2 * (q[0] * q[1] - q[2] * q[3])\n    R[1, 1] = 1 - 2 * (q[0] * q[0] + q[2] * q[2])\n    R[1, 2] = 2 * (q[1] * q[2] + q[3] * q[0])\n\n    R[2, 0] = 2 * (q[0] * q[2] + q[3] * q[1])\n    R[2, 1] = 2 * (q[1] * q[2] - q[3] * q[0])\n    R[2, 2] = 1 - 2 * (q[0] * q[0] + q[1] * q[1])\n\n    obj_points = (R @ obj_points.T).T\n\n    # Transform to canonical coordintes obj_point in [-1,1]^3\n    obj_point = obj_points / scale\n\n    # Compute cell and cell position\n    res = sdf.shape[0]  # assuming same resolution along each axis\n    grid_size = 2.0 / (res - 1)\n    c = torch.floor((obj_point + 1.0) * (res - 1) * 0.5)\n    mask = torch.logical_or(\n        torch.min(c, dim=1)[0] &lt; 0, torch.max(c, dim=1)[0] &gt; res - 2\n    )\n    c = torch.clip(c, 0, res - 2)  # base cell index of each point\n    cell_position = c * grid_size - 1.0  # base cell position of each point\n    sdf_indices = c.new_empty((obj_point.shape[0], 8), dtype=torch.long)\n    sdf_indices[:, 0] = c[:, 0] * res**2 + c[:, 1] * res + c[:, 2]\n    sdf_indices[:, 1] = c[:, 0] * res**2 + c[:, 1] * res + c[:, 2] + 1\n    sdf_indices[:, 2] = c[:, 0] * res**2 + (c[:, 1] + 1) * res + c[:, 2]\n    sdf_indices[:, 3] = c[:, 0] * res**2 + (c[:, 1] + 1) * res + c[:, 2] + 1\n    sdf_indices[:, 4] = (c[:, 0] + 1) * res**2 + c[:, 1] * res + c[:, 2]\n    sdf_indices[:, 5] = (c[:, 0] + 1) * res**2 + c[:, 1] * res + c[:, 2] + 1\n    sdf_indices[:, 6] = (c[:, 0] + 1) * res**2 + (c[:, 1] + 1) * res + c[:, 2]\n    sdf_indices[:, 7] = (c[:, 0] + 1) * res**2 + (c[:, 1] + 1) * res + c[:, 2] + 1\n    sdf_view = sdf.view([-1])\n    point_cell_position = (obj_point - cell_position) / grid_size  # [0,1]^3\n    sdf_values = torch.take(sdf_view, sdf_indices)\n\n    # trilinear interpolation\n    sdf_value = sdf_values.new_empty(obj_points.shape[0])\n    # sdf_value = obj_point[:, 0]\n    sdf_value = (\n        (\n            sdf_values[:, 0] * (1 - point_cell_position[:, 0])\n            + sdf_values[:, 4] * point_cell_position[:, 0]\n        )\n        * (1 - point_cell_position[:, 1])\n        + (\n            sdf_values[:, 2] * (1 - point_cell_position[:, 0])\n            + sdf_values[:, 6] * point_cell_position[:, 0]\n        )\n        * point_cell_position[:, 1]\n    ) * (1 - point_cell_position[:, 2]) + (\n        (\n            sdf_values[:, 1] * (1 - point_cell_position[:, 0])\n            + sdf_values[:, 5] * point_cell_position[:, 0]\n        )\n        * (1 - point_cell_position[:, 1])\n        + (\n            sdf_values[:, 3] * (1 - point_cell_position[:, 0])\n            + sdf_values[:, 7] * point_cell_position[:, 0]\n        )\n        * point_cell_position[:, 1]\n    ) * point_cell_position[\n        :, 2\n    ]\n    sdf_value[mask] = 0\n    return sdf_value\n</code></pre>"},{"location":"reference/vae/scripts/train/#sdfest.vae.scripts.train.train","title":"<code>train(config)</code>","text":"<p>Train the model.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The training and model config.</p> required Source code in <code>sdfest/vae/scripts/train.py</code> <pre><code>def train(config):\n    \"\"\"Train the model.\n\n    Args:\n        config: The training and model config.\n    \"\"\"\n    batch_size = wandb.config.batch_size = config[\"batch_size\"]\n    iterations = wandb.config.iterations = config[\"iterations\"]\n    learning_rate = wandb.config.learning_rate = config[\"learning_rate\"]\n    latent_size = wandb.config.latent_size = config[\"latent_size\"]\n    l2_large_weight = wandb.config.l2_large_weight = config[\"l2_large_weight\"]\n    l2_small_weight = wandb.config.l2_small_weight = config[\"l2_small_weight\"]\n    l1_large_weight = wandb.config.l1_large_weight = config[\"l1_large_weight\"]\n    l1_small_weight = wandb.config.l1_small_weight = config[\"l1_small_weight\"]\n    kld_weight = wandb.config.kld_weight = config[\"kld_weight\"]\n    pc_weight = wandb.config.pc_weight = config[\"pc_weight\"]\n    encoder_dict = wandb.config.encoder_dict = config[\"encoder\"]\n    decoder_dict = wandb.config.decoder_dict = config[\"decoder\"]\n    dataset_path = wandb.config.dataset_path = config[\"dataset_path\"]\n    tsdf = wandb.config.tsdf = config[\"tsdf\"]\n\n    # init dataset, dataloader, model and optimizer\n    dataset = SDFDataset(dataset_path)\n    data_loader = torch.utils.data.DataLoader(\n        dataset=dataset, batch_size=batch_size, shuffle=True, drop_last=True\n    )\n\n    camera = Camera(640, 480, 320, 320, 320, 240, pixel_center=0.5)\n\n    if \"device\" not in config or config[\"device\"] is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    else:\n        device = torch.device(config[\"device\"])\n\n    sdfvae = SDFVAE(\n        sdf_size=64,\n        latent_size=latent_size,\n        encoder_dict=encoder_dict,\n        decoder_dict=decoder_dict,\n        device=device,\n        tsdf=tsdf,\n    ).to(device)\n\n    print(str(sdfvae))\n\n    summary(sdfvae, (1, 1, 64, 64, 64), device=device)\n\n    optimizer = torch.optim.Adam(sdfvae.parameters(), lr=learning_rate)\n\n    # load checkpoint if provided\n    if \"checkpoint\" in config and config[\"checkpoint\"] is not None:\n        # TODO: checkpoint should always go together with model config!\n        model, optimizer, current_iteration, run_name, epoch = utils.load_checkpoint(\n            config[\"checkpoint\"], sdfvae, optimizer\n        )\n    else:\n        current_iteration = 0\n        current_epoch = 0\n        run_name = f\"sdfvae_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S-%f')}\"\n\n    # create SummaryWriter for logging and intermediate output\n    writer = torch.utils.tensorboard.SummaryWriter(f\"runs/\" + run_name)\n\n    model_base_path = os.path.join(os.getcwd(), \"models\", run_name)\n    program_starts = time.time()\n    warm_up_iterations = 1000\n    stop = False\n    while current_iteration &lt;= iterations:\n        current_epoch += 1\n        for sdf_volumes in data_loader:\n            sdf_volumes = sdf_volumes.to(device)\n\n            # train first N iters with SDF instead of TSDF to stabilize early training\n            if current_iteration &gt; warm_up_iterations:\n                sdfvae.prepare_input(sdf_volumes)\n\n            recon_sdf_volumes, mean, log_var, z = sdfvae(\n                sdf_volumes, enforce_tsdf=False\n            )\n\n            if tsdf is not False and current_iteration &gt; warm_up_iterations:\n                # during training, clamp the reconstructed SDF only where both target\n                # and output are outside the TSDF range\n                mask = torch.logical_and(\n                    torch.abs(sdf_volumes) &gt;= tsdf, torch.abs(recon_sdf_volumes) &gt;= tsdf\n                )\n                recon_sdf_volumes_temp = recon_sdf_volumes\n                recon_sdf_volumes = recon_sdf_volumes_temp.clone()\n                recon_sdf_volumes[mask] = recon_sdf_volumes_temp[mask].clamp(\n                    -tsdf, tsdf\n                )\n\n            # Compute losses, use negative log-likelihood here\n            # Note: for average negative log-likelihood all of these have to be divided\n            # by the batch size. Probably would be better to keep losses comparable for\n            # varying batch size.\n            l1_error = torch.abs(recon_sdf_volumes - sdf_volumes)\n            l2_error = l1_error**2\n            loss_l2_small = torch.sum(l2_error[torch.abs(sdf_volumes) &lt; 0.1])\n            loss_l2_large = torch.sum(l2_error[torch.abs(sdf_volumes) &gt;= 0.1])\n            loss_l1_small = torch.sum(l1_error[torch.abs(sdf_volumes) &lt; 0.1])\n            loss_l1_large = torch.sum(l1_error[torch.abs(sdf_volumes) &gt;= 0.1])\n            loss_pc = 0\n\n            depth_images = torch.empty((batch_size, 480, 640), device=device)\n            pointclouds = []\n            # compute point clouds from sdf volumes at zero crossings\n            for recon_sdf_volume, sdf_volume, depth_image in zip(\n                recon_sdf_volumes, sdf_volumes, depth_images\n            ):\n                with torch.no_grad():\n                    import random\n                    import math\n\n                    u1, u2, u3 = random.random(), random.random(), random.random()\n                    q = (\n                        torch.tensor(\n                            [\n                                math.sqrt(1 - u1) * math.sin(2 * math.pi * u2),\n                                math.sqrt(1 - u1) * math.cos(2 * math.pi * u2),\n                                math.sqrt(u1) * math.sin(2 * math.pi * u3),\n                                math.sqrt(u1) * math.cos(2 * math.pi * u3),\n                            ]\n                        )\n                        .unsqueeze(0)\n                        .to(device)\n                    )\n                    s = torch.Tensor([1.0]).to(device)\n                    p = torch.Tensor([0.0, 0.0, -5.0]).to(device)\n\n                    depth_image = render_depth_gpu(\n                        sdf_volume[0],\n                        p,\n                        q,\n                        s,\n                        threshold=0.01,\n                        camera=camera,\n                    )\n                    pointcloud = pointset_utils.depth_to_pointcloud(depth_image, camera)\n                loss_pc = loss_pc + torch.sum(\n                    pc_loss(pointcloud, p, q[0], s, recon_sdf_volume[0]) ** 2\n                )\n\n            loss_kld = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n\n            loss = (\n                l2_small_weight * loss_l2_small\n                + l2_large_weight * loss_l2_large\n                + l1_small_weight * loss_l1_small\n                + l1_large_weight * loss_l1_large\n                + pc_weight * loss_pc\n                + loss_kld\n                * (kld_weight if current_iteration &gt; warm_up_iterations else 0)\n            )\n\n            print(f\"Iteration {current_iteration}, epoch {current_epoch}, loss {loss}\")\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            writer.add_scalar(\"loss\", loss.item(), current_iteration)\n            writer.add_scalar(\"loss l2 small\", loss_l2_small.item(), current_iteration)\n            writer.add_scalar(\"loss l2 large\", loss_l2_large.item(), current_iteration)\n            writer.add_scalar(\"loss l1 small\", loss_l1_small.item(), current_iteration)\n            writer.add_scalar(\"loss l1 large\", loss_l1_large.item(), current_iteration)\n            writer.add_scalar(\"loss pc\", loss_pc.item(), current_iteration)\n            writer.add_scalar(\"loss kl\", loss_kld, current_iteration)\n\n            wandb.log(\n                {\n                    \"total loss\": loss.item(),\n                    \"loss l2 small\": loss_l2_small.item(),\n                    \"loss l2 large\": loss_l2_large.item(),\n                    \"loss l1 small\": loss_l1_small.item(),\n                    \"loss l1 large\": loss_l1_large.item(),\n                    \"loss kl\": loss_kld.item(),\n                    \"loss pc\": loss_pc.item(),\n                },\n                step=current_iteration,\n            )\n\n            current_iteration += 1\n\n            with torch.no_grad():\n                if current_iteration % 1000 == 0:\n                    # show one reconstruction\n                    mean = mean[0].cpu()\n                    figure = sdf_utils.visualize_sdf_reconstruction(\n                        sdf_volumes[0, 0].cpu().numpy(),\n                        recon_sdf_volumes[0, 0].cpu().numpy(),\n                    )\n                    if figure.gca().has_data():\n                        writer.add_figure(\n                            tag=\"reconstruction\",\n                            figure=figure,\n                            global_step=current_iteration,\n                        )\n                        wandb.log({\"reconstruction\": figure}, step=current_iteration)\n\n                    # generate 8 samples from the prior\n                    output, _ = sdfvae.inference(n=8, enforce_tsdf=True)\n                    figure = sdf_utils.visualize_sdf_batch(\n                        output.squeeze().cpu().numpy()\n                    )\n                    if figure.gca().has_data():\n                        writer.add_figure(\n                            tag=\"samples from prior\",\n                            figure=figure,\n                            global_step=current_iteration,\n                        )\n                        wandb.log(\n                            {\"samples from prior\": figure}, step=current_iteration\n                        )\n\n                    # generate 4 samples from prior\n                    output, _ = sdfvae.inference(n=4, enforce_tsdf=True)\n                    figure = sdf_utils.visualize_sdf_batch_columns(\n                        output.squeeze().cpu().numpy()\n                    )\n                    if figure.gca().has_data():\n                        writer.add_figure(\n                            tag=\"samples from prior, columns\",\n                            figure=figure,\n                            global_step=current_iteration,\n                        )\n                        wandb.log(\n                            {\"samples from prior, columns\": figure},\n                            step=current_iteration,\n                        )\n\n            #     if current_iteration % 10000 == 0:\n            #         os.makedirs(model_base_path, exist_ok=True)\n            #         checkpoint_path = os.path.join(model_base_path, F\"{current_iteration}.ckp\")\n            #         utils.save_checkpoint(\n            #             path=checkpoint_path,\n            #             model=sdfvae,\n            #             optimizer=optimizer,\n            #             iteration=current_iteration,\n            #             run_name=run_name)\n\n            if current_iteration &gt; iterations:\n                break\n        if current_iteration &gt; iterations:\n            break\n\n    now = time.time()\n    print(\"It has been {0} seconds since the loop started\".format(now - program_starts))\n\n    # save the final model\n    torch.save(sdfvae.state_dict(), os.path.join(wandb.run.dir, f\"{wandb.run.name}.pt\"))\n    config_path = os.path.join(wandb.run.dir, f\"{wandb.run.name}.yaml\")\n    config[\"model\"] = os.path.join(\".\", f\"{wandb.run.name}.pt\")\n    yoco.save_config_to_file(config_path, config)\n</code></pre>"},{"location":"reference/vae/scripts/visualizer/","title":"visualizer","text":"<p>Application to explore and visualize VAE.</p>"},{"location":"reference/vae/scripts/visualizer/#sdfest.vae.scripts.visualizer.VAEVisualizer","title":"<code>VAEVisualizer</code>","text":"<p>               Bases: <code>QDialog</code></p> <p>User interface to explore VAE model.</p> Source code in <code>sdfest/vae/scripts/visualizer.py</code> <pre><code>class VAEVisualizer(QDialog):\n    \"\"\"User interface to explore VAE model.\"\"\"\n\n    LATENT_ABS_MAX = 4\n    LATENT_TICKS = 100\n\n    def __init__(self, parent=None):\n        super(VAEVisualizer, self).__init__(parent)\n\n        # define attributes\n        self._single_sdf_input = None\n        self._single_sdf_latent = None\n        self._single_sdf_output = None\n\n        self._transition_sdf_first = None\n        self._transition_sdf_first_latent = None\n        self._transition_sdf_first_out = None\n        self._transition_sdf_second = None\n        self._transition_sdf_second_latent = None\n        self._transition_sdf_second_out = None\n        self._transition_sdf_out = None\n\n        self._config = None\n        self._state_dict = None\n        self._model = None\n        self._latent_sliders = []\n\n        self._isosurface_level = 0\n\n        # split layout into two halfs\n        # (left for loading model, right is for visualization)\n        layout = QHBoxLayout()\n        model_layout = QVBoxLayout()\n\n        # Load model button\n        load_model_button = QPushButton(\"Load model/config\")\n        load_model_button.clicked.connect(self.load_model)\n        load_model_button.setSizePolicy(\n            QSizePolicy.Policy.Expanding, QSizePolicy.Policy.Maximum\n        )\n        model_layout.addWidget(load_model_button)\n\n        # Confg line edit\n        self._config_line_edit = QLineEdit()\n        self._config_line_edit.textChanged.connect(self.config_changed)\n        self._config_line_edit.setSizePolicy(\n            QSizePolicy.Policy.Expanding, QSizePolicy.Policy.Maximum\n        )\n        model_layout.addWidget(self._config_line_edit)\n\n        # Isosurface leve spin box\n        self._iso_level_spinbox = QDoubleSpinBox()\n        self._iso_level_spinbox.setDecimals(2)\n        self._iso_level_spinbox.setSingleStep(0.01)\n        self._iso_level_spinbox.valueChanged.connect(self.update_isosurface_level)\n        model_layout.addWidget(self._iso_level_spinbox)\n\n        # Model status\n        self._model_status_label = QLabel()\n        model_layout.addWidget(self._model_status_label)\n\n        # Current config\n        scroll_area = QScrollArea(widgetResizable=True)\n        scroll_area.setSizePolicy(\n            QSizePolicy.Policy.Expanding, QSizePolicy.Policy.Expanding\n        )\n        scroll_area.setMinimumWidth(270)\n        self._config_label = QLabel()\n        self._config_label.setAlignment(QtCore.Qt.AlignTop)\n        scroll_area.setWidget(self._config_label)\n\n        model_layout.addWidget(scroll_area)\n\n        # Single object explorer\n        single_object_widget = QWidget()\n        single_object_widget_layout = QVBoxLayout()\n        single_object_widget_layout.setAlignment(QtCore.Qt.AlignTop)\n        single_object_widget_bottom = QWidget()\n        single_object_widget_bottom_layout = QHBoxLayout()\n        single_object_save_buttons_widget = QWidget()\n        single_object_save_buttons_layout = QHBoxLayout()\n        self.slider_group_widget = QWidget()\n        self.slider_group_widget_layout = QVBoxLayout()\n        load_sdf_button = QPushButton(\"Load SDF\")\n        load_sdf_button.clicked.connect(self.load_single_sdf)\n        generate_sdf_button = QPushButton(\"Generate SDF from prior\")\n        generate_sdf_button.clicked.connect(self.generate_sdf)\n        save_figure_button = QPushButton(\"Save figure\")\n        save_figure_button.clicked.connect(self.save_figure)\n        save_mesh_button = QPushButton(\"Save mesh\")\n        save_mesh_button.clicked.connect(self.save_mesh)\n        save_sdf_button = QPushButton(\"Save sdf\")\n        save_sdf_button.clicked.connect(self.save_sdf)\n\n        self.single_object_figure = Figure(\n            dpi=85, facecolor=(1, 1, 1), edgecolor=(0, 0, 0), tight_layout=True\n        )\n        single_object_canvas = FigureCanvas(self.single_object_figure)\n        single_object_canvas.setSizePolicy(QSizePolicy.Expanding, QSizePolicy.Expanding)\n\n        self.scroll_area_sliders = QScrollArea(widgetResizable=True)\n        # widgetResizable indicates that the widget insided can be resized\n        self.scroll_area_sliders.setFixedWidth(130)\n\n        self.slider_group_widget.setLayout(self.slider_group_widget_layout)\n        self.scroll_area_sliders.setWidget(self.slider_group_widget)\n        self.scroll_area_sliders.show()\n\n        # Single object bottom\n        single_object_widget_bottom_layout.addWidget(single_object_canvas)\n        single_object_widget_bottom_layout.addWidget(self.scroll_area_sliders)\n        single_object_widget_bottom.setLayout(single_object_widget_bottom_layout)\n\n        # Single object save buttons\n        single_object_save_buttons_layout.addWidget(save_figure_button)\n        single_object_save_buttons_layout.addWidget(save_mesh_button)\n        single_object_save_buttons_layout.addWidget(save_sdf_button)\n        single_object_save_buttons_widget.setLayout(single_object_save_buttons_layout)\n        single_object_save_buttons_layout.setMargin(0)\n\n        # Single object tab\n        single_object_widget_layout.addWidget(load_sdf_button)\n        single_object_widget_layout.addWidget(generate_sdf_button)\n        single_object_widget_layout.addWidget(single_object_save_buttons_widget)\n        single_object_widget_layout.addWidget(single_object_widget_bottom)\n        single_object_widget.setLayout(single_object_widget_layout)\n\n        # Transition widget\n        transition_widget = QWidget()\n\n        transition_widget_layout = QVBoxLayout()\n        transition_widget_layout.setAlignment(QtCore.Qt.AlignTop)\n\n        transition_widget_button_layout = QHBoxLayout()\n\n        load_first_sdf_button = QPushButton(\"Load SDF 1\")\n        load_first_sdf_button.clicked.connect(lambda: self.load_transition_sdf(\"first\"))\n\n        load_second_sdf_button = QPushButton(\"Load SDF 2\")\n        load_second_sdf_button.clicked.connect(\n            lambda: self.load_transition_sdf(\"second\")\n        )\n\n        self.transition_figure = Figure(\n            dpi=85, facecolor=(1, 1, 1), edgecolor=(0, 0, 0)\n        )\n        transition_canvas = FigureCanvas(self.transition_figure)\n        transition_canvas.setSizePolicy(QSizePolicy.Expanding, QSizePolicy.Expanding)\n        self.transition_figure_axes = self.transition_figure.subplots(2, 3)\n        self.transition_figure.delaxes(self.transition_figure_axes[0, 1])\n\n        self.transition_slider = QSlider(QtCore.Qt.Horizontal)\n        self.transition_slider.setMinimum(0)\n        self.transition_slider.setMaximum(100)\n        self.transition_slider.valueChanged.connect(self.update_transition_slider)\n\n        transition_widget_button_layout.addWidget(load_first_sdf_button)\n        transition_widget_button_layout.addWidget(load_second_sdf_button)\n        transition_widget_layout.addLayout(transition_widget_button_layout)\n        transition_widget_layout.addWidget(transition_canvas)\n        transition_widget_layout.addWidget(self.transition_slider)\n        transition_widget.setLayout(transition_widget_layout)\n\n        # Animation widget\n        animation_widget = QWidget()\n        animation_widget_layout = QVBoxLayout()\n        animation_widget_layout.setAlignment(QtCore.Qt.AlignTop)\n\n        add_kf_from_single_view_button = QPushButton(\"Add from single view\")\n        add_kf_from_single_view_button.clicked.connect(\n            lambda: self.add_kf_from_single_view()\n        )\n\n        delete_kf_button = QPushButton(\"Delete keyframe\")\n        delete_kf_button.clicked.connect(lambda: self.delete_kf())\n\n        generate_animation_button = QPushButton(\"Generate animation\")\n        generate_animation_button.clicked.connect(lambda: self.generate_animation())\n\n        self._keyframe_list = QListWidget()\n\n        animation_widget.setLayout(animation_widget_layout)\n        animation_widget_layout.addWidget(add_kf_from_single_view_button)\n        animation_widget_layout.addWidget(delete_kf_button)\n        animation_widget_layout.addWidget(self._keyframe_list)\n        animation_widget_layout.addWidget(generate_animation_button)\n\n        # Tab widget\n        tab_widget = QTabWidget()\n        tab_widget.addTab(single_object_widget, \"Single SDF\")\n        tab_widget.addTab(transition_widget, \"Transition\")\n        tab_widget.addTab(animation_widget, \"Animation\")\n\n        layout.addLayout(model_layout)\n        layout.addWidget(tab_widget)\n        layout.setStretch(0, 0)\n        layout.setStretch(1, 2)\n\n        self.setLayout(layout)\n\n        self.parse_config()\n        self.update_model()\n\n        # TODO make these adjustable from GUI\n        self.transform = np.array(\n            [\n                [0, -1, 0, 0],\n                [0, 0, 1, 0],\n                [-1, 0, 0, 0],\n                [0, 0, 0, 1],\n            ]\n        )\n        self.azimuth = 0\n        self.polar_angle = -np.pi / 4\n\n    def reset_output(self) -&gt; None:\n        self._single_sdf_latent = None\n        self._single_sdf_output = None\n        self._transition_sdf_first_latent = None\n        self._transition_sdf_first_out = None\n        self._transition_sdf_second_latent = None\n        self._transition_sdf_second_out = None\n\n    def add_kf_from_single_view(self) -&gt; None:\n        \"\"\"Add keyframe based on current latent in Single SDF tab.\"\"\"\n        if self._single_sdf_latent is not None:\n            item = QListWidgetItem()\n            item.setText(str(self._single_sdf_latent))\n            item.setData(QtCore.Qt.UserRole, self._single_sdf_latent)\n            self._keyframe_list.addItem(item)\n        else:\n            print(\"No valid latent currently set in Single SDF tab\")\n\n    def delete_kf(self) -&gt; None:\n        \"\"\"Delete currently selected keyframe.\"\"\"\n        row = self._keyframe_list.currentRow()\n        self._keyframe_list.takeItem(row)\n\n    def generate_animation(self) -&gt; None:\n        \"\"\"Generate animation based on keyframes.\"\"\"\n        fps = 30\n        distance_per_frame = 0.1\n        loop = True\n        keyframes = []\n        for i in range(self._keyframe_list.count()):\n            keyframes.append(self._keyframe_list.item(i).data(QtCore.Qt.UserRole))\n        if loop:\n            keyframes.append(keyframes[0])\n\n        # generate frames\n        with torch.no_grad():\n            fig = Figure(dpi=85, facecolor=(1, 1, 1), edgecolor=(0, 0, 0))\n            now = datetime.datetime.now()\n            folder = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n            os.makedirs(folder)\n            frame_number = 0\n            current = keyframes.pop(0).clone()\n            remaining_distance = distance_per_frame\n            while keyframes:\n                next_target = keyframes[0]\n                delta = next_target - current\n                distance = torch.linalg.norm(delta)\n                direction = delta / torch.linalg.norm(delta)\n                if distance &gt; distance_per_frame:\n                    current += direction * distance_per_frame\n                    remaining_distance = 0\n                else:\n                    current = keyframes.pop(0).clone()\n                    remaining_distance -= distance\n\n                if remaining_distance &lt;= 0:\n                    # generate frame\n                    sdf = self._model.decode(current)\n                    mesh = sdf_utils.mesh_from_sdf(\n                        sdf[0, 0].cpu().numpy(),\n                        self._isosurface_level,\n                        complete_mesh=True,\n                    )\n                    if mesh is None:\n                        print(\"No surface boundaries in SDF.\")\n                    else:\n                        ax = fig.subplots(1, 1)\n                        sdf_utils.plot_mesh(\n                            mesh,\n                            plot_object=ax,\n                            transform=self.transform,\n                            azimuth=self.azimuth,\n                            polar_angle=self.polar_angle,\n                        )\n                        ax.xaxis.set_major_locator(NullLocator())\n                        ax.yaxis.set_major_locator(NullLocator())\n                        fig.savefig(\n                            os.path.join(folder, f\"{frame_number:05d}.png\"), dpi=200\n                        )\n                        frame_number += 1\n                        fig.clear()\n\n        # convert frames to video\n        video_name = os.path.join(f\"{folder}.mp4\")\n        ffmpeg.input(\n            os.path.join(folder, \"*.png\"), pattern_type=\"glob\", framerate=fps\n        ).output(video_name).run()\n\n    def update_model(self):\n        if self._config is None:\n            return\n        try:\n            self._model = SDFVAE(\n                sdf_size=64,\n                latent_size=self._config[\"latent_size\"],\n                encoder_dict=self._config[\"encoder\"],\n                decoder_dict=self._config[\"decoder\"],\n                device=\"cpu\",\n                tsdf=self._config[\"tsdf\"],\n            )\n\n            if self._state_dict is not None:\n                self._model.load_state_dict(self._state_dict)\n                self._model_status_label.setText(\n                    \"Model (with weights) successfully loaded.\"\n                )\n            else:\n                self._model_status_label.setText(\n                    \"Model (no weights) successfully loaded.\"\n                )\n            self._model_status_label.setStyleSheet(\"color: green\")\n            self.reset_output()\n        except RuntimeError:\n            self._model_status_label.setText(\"Model config and weights do not match.\")\n            self._model_status_label.setStyleSheet(\"color: red\")\n            self._config_label.setText(\"\")\n            self._model = None\n        except KeyError:\n            self._model_status_label.setText(\"Model config incomplete.\")\n            self._model_status_label.setStyleSheet(\"color: red\")\n            self._config_label.setText(\"\")\n            self._model = None\n\n        self.update_single()\n        self.update_transition()\n\n    def load_sdf(self):\n        path, _ = QFileDialog.getOpenFileName(self, \"Open file\", filter=\"*.npy\")\n        if path == \"\":\n            return None\n        try:\n            sdf_np = np.load(path)\n            return torch.as_tensor(sdf_np).unsqueeze(0).unsqueeze(0)\n        except FileNotFoundError:\n            print(\"Error loading SDF.\")\n            return None\n\n    def parse_config(self):\n        try:\n            arg_list = self._config_line_edit.text().split(\" \")\n            arg_list = [x for x in arg_list if x != \"\"]\n            parser = ThrowingArgumentParser()\n            parser.add_argument(\"--tsdf\", type=utils.str_to_tsdf, default=False)\n            parser.add_argument(\"--latent_size\", type=lambda x: int(float(x)))\n            parser.add_argument(\"--config\", default=\"configs/default.yaml\", nargs=\"+\")\n            self._config = yoco.load_config_from_args(parser, arg_list)\n            self._config_label.setText(json.dumps(self._config, indent=4))\n        except ArgumentParserError:\n            self._model_status_label.setText(\"Error in config string.\")\n            self._model_status_label.setStyleSheet(\"color: red\")\n            self._config_label.setText(\"\")\n            self._model = None\n            self._config = None\n        except (FileNotFoundError, IsADirectoryError):\n            self._model_status_label.setText(\"Specified config file not found.\")\n            self._model_status_label.setStyleSheet(\"color: red\")\n            self._config_label.setText(\"\")\n            self._model = None\n            self._config = None\n        except RuntimeError:\n            self._model_status_label.setText(\"Model config and weights do not match.\")\n            self._model_status_label.setStyleSheet(\"color: red\")\n            self._config_label.setText(\"\")\n            self._model = None\n            self._config = None\n\n    def load_transition_sdf(self, which):\n        sdf = self.load_sdf()\n        if which == \"first\":\n            self._transition_sdf_first = sdf\n            self._transition_sdf_first_out = None\n            self._transition_sdf_first_latent = None\n        elif which == \"second\":\n            self._transition_sdf_second = sdf\n            self._transition_sdf_second_out = None\n            self._transition_sdf_second_latent = None\n        else:\n            raise ValueError(\"which must be either 'first' or 'second'.\")\n        self.update_transition()\n\n    def update_single(self, evaluate=True):\n        # Visualize input sdf\n        self.single_object_figure.clear()\n        if self._single_sdf_input is not None:\n            ax1, ax2 = self.single_object_figure.subplots(1, 2)\n            self.render_sdf(ax1, self._single_sdf_input[0, 0].numpy())\n        else:\n            ax2 = self.single_object_figure.subplots(1, 1)\n\n        if self._model is not None:\n            with torch.no_grad():\n                if (\n                    self._single_sdf_input is not None\n                    and self._single_sdf_latent is None\n                ):\n                    inp = self._single_sdf_input.clone()\n                    self._model.prepare_input(inp)\n                    self._single_sdf_latent, _, _ = self._model.encode(inp)\n                    self.update_sliders()\n                if self._single_sdf_latent is not None:\n                    if evaluate:\n                        t1 = time.time()\n                        self._single_sdf_output = self._model.decode(\n                            self._single_sdf_latent, enforce_tsdf=True\n                        )\n                        print(f\"Decoding took {time.time() - t1}s\")\n                    t1 = time.time()\n                    self.render_sdf(ax2, self._single_sdf_output[0, 0].detach().numpy())\n                    print(f\"Rendering took {time.time() - t1}s\")\n\n        self.single_object_figure.canvas.draw()\n\n    def update_sliders(self):\n        for current_slider in self._latent_sliders:\n            self.slider_group_widget_layout.removeWidget(current_slider)\n            current_slider.deleteLater()\n\n        self._latent_sliders = []\n        for latent in self._single_sdf_latent.view(-1).numpy():\n            slider = QSlider(QtCore.Qt.Horizontal)\n            slider.setMinimum(-self.LATENT_ABS_MAX * self.LATENT_TICKS / 2.0)\n            slider.setMaximum(+self.LATENT_ABS_MAX * self.LATENT_TICKS / 2.0)\n            slider.setValue(latent * self.LATENT_TICKS / 2.0)\n            slider.valueChanged.connect(self.update_latent)\n            self._latent_sliders.append(slider)\n            self.slider_group_widget_layout.addWidget(slider)\n\n    def update_latent(self):\n        for i, current_slider in enumerate(self._latent_sliders):\n            self._single_sdf_latent[0, i] = (\n                current_slider.value() * 2.0 / self.LATENT_TICKS\n            )\n        self.update_single()\n\n    def update_transition_slider(self):\n        self.update_transition()\n\n    def update_transition(self, update_iso_level_only=False):\n        if update_iso_level_only:\n            if self._transition_sdf_first_out is not None:\n                self.render_sdf(\n                    self.transition_figure_axes[1, 0],\n                    self._transition_sdf_first_out[0, 0].detach().numpy(),\n                )\n            if self._transition_sdf_second_out is not None:\n                self.render_sdf(\n                    self.transition_figure_axes[1, 2],\n                    self._transition_sdf_second_out[0, 0].detach().numpy(),\n                )\n            if self._transition_sdf_out is not None:\n                self.render_sdf(\n                    self.transition_figure_axes[1, 1],\n                    self._transition_sdf_out[0, 0].detach().numpy(),\n                )\n            self.transition_figure.canvas.draw()\n            return\n\n        with torch.no_grad():\n            if (\n                self._transition_sdf_first is not None\n                and self._transition_sdf_first_out is None\n            ):\n                print(\"render first sdf\")\n                self.render_sdf(\n                    self.transition_figure_axes[0, 0],\n                    self._transition_sdf_first[0, 0].detach().numpy(),\n                )\n                if (\n                    self._model is not None\n                    and self._transition_sdf_first_latent is None\n                ):\n                    inp = self._transition_sdf_first.clone()\n                    self._model.prepare_input(inp)\n                    self._transition_sdf_first_latent, _, _ = self._model.encode(inp)\n                    self._transition_sdf_first_out = self._model.decode(\n                        self._transition_sdf_first_latent\n                    )\n                    self.render_sdf(\n                        self.transition_figure_axes[1, 0],\n                        self._transition_sdf_first_out[0, 0].detach().numpy(),\n                    )\n            if (\n                self._transition_sdf_second is not None\n                and self._transition_sdf_second_out is None\n            ):\n                self.render_sdf(\n                    self.transition_figure_axes[0, 2],\n                    self._transition_sdf_second[0, 0].detach().numpy(),\n                )\n                if (\n                    self._model is not None\n                    and self._transition_sdf_second_latent is None\n                ):\n                    inp = self._transition_sdf_second.clone()\n                    self._model.prepare_input(inp)\n                    self._transition_sdf_second_latent, _, _ = self._model.encode(inp)\n                    self._transition_sdf_second_out = self._model.decode(\n                        self._transition_sdf_second_latent\n                    )\n                    self.render_sdf(\n                        self.transition_figure_axes[1, 2],\n                        self._transition_sdf_second_out[0, 0].detach().numpy(),\n                    )\n\n            if (\n                self._transition_sdf_first_latent is not None\n                and self._transition_sdf_second_latent is not None\n            ):\n                t = self.transition_slider.value() / 100.0\n                interpolated_latent = (1 - t) * self._transition_sdf_first_latent + (\n                    t\n                ) * self._transition_sdf_second_latent\n                self._transition_sdf_out = self._model.decode(interpolated_latent)\n                self.render_sdf(\n                    self.transition_figure_axes[1, 1],\n                    self._transition_sdf_out[0, 0].detach().numpy(),\n                )\n        self.transition_figure.canvas.draw()\n\n    def load_single_sdf(self):\n        sdf = self.load_sdf()\n        self._single_sdf_input = sdf\n        self._single_sdf_latent = None\n        self.update_single()\n\n    def generate_sdf(self):\n        self._single_sdf_input = None\n        if self._model is not None:\n            self._single_sdf_latent = self._model.sample(1) * 0.5\n            self.update_single()\n            self.update_sliders()\n        else:\n            print(\"Can't generate without loaded model.\")\n\n    def save_figure(self):\n        \"\"\"Save current single sample figure.\"\"\"\n        now = datetime.datetime.now()\n        filename = now.strftime(\"%Y-%m-%d_%H-%M-%S.png\")\n        self.single_object_figure.savefig(filename, pad_inches=0, dpi=200)\n        print(f\"Saved as {filename}\")\n\n    def save_mesh(self):\n        \"\"\"Save current object mesh.\"\"\"\n        if self._single_sdf_output is None:\n            print(\"Can't save mesh without generated SDF.\")\n            return\n        sdf = self._single_sdf_output[0, 0].detach().numpy()\n        mesh = sdf_utils.mesh_from_sdf(sdf, self._isosurface_level, complete_mesh=True)\n        mesh_filename = (\n            f\"mesh_{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S-%f')}.obj\"\n        )\n        mesh_path = os.path.join(os.getcwd(), mesh_filename)\n        with open(mesh_path, \"w\") as f:\n            f.write(trimesh.exchange.obj.export_obj(mesh))\n\n    def save_sdf(self):\n        \"\"\"Save current object sdf.\"\"\"\n        if self._single_sdf_output is None:\n            print(\"Can't save SDF without generated SDF.\")\n            return\n        sdf = self._single_sdf_output[0, 0].detach().numpy()\n        sdf_filename = (\n            f\"sdf_{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S-%f')}.npy\"\n        )\n        sdf_path = os.path.join(os.getcwd(), sdf_filename)\n        np.save(sdf_path, sdf)\n\n    def config_changed(self, string):\n        self.parse_config()\n        self.update_model()\n\n    def load_model(self):\n        path, _ = QFileDialog.getOpenFileName(self, \"Open file\")\n        if path.endswith(\".yml\") or path.endswith(\".yaml\"):\n            self._config_line_edit.setText(f\"--config {path}\")\n            self.parse_config()\n            print(self._config)\n            if \"model\" in self._config:\n                self._state_dict = torch.load(self._config[\"model\"], map_location=\"cpu\")\n            self.update_model()\n        elif path.endswith(\".pt\"):\n            self._state_dict = torch.load(path, map_location=\"cpu\")\n            self.update_model()\n        else:\n            return\n\n    def render_sdf(self, ax, sdf):\n        mesh = sdf_utils.mesh_from_sdf(sdf, self._isosurface_level, complete_mesh=True)\n        if mesh is None:\n            print(\"No surface boundaries in SDF.\")\n        else:\n            sdf_utils.plot_mesh(\n                mesh,\n                plot_object=ax,\n                transform=self.transform,\n                azimuth=self.azimuth,\n                polar_angle=self.polar_angle,\n            )\n\n    def update_isosurface_level(self, d):\n        self._isosurface_level = d\n        self.update_single(False)\n        self.update_transition(True)\n</code></pre>"},{"location":"reference/vae/scripts/visualizer/#sdfest.vae.scripts.visualizer.VAEVisualizer.add_kf_from_single_view","title":"<code>add_kf_from_single_view()</code>","text":"<p>Add keyframe based on current latent in Single SDF tab.</p> Source code in <code>sdfest/vae/scripts/visualizer.py</code> <pre><code>def add_kf_from_single_view(self) -&gt; None:\n    \"\"\"Add keyframe based on current latent in Single SDF tab.\"\"\"\n    if self._single_sdf_latent is not None:\n        item = QListWidgetItem()\n        item.setText(str(self._single_sdf_latent))\n        item.setData(QtCore.Qt.UserRole, self._single_sdf_latent)\n        self._keyframe_list.addItem(item)\n    else:\n        print(\"No valid latent currently set in Single SDF tab\")\n</code></pre>"},{"location":"reference/vae/scripts/visualizer/#sdfest.vae.scripts.visualizer.VAEVisualizer.delete_kf","title":"<code>delete_kf()</code>","text":"<p>Delete currently selected keyframe.</p> Source code in <code>sdfest/vae/scripts/visualizer.py</code> <pre><code>def delete_kf(self) -&gt; None:\n    \"\"\"Delete currently selected keyframe.\"\"\"\n    row = self._keyframe_list.currentRow()\n    self._keyframe_list.takeItem(row)\n</code></pre>"},{"location":"reference/vae/scripts/visualizer/#sdfest.vae.scripts.visualizer.VAEVisualizer.generate_animation","title":"<code>generate_animation()</code>","text":"<p>Generate animation based on keyframes.</p> Source code in <code>sdfest/vae/scripts/visualizer.py</code> <pre><code>def generate_animation(self) -&gt; None:\n    \"\"\"Generate animation based on keyframes.\"\"\"\n    fps = 30\n    distance_per_frame = 0.1\n    loop = True\n    keyframes = []\n    for i in range(self._keyframe_list.count()):\n        keyframes.append(self._keyframe_list.item(i).data(QtCore.Qt.UserRole))\n    if loop:\n        keyframes.append(keyframes[0])\n\n    # generate frames\n    with torch.no_grad():\n        fig = Figure(dpi=85, facecolor=(1, 1, 1), edgecolor=(0, 0, 0))\n        now = datetime.datetime.now()\n        folder = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n        os.makedirs(folder)\n        frame_number = 0\n        current = keyframes.pop(0).clone()\n        remaining_distance = distance_per_frame\n        while keyframes:\n            next_target = keyframes[0]\n            delta = next_target - current\n            distance = torch.linalg.norm(delta)\n            direction = delta / torch.linalg.norm(delta)\n            if distance &gt; distance_per_frame:\n                current += direction * distance_per_frame\n                remaining_distance = 0\n            else:\n                current = keyframes.pop(0).clone()\n                remaining_distance -= distance\n\n            if remaining_distance &lt;= 0:\n                # generate frame\n                sdf = self._model.decode(current)\n                mesh = sdf_utils.mesh_from_sdf(\n                    sdf[0, 0].cpu().numpy(),\n                    self._isosurface_level,\n                    complete_mesh=True,\n                )\n                if mesh is None:\n                    print(\"No surface boundaries in SDF.\")\n                else:\n                    ax = fig.subplots(1, 1)\n                    sdf_utils.plot_mesh(\n                        mesh,\n                        plot_object=ax,\n                        transform=self.transform,\n                        azimuth=self.azimuth,\n                        polar_angle=self.polar_angle,\n                    )\n                    ax.xaxis.set_major_locator(NullLocator())\n                    ax.yaxis.set_major_locator(NullLocator())\n                    fig.savefig(\n                        os.path.join(folder, f\"{frame_number:05d}.png\"), dpi=200\n                    )\n                    frame_number += 1\n                    fig.clear()\n\n    # convert frames to video\n    video_name = os.path.join(f\"{folder}.mp4\")\n    ffmpeg.input(\n        os.path.join(folder, \"*.png\"), pattern_type=\"glob\", framerate=fps\n    ).output(video_name).run()\n</code></pre>"},{"location":"reference/vae/scripts/visualizer/#sdfest.vae.scripts.visualizer.VAEVisualizer.save_figure","title":"<code>save_figure()</code>","text":"<p>Save current single sample figure.</p> Source code in <code>sdfest/vae/scripts/visualizer.py</code> <pre><code>def save_figure(self):\n    \"\"\"Save current single sample figure.\"\"\"\n    now = datetime.datetime.now()\n    filename = now.strftime(\"%Y-%m-%d_%H-%M-%S.png\")\n    self.single_object_figure.savefig(filename, pad_inches=0, dpi=200)\n    print(f\"Saved as {filename}\")\n</code></pre>"},{"location":"reference/vae/scripts/visualizer/#sdfest.vae.scripts.visualizer.VAEVisualizer.save_mesh","title":"<code>save_mesh()</code>","text":"<p>Save current object mesh.</p> Source code in <code>sdfest/vae/scripts/visualizer.py</code> <pre><code>def save_mesh(self):\n    \"\"\"Save current object mesh.\"\"\"\n    if self._single_sdf_output is None:\n        print(\"Can't save mesh without generated SDF.\")\n        return\n    sdf = self._single_sdf_output[0, 0].detach().numpy()\n    mesh = sdf_utils.mesh_from_sdf(sdf, self._isosurface_level, complete_mesh=True)\n    mesh_filename = (\n        f\"mesh_{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S-%f')}.obj\"\n    )\n    mesh_path = os.path.join(os.getcwd(), mesh_filename)\n    with open(mesh_path, \"w\") as f:\n        f.write(trimesh.exchange.obj.export_obj(mesh))\n</code></pre>"},{"location":"reference/vae/scripts/visualizer/#sdfest.vae.scripts.visualizer.VAEVisualizer.save_sdf","title":"<code>save_sdf()</code>","text":"<p>Save current object sdf.</p> Source code in <code>sdfest/vae/scripts/visualizer.py</code> <pre><code>def save_sdf(self):\n    \"\"\"Save current object sdf.\"\"\"\n    if self._single_sdf_output is None:\n        print(\"Can't save SDF without generated SDF.\")\n        return\n    sdf = self._single_sdf_output[0, 0].detach().numpy()\n    sdf_filename = (\n        f\"sdf_{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S-%f')}.npy\"\n    )\n    sdf_path = os.path.join(os.getcwd(), sdf_filename)\n    np.save(sdf_path, sdf)\n</code></pre>"},{"location":"reference/vae/scripts/visualizer/#sdfest.vae.scripts.visualizer.main","title":"<code>main()</code>","text":"<p>Start the UI.</p> Source code in <code>sdfest/vae/scripts/visualizer.py</code> <pre><code>def main():\n    \"\"\"Start the UI.\"\"\"\n    app = QApplication(sys.argv)\n\n    gui = VAEVisualizer()\n    gui.show()\n\n    app.exec_()\n</code></pre>"}]}