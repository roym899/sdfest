{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Getting Started SDFEst is a method for RGB-D-based shape and pose estimation using discretized signed distance fields. Installation Run pip install sdfest to install the latest release of the package. API Reference sdfest","title":"Getting started"},{"location":"#getting-started","text":"SDFEst is a method for RGB-D-based shape and pose estimation using discretized signed distance fields.","title":"Getting Started"},{"location":"#installation","text":"Run pip install sdfest to install the latest release of the package.","title":"Installation"},{"location":"#api-reference","text":"","title":"API Reference"},{"location":"#sdfest","text":"","title":"sdfest"},{"location":"reference/SUMMARY/","text":"differentiable_renderer scripts experiments sdf_renderer estimation losses metrics scripts play_log real_data rendering_evaluation simple_setup synthetic initialization datasets dataset_utils generated_dataset nocs_dataset nocs_utils redwood_dataset pointnet pointset_utils quaternion_utils scripts train sdf_pose_network sdf_utils so3grid utils utils vae scripts benchmark process_shapenet train visualizer sdf_dataset sdf_utils sdf_vae torch_utils utils","title":"SUMMARY"},{"location":"reference/utils/","text":"sdfest.utils Common utility functions. load_model_weights load_model_weights ( path : str , model : torch . nn . Module , map_location : torch . device , model_weights_url : Optional [ str ] = None , ) -> None Load model weights from path or download weights from URL if file does not exist. PARAMETER DESCRIPTION path Path to model weights. TYPE: str model Path to model weights. TYPE: torch . nn . Module map_location See torch.load. TYPE: torch . device model_weights_url URL to download model weights from if path does not exist. TYPE: Optional [ str ] DEFAULT: None Source code in sdfest/utils.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def load_model_weights ( path : str , model : torch . nn . Module , map_location : torch . device , model_weights_url : Optional [ str ] = None , ) -> None : \"\"\"Load model weights from path or download weights from URL if file does not exist. Args: path: Path to model weights. model: Path to model weights. map_location: See torch.load. model_weights_url: URL to download model weights from if path does not exist. \"\"\" resolved_path = yoco . resolve_path ( path , search_paths = [ \".\" , \"~/.sdfest/model_weights/\" ] ) if not os . path . exists ( resolved_path ): if model_weights_url is not None : if not os . path . isabs ( resolved_path ): resolved_path = os . path . expanduser ( os . path . join ( \"~/.sdfest/model_weights\" , resolved_path ) ) os . makedirs ( os . path . dirname ( resolved_path ), exist_ok = True ) print ( f \"Model weights { resolved_path } not found.\" ) print ( f \"Downloading from { model_weights_url } \" ) download ( model_weights_url , resolved_path ) else : print ( f \"Model weights { resolved_path } not found. Aborting.\" ) exit ( 0 ) state_dict = torch . load ( resolved_path , map_location = map_location ) model . load_state_dict ( state_dict )","title":"utils"},{"location":"reference/utils/#sdfest.utils","text":"Common utility functions.","title":"utils"},{"location":"reference/utils/#sdfest.utils.load_model_weights","text":"load_model_weights ( path : str , model : torch . nn . Module , map_location : torch . device , model_weights_url : Optional [ str ] = None , ) -> None Load model weights from path or download weights from URL if file does not exist. PARAMETER DESCRIPTION path Path to model weights. TYPE: str model Path to model weights. TYPE: torch . nn . Module map_location See torch.load. TYPE: torch . device model_weights_url URL to download model weights from if path does not exist. TYPE: Optional [ str ] DEFAULT: None Source code in sdfest/utils.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def load_model_weights ( path : str , model : torch . nn . Module , map_location : torch . device , model_weights_url : Optional [ str ] = None , ) -> None : \"\"\"Load model weights from path or download weights from URL if file does not exist. Args: path: Path to model weights. model: Path to model weights. map_location: See torch.load. model_weights_url: URL to download model weights from if path does not exist. \"\"\" resolved_path = yoco . resolve_path ( path , search_paths = [ \".\" , \"~/.sdfest/model_weights/\" ] ) if not os . path . exists ( resolved_path ): if model_weights_url is not None : if not os . path . isabs ( resolved_path ): resolved_path = os . path . expanduser ( os . path . join ( \"~/.sdfest/model_weights\" , resolved_path ) ) os . makedirs ( os . path . dirname ( resolved_path ), exist_ok = True ) print ( f \"Model weights { resolved_path } not found.\" ) print ( f \"Downloading from { model_weights_url } \" ) download ( model_weights_url , resolved_path ) else : print ( f \"Model weights { resolved_path } not found. Aborting.\" ) exit ( 0 ) state_dict = torch . load ( resolved_path , map_location = map_location ) model . load_state_dict ( state_dict )","title":"load_model_weights()"},{"location":"reference/differentiable_renderer/sdf_renderer/","text":"sdfest.differentiable_renderer.sdf_renderer PyTorch interface for diffferentiable renderer. This module provides two functions render_depth: numpy-based CPU implementation (not recommended, only for development) render_depth_gpu: CUDA implementation (fast) Camera Pinhole camera parameters. This class allows conversion between different pixel conventions, i.e., pixel center at (0.5, 0.5) (as common in computer graphics), and (0, 0) as common in computer vision. Source code in sdfest/differentiable_renderer/sdf_renderer.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 class Camera : \"\"\"Pinhole camera parameters. This class allows conversion between different pixel conventions, i.e., pixel center at (0.5, 0.5) (as common in computer graphics), and (0, 0) as common in computer vision. \"\"\" def __init__ ( self , width : int , height : int , fx : float , fy : float , cx : float , cy : float , s : float = 0.0 , pixel_center : float = 0.0 , ): \"\"\"Initialize camera parameters. Note that the principal point is only fully defined in combination with pixel_center. The pixel_center defines the relation between continuous image plane coordinates and discrete pixel coordinates. A discrete image coordinate (x, y) will correspond to the continuous image coordinate (x + pixel_center, y + pixel_center). Normally pixel_center will be either 0 or 0.5. During calibration it depends on the convention the point features used to compute the calibration matrix. Note that if pixel_center == 0, the corresponding continuous coordinate interval for a pixel are [x-0.5, x+0.5). I.e., proper rounding has to be done to convert from continuous coordinate to the corresponding discrete coordinate. For pixel_center == 0.5, the corresponding continuous coordinate interval for a pixel are [x, x+1). I.e., floor is sufficient to convert from continuous coordinate to the corresponding discrete coordinate. Args: width: Number of pixels in horizontal direction. height: Number of pixels in vertical direction. fx: Horizontal focal length. fy: Vertical focal length. cx: Principal point x-coordinate. cy: Principal point y-coordinate. s: Skew. pixel_center: The center offset for the provided principal point. \"\"\" # focal length self . fx = fx self . fy = fy # principal point self . cx = cx self . cy = cy self . pixel_center = pixel_center # skew self . s = s # image dimensions self . width = width self . height = height def get_o3d_pinhole_camera_parameters ( self ) -> o3d . camera . PinholeCameraParameters (): \"\"\"Convert camera to Open3D pinhole camera parameters. Open3D camera is at (0,0,0) looking along positive z axis (i.e., positive z values are in front of camera). Open3D expects camera with pixel_center = 0 and does not support skew. Returns: The pinhole camera parameters. \"\"\" fx , fy , cx , cy , _ = self . get_pinhole_camera_parameters ( 0 ) params = o3d . camera . PinholeCameraParameters () params . intrinsic . set_intrinsics ( self . width , self . height , fx , fy , cx , cy ) params . extrinsic = np . array ( [[ 1 , 0 , 0 , 0 ], [ 0 , 1 , 0 , 0 ], [ 0 , 0 , 1 , 0 ], [ 0 , 0 , 0 , 1 ]] ) return params def get_pinhole_camera_parameters ( self , pixel_center : float ) -> Tuple : \"\"\"Convert camera to general camera parameters. Args: pixel_center: At which ratio of a square the pixel center should be for the resulting parameters. Typically 0 or 0.5. See class documentation for more info. Returns: - fx, fy: The horizontal and vertical focal length - cx, cy: The position of the principal point in continuous image plane coordinates considering the provided pixel center and the pixel center specified during the construction. - s: The skew. \"\"\" cx_corrected = self . cx - self . pixel_center + pixel_center cy_corrected = self . cy - self . pixel_center + pixel_center return self . fx , self . fy , cx_corrected , cy_corrected , self . s __init__ __init__ ( width : int , height : int , fx : float , fy : float , cx : float , cy : float , s : float = 0.0 , pixel_center : float = 0.0 , ) Initialize camera parameters. Note that the principal point is only fully defined in combination with pixel_center. The pixel_center defines the relation between continuous image plane coordinates and discrete pixel coordinates. A discrete image coordinate (x, y) will correspond to the continuous image coordinate (x + pixel_center, y + pixel_center). Normally pixel_center will be either 0 or 0.5. During calibration it depends on the convention the point features used to compute the calibration matrix. Note that if pixel_center == 0, the corresponding continuous coordinate interval for a pixel are [x-0.5, x+0.5). I.e., proper rounding has to be done to convert from continuous coordinate to the corresponding discrete coordinate. For pixel_center == 0.5, the corresponding continuous coordinate interval for a pixel are [x, x+1). I.e., floor is sufficient to convert from continuous coordinate to the corresponding discrete coordinate. PARAMETER DESCRIPTION width Number of pixels in horizontal direction. TYPE: int height Number of pixels in vertical direction. TYPE: int fx Horizontal focal length. TYPE: float fy Vertical focal length. TYPE: float cx Principal point x-coordinate. TYPE: float cy Principal point y-coordinate. TYPE: float s Skew. TYPE: float DEFAULT: 0.0 pixel_center The center offset for the provided principal point. TYPE: float DEFAULT: 0.0 Source code in sdfest/differentiable_renderer/sdf_renderer.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def __init__ ( self , width : int , height : int , fx : float , fy : float , cx : float , cy : float , s : float = 0.0 , pixel_center : float = 0.0 , ): \"\"\"Initialize camera parameters. Note that the principal point is only fully defined in combination with pixel_center. The pixel_center defines the relation between continuous image plane coordinates and discrete pixel coordinates. A discrete image coordinate (x, y) will correspond to the continuous image coordinate (x + pixel_center, y + pixel_center). Normally pixel_center will be either 0 or 0.5. During calibration it depends on the convention the point features used to compute the calibration matrix. Note that if pixel_center == 0, the corresponding continuous coordinate interval for a pixel are [x-0.5, x+0.5). I.e., proper rounding has to be done to convert from continuous coordinate to the corresponding discrete coordinate. For pixel_center == 0.5, the corresponding continuous coordinate interval for a pixel are [x, x+1). I.e., floor is sufficient to convert from continuous coordinate to the corresponding discrete coordinate. Args: width: Number of pixels in horizontal direction. height: Number of pixels in vertical direction. fx: Horizontal focal length. fy: Vertical focal length. cx: Principal point x-coordinate. cy: Principal point y-coordinate. s: Skew. pixel_center: The center offset for the provided principal point. \"\"\" # focal length self . fx = fx self . fy = fy # principal point self . cx = cx self . cy = cy self . pixel_center = pixel_center # skew self . s = s # image dimensions self . width = width self . height = height get_o3d_pinhole_camera_parameters get_o3d_pinhole_camera_parameters () -> o3d . camera . PinholeCameraParameters () Convert camera to Open3D pinhole camera parameters. Open3D camera is at (0,0,0) looking along positive z axis (i.e., positive z values are in front of camera). Open3D expects camera with pixel_center = 0 and does not support skew. RETURNS DESCRIPTION o3d . camera . PinholeCameraParameters () The pinhole camera parameters. Source code in sdfest/differentiable_renderer/sdf_renderer.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def get_o3d_pinhole_camera_parameters ( self ) -> o3d . camera . PinholeCameraParameters (): \"\"\"Convert camera to Open3D pinhole camera parameters. Open3D camera is at (0,0,0) looking along positive z axis (i.e., positive z values are in front of camera). Open3D expects camera with pixel_center = 0 and does not support skew. Returns: The pinhole camera parameters. \"\"\" fx , fy , cx , cy , _ = self . get_pinhole_camera_parameters ( 0 ) params = o3d . camera . PinholeCameraParameters () params . intrinsic . set_intrinsics ( self . width , self . height , fx , fy , cx , cy ) params . extrinsic = np . array ( [[ 1 , 0 , 0 , 0 ], [ 0 , 1 , 0 , 0 ], [ 0 , 0 , 1 , 0 ], [ 0 , 0 , 0 , 1 ]] ) return params get_pinhole_camera_parameters get_pinhole_camera_parameters ( pixel_center : float ) -> Tuple Convert camera to general camera parameters. PARAMETER DESCRIPTION pixel_center At which ratio of a square the pixel center should be for the resulting parameters. Typically 0 or 0.5. See class documentation for more info. TYPE: float RETURNS DESCRIPTION Tuple fx, fy: The horizontal and vertical focal length Tuple cx, cy: The position of the principal point in continuous image plane coordinates considering the provided pixel center and the pixel center specified during the construction. Tuple s: The skew. Source code in sdfest/differentiable_renderer/sdf_renderer.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 def get_pinhole_camera_parameters ( self , pixel_center : float ) -> Tuple : \"\"\"Convert camera to general camera parameters. Args: pixel_center: At which ratio of a square the pixel center should be for the resulting parameters. Typically 0 or 0.5. See class documentation for more info. Returns: - fx, fy: The horizontal and vertical focal length - cx, cy: The position of the principal point in continuous image plane coordinates considering the provided pixel center and the pixel center specified during the construction. - s: The skew. \"\"\" cx_corrected = self . cx - self . pixel_center + pixel_center cy_corrected = self . cy - self . pixel_center + pixel_center return self . fx , self . fy , cx_corrected , cy_corrected , self . s SDFRendererFunction Bases: torch . autograd . Function Renderer function for signed distance fields. Source code in sdfest/differentiable_renderer/sdf_renderer.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 class SDFRendererFunction ( torch . autograd . Function ): \"\"\"Renderer function for signed distance fields.\"\"\" @staticmethod def forward ( ctx , sdf : torch . Tensor , position : torch . Tensor , orientation : torch . Tensor , inv_scale : torch . Tensor , width : Optional [ int ] = None , height : Optional [ int ] = None , fov_deg : Optional [ float ] = None , threshold : Optional [ float ] = 0.0 , camera : Optional [ Camera ] = None , ) -> torch . Tensor : \"\"\"Render depth image of a 7-DOF discrete signed distance field. The SDF position is assumed to be in the camera frame under OpenGL convention. That is, camera looks along negative z-axis, y pointing upwards and x to the right. Note that the rendered image will still follow the classical computer vision convention, of first row being up in the camera frame. This function internally usese numpy and is very slow due to the fully serial implementation. This is only for testing purposes. Use the GPU version for practical performance. Camera can be specified either via camera parameter giving most flexbility or alternatively by providing width, height and fov_deg. Args: ctx: Context object to stash information. See https://pytorch.org/docs/stable/notes/extending.html. sdf: Discrete signed distance field with shape (M, M, M). Arbitrary (but uniform) resolutions are supported. position: The position of the signed distance field origin in the camera frame. orientation: The orientation of the SDF as a normalized quaternion. inv_scale: The inverted scale of the SDF. The scale of an SDF the half-width of the full SDF volume. width: Number of pixels in x direction. Recommended to use camera instead. height: Number of pixels in y direction. Recommended to use camera instead. fov_deg: The horizontal field of view (i.e., in x direction). Pixels are assumed to be square, i.e., fx=fy, computed based on width and fov_deg. Recommended to use camera instead. threshold: The distance threshold at which sphere tracing should be stopped. Smaller value will be more accurate, but slower and might potentially lead to holes in the rendering for thin structures in the SDF. Larger values will be faster, but will overestimate the thickness. Should always be positive to guarantee convergence. camera: Camera parameters (not supported right now). Returns: The rendered depth image. \"\"\" if None not in [ width , height , fov_deg ] and camera is not None : raise ValueError ( \"Either width+height+fov_dev or camera must be provided.\" ) if camera is not None : raise NotImplementedError ( \"Only width+height+fov_dev currently supported for CPU\" ) # for simplicity use numpy internally ctx . save_for_backward ( sdf , position , orientation , inv_scale ) sdf = sdf . detach () . numpy () position = position . detach () . numpy () orientation = orientation . detach () . numpy () inv_scale = inv_scale . detach () . numpy () sdf_object = SDFObject ( sdf ) image , derivatives = _render_depth ( sdf_object , width , height , fov_deg , \"d\" , threshold , position , orientation , inv_scale , ) ctx . derivatives = derivatives return torch . from_numpy ( image ) @staticmethod def backward ( ctx , inp : torch . Tensor ): \"\"\"Compute gradients of inputs with respect to the provided gradients. Normally called by PyTorch as part of a call to backward() on a loss. Args: grad_depth_image: Returns: Gradients of discretized signed distance field, position, orientation, inverted scale followed by None for all the non-supported variables passed to forward. \"\"\" derivatives = ctx . derivatives sdf , pos , quat , inv_s = ctx . saved_tensors g_image = inp . numpy () g_sdf = g_p = g_q = g_is = g_w = g_h = g_fov = g_thresh = g_camera = None g_sdf = torch . zeros_like ( sdf ) g_p = torch . empty_like ( pos ) g_q = torch . empty_like ( quat ) g_is = torch . empty_like ( inv_s ) g_p [ 0 ] = torch . tensor ( np . sum ( derivatives [ \"x\" ] * g_image )) g_p [ 1 ] = torch . tensor ( np . sum ( derivatives [ \"y\" ] * g_image )) g_p [ 2 ] = torch . tensor ( np . sum ( derivatives [ \"z\" ] * g_image )) g_q [ 0 ] = torch . tensor ( np . sum ( derivatives [ \"qx\" ] * g_image )) g_q [ 1 ] = torch . tensor ( np . sum ( derivatives [ \"qy\" ] * g_image )) g_q [ 2 ] = torch . tensor ( np . sum ( derivatives [ \"qz\" ] * g_image )) g_q [ 3 ] = torch . tensor ( np . sum ( derivatives [ \"qw\" ] * g_image )) g_is = torch . tensor ( np . sum ( derivatives [ \"s_inv\" ] * g_image )) if \"sdf\" in derivatives : for k , v in derivatives [ \"sdf\" ] . items (): g_sdf [ k ] = torch . tensor ( np . sum ( v * g_image )) return g_sdf , g_p , g_q , g_is , g_w , g_h , g_fov , g_thresh , g_camera forward staticmethod forward ( ctx , sdf : torch . Tensor , position : torch . Tensor , orientation : torch . Tensor , inv_scale : torch . Tensor , width : Optional [ int ] = None , height : Optional [ int ] = None , fov_deg : Optional [ float ] = None , threshold : Optional [ float ] = 0.0 , camera : Optional [ Camera ] = None , ) -> torch . Tensor Render depth image of a 7-DOF discrete signed distance field. The SDF position is assumed to be in the camera frame under OpenGL convention. That is, camera looks along negative z-axis, y pointing upwards and x to the right. Note that the rendered image will still follow the classical computer vision convention, of first row being up in the camera frame. This function internally usese numpy and is very slow due to the fully serial implementation. This is only for testing purposes. Use the GPU version for practical performance. Camera can be specified either via camera parameter giving most flexbility or alternatively by providing width, height and fov_deg. PARAMETER DESCRIPTION ctx Context object to stash information. See https://pytorch.org/docs/stable/notes/extending.html. sdf Discrete signed distance field with shape (M, M, M). Arbitrary (but uniform) resolutions are supported. TYPE: torch . Tensor position The position of the signed distance field origin in the camera frame. TYPE: torch . Tensor orientation The orientation of the SDF as a normalized quaternion. TYPE: torch . Tensor inv_scale The inverted scale of the SDF. The scale of an SDF the half-width of the full SDF volume. TYPE: torch . Tensor width Number of pixels in x direction. Recommended to use camera instead. TYPE: Optional [ int ] DEFAULT: None height Number of pixels in y direction. Recommended to use camera instead. TYPE: Optional [ int ] DEFAULT: None fov_deg The horizontal field of view (i.e., in x direction). Pixels are assumed to be square, i.e., fx=fy, computed based on width and fov_deg. Recommended to use camera instead. TYPE: Optional [ float ] DEFAULT: None threshold The distance threshold at which sphere tracing should be stopped. Smaller value will be more accurate, but slower and might potentially lead to holes in the rendering for thin structures in the SDF. Larger values will be faster, but will overestimate the thickness. Should always be positive to guarantee convergence. TYPE: Optional [ float ] DEFAULT: 0.0 camera Camera parameters (not supported right now). TYPE: Optional [ Camera ] DEFAULT: None RETURNS DESCRIPTION torch . Tensor The rendered depth image. Source code in sdfest/differentiable_renderer/sdf_renderer.py 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 @staticmethod def forward ( ctx , sdf : torch . Tensor , position : torch . Tensor , orientation : torch . Tensor , inv_scale : torch . Tensor , width : Optional [ int ] = None , height : Optional [ int ] = None , fov_deg : Optional [ float ] = None , threshold : Optional [ float ] = 0.0 , camera : Optional [ Camera ] = None , ) -> torch . Tensor : \"\"\"Render depth image of a 7-DOF discrete signed distance field. The SDF position is assumed to be in the camera frame under OpenGL convention. That is, camera looks along negative z-axis, y pointing upwards and x to the right. Note that the rendered image will still follow the classical computer vision convention, of first row being up in the camera frame. This function internally usese numpy and is very slow due to the fully serial implementation. This is only for testing purposes. Use the GPU version for practical performance. Camera can be specified either via camera parameter giving most flexbility or alternatively by providing width, height and fov_deg. Args: ctx: Context object to stash information. See https://pytorch.org/docs/stable/notes/extending.html. sdf: Discrete signed distance field with shape (M, M, M). Arbitrary (but uniform) resolutions are supported. position: The position of the signed distance field origin in the camera frame. orientation: The orientation of the SDF as a normalized quaternion. inv_scale: The inverted scale of the SDF. The scale of an SDF the half-width of the full SDF volume. width: Number of pixels in x direction. Recommended to use camera instead. height: Number of pixels in y direction. Recommended to use camera instead. fov_deg: The horizontal field of view (i.e., in x direction). Pixels are assumed to be square, i.e., fx=fy, computed based on width and fov_deg. Recommended to use camera instead. threshold: The distance threshold at which sphere tracing should be stopped. Smaller value will be more accurate, but slower and might potentially lead to holes in the rendering for thin structures in the SDF. Larger values will be faster, but will overestimate the thickness. Should always be positive to guarantee convergence. camera: Camera parameters (not supported right now). Returns: The rendered depth image. \"\"\" if None not in [ width , height , fov_deg ] and camera is not None : raise ValueError ( \"Either width+height+fov_dev or camera must be provided.\" ) if camera is not None : raise NotImplementedError ( \"Only width+height+fov_dev currently supported for CPU\" ) # for simplicity use numpy internally ctx . save_for_backward ( sdf , position , orientation , inv_scale ) sdf = sdf . detach () . numpy () position = position . detach () . numpy () orientation = orientation . detach () . numpy () inv_scale = inv_scale . detach () . numpy () sdf_object = SDFObject ( sdf ) image , derivatives = _render_depth ( sdf_object , width , height , fov_deg , \"d\" , threshold , position , orientation , inv_scale , ) ctx . derivatives = derivatives return torch . from_numpy ( image ) backward staticmethod backward ( ctx , inp : torch . Tensor ) Compute gradients of inputs with respect to the provided gradients. Normally called by PyTorch as part of a call to backward() on a loss. PARAMETER DESCRIPTION grad_depth_image RETURNS DESCRIPTION Gradients of discretized signed distance field, position, orientation, inverted scale followed by None for all the non-supported variables passed to forward. Source code in sdfest/differentiable_renderer/sdf_renderer.py 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 @staticmethod def backward ( ctx , inp : torch . Tensor ): \"\"\"Compute gradients of inputs with respect to the provided gradients. Normally called by PyTorch as part of a call to backward() on a loss. Args: grad_depth_image: Returns: Gradients of discretized signed distance field, position, orientation, inverted scale followed by None for all the non-supported variables passed to forward. \"\"\" derivatives = ctx . derivatives sdf , pos , quat , inv_s = ctx . saved_tensors g_image = inp . numpy () g_sdf = g_p = g_q = g_is = g_w = g_h = g_fov = g_thresh = g_camera = None g_sdf = torch . zeros_like ( sdf ) g_p = torch . empty_like ( pos ) g_q = torch . empty_like ( quat ) g_is = torch . empty_like ( inv_s ) g_p [ 0 ] = torch . tensor ( np . sum ( derivatives [ \"x\" ] * g_image )) g_p [ 1 ] = torch . tensor ( np . sum ( derivatives [ \"y\" ] * g_image )) g_p [ 2 ] = torch . tensor ( np . sum ( derivatives [ \"z\" ] * g_image )) g_q [ 0 ] = torch . tensor ( np . sum ( derivatives [ \"qx\" ] * g_image )) g_q [ 1 ] = torch . tensor ( np . sum ( derivatives [ \"qy\" ] * g_image )) g_q [ 2 ] = torch . tensor ( np . sum ( derivatives [ \"qz\" ] * g_image )) g_q [ 3 ] = torch . tensor ( np . sum ( derivatives [ \"qw\" ] * g_image )) g_is = torch . tensor ( np . sum ( derivatives [ \"s_inv\" ] * g_image )) if \"sdf\" in derivatives : for k , v in derivatives [ \"sdf\" ] . items (): g_sdf [ k ] = torch . tensor ( np . sum ( v * g_image )) return g_sdf , g_p , g_q , g_is , g_w , g_h , g_fov , g_thresh , g_camera SDFRendererFunctionGPU Bases: torch . autograd . Function Renderer function for signed distance fields. Source code in sdfest/differentiable_renderer/sdf_renderer.py 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 class SDFRendererFunctionGPU ( torch . autograd . Function ): \"\"\"Renderer function for signed distance fields.\"\"\" @staticmethod def forward ( ctx , sdf : torch . Tensor , position : torch . Tensor , orientation : torch . Tensor , inv_scale : torch . Tensor , threshold : Optional [ float ] = 0.0 , camera : Optional [ Camera ] = None , ) -> torch . Tensor : \"\"\"Render depth image of a 7-DOF discrete signed distance field on the GPU. Also see render_depth_gpu for documentation. Args: ctx: Context object to stash information. See https://pytorch.org/docs/stable/notes/extending.html. sdf: Discrete signed distance field with shape (M, M, M). Arbitrary (but uniform) resolutions are supported. position: The position of the signed distance field origin in the camera frame. orientation: The orientation of the SDF as a normalized quaternion. inv_scale: The inverted scale of the SDF. The scale of an SDF the half-width of the full SDF volume. threshold: The distance threshold at which sphere tracing should be stopped. Smaller value will be more accurate, but slower and might potentially lead to holes in the rendering for thin structures in the SDF. Larger values will be faster, but will overestimate the thickness. Should always be positive to guarantee convergence. camera: Camera parameters. Returns: The rendered depth image. \"\"\" fx , fy , cx , cy , _ = camera . get_pinhole_camera_parameters ( 0.5 ) ( image ,) = sdf_renderer_cpp . forward ( sdf , position , orientation , inv_scale , camera . width , camera . height , cx , cy , fx , fy , threshold , ) ctx . save_for_backward ( image , sdf , position , orientation , inv_scale ) ctx . width = camera . width ctx . height = camera . height ctx . fx = fx ctx . fy = fy ctx . cx = cx ctx . cy = cy return image @staticmethod def backward ( ctx , grad_depth_image : torch . Tensor ): \"\"\"Compute gradients of inputs with respect to the provided gradients. Normally called by PyTorch as part of a call to backward() on a loss. Args: grad_depth_image: Returns: Gradients of discretized signed distance field, position, orientation, inverted scale followed by None for all the non-supported variables passed to forward. \"\"\" g_sdf = g_p = g_q = g_is = g_thresh = g_camera = None g_sdf , g_p , g_q , g_is = sdf_renderer_cpp . backward ( grad_depth_image , * ctx . saved_tensors , ctx . width , ctx . height , ctx . cx , ctx . cy , ctx . fx , ctx . fy ) return g_sdf , g_p , g_q , g_is , g_thresh , g_camera forward staticmethod forward ( ctx , sdf : torch . Tensor , position : torch . Tensor , orientation : torch . Tensor , inv_scale : torch . Tensor , threshold : Optional [ float ] = 0.0 , camera : Optional [ Camera ] = None , ) -> torch . Tensor Render depth image of a 7-DOF discrete signed distance field on the GPU. Also see render_depth_gpu for documentation. PARAMETER DESCRIPTION ctx Context object to stash information. See https://pytorch.org/docs/stable/notes/extending.html. sdf Discrete signed distance field with shape (M, M, M). Arbitrary (but uniform) resolutions are supported. TYPE: torch . Tensor position The position of the signed distance field origin in the camera frame. TYPE: torch . Tensor orientation The orientation of the SDF as a normalized quaternion. TYPE: torch . Tensor inv_scale The inverted scale of the SDF. The scale of an SDF the half-width of the full SDF volume. TYPE: torch . Tensor threshold The distance threshold at which sphere tracing should be stopped. Smaller value will be more accurate, but slower and might potentially lead to holes in the rendering for thin structures in the SDF. Larger values will be faster, but will overestimate the thickness. Should always be positive to guarantee convergence. TYPE: Optional [ float ] DEFAULT: 0.0 camera Camera parameters. TYPE: Optional [ Camera ] DEFAULT: None RETURNS DESCRIPTION torch . Tensor The rendered depth image. Source code in sdfest/differentiable_renderer/sdf_renderer.py 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 @staticmethod def forward ( ctx , sdf : torch . Tensor , position : torch . Tensor , orientation : torch . Tensor , inv_scale : torch . Tensor , threshold : Optional [ float ] = 0.0 , camera : Optional [ Camera ] = None , ) -> torch . Tensor : \"\"\"Render depth image of a 7-DOF discrete signed distance field on the GPU. Also see render_depth_gpu for documentation. Args: ctx: Context object to stash information. See https://pytorch.org/docs/stable/notes/extending.html. sdf: Discrete signed distance field with shape (M, M, M). Arbitrary (but uniform) resolutions are supported. position: The position of the signed distance field origin in the camera frame. orientation: The orientation of the SDF as a normalized quaternion. inv_scale: The inverted scale of the SDF. The scale of an SDF the half-width of the full SDF volume. threshold: The distance threshold at which sphere tracing should be stopped. Smaller value will be more accurate, but slower and might potentially lead to holes in the rendering for thin structures in the SDF. Larger values will be faster, but will overestimate the thickness. Should always be positive to guarantee convergence. camera: Camera parameters. Returns: The rendered depth image. \"\"\" fx , fy , cx , cy , _ = camera . get_pinhole_camera_parameters ( 0.5 ) ( image ,) = sdf_renderer_cpp . forward ( sdf , position , orientation , inv_scale , camera . width , camera . height , cx , cy , fx , fy , threshold , ) ctx . save_for_backward ( image , sdf , position , orientation , inv_scale ) ctx . width = camera . width ctx . height = camera . height ctx . fx = fx ctx . fy = fy ctx . cx = cx ctx . cy = cy return image backward staticmethod backward ( ctx , grad_depth_image : torch . Tensor ) Compute gradients of inputs with respect to the provided gradients. Normally called by PyTorch as part of a call to backward() on a loss. PARAMETER DESCRIPTION grad_depth_image TYPE: torch . Tensor RETURNS DESCRIPTION Gradients of discretized signed distance field, position, orientation, inverted scale followed by None for all the non-supported variables passed to forward. Source code in sdfest/differentiable_renderer/sdf_renderer.py 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 @staticmethod def backward ( ctx , grad_depth_image : torch . Tensor ): \"\"\"Compute gradients of inputs with respect to the provided gradients. Normally called by PyTorch as part of a call to backward() on a loss. Args: grad_depth_image: Returns: Gradients of discretized signed distance field, position, orientation, inverted scale followed by None for all the non-supported variables passed to forward. \"\"\" g_sdf = g_p = g_q = g_is = g_thresh = g_camera = None g_sdf , g_p , g_q , g_is = sdf_renderer_cpp . backward ( grad_depth_image , * ctx . saved_tensors , ctx . width , ctx . height , ctx . cx , ctx . cy , ctx . fx , ctx . fy ) return g_sdf , g_p , g_q , g_is , g_thresh , g_camera render_depth_gpu render_depth_gpu ( sdf : torch . Tensor , position : torch . Tensor , orientation : torch . Tensor , inv_scale : torch . Tensor , width : Optional [ int ] = None , height : Optional [ int ] = None , fov_deg : Optional [ float ] = None , threshold : Optional [ float ] = 0.0 , camera : Optional [ Camera ] = None , ) Render depth image of a 7-DOF discrete signed distance field on the GPU. The SDF position is assumed to be in the camera frame under OpenGL convention. That is, camera looks along negative z-axis, y pointing upwards and x to the right. Note that the rendered image will still follow the classical computer vision convention, of first row being up in the camera frame. Camera can be specified either via camera parameter giving most flexbility or alternatively by providing width, height and fov_deg. All provided tensors must reside on the GPU. PARAMETER DESCRIPTION sdf Discrete signed distance field with shape (M, M, M). Arbitrary (but uniform) resolutions are supported. TYPE: torch . Tensor position The position of the signed distance field origin in the camera frame. TYPE: torch . Tensor orientation The orientation of the SDF as a normalized quaternion. TYPE: torch . Tensor inv_scale The inverted scale of the SDF. The scale of an SDF the half-width of the full SDF volume. TYPE: torch . Tensor width Number of pixels in x direction. Recommended to use camera instead. TYPE: Optional [ int ] DEFAULT: None height Number of pixels in y direction. Recommended to use camera instead. TYPE: Optional [ int ] DEFAULT: None fov_deg The horizontal field of view (i.e., in x direction). Pixels are assumed to be square, i.e., fx=fy, computed based on width and fov_deg. Recommended to use camera instead. TYPE: Optional [ float ] DEFAULT: None threshold The distance threshold at which sphere tracing should be stopped. Smaller value will be more accurate, but slower and might potentially lead to holes in the rendering for thin structures in the SDF. Larger values will be faster, but will overestimate the thickness. Should always be positive to guarantee convergence. TYPE: Optional [ float ] DEFAULT: 0.0 camera Camera parameters. TYPE: Optional [ Camera ] DEFAULT: None RETURNS DESCRIPTION The rendered depth image. Source code in sdfest/differentiable_renderer/sdf_renderer.py 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 def render_depth_gpu ( sdf : torch . Tensor , position : torch . Tensor , orientation : torch . Tensor , inv_scale : torch . Tensor , width : Optional [ int ] = None , height : Optional [ int ] = None , fov_deg : Optional [ float ] = None , threshold : Optional [ float ] = 0.0 , camera : Optional [ Camera ] = None , ): \"\"\"Render depth image of a 7-DOF discrete signed distance field on the GPU. The SDF position is assumed to be in the camera frame under OpenGL convention. That is, camera looks along negative z-axis, y pointing upwards and x to the right. Note that the rendered image will still follow the classical computer vision convention, of first row being up in the camera frame. Camera can be specified either via camera parameter giving most flexbility or alternatively by providing width, height and fov_deg. All provided tensors must reside on the GPU. Args: sdf: Discrete signed distance field with shape (M, M, M). Arbitrary (but uniform) resolutions are supported. position: The position of the signed distance field origin in the camera frame. orientation: The orientation of the SDF as a normalized quaternion. inv_scale: The inverted scale of the SDF. The scale of an SDF the half-width of the full SDF volume. width: Number of pixels in x direction. Recommended to use camera instead. height: Number of pixels in y direction. Recommended to use camera instead. fov_deg: The horizontal field of view (i.e., in x direction). Pixels are assumed to be square, i.e., fx=fy, computed based on width and fov_deg. Recommended to use camera instead. threshold: The distance threshold at which sphere tracing should be stopped. Smaller value will be more accurate, but slower and might potentially lead to holes in the rendering for thin structures in the SDF. Larger values will be faster, but will overestimate the thickness. Should always be positive to guarantee convergence. camera: Camera parameters. Returns: The rendered depth image. \"\"\" if None not in [ width , height , fov_deg ] and camera is not None : raise ValueError ( \"Either width+height+fov_dev or camera must be provided.\" ) if camera is None : f = width / math . tan ( fov_deg * math . pi / 180.0 / 2.0 ) / 2 camera = Camera ( width , height , f , f , width / 2 , height / 2 , pixel_center = 0.5 ) return SDFRendererFunctionGPU . apply ( sdf , position , orientation , inv_scale , threshold , camera )","title":"sdf_renderer"},{"location":"reference/differentiable_renderer/sdf_renderer/#sdfest.differentiable_renderer.sdf_renderer","text":"PyTorch interface for diffferentiable renderer. This module provides two functions render_depth: numpy-based CPU implementation (not recommended, only for development) render_depth_gpu: CUDA implementation (fast)","title":"sdf_renderer"},{"location":"reference/differentiable_renderer/sdf_renderer/#sdfest.differentiable_renderer.sdf_renderer.Camera","text":"Pinhole camera parameters. This class allows conversion between different pixel conventions, i.e., pixel center at (0.5, 0.5) (as common in computer graphics), and (0, 0) as common in computer vision. Source code in sdfest/differentiable_renderer/sdf_renderer.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 class Camera : \"\"\"Pinhole camera parameters. This class allows conversion between different pixel conventions, i.e., pixel center at (0.5, 0.5) (as common in computer graphics), and (0, 0) as common in computer vision. \"\"\" def __init__ ( self , width : int , height : int , fx : float , fy : float , cx : float , cy : float , s : float = 0.0 , pixel_center : float = 0.0 , ): \"\"\"Initialize camera parameters. Note that the principal point is only fully defined in combination with pixel_center. The pixel_center defines the relation between continuous image plane coordinates and discrete pixel coordinates. A discrete image coordinate (x, y) will correspond to the continuous image coordinate (x + pixel_center, y + pixel_center). Normally pixel_center will be either 0 or 0.5. During calibration it depends on the convention the point features used to compute the calibration matrix. Note that if pixel_center == 0, the corresponding continuous coordinate interval for a pixel are [x-0.5, x+0.5). I.e., proper rounding has to be done to convert from continuous coordinate to the corresponding discrete coordinate. For pixel_center == 0.5, the corresponding continuous coordinate interval for a pixel are [x, x+1). I.e., floor is sufficient to convert from continuous coordinate to the corresponding discrete coordinate. Args: width: Number of pixels in horizontal direction. height: Number of pixels in vertical direction. fx: Horizontal focal length. fy: Vertical focal length. cx: Principal point x-coordinate. cy: Principal point y-coordinate. s: Skew. pixel_center: The center offset for the provided principal point. \"\"\" # focal length self . fx = fx self . fy = fy # principal point self . cx = cx self . cy = cy self . pixel_center = pixel_center # skew self . s = s # image dimensions self . width = width self . height = height def get_o3d_pinhole_camera_parameters ( self ) -> o3d . camera . PinholeCameraParameters (): \"\"\"Convert camera to Open3D pinhole camera parameters. Open3D camera is at (0,0,0) looking along positive z axis (i.e., positive z values are in front of camera). Open3D expects camera with pixel_center = 0 and does not support skew. Returns: The pinhole camera parameters. \"\"\" fx , fy , cx , cy , _ = self . get_pinhole_camera_parameters ( 0 ) params = o3d . camera . PinholeCameraParameters () params . intrinsic . set_intrinsics ( self . width , self . height , fx , fy , cx , cy ) params . extrinsic = np . array ( [[ 1 , 0 , 0 , 0 ], [ 0 , 1 , 0 , 0 ], [ 0 , 0 , 1 , 0 ], [ 0 , 0 , 0 , 1 ]] ) return params def get_pinhole_camera_parameters ( self , pixel_center : float ) -> Tuple : \"\"\"Convert camera to general camera parameters. Args: pixel_center: At which ratio of a square the pixel center should be for the resulting parameters. Typically 0 or 0.5. See class documentation for more info. Returns: - fx, fy: The horizontal and vertical focal length - cx, cy: The position of the principal point in continuous image plane coordinates considering the provided pixel center and the pixel center specified during the construction. - s: The skew. \"\"\" cx_corrected = self . cx - self . pixel_center + pixel_center cy_corrected = self . cy - self . pixel_center + pixel_center return self . fx , self . fy , cx_corrected , cy_corrected , self . s","title":"Camera"},{"location":"reference/differentiable_renderer/sdf_renderer/#sdfest.differentiable_renderer.sdf_renderer.Camera.__init__","text":"__init__ ( width : int , height : int , fx : float , fy : float , cx : float , cy : float , s : float = 0.0 , pixel_center : float = 0.0 , ) Initialize camera parameters. Note that the principal point is only fully defined in combination with pixel_center. The pixel_center defines the relation between continuous image plane coordinates and discrete pixel coordinates. A discrete image coordinate (x, y) will correspond to the continuous image coordinate (x + pixel_center, y + pixel_center). Normally pixel_center will be either 0 or 0.5. During calibration it depends on the convention the point features used to compute the calibration matrix. Note that if pixel_center == 0, the corresponding continuous coordinate interval for a pixel are [x-0.5, x+0.5). I.e., proper rounding has to be done to convert from continuous coordinate to the corresponding discrete coordinate. For pixel_center == 0.5, the corresponding continuous coordinate interval for a pixel are [x, x+1). I.e., floor is sufficient to convert from continuous coordinate to the corresponding discrete coordinate. PARAMETER DESCRIPTION width Number of pixels in horizontal direction. TYPE: int height Number of pixels in vertical direction. TYPE: int fx Horizontal focal length. TYPE: float fy Vertical focal length. TYPE: float cx Principal point x-coordinate. TYPE: float cy Principal point y-coordinate. TYPE: float s Skew. TYPE: float DEFAULT: 0.0 pixel_center The center offset for the provided principal point. TYPE: float DEFAULT: 0.0 Source code in sdfest/differentiable_renderer/sdf_renderer.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def __init__ ( self , width : int , height : int , fx : float , fy : float , cx : float , cy : float , s : float = 0.0 , pixel_center : float = 0.0 , ): \"\"\"Initialize camera parameters. Note that the principal point is only fully defined in combination with pixel_center. The pixel_center defines the relation between continuous image plane coordinates and discrete pixel coordinates. A discrete image coordinate (x, y) will correspond to the continuous image coordinate (x + pixel_center, y + pixel_center). Normally pixel_center will be either 0 or 0.5. During calibration it depends on the convention the point features used to compute the calibration matrix. Note that if pixel_center == 0, the corresponding continuous coordinate interval for a pixel are [x-0.5, x+0.5). I.e., proper rounding has to be done to convert from continuous coordinate to the corresponding discrete coordinate. For pixel_center == 0.5, the corresponding continuous coordinate interval for a pixel are [x, x+1). I.e., floor is sufficient to convert from continuous coordinate to the corresponding discrete coordinate. Args: width: Number of pixels in horizontal direction. height: Number of pixels in vertical direction. fx: Horizontal focal length. fy: Vertical focal length. cx: Principal point x-coordinate. cy: Principal point y-coordinate. s: Skew. pixel_center: The center offset for the provided principal point. \"\"\" # focal length self . fx = fx self . fy = fy # principal point self . cx = cx self . cy = cy self . pixel_center = pixel_center # skew self . s = s # image dimensions self . width = width self . height = height","title":"__init__()"},{"location":"reference/differentiable_renderer/sdf_renderer/#sdfest.differentiable_renderer.sdf_renderer.Camera.get_o3d_pinhole_camera_parameters","text":"get_o3d_pinhole_camera_parameters () -> o3d . camera . PinholeCameraParameters () Convert camera to Open3D pinhole camera parameters. Open3D camera is at (0,0,0) looking along positive z axis (i.e., positive z values are in front of camera). Open3D expects camera with pixel_center = 0 and does not support skew. RETURNS DESCRIPTION o3d . camera . PinholeCameraParameters () The pinhole camera parameters. Source code in sdfest/differentiable_renderer/sdf_renderer.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def get_o3d_pinhole_camera_parameters ( self ) -> o3d . camera . PinholeCameraParameters (): \"\"\"Convert camera to Open3D pinhole camera parameters. Open3D camera is at (0,0,0) looking along positive z axis (i.e., positive z values are in front of camera). Open3D expects camera with pixel_center = 0 and does not support skew. Returns: The pinhole camera parameters. \"\"\" fx , fy , cx , cy , _ = self . get_pinhole_camera_parameters ( 0 ) params = o3d . camera . PinholeCameraParameters () params . intrinsic . set_intrinsics ( self . width , self . height , fx , fy , cx , cy ) params . extrinsic = np . array ( [[ 1 , 0 , 0 , 0 ], [ 0 , 1 , 0 , 0 ], [ 0 , 0 , 1 , 0 ], [ 0 , 0 , 0 , 1 ]] ) return params","title":"get_o3d_pinhole_camera_parameters()"},{"location":"reference/differentiable_renderer/sdf_renderer/#sdfest.differentiable_renderer.sdf_renderer.Camera.get_pinhole_camera_parameters","text":"get_pinhole_camera_parameters ( pixel_center : float ) -> Tuple Convert camera to general camera parameters. PARAMETER DESCRIPTION pixel_center At which ratio of a square the pixel center should be for the resulting parameters. Typically 0 or 0.5. See class documentation for more info. TYPE: float RETURNS DESCRIPTION Tuple fx, fy: The horizontal and vertical focal length Tuple cx, cy: The position of the principal point in continuous image plane coordinates considering the provided pixel center and the pixel center specified during the construction. Tuple s: The skew. Source code in sdfest/differentiable_renderer/sdf_renderer.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 def get_pinhole_camera_parameters ( self , pixel_center : float ) -> Tuple : \"\"\"Convert camera to general camera parameters. Args: pixel_center: At which ratio of a square the pixel center should be for the resulting parameters. Typically 0 or 0.5. See class documentation for more info. Returns: - fx, fy: The horizontal and vertical focal length - cx, cy: The position of the principal point in continuous image plane coordinates considering the provided pixel center and the pixel center specified during the construction. - s: The skew. \"\"\" cx_corrected = self . cx - self . pixel_center + pixel_center cy_corrected = self . cy - self . pixel_center + pixel_center return self . fx , self . fy , cx_corrected , cy_corrected , self . s","title":"get_pinhole_camera_parameters()"},{"location":"reference/differentiable_renderer/sdf_renderer/#sdfest.differentiable_renderer.sdf_renderer.SDFRendererFunction","text":"Bases: torch . autograd . Function Renderer function for signed distance fields. Source code in sdfest/differentiable_renderer/sdf_renderer.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 class SDFRendererFunction ( torch . autograd . Function ): \"\"\"Renderer function for signed distance fields.\"\"\" @staticmethod def forward ( ctx , sdf : torch . Tensor , position : torch . Tensor , orientation : torch . Tensor , inv_scale : torch . Tensor , width : Optional [ int ] = None , height : Optional [ int ] = None , fov_deg : Optional [ float ] = None , threshold : Optional [ float ] = 0.0 , camera : Optional [ Camera ] = None , ) -> torch . Tensor : \"\"\"Render depth image of a 7-DOF discrete signed distance field. The SDF position is assumed to be in the camera frame under OpenGL convention. That is, camera looks along negative z-axis, y pointing upwards and x to the right. Note that the rendered image will still follow the classical computer vision convention, of first row being up in the camera frame. This function internally usese numpy and is very slow due to the fully serial implementation. This is only for testing purposes. Use the GPU version for practical performance. Camera can be specified either via camera parameter giving most flexbility or alternatively by providing width, height and fov_deg. Args: ctx: Context object to stash information. See https://pytorch.org/docs/stable/notes/extending.html. sdf: Discrete signed distance field with shape (M, M, M). Arbitrary (but uniform) resolutions are supported. position: The position of the signed distance field origin in the camera frame. orientation: The orientation of the SDF as a normalized quaternion. inv_scale: The inverted scale of the SDF. The scale of an SDF the half-width of the full SDF volume. width: Number of pixels in x direction. Recommended to use camera instead. height: Number of pixels in y direction. Recommended to use camera instead. fov_deg: The horizontal field of view (i.e., in x direction). Pixels are assumed to be square, i.e., fx=fy, computed based on width and fov_deg. Recommended to use camera instead. threshold: The distance threshold at which sphere tracing should be stopped. Smaller value will be more accurate, but slower and might potentially lead to holes in the rendering for thin structures in the SDF. Larger values will be faster, but will overestimate the thickness. Should always be positive to guarantee convergence. camera: Camera parameters (not supported right now). Returns: The rendered depth image. \"\"\" if None not in [ width , height , fov_deg ] and camera is not None : raise ValueError ( \"Either width+height+fov_dev or camera must be provided.\" ) if camera is not None : raise NotImplementedError ( \"Only width+height+fov_dev currently supported for CPU\" ) # for simplicity use numpy internally ctx . save_for_backward ( sdf , position , orientation , inv_scale ) sdf = sdf . detach () . numpy () position = position . detach () . numpy () orientation = orientation . detach () . numpy () inv_scale = inv_scale . detach () . numpy () sdf_object = SDFObject ( sdf ) image , derivatives = _render_depth ( sdf_object , width , height , fov_deg , \"d\" , threshold , position , orientation , inv_scale , ) ctx . derivatives = derivatives return torch . from_numpy ( image ) @staticmethod def backward ( ctx , inp : torch . Tensor ): \"\"\"Compute gradients of inputs with respect to the provided gradients. Normally called by PyTorch as part of a call to backward() on a loss. Args: grad_depth_image: Returns: Gradients of discretized signed distance field, position, orientation, inverted scale followed by None for all the non-supported variables passed to forward. \"\"\" derivatives = ctx . derivatives sdf , pos , quat , inv_s = ctx . saved_tensors g_image = inp . numpy () g_sdf = g_p = g_q = g_is = g_w = g_h = g_fov = g_thresh = g_camera = None g_sdf = torch . zeros_like ( sdf ) g_p = torch . empty_like ( pos ) g_q = torch . empty_like ( quat ) g_is = torch . empty_like ( inv_s ) g_p [ 0 ] = torch . tensor ( np . sum ( derivatives [ \"x\" ] * g_image )) g_p [ 1 ] = torch . tensor ( np . sum ( derivatives [ \"y\" ] * g_image )) g_p [ 2 ] = torch . tensor ( np . sum ( derivatives [ \"z\" ] * g_image )) g_q [ 0 ] = torch . tensor ( np . sum ( derivatives [ \"qx\" ] * g_image )) g_q [ 1 ] = torch . tensor ( np . sum ( derivatives [ \"qy\" ] * g_image )) g_q [ 2 ] = torch . tensor ( np . sum ( derivatives [ \"qz\" ] * g_image )) g_q [ 3 ] = torch . tensor ( np . sum ( derivatives [ \"qw\" ] * g_image )) g_is = torch . tensor ( np . sum ( derivatives [ \"s_inv\" ] * g_image )) if \"sdf\" in derivatives : for k , v in derivatives [ \"sdf\" ] . items (): g_sdf [ k ] = torch . tensor ( np . sum ( v * g_image )) return g_sdf , g_p , g_q , g_is , g_w , g_h , g_fov , g_thresh , g_camera","title":"SDFRendererFunction"},{"location":"reference/differentiable_renderer/sdf_renderer/#sdfest.differentiable_renderer.sdf_renderer.SDFRendererFunction.forward","text":"forward ( ctx , sdf : torch . Tensor , position : torch . Tensor , orientation : torch . Tensor , inv_scale : torch . Tensor , width : Optional [ int ] = None , height : Optional [ int ] = None , fov_deg : Optional [ float ] = None , threshold : Optional [ float ] = 0.0 , camera : Optional [ Camera ] = None , ) -> torch . Tensor Render depth image of a 7-DOF discrete signed distance field. The SDF position is assumed to be in the camera frame under OpenGL convention. That is, camera looks along negative z-axis, y pointing upwards and x to the right. Note that the rendered image will still follow the classical computer vision convention, of first row being up in the camera frame. This function internally usese numpy and is very slow due to the fully serial implementation. This is only for testing purposes. Use the GPU version for practical performance. Camera can be specified either via camera parameter giving most flexbility or alternatively by providing width, height and fov_deg. PARAMETER DESCRIPTION ctx Context object to stash information. See https://pytorch.org/docs/stable/notes/extending.html. sdf Discrete signed distance field with shape (M, M, M). Arbitrary (but uniform) resolutions are supported. TYPE: torch . Tensor position The position of the signed distance field origin in the camera frame. TYPE: torch . Tensor orientation The orientation of the SDF as a normalized quaternion. TYPE: torch . Tensor inv_scale The inverted scale of the SDF. The scale of an SDF the half-width of the full SDF volume. TYPE: torch . Tensor width Number of pixels in x direction. Recommended to use camera instead. TYPE: Optional [ int ] DEFAULT: None height Number of pixels in y direction. Recommended to use camera instead. TYPE: Optional [ int ] DEFAULT: None fov_deg The horizontal field of view (i.e., in x direction). Pixels are assumed to be square, i.e., fx=fy, computed based on width and fov_deg. Recommended to use camera instead. TYPE: Optional [ float ] DEFAULT: None threshold The distance threshold at which sphere tracing should be stopped. Smaller value will be more accurate, but slower and might potentially lead to holes in the rendering for thin structures in the SDF. Larger values will be faster, but will overestimate the thickness. Should always be positive to guarantee convergence. TYPE: Optional [ float ] DEFAULT: 0.0 camera Camera parameters (not supported right now). TYPE: Optional [ Camera ] DEFAULT: None RETURNS DESCRIPTION torch . Tensor The rendered depth image. Source code in sdfest/differentiable_renderer/sdf_renderer.py 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 @staticmethod def forward ( ctx , sdf : torch . Tensor , position : torch . Tensor , orientation : torch . Tensor , inv_scale : torch . Tensor , width : Optional [ int ] = None , height : Optional [ int ] = None , fov_deg : Optional [ float ] = None , threshold : Optional [ float ] = 0.0 , camera : Optional [ Camera ] = None , ) -> torch . Tensor : \"\"\"Render depth image of a 7-DOF discrete signed distance field. The SDF position is assumed to be in the camera frame under OpenGL convention. That is, camera looks along negative z-axis, y pointing upwards and x to the right. Note that the rendered image will still follow the classical computer vision convention, of first row being up in the camera frame. This function internally usese numpy and is very slow due to the fully serial implementation. This is only for testing purposes. Use the GPU version for practical performance. Camera can be specified either via camera parameter giving most flexbility or alternatively by providing width, height and fov_deg. Args: ctx: Context object to stash information. See https://pytorch.org/docs/stable/notes/extending.html. sdf: Discrete signed distance field with shape (M, M, M). Arbitrary (but uniform) resolutions are supported. position: The position of the signed distance field origin in the camera frame. orientation: The orientation of the SDF as a normalized quaternion. inv_scale: The inverted scale of the SDF. The scale of an SDF the half-width of the full SDF volume. width: Number of pixels in x direction. Recommended to use camera instead. height: Number of pixels in y direction. Recommended to use camera instead. fov_deg: The horizontal field of view (i.e., in x direction). Pixels are assumed to be square, i.e., fx=fy, computed based on width and fov_deg. Recommended to use camera instead. threshold: The distance threshold at which sphere tracing should be stopped. Smaller value will be more accurate, but slower and might potentially lead to holes in the rendering for thin structures in the SDF. Larger values will be faster, but will overestimate the thickness. Should always be positive to guarantee convergence. camera: Camera parameters (not supported right now). Returns: The rendered depth image. \"\"\" if None not in [ width , height , fov_deg ] and camera is not None : raise ValueError ( \"Either width+height+fov_dev or camera must be provided.\" ) if camera is not None : raise NotImplementedError ( \"Only width+height+fov_dev currently supported for CPU\" ) # for simplicity use numpy internally ctx . save_for_backward ( sdf , position , orientation , inv_scale ) sdf = sdf . detach () . numpy () position = position . detach () . numpy () orientation = orientation . detach () . numpy () inv_scale = inv_scale . detach () . numpy () sdf_object = SDFObject ( sdf ) image , derivatives = _render_depth ( sdf_object , width , height , fov_deg , \"d\" , threshold , position , orientation , inv_scale , ) ctx . derivatives = derivatives return torch . from_numpy ( image )","title":"forward()"},{"location":"reference/differentiable_renderer/sdf_renderer/#sdfest.differentiable_renderer.sdf_renderer.SDFRendererFunction.backward","text":"backward ( ctx , inp : torch . Tensor ) Compute gradients of inputs with respect to the provided gradients. Normally called by PyTorch as part of a call to backward() on a loss. PARAMETER DESCRIPTION grad_depth_image RETURNS DESCRIPTION Gradients of discretized signed distance field, position, orientation, inverted scale followed by None for all the non-supported variables passed to forward. Source code in sdfest/differentiable_renderer/sdf_renderer.py 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 @staticmethod def backward ( ctx , inp : torch . Tensor ): \"\"\"Compute gradients of inputs with respect to the provided gradients. Normally called by PyTorch as part of a call to backward() on a loss. Args: grad_depth_image: Returns: Gradients of discretized signed distance field, position, orientation, inverted scale followed by None for all the non-supported variables passed to forward. \"\"\" derivatives = ctx . derivatives sdf , pos , quat , inv_s = ctx . saved_tensors g_image = inp . numpy () g_sdf = g_p = g_q = g_is = g_w = g_h = g_fov = g_thresh = g_camera = None g_sdf = torch . zeros_like ( sdf ) g_p = torch . empty_like ( pos ) g_q = torch . empty_like ( quat ) g_is = torch . empty_like ( inv_s ) g_p [ 0 ] = torch . tensor ( np . sum ( derivatives [ \"x\" ] * g_image )) g_p [ 1 ] = torch . tensor ( np . sum ( derivatives [ \"y\" ] * g_image )) g_p [ 2 ] = torch . tensor ( np . sum ( derivatives [ \"z\" ] * g_image )) g_q [ 0 ] = torch . tensor ( np . sum ( derivatives [ \"qx\" ] * g_image )) g_q [ 1 ] = torch . tensor ( np . sum ( derivatives [ \"qy\" ] * g_image )) g_q [ 2 ] = torch . tensor ( np . sum ( derivatives [ \"qz\" ] * g_image )) g_q [ 3 ] = torch . tensor ( np . sum ( derivatives [ \"qw\" ] * g_image )) g_is = torch . tensor ( np . sum ( derivatives [ \"s_inv\" ] * g_image )) if \"sdf\" in derivatives : for k , v in derivatives [ \"sdf\" ] . items (): g_sdf [ k ] = torch . tensor ( np . sum ( v * g_image )) return g_sdf , g_p , g_q , g_is , g_w , g_h , g_fov , g_thresh , g_camera","title":"backward()"},{"location":"reference/differentiable_renderer/sdf_renderer/#sdfest.differentiable_renderer.sdf_renderer.SDFRendererFunctionGPU","text":"Bases: torch . autograd . Function Renderer function for signed distance fields. Source code in sdfest/differentiable_renderer/sdf_renderer.py 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 class SDFRendererFunctionGPU ( torch . autograd . Function ): \"\"\"Renderer function for signed distance fields.\"\"\" @staticmethod def forward ( ctx , sdf : torch . Tensor , position : torch . Tensor , orientation : torch . Tensor , inv_scale : torch . Tensor , threshold : Optional [ float ] = 0.0 , camera : Optional [ Camera ] = None , ) -> torch . Tensor : \"\"\"Render depth image of a 7-DOF discrete signed distance field on the GPU. Also see render_depth_gpu for documentation. Args: ctx: Context object to stash information. See https://pytorch.org/docs/stable/notes/extending.html. sdf: Discrete signed distance field with shape (M, M, M). Arbitrary (but uniform) resolutions are supported. position: The position of the signed distance field origin in the camera frame. orientation: The orientation of the SDF as a normalized quaternion. inv_scale: The inverted scale of the SDF. The scale of an SDF the half-width of the full SDF volume. threshold: The distance threshold at which sphere tracing should be stopped. Smaller value will be more accurate, but slower and might potentially lead to holes in the rendering for thin structures in the SDF. Larger values will be faster, but will overestimate the thickness. Should always be positive to guarantee convergence. camera: Camera parameters. Returns: The rendered depth image. \"\"\" fx , fy , cx , cy , _ = camera . get_pinhole_camera_parameters ( 0.5 ) ( image ,) = sdf_renderer_cpp . forward ( sdf , position , orientation , inv_scale , camera . width , camera . height , cx , cy , fx , fy , threshold , ) ctx . save_for_backward ( image , sdf , position , orientation , inv_scale ) ctx . width = camera . width ctx . height = camera . height ctx . fx = fx ctx . fy = fy ctx . cx = cx ctx . cy = cy return image @staticmethod def backward ( ctx , grad_depth_image : torch . Tensor ): \"\"\"Compute gradients of inputs with respect to the provided gradients. Normally called by PyTorch as part of a call to backward() on a loss. Args: grad_depth_image: Returns: Gradients of discretized signed distance field, position, orientation, inverted scale followed by None for all the non-supported variables passed to forward. \"\"\" g_sdf = g_p = g_q = g_is = g_thresh = g_camera = None g_sdf , g_p , g_q , g_is = sdf_renderer_cpp . backward ( grad_depth_image , * ctx . saved_tensors , ctx . width , ctx . height , ctx . cx , ctx . cy , ctx . fx , ctx . fy ) return g_sdf , g_p , g_q , g_is , g_thresh , g_camera","title":"SDFRendererFunctionGPU"},{"location":"reference/differentiable_renderer/sdf_renderer/#sdfest.differentiable_renderer.sdf_renderer.SDFRendererFunctionGPU.forward","text":"forward ( ctx , sdf : torch . Tensor , position : torch . Tensor , orientation : torch . Tensor , inv_scale : torch . Tensor , threshold : Optional [ float ] = 0.0 , camera : Optional [ Camera ] = None , ) -> torch . Tensor Render depth image of a 7-DOF discrete signed distance field on the GPU. Also see render_depth_gpu for documentation. PARAMETER DESCRIPTION ctx Context object to stash information. See https://pytorch.org/docs/stable/notes/extending.html. sdf Discrete signed distance field with shape (M, M, M). Arbitrary (but uniform) resolutions are supported. TYPE: torch . Tensor position The position of the signed distance field origin in the camera frame. TYPE: torch . Tensor orientation The orientation of the SDF as a normalized quaternion. TYPE: torch . Tensor inv_scale The inverted scale of the SDF. The scale of an SDF the half-width of the full SDF volume. TYPE: torch . Tensor threshold The distance threshold at which sphere tracing should be stopped. Smaller value will be more accurate, but slower and might potentially lead to holes in the rendering for thin structures in the SDF. Larger values will be faster, but will overestimate the thickness. Should always be positive to guarantee convergence. TYPE: Optional [ float ] DEFAULT: 0.0 camera Camera parameters. TYPE: Optional [ Camera ] DEFAULT: None RETURNS DESCRIPTION torch . Tensor The rendered depth image. Source code in sdfest/differentiable_renderer/sdf_renderer.py 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 @staticmethod def forward ( ctx , sdf : torch . Tensor , position : torch . Tensor , orientation : torch . Tensor , inv_scale : torch . Tensor , threshold : Optional [ float ] = 0.0 , camera : Optional [ Camera ] = None , ) -> torch . Tensor : \"\"\"Render depth image of a 7-DOF discrete signed distance field on the GPU. Also see render_depth_gpu for documentation. Args: ctx: Context object to stash information. See https://pytorch.org/docs/stable/notes/extending.html. sdf: Discrete signed distance field with shape (M, M, M). Arbitrary (but uniform) resolutions are supported. position: The position of the signed distance field origin in the camera frame. orientation: The orientation of the SDF as a normalized quaternion. inv_scale: The inverted scale of the SDF. The scale of an SDF the half-width of the full SDF volume. threshold: The distance threshold at which sphere tracing should be stopped. Smaller value will be more accurate, but slower and might potentially lead to holes in the rendering for thin structures in the SDF. Larger values will be faster, but will overestimate the thickness. Should always be positive to guarantee convergence. camera: Camera parameters. Returns: The rendered depth image. \"\"\" fx , fy , cx , cy , _ = camera . get_pinhole_camera_parameters ( 0.5 ) ( image ,) = sdf_renderer_cpp . forward ( sdf , position , orientation , inv_scale , camera . width , camera . height , cx , cy , fx , fy , threshold , ) ctx . save_for_backward ( image , sdf , position , orientation , inv_scale ) ctx . width = camera . width ctx . height = camera . height ctx . fx = fx ctx . fy = fy ctx . cx = cx ctx . cy = cy return image","title":"forward()"},{"location":"reference/differentiable_renderer/sdf_renderer/#sdfest.differentiable_renderer.sdf_renderer.SDFRendererFunctionGPU.backward","text":"backward ( ctx , grad_depth_image : torch . Tensor ) Compute gradients of inputs with respect to the provided gradients. Normally called by PyTorch as part of a call to backward() on a loss. PARAMETER DESCRIPTION grad_depth_image TYPE: torch . Tensor RETURNS DESCRIPTION Gradients of discretized signed distance field, position, orientation, inverted scale followed by None for all the non-supported variables passed to forward. Source code in sdfest/differentiable_renderer/sdf_renderer.py 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 @staticmethod def backward ( ctx , grad_depth_image : torch . Tensor ): \"\"\"Compute gradients of inputs with respect to the provided gradients. Normally called by PyTorch as part of a call to backward() on a loss. Args: grad_depth_image: Returns: Gradients of discretized signed distance field, position, orientation, inverted scale followed by None for all the non-supported variables passed to forward. \"\"\" g_sdf = g_p = g_q = g_is = g_thresh = g_camera = None g_sdf , g_p , g_q , g_is = sdf_renderer_cpp . backward ( grad_depth_image , * ctx . saved_tensors , ctx . width , ctx . height , ctx . cx , ctx . cy , ctx . fx , ctx . fy ) return g_sdf , g_p , g_q , g_is , g_thresh , g_camera","title":"backward()"},{"location":"reference/differentiable_renderer/sdf_renderer/#sdfest.differentiable_renderer.sdf_renderer.render_depth_gpu","text":"render_depth_gpu ( sdf : torch . Tensor , position : torch . Tensor , orientation : torch . Tensor , inv_scale : torch . Tensor , width : Optional [ int ] = None , height : Optional [ int ] = None , fov_deg : Optional [ float ] = None , threshold : Optional [ float ] = 0.0 , camera : Optional [ Camera ] = None , ) Render depth image of a 7-DOF discrete signed distance field on the GPU. The SDF position is assumed to be in the camera frame under OpenGL convention. That is, camera looks along negative z-axis, y pointing upwards and x to the right. Note that the rendered image will still follow the classical computer vision convention, of first row being up in the camera frame. Camera can be specified either via camera parameter giving most flexbility or alternatively by providing width, height and fov_deg. All provided tensors must reside on the GPU. PARAMETER DESCRIPTION sdf Discrete signed distance field with shape (M, M, M). Arbitrary (but uniform) resolutions are supported. TYPE: torch . Tensor position The position of the signed distance field origin in the camera frame. TYPE: torch . Tensor orientation The orientation of the SDF as a normalized quaternion. TYPE: torch . Tensor inv_scale The inverted scale of the SDF. The scale of an SDF the half-width of the full SDF volume. TYPE: torch . Tensor width Number of pixels in x direction. Recommended to use camera instead. TYPE: Optional [ int ] DEFAULT: None height Number of pixels in y direction. Recommended to use camera instead. TYPE: Optional [ int ] DEFAULT: None fov_deg The horizontal field of view (i.e., in x direction). Pixels are assumed to be square, i.e., fx=fy, computed based on width and fov_deg. Recommended to use camera instead. TYPE: Optional [ float ] DEFAULT: None threshold The distance threshold at which sphere tracing should be stopped. Smaller value will be more accurate, but slower and might potentially lead to holes in the rendering for thin structures in the SDF. Larger values will be faster, but will overestimate the thickness. Should always be positive to guarantee convergence. TYPE: Optional [ float ] DEFAULT: 0.0 camera Camera parameters. TYPE: Optional [ Camera ] DEFAULT: None RETURNS DESCRIPTION The rendered depth image. Source code in sdfest/differentiable_renderer/sdf_renderer.py 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 def render_depth_gpu ( sdf : torch . Tensor , position : torch . Tensor , orientation : torch . Tensor , inv_scale : torch . Tensor , width : Optional [ int ] = None , height : Optional [ int ] = None , fov_deg : Optional [ float ] = None , threshold : Optional [ float ] = 0.0 , camera : Optional [ Camera ] = None , ): \"\"\"Render depth image of a 7-DOF discrete signed distance field on the GPU. The SDF position is assumed to be in the camera frame under OpenGL convention. That is, camera looks along negative z-axis, y pointing upwards and x to the right. Note that the rendered image will still follow the classical computer vision convention, of first row being up in the camera frame. Camera can be specified either via camera parameter giving most flexbility or alternatively by providing width, height and fov_deg. All provided tensors must reside on the GPU. Args: sdf: Discrete signed distance field with shape (M, M, M). Arbitrary (but uniform) resolutions are supported. position: The position of the signed distance field origin in the camera frame. orientation: The orientation of the SDF as a normalized quaternion. inv_scale: The inverted scale of the SDF. The scale of an SDF the half-width of the full SDF volume. width: Number of pixels in x direction. Recommended to use camera instead. height: Number of pixels in y direction. Recommended to use camera instead. fov_deg: The horizontal field of view (i.e., in x direction). Pixels are assumed to be square, i.e., fx=fy, computed based on width and fov_deg. Recommended to use camera instead. threshold: The distance threshold at which sphere tracing should be stopped. Smaller value will be more accurate, but slower and might potentially lead to holes in the rendering for thin structures in the SDF. Larger values will be faster, but will overestimate the thickness. Should always be positive to guarantee convergence. camera: Camera parameters. Returns: The rendered depth image. \"\"\" if None not in [ width , height , fov_deg ] and camera is not None : raise ValueError ( \"Either width+height+fov_dev or camera must be provided.\" ) if camera is None : f = width / math . tan ( fov_deg * math . pi / 180.0 / 2.0 ) / 2 camera = Camera ( width , height , f , f , width / 2 , height / 2 , pixel_center = 0.5 ) return SDFRendererFunctionGPU . apply ( sdf , position , orientation , inv_scale , threshold , camera )","title":"render_depth_gpu()"},{"location":"reference/differentiable_renderer/scripts/experiments/","text":"sdfest.differentiable_renderer.scripts.experiments Experiments with differentiable SDF renderer. Usage python -m sdfest.differentiable_renderer.scripts.experiments --sdf_path data/shapenet_processed/mug_filtered/00001.npy --scale 0.15 --scale_off --pos 0 0 -0.4 --pos_off 0.03 0.03 0.03 --rot 400 90 0 --rot_off 10 10 10 --gpu --visualize 10 --steps 1000","title":"experiments"},{"location":"reference/differentiable_renderer/scripts/experiments/#sdfest.differentiable_renderer.scripts.experiments","text":"Experiments with differentiable SDF renderer. Usage python -m sdfest.differentiable_renderer.scripts.experiments --sdf_path data/shapenet_processed/mug_filtered/00001.npy --scale 0.15 --scale_off --pos 0 0 -0.4 --pos_off 0.03 0.03 0.03 --rot 400 90 0 --rot_off 10 10 10 --gpu --visualize 10 --steps 1000","title":"experiments"},{"location":"reference/estimation/losses/","text":"sdfest.estimation.losses Module containing loss functions. nn_loss nn_loss ( points_from : torch . Tensor , points_to : torch . Tensor ) -> torch . Tensor Compute the distance to the closest neighbor in the other set of points. PARAMETER DESCRIPTION points_from The first point set. Shape NxD, with N points of dimension D. TYPE: torch . Tensor points_to The second point set. Shape MxD, with M points of dimension D. TYPE: torch . Tensor RETURNS DESCRIPTION torch . Tensor Squared distance from all points in the points_from set to the closest point in torch . Tensor points to set. Output shape is (N,). Source code in sdfest/estimation/losses.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def nn_loss ( points_from : torch . Tensor , points_to : torch . Tensor ) -> torch . Tensor : \"\"\"Compute the distance to the closest neighbor in the other set of points. Params: points_from: The first point set. Shape NxD, with N points of dimension D. points_to: The second point set. Shape MxD, with M points of dimension D. Returns: Squared distance from all points in the points_from set to the closest point in points to set. Output shape is (N,). \"\"\" a = torch . sum ( points_from ** 2 , dim = 1 ) b = torch . mm ( points_from , points_to . t ()) c = torch . sum ( points_to ** 2 , dim = 1 ) # compute the distance matrix d = - 2 * b + a . unsqueeze ( 1 ) + c . unsqueeze ( 0 ) d [ d < 0 ] = 0 # TODO why is it negative sometimes? numerical issues? d , _ = d . min ( 1 ) return d pc_loss pc_loss ( points : torch . Tensor , position : torch . Tensor , orientation : torch . Tensor , scale : torch . Tensor , sdf : torch . Tensor , ) -> torch . Tensor Compute trilinerly interpolated SDF value at point positions. PARAMETER DESCRIPTION points pointcloud in camera frame, shape (M, 4) TYPE: torch . Tensor position position of SDF center in camera frame, shape (3,) TYPE: torch . Tensor orientation quaternion representing orientation of SDF, shape (4,) TYPE: torch . Tensor scale half-width of SDF volume TYPE: torch . Tensor sdf volumetric signed distance field, shape (res, res, res), assuming same resolution along each axis TYPE: torch . Tensor RETURNS DESCRIPTION torch . Tensor Trilinearly interpolated distance at the passed points 0 if outside of SDF torch . Tensor volume. Distance is in world coordinates (i.e., after scaling the SDF). Source code in sdfest/estimation/losses.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def pc_loss ( points : torch . Tensor , position : torch . Tensor , orientation : torch . Tensor , scale : torch . Tensor , sdf : torch . Tensor , ) -> torch . Tensor : \"\"\"Compute trilinerly interpolated SDF value at point positions. Args: points: pointcloud in camera frame, shape (M, 4) position: position of SDF center in camera frame, shape (3,) orientation: quaternion representing orientation of SDF, shape (4,) scale: half-width of SDF volume sdf: volumetric signed distance field, shape (res, res, res), assuming same resolution along each axis Returns: Trilinearly interpolated distance at the passed points 0 if outside of SDF volume. Distance is in world coordinates (i.e., after scaling the SDF). \"\"\" q = orientation / torch . norm ( orientation ) # to get normalization gradients obj_points = points - position . unsqueeze ( 0 ) # Quaternion to rotation matrix # Note that we use conjugate here since we want to transform points from # world to object coordinates and the quaternion describes rotation of # object coordinate system in world coordinates. R = obj_points . new_zeros ( 3 , 3 ) R [ 0 , 0 ] = 1 - 2 * ( q [ 1 ] * q [ 1 ] + q [ 2 ] * q [ 2 ]) R [ 0 , 1 ] = 2 * ( q [ 0 ] * q [ 1 ] + q [ 2 ] * q [ 3 ]) R [ 0 , 2 ] = 2 * ( q [ 0 ] * q [ 2 ] - q [ 3 ] * q [ 1 ]) R [ 1 , 0 ] = 2 * ( q [ 0 ] * q [ 1 ] - q [ 2 ] * q [ 3 ]) R [ 1 , 1 ] = 1 - 2 * ( q [ 0 ] * q [ 0 ] + q [ 2 ] * q [ 2 ]) R [ 1 , 2 ] = 2 * ( q [ 1 ] * q [ 2 ] + q [ 3 ] * q [ 0 ]) R [ 2 , 0 ] = 2 * ( q [ 0 ] * q [ 2 ] + q [ 3 ] * q [ 1 ]) R [ 2 , 1 ] = 2 * ( q [ 1 ] * q [ 2 ] - q [ 3 ] * q [ 0 ]) R [ 2 , 2 ] = 1 - 2 * ( q [ 0 ] * q [ 0 ] + q [ 1 ] * q [ 1 ]) obj_points = ( R @ obj_points . T ) . T # Transform to canonical coordintes obj_point in [-1,1]^3 obj_point = obj_points / scale # Compute cell and cell position res = sdf . shape [ 0 ] # assuming same resolution along each axis grid_size = 2.0 / ( res - 1 ) c = torch . floor (( obj_point + 1.0 ) * ( res - 1 ) * 0.5 ) mask = torch . logical_or ( torch . min ( c , dim = 1 )[ 0 ] < 0 , torch . max ( c , dim = 1 )[ 0 ] > res - 2 ) c = torch . clip ( c , 0 , res - 2 ) # base cell index of each point cell_position = c * grid_size - 1.0 # base cell position of each point sdf_indices = c . new_empty (( obj_point . shape [ 0 ], 8 ), dtype = torch . long ) sdf_indices [:, 0 ] = c [:, 0 ] * res ** 2 + c [:, 1 ] * res + c [:, 2 ] sdf_indices [:, 1 ] = c [:, 0 ] * res ** 2 + c [:, 1 ] * res + c [:, 2 ] + 1 sdf_indices [:, 2 ] = c [:, 0 ] * res ** 2 + ( c [:, 1 ] + 1 ) * res + c [:, 2 ] sdf_indices [:, 3 ] = c [:, 0 ] * res ** 2 + ( c [:, 1 ] + 1 ) * res + c [:, 2 ] + 1 sdf_indices [:, 4 ] = ( c [:, 0 ] + 1 ) * res ** 2 + c [:, 1 ] * res + c [:, 2 ] sdf_indices [:, 5 ] = ( c [:, 0 ] + 1 ) * res ** 2 + c [:, 1 ] * res + c [:, 2 ] + 1 sdf_indices [:, 6 ] = ( c [:, 0 ] + 1 ) * res ** 2 + ( c [:, 1 ] + 1 ) * res + c [:, 2 ] sdf_indices [:, 7 ] = ( c [:, 0 ] + 1 ) * res ** 2 + ( c [:, 1 ] + 1 ) * res + c [:, 2 ] + 1 sdf_view = sdf . view ([ - 1 ]) point_cell_position = ( obj_point - cell_position ) / grid_size # [0,1]^3 sdf_values = torch . take ( sdf_view , sdf_indices ) # trilinear interpolation sdf_value = sdf_values . new_empty ( obj_points . shape [ 0 ]) # sdf_value = obj_point[:, 0] sdf_value = ( ( sdf_values [:, 0 ] * ( 1 - point_cell_position [:, 0 ]) + sdf_values [:, 4 ] * point_cell_position [:, 0 ] ) * ( 1 - point_cell_position [:, 1 ]) + ( sdf_values [:, 2 ] * ( 1 - point_cell_position [:, 0 ]) + sdf_values [:, 6 ] * point_cell_position [:, 0 ] ) * point_cell_position [:, 1 ] ) * ( 1 - point_cell_position [:, 2 ]) + ( ( sdf_values [:, 1 ] * ( 1 - point_cell_position [:, 0 ]) + sdf_values [:, 5 ] * point_cell_position [:, 0 ] ) * ( 1 - point_cell_position [:, 1 ]) + ( sdf_values [:, 3 ] * ( 1 - point_cell_position [:, 0 ]) + sdf_values [:, 7 ] * point_cell_position [:, 0 ] ) * point_cell_position [:, 1 ] ) * point_cell_position [ :, 2 ] sdf_value [ mask ] = 0 return sdf_value * scale point_constraint_loss point_constraint_loss ( orientation_q : torch . Tensor , source : torch . Tensor , target : torch . Tensor ) -> torch . Tensor Compute Euclidean distance between rotated source point and target point. PARAMETER DESCRIPTION orientation_q Orientation of object in world / camera frame as quaternion. Scalar-last convention. Shape (4,). TYPE: torch . Tensor source Point in object frame, which will be transformed. (3,). TYPE: torch . Tensor target Point in rotated oject frame. Shape (3,). TYPE: torch . Tensor RETURNS DESCRIPTION torch . Tensor Euclidean norm between R(orientation_q) @ source - target. Scalar. Source code in sdfest/estimation/losses.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def point_constraint_loss ( orientation_q : torch . Tensor , source : torch . Tensor , target : torch . Tensor ) -> torch . Tensor : \"\"\"Compute Euclidean distance between rotated source point and target point. Args: orientation_q: Orientation of object in world / camera frame as quaternion. Scalar-last convention. Shape (4,). source: Point in object frame, which will be transformed. (3,). target: Point in rotated oject frame. Shape (3,). Returns: Euclidean norm between R(orientation_q) @ source - target. Scalar. \"\"\" rotated_source = quaternion_utils . quaternion_apply ( orientation_q , source ) return torch . linalg . norm ( rotated_source - target )","title":"losses"},{"location":"reference/estimation/losses/#sdfest.estimation.losses","text":"Module containing loss functions.","title":"losses"},{"location":"reference/estimation/losses/#sdfest.estimation.losses.nn_loss","text":"nn_loss ( points_from : torch . Tensor , points_to : torch . Tensor ) -> torch . Tensor Compute the distance to the closest neighbor in the other set of points. PARAMETER DESCRIPTION points_from The first point set. Shape NxD, with N points of dimension D. TYPE: torch . Tensor points_to The second point set. Shape MxD, with M points of dimension D. TYPE: torch . Tensor RETURNS DESCRIPTION torch . Tensor Squared distance from all points in the points_from set to the closest point in torch . Tensor points to set. Output shape is (N,). Source code in sdfest/estimation/losses.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def nn_loss ( points_from : torch . Tensor , points_to : torch . Tensor ) -> torch . Tensor : \"\"\"Compute the distance to the closest neighbor in the other set of points. Params: points_from: The first point set. Shape NxD, with N points of dimension D. points_to: The second point set. Shape MxD, with M points of dimension D. Returns: Squared distance from all points in the points_from set to the closest point in points to set. Output shape is (N,). \"\"\" a = torch . sum ( points_from ** 2 , dim = 1 ) b = torch . mm ( points_from , points_to . t ()) c = torch . sum ( points_to ** 2 , dim = 1 ) # compute the distance matrix d = - 2 * b + a . unsqueeze ( 1 ) + c . unsqueeze ( 0 ) d [ d < 0 ] = 0 # TODO why is it negative sometimes? numerical issues? d , _ = d . min ( 1 ) return d","title":"nn_loss()"},{"location":"reference/estimation/losses/#sdfest.estimation.losses.pc_loss","text":"pc_loss ( points : torch . Tensor , position : torch . Tensor , orientation : torch . Tensor , scale : torch . Tensor , sdf : torch . Tensor , ) -> torch . Tensor Compute trilinerly interpolated SDF value at point positions. PARAMETER DESCRIPTION points pointcloud in camera frame, shape (M, 4) TYPE: torch . Tensor position position of SDF center in camera frame, shape (3,) TYPE: torch . Tensor orientation quaternion representing orientation of SDF, shape (4,) TYPE: torch . Tensor scale half-width of SDF volume TYPE: torch . Tensor sdf volumetric signed distance field, shape (res, res, res), assuming same resolution along each axis TYPE: torch . Tensor RETURNS DESCRIPTION torch . Tensor Trilinearly interpolated distance at the passed points 0 if outside of SDF torch . Tensor volume. Distance is in world coordinates (i.e., after scaling the SDF). Source code in sdfest/estimation/losses.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def pc_loss ( points : torch . Tensor , position : torch . Tensor , orientation : torch . Tensor , scale : torch . Tensor , sdf : torch . Tensor , ) -> torch . Tensor : \"\"\"Compute trilinerly interpolated SDF value at point positions. Args: points: pointcloud in camera frame, shape (M, 4) position: position of SDF center in camera frame, shape (3,) orientation: quaternion representing orientation of SDF, shape (4,) scale: half-width of SDF volume sdf: volumetric signed distance field, shape (res, res, res), assuming same resolution along each axis Returns: Trilinearly interpolated distance at the passed points 0 if outside of SDF volume. Distance is in world coordinates (i.e., after scaling the SDF). \"\"\" q = orientation / torch . norm ( orientation ) # to get normalization gradients obj_points = points - position . unsqueeze ( 0 ) # Quaternion to rotation matrix # Note that we use conjugate here since we want to transform points from # world to object coordinates and the quaternion describes rotation of # object coordinate system in world coordinates. R = obj_points . new_zeros ( 3 , 3 ) R [ 0 , 0 ] = 1 - 2 * ( q [ 1 ] * q [ 1 ] + q [ 2 ] * q [ 2 ]) R [ 0 , 1 ] = 2 * ( q [ 0 ] * q [ 1 ] + q [ 2 ] * q [ 3 ]) R [ 0 , 2 ] = 2 * ( q [ 0 ] * q [ 2 ] - q [ 3 ] * q [ 1 ]) R [ 1 , 0 ] = 2 * ( q [ 0 ] * q [ 1 ] - q [ 2 ] * q [ 3 ]) R [ 1 , 1 ] = 1 - 2 * ( q [ 0 ] * q [ 0 ] + q [ 2 ] * q [ 2 ]) R [ 1 , 2 ] = 2 * ( q [ 1 ] * q [ 2 ] + q [ 3 ] * q [ 0 ]) R [ 2 , 0 ] = 2 * ( q [ 0 ] * q [ 2 ] + q [ 3 ] * q [ 1 ]) R [ 2 , 1 ] = 2 * ( q [ 1 ] * q [ 2 ] - q [ 3 ] * q [ 0 ]) R [ 2 , 2 ] = 1 - 2 * ( q [ 0 ] * q [ 0 ] + q [ 1 ] * q [ 1 ]) obj_points = ( R @ obj_points . T ) . T # Transform to canonical coordintes obj_point in [-1,1]^3 obj_point = obj_points / scale # Compute cell and cell position res = sdf . shape [ 0 ] # assuming same resolution along each axis grid_size = 2.0 / ( res - 1 ) c = torch . floor (( obj_point + 1.0 ) * ( res - 1 ) * 0.5 ) mask = torch . logical_or ( torch . min ( c , dim = 1 )[ 0 ] < 0 , torch . max ( c , dim = 1 )[ 0 ] > res - 2 ) c = torch . clip ( c , 0 , res - 2 ) # base cell index of each point cell_position = c * grid_size - 1.0 # base cell position of each point sdf_indices = c . new_empty (( obj_point . shape [ 0 ], 8 ), dtype = torch . long ) sdf_indices [:, 0 ] = c [:, 0 ] * res ** 2 + c [:, 1 ] * res + c [:, 2 ] sdf_indices [:, 1 ] = c [:, 0 ] * res ** 2 + c [:, 1 ] * res + c [:, 2 ] + 1 sdf_indices [:, 2 ] = c [:, 0 ] * res ** 2 + ( c [:, 1 ] + 1 ) * res + c [:, 2 ] sdf_indices [:, 3 ] = c [:, 0 ] * res ** 2 + ( c [:, 1 ] + 1 ) * res + c [:, 2 ] + 1 sdf_indices [:, 4 ] = ( c [:, 0 ] + 1 ) * res ** 2 + c [:, 1 ] * res + c [:, 2 ] sdf_indices [:, 5 ] = ( c [:, 0 ] + 1 ) * res ** 2 + c [:, 1 ] * res + c [:, 2 ] + 1 sdf_indices [:, 6 ] = ( c [:, 0 ] + 1 ) * res ** 2 + ( c [:, 1 ] + 1 ) * res + c [:, 2 ] sdf_indices [:, 7 ] = ( c [:, 0 ] + 1 ) * res ** 2 + ( c [:, 1 ] + 1 ) * res + c [:, 2 ] + 1 sdf_view = sdf . view ([ - 1 ]) point_cell_position = ( obj_point - cell_position ) / grid_size # [0,1]^3 sdf_values = torch . take ( sdf_view , sdf_indices ) # trilinear interpolation sdf_value = sdf_values . new_empty ( obj_points . shape [ 0 ]) # sdf_value = obj_point[:, 0] sdf_value = ( ( sdf_values [:, 0 ] * ( 1 - point_cell_position [:, 0 ]) + sdf_values [:, 4 ] * point_cell_position [:, 0 ] ) * ( 1 - point_cell_position [:, 1 ]) + ( sdf_values [:, 2 ] * ( 1 - point_cell_position [:, 0 ]) + sdf_values [:, 6 ] * point_cell_position [:, 0 ] ) * point_cell_position [:, 1 ] ) * ( 1 - point_cell_position [:, 2 ]) + ( ( sdf_values [:, 1 ] * ( 1 - point_cell_position [:, 0 ]) + sdf_values [:, 5 ] * point_cell_position [:, 0 ] ) * ( 1 - point_cell_position [:, 1 ]) + ( sdf_values [:, 3 ] * ( 1 - point_cell_position [:, 0 ]) + sdf_values [:, 7 ] * point_cell_position [:, 0 ] ) * point_cell_position [:, 1 ] ) * point_cell_position [ :, 2 ] sdf_value [ mask ] = 0 return sdf_value * scale","title":"pc_loss()"},{"location":"reference/estimation/losses/#sdfest.estimation.losses.point_constraint_loss","text":"point_constraint_loss ( orientation_q : torch . Tensor , source : torch . Tensor , target : torch . Tensor ) -> torch . Tensor Compute Euclidean distance between rotated source point and target point. PARAMETER DESCRIPTION orientation_q Orientation of object in world / camera frame as quaternion. Scalar-last convention. Shape (4,). TYPE: torch . Tensor source Point in object frame, which will be transformed. (3,). TYPE: torch . Tensor target Point in rotated oject frame. Shape (3,). TYPE: torch . Tensor RETURNS DESCRIPTION torch . Tensor Euclidean norm between R(orientation_q) @ source - target. Scalar. Source code in sdfest/estimation/losses.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def point_constraint_loss ( orientation_q : torch . Tensor , source : torch . Tensor , target : torch . Tensor ) -> torch . Tensor : \"\"\"Compute Euclidean distance between rotated source point and target point. Args: orientation_q: Orientation of object in world / camera frame as quaternion. Scalar-last convention. Shape (4,). source: Point in object frame, which will be transformed. (3,). target: Point in rotated oject frame. Shape (3,). Returns: Euclidean norm between R(orientation_q) @ source - target. Scalar. \"\"\" rotated_source = quaternion_utils . quaternion_apply ( orientation_q , source ) return torch . linalg . norm ( rotated_source - target )","title":"point_constraint_loss()"},{"location":"reference/estimation/metrics/","text":"sdfest.estimation.metrics Metrics for shape evaluation. correct_thresh correct_thresh ( position_gt : np . ndarray , position_prediction : np . ndarray , orientation_gt : Rotation , orientation_prediction : Rotation , extent_gt : Optional [ np . ndarray ] = None , extent_prediction : Optional [ np . ndarray ] = None , points_gt : Optional [ np . ndarray ] = None , points_prediction : Optional [ np . ndarray ] = None , position_threshold : Optional [ float ] = None , degree_threshold : Optional [ float ] = None , iou_3d_threshold : Optional [ float ] = None , fscore_threshold : Optional [ float ] = None , rotational_symmetry_axis : Optional [ int ] = None , ) -> int Classify a pose prediction as correct or incorrect. PARAMETER DESCRIPTION position_gt ground truth position, expected shape (3,). TYPE: np . ndarray position_prediction predicted position, expected shape (3,). TYPE: np . ndarray position_threshold position threshold in meters, no threshold if None TYPE: Optional [ float ] DEFAULT: None orientation_q_qt ground truth orientation, scalar-last quaternion, shape (4,) orientation_q_prediction predicted orientation, scalar-last quaternion, shape (4,) extent_gt bounding box extents, shape (3,) only used if IoU threshold specified TYPE: Optional [ np . ndarray ] DEFAULT: None extent_prediction bounding box extents, shape (3,) only used if IoU threshold specified TYPE: Optional [ np . ndarray ] DEFAULT: None point_gt set of true points, expected shape (N,3) points_rec set of reconstructed points, expected shape (M,3) degree_threshold orientation threshold in degrees, no threshold if None TYPE: Optional [ float ] DEFAULT: None iou_3d_threshold 3D IoU threshold, no threshold if None TYPE: Optional [ float ] DEFAULT: None rotational_symmetry_axis Specify axis along which rotation is ignored. If None, no axis is ignored. 0 for x-axis, 1 for y-axis, 2 for z-axis. TYPE: Optional [ int ] DEFAULT: None RETURNS DESCRIPTION int 1 if error is below all provided thresholds. 0 if error is above one provided int threshold. Source code in sdfest/estimation/metrics.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def correct_thresh ( position_gt : np . ndarray , position_prediction : np . ndarray , orientation_gt : Rotation , orientation_prediction : Rotation , extent_gt : Optional [ np . ndarray ] = None , extent_prediction : Optional [ np . ndarray ] = None , points_gt : Optional [ np . ndarray ] = None , points_prediction : Optional [ np . ndarray ] = None , position_threshold : Optional [ float ] = None , degree_threshold : Optional [ float ] = None , iou_3d_threshold : Optional [ float ] = None , fscore_threshold : Optional [ float ] = None , rotational_symmetry_axis : Optional [ int ] = None , ) -> int : \"\"\"Classify a pose prediction as correct or incorrect. Args: position_gt: ground truth position, expected shape (3,). position_prediction: predicted position, expected shape (3,). position_threshold: position threshold in meters, no threshold if None orientation_q_qt: ground truth orientation, scalar-last quaternion, shape (4,) orientation_q_prediction: predicted orientation, scalar-last quaternion, shape (4,) extent_gt: bounding box extents, shape (3,) only used if IoU threshold specified extent_prediction: bounding box extents, shape (3,) only used if IoU threshold specified point_gt: set of true points, expected shape (N,3) points_rec: set of reconstructed points, expected shape (M,3) degree_threshold: orientation threshold in degrees, no threshold if None iou_3d_threshold: 3D IoU threshold, no threshold if None rotational_symmetry_axis: Specify axis along which rotation is ignored. If None, no axis is ignored. 0 for x-axis, 1 for y-axis, 2 for z-axis. Returns: 1 if error is below all provided thresholds. 0 if error is above one provided threshold. \"\"\" if position_threshold is not None : position_error = np . linalg . norm ( position_gt - position_prediction ) if position_error > position_threshold : return 0 if degree_threshold is not None : rad_threshold = degree_threshold * np . pi / 180.0 if rotational_symmetry_axis is not None : p = np . array ([ 0.0 , 0.0 , 0.0 ]) p [ rotational_symmetry_axis ] = 1.0 p1 = orientation_gt . apply ( p ) p2 = orientation_prediction . apply ( p ) rad_error = np . arccos ( p1 @ p2 ) else : rad_error = ( orientation_gt * orientation_prediction . inv ()) . magnitude () if rad_error > rad_threshold : return 0 if iou_3d_threshold is not None : raise NotImplementedError ( \"3D IoU is not impemented yet.\" ) # TODO implement 3D IoU # starting point for proper implementation: https://github.com/google-research-datasets/Objectron/blob/c06a65165a18396e1e00091981fd1652875c97b5/objectron/dataset/iou.py#L6 pass if fscore_threshold is not None : fscore = reconstruction_fscore ( points_gt , points_prediction , 0.01 ) if fscore < fscore_threshold : return 0 return 1 mean_accuracy mean_accuracy ( points_gt : np . ndarray , points_rec : np . ndarray , p_norm : int = 2 , normalize : bool = False , ) -> float Compute accuracy metric. Accuracy metric is the same as asymmetric chamfer distance from rec to gt. See, for example, Occupancy Networks Learning 3D Reconstruction in Function Space, Mescheder et al., 2019. PARAMETER DESCRIPTION points_gt set of true points, expected shape (N,3) TYPE: np . ndarray points_rec set of reconstructed points, expected shape (M,3) TYPE: np . ndarray p_norm which Minkowski p-norm is used for distance and nearest neighbor query TYPE: int DEFAULT: 2 normalize whether to divide result by Euclidean extent of points_gt TYPE: bool DEFAULT: False RETURNS DESCRIPTION float Arithmetic mean of p-norm from reconstructed points to closest (in p-norm) float ground truth points. Source code in sdfest/estimation/metrics.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def mean_accuracy ( points_gt : np . ndarray , points_rec : np . ndarray , p_norm : int = 2 , normalize : bool = False , ) -> float : \"\"\"Compute accuracy metric. Accuracy metric is the same as asymmetric chamfer distance from rec to gt. See, for example, Occupancy Networks Learning 3D Reconstruction in Function Space, Mescheder et al., 2019. Args: points_gt: set of true points, expected shape (N,3) points_rec: set of reconstructed points, expected shape (M,3) p_norm: which Minkowski p-norm is used for distance and nearest neighbor query normalize: whether to divide result by Euclidean extent of points_gt Returns: Arithmetic mean of p-norm from reconstructed points to closest (in p-norm) ground truth points. \"\"\" kd_tree = scipy . spatial . KDTree ( points_gt ) d , _ = kd_tree . query ( points_rec , p = p_norm ) if normalize : return np . mean ( d ) / extent ( points_gt ) else : return np . mean ( d ) mean_completeness mean_completeness ( points_gt : np . ndarray , points_rec : np . ndarray , p_norm : int = 2 , normalize : bool = False , ) -> float Compute completeness metric. Completeness metric is the same as asymmetric chamfer distance from gt to rec. See, for example, Occupancy Networks Learning 3D Reconstruction in Function Space, Mescheder et al., 2019. PARAMETER DESCRIPTION points_gt set of true points, expected shape (N,3) TYPE: np . ndarray points_rec set of reconstructed points, expected shape (M,3) TYPE: np . ndarray p_norm which Minkowski p-norm is used for distance and nearest neighbor query TYPE: int DEFAULT: 2 normalize whether to divide result by Euclidean extent of points_gt TYPE: bool DEFAULT: False RETURNS DESCRIPTION float Arithmetic mean of p-norm from ground truth points to closest (in p-norm) float reconstructed points. Source code in sdfest/estimation/metrics.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def mean_completeness ( points_gt : np . ndarray , points_rec : np . ndarray , p_norm : int = 2 , normalize : bool = False , ) -> float : \"\"\"Compute completeness metric. Completeness metric is the same as asymmetric chamfer distance from gt to rec. See, for example, Occupancy Networks Learning 3D Reconstruction in Function Space, Mescheder et al., 2019. Args: points_gt: set of true points, expected shape (N,3) points_rec: set of reconstructed points, expected shape (M,3) p_norm: which Minkowski p-norm is used for distance and nearest neighbor query normalize: whether to divide result by Euclidean extent of points_gt Returns: Arithmetic mean of p-norm from ground truth points to closest (in p-norm) reconstructed points. \"\"\" kd_tree = scipy . spatial . KDTree ( points_rec ) d , _ = kd_tree . query ( points_gt , p = p_norm ) if normalize : return np . mean ( d ) / extent ( points_gt ) else : return np . mean ( d ) symmetric_chamfer symmetric_chamfer ( points_gt : np . ndarray , points_rec : np . ndarray , p_norm : int = 2 , normalize : bool = False , ) -> float Compute symmetric chamfer distance. There are various slightly different definitions for the chamfer distance. Note that completeness and accuracy are themselves sometimes referred to as chamfer distances, with symmetric chamfer distance being the combination of the two. Chamfer L1 in the literature (see, for example, Occupancy Networks Learning 3D Reconstruction in Function Space, Mescheder et al., 2019) refers to using arithmetic mean (note that this is actually differently scaled from L1) when combining accuracy and completeness. PARAMETER DESCRIPTION points_gt set of true points, expected shape (N,3) TYPE: np . ndarray points_rec set of reconstructed points, expected shape (M,3) TYPE: np . ndarray p_norm which Minkowski p-norm is used for distance and nearest neighbor query TYPE: int DEFAULT: 2 normalize whether to divide result by Euclidean extent of points_gt TYPE: bool DEFAULT: False RETURNS DESCRIPTION float Arithmetic mean of accuracy and completeness metrics using the specified p-norm. Source code in sdfest/estimation/metrics.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 def symmetric_chamfer ( points_gt : np . ndarray , points_rec : np . ndarray , p_norm : int = 2 , normalize : bool = False , ) -> float : \"\"\"Compute symmetric chamfer distance. There are various slightly different definitions for the chamfer distance. Note that completeness and accuracy are themselves sometimes referred to as chamfer distances, with symmetric chamfer distance being the combination of the two. Chamfer L1 in the literature (see, for example, Occupancy Networks Learning 3D Reconstruction in Function Space, Mescheder et al., 2019) refers to using arithmetic mean (note that this is actually differently scaled from L1) when combining accuracy and completeness. Args: points_gt: set of true points, expected shape (N,3) points_rec: set of reconstructed points, expected shape (M,3) p_norm: which Minkowski p-norm is used for distance and nearest neighbor query normalize: whether to divide result by Euclidean extent of points_gt Returns: Arithmetic mean of accuracy and completeness metrics using the specified p-norm. \"\"\" return ( mean_completeness ( points_gt , points_rec , p_norm = p_norm , normalize = normalize ) + mean_accuracy ( points_gt , points_rec , p_norm = p_norm , normalize = normalize ) ) / 2 completeness_thresh completeness_thresh ( points_gt : np . ndarray , points_rec : np . ndarray , threshold : float , p_norm : int = 2 , normalize : bool = False , ) -> float Compute thresholded completion metric. See FroDO: From Detections to 3D Objects, Ru\u0308nz et al., 2020. PARAMETER DESCRIPTION points_gt set of true points, expected shape (N,3) TYPE: np . ndarray points_rec set of reconstructed points, expected shape (M,3) TYPE: np . ndarray threshold distance threshold to count a point as correct TYPE: float p_norm which Minkowski p-norm is used for distance and nearest neighbor query TYPE: int DEFAULT: 2 normalize whether to divide distances by Euclidean extent of points_gt TYPE: bool DEFAULT: False RETURNS DESCRIPTION float Ratio of ground truth points with closest reconstructed point closer than float threshold (in p-norm). Source code in sdfest/estimation/metrics.py 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 def completeness_thresh ( points_gt : np . ndarray , points_rec : np . ndarray , threshold : float , p_norm : int = 2 , normalize : bool = False , ) -> float : \"\"\"Compute thresholded completion metric. See FroDO: From Detections to 3D Objects, Ru\u0308nz et al., 2020. Args: points_gt: set of true points, expected shape (N,3) points_rec: set of reconstructed points, expected shape (M,3) threshold: distance threshold to count a point as correct p_norm: which Minkowski p-norm is used for distance and nearest neighbor query normalize: whether to divide distances by Euclidean extent of points_gt Returns: Ratio of ground truth points with closest reconstructed point closer than threshold (in p-norm). \"\"\" kd_tree = scipy . spatial . KDTree ( points_rec ) d , _ = kd_tree . query ( points_gt , p = p_norm ) if normalize : return np . sum ( d / extent ( points_gt ) < threshold ) / points_gt . shape [ 0 ] else : return np . sum ( d < threshold ) / points_gt . shape [ 0 ] accuracy_thresh accuracy_thresh ( points_gt : np . ndarray , points_rec : np . ndarray , threshold : float , p_norm : int = 2 , normalize : bool = False , ) -> float Compute thresholded accuracy metric. See FroDO: From Detections to 3D Objects, Ru\u0308nz et al., 2020. PARAMETER DESCRIPTION points_gt set of true points, expected shape (N,3) TYPE: np . ndarray points_rec set of reconstructed points, expected shape (M,3) TYPE: np . ndarray threshold distance threshold to count a point as correct TYPE: float p_norm which Minkowski p-norm is used for distance and nearest neighbor query TYPE: int DEFAULT: 2 normalize whether to divide distances by Euclidean extent of points_gt TYPE: bool DEFAULT: False RETURNS DESCRIPTION float Ratio of reconstructed points with closest ground truth point closer than float threshold (in p-norm). Source code in sdfest/estimation/metrics.py 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 def accuracy_thresh ( points_gt : np . ndarray , points_rec : np . ndarray , threshold : float , p_norm : int = 2 , normalize : bool = False , ) -> float : \"\"\"Compute thresholded accuracy metric. See FroDO: From Detections to 3D Objects, Ru\u0308nz et al., 2020. Args: points_gt: set of true points, expected shape (N,3) points_rec: set of reconstructed points, expected shape (M,3) threshold: distance threshold to count a point as correct p_norm: which Minkowski p-norm is used for distance and nearest neighbor query normalize: whether to divide distances by Euclidean extent of points_gt Returns: Ratio of reconstructed points with closest ground truth point closer than threshold (in p-norm). \"\"\" kd_tree = scipy . spatial . KDTree ( points_gt ) d , _ = kd_tree . query ( points_rec , p = p_norm ) if normalize : return np . sum ( d / extent ( points_gt ) < threshold ) / points_rec . shape [ 0 ] else : return np . sum ( d < threshold ) / points_rec . shape [ 0 ] reconstruction_fscore reconstruction_fscore ( points_gt : np . ndarray , points_rec : np . ndarray , threshold : float , p_norm : int = 2 , normalize : bool = False , ) -> float Compute reconstruction fscore. See What Do Single-View 3D Reconstruction Networks Learn, Tatarchenko, 2019 PARAMETER DESCRIPTION points_gt set of true points, expected shape (N,3) TYPE: np . ndarray points_rec set of reconstructed points, expected shape (M,3) TYPE: np . ndarray threshold distance threshold to count a point as correct TYPE: float p_norm which Minkowski p-norm is used for distance and nearest neighbor query TYPE: int DEFAULT: 2 normalize whether to divide distances by Euclidean extent of points_gt TYPE: bool DEFAULT: False RETURNS DESCRIPTION float Harmonic mean of precision (thresholded accuracy) and recall (thresholded float completeness). Source code in sdfest/estimation/metrics.py 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 def reconstruction_fscore ( points_gt : np . ndarray , points_rec : np . ndarray , threshold : float , p_norm : int = 2 , normalize : bool = False , ) -> float : \"\"\"Compute reconstruction fscore. See What Do Single-View 3D Reconstruction Networks Learn, Tatarchenko, 2019 Args: points_gt: set of true points, expected shape (N,3) points_rec: set of reconstructed points, expected shape (M,3) threshold: distance threshold to count a point as correct p_norm: which Minkowski p-norm is used for distance and nearest neighbor query normalize: whether to divide distances by Euclidean extent of points_gt Returns: Harmonic mean of precision (thresholded accuracy) and recall (thresholded completeness). \"\"\" recall = completeness_thresh ( points_gt , points_rec , threshold , p_norm = p_norm , normalize = normalize ) precision = accuracy_thresh ( points_gt , points_rec , threshold , p_norm = p_norm , normalize = normalize ) if recall < 1e-7 or precision < 1e-7 : return 0 return 2 / ( 1 / recall + 1 / precision ) extent extent ( points : np . ndarray ) -> float Compute largest Euclidean distance between any two points. PARAMETER DESCRIPTION points_gt set of true p_norm which Minkowski p-norm is used for distance and nearest neighbor query RETURNS DESCRIPTION float Ratio of reconstructed points with closest ground truth point closer than float threshold (in p-norm). Source code in sdfest/estimation/metrics.py 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 def extent ( points : np . ndarray ) -> float : \"\"\"Compute largest Euclidean distance between any two points. Args: points_gt: set of true p_norm: which Minkowski p-norm is used for distance and nearest neighbor query Returns: Ratio of reconstructed points with closest ground truth point closer than threshold (in p-norm). \"\"\" try : hull = scipy . spatial . ConvexHull ( points ) except scipy . spatial . qhull . QhullError : # fallback to brute force distance matrix return np . max ( scipy . spatial . distance_matrix ( points , points )) # this is wasteful, if too slow implement rotating caliper method return np . max ( scipy . spatial . distance_matrix ( points [ hull . vertices ], points [ hull . vertices ]) )","title":"metrics"},{"location":"reference/estimation/metrics/#sdfest.estimation.metrics","text":"Metrics for shape evaluation.","title":"metrics"},{"location":"reference/estimation/metrics/#sdfest.estimation.metrics.correct_thresh","text":"correct_thresh ( position_gt : np . ndarray , position_prediction : np . ndarray , orientation_gt : Rotation , orientation_prediction : Rotation , extent_gt : Optional [ np . ndarray ] = None , extent_prediction : Optional [ np . ndarray ] = None , points_gt : Optional [ np . ndarray ] = None , points_prediction : Optional [ np . ndarray ] = None , position_threshold : Optional [ float ] = None , degree_threshold : Optional [ float ] = None , iou_3d_threshold : Optional [ float ] = None , fscore_threshold : Optional [ float ] = None , rotational_symmetry_axis : Optional [ int ] = None , ) -> int Classify a pose prediction as correct or incorrect. PARAMETER DESCRIPTION position_gt ground truth position, expected shape (3,). TYPE: np . ndarray position_prediction predicted position, expected shape (3,). TYPE: np . ndarray position_threshold position threshold in meters, no threshold if None TYPE: Optional [ float ] DEFAULT: None orientation_q_qt ground truth orientation, scalar-last quaternion, shape (4,) orientation_q_prediction predicted orientation, scalar-last quaternion, shape (4,) extent_gt bounding box extents, shape (3,) only used if IoU threshold specified TYPE: Optional [ np . ndarray ] DEFAULT: None extent_prediction bounding box extents, shape (3,) only used if IoU threshold specified TYPE: Optional [ np . ndarray ] DEFAULT: None point_gt set of true points, expected shape (N,3) points_rec set of reconstructed points, expected shape (M,3) degree_threshold orientation threshold in degrees, no threshold if None TYPE: Optional [ float ] DEFAULT: None iou_3d_threshold 3D IoU threshold, no threshold if None TYPE: Optional [ float ] DEFAULT: None rotational_symmetry_axis Specify axis along which rotation is ignored. If None, no axis is ignored. 0 for x-axis, 1 for y-axis, 2 for z-axis. TYPE: Optional [ int ] DEFAULT: None RETURNS DESCRIPTION int 1 if error is below all provided thresholds. 0 if error is above one provided int threshold. Source code in sdfest/estimation/metrics.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def correct_thresh ( position_gt : np . ndarray , position_prediction : np . ndarray , orientation_gt : Rotation , orientation_prediction : Rotation , extent_gt : Optional [ np . ndarray ] = None , extent_prediction : Optional [ np . ndarray ] = None , points_gt : Optional [ np . ndarray ] = None , points_prediction : Optional [ np . ndarray ] = None , position_threshold : Optional [ float ] = None , degree_threshold : Optional [ float ] = None , iou_3d_threshold : Optional [ float ] = None , fscore_threshold : Optional [ float ] = None , rotational_symmetry_axis : Optional [ int ] = None , ) -> int : \"\"\"Classify a pose prediction as correct or incorrect. Args: position_gt: ground truth position, expected shape (3,). position_prediction: predicted position, expected shape (3,). position_threshold: position threshold in meters, no threshold if None orientation_q_qt: ground truth orientation, scalar-last quaternion, shape (4,) orientation_q_prediction: predicted orientation, scalar-last quaternion, shape (4,) extent_gt: bounding box extents, shape (3,) only used if IoU threshold specified extent_prediction: bounding box extents, shape (3,) only used if IoU threshold specified point_gt: set of true points, expected shape (N,3) points_rec: set of reconstructed points, expected shape (M,3) degree_threshold: orientation threshold in degrees, no threshold if None iou_3d_threshold: 3D IoU threshold, no threshold if None rotational_symmetry_axis: Specify axis along which rotation is ignored. If None, no axis is ignored. 0 for x-axis, 1 for y-axis, 2 for z-axis. Returns: 1 if error is below all provided thresholds. 0 if error is above one provided threshold. \"\"\" if position_threshold is not None : position_error = np . linalg . norm ( position_gt - position_prediction ) if position_error > position_threshold : return 0 if degree_threshold is not None : rad_threshold = degree_threshold * np . pi / 180.0 if rotational_symmetry_axis is not None : p = np . array ([ 0.0 , 0.0 , 0.0 ]) p [ rotational_symmetry_axis ] = 1.0 p1 = orientation_gt . apply ( p ) p2 = orientation_prediction . apply ( p ) rad_error = np . arccos ( p1 @ p2 ) else : rad_error = ( orientation_gt * orientation_prediction . inv ()) . magnitude () if rad_error > rad_threshold : return 0 if iou_3d_threshold is not None : raise NotImplementedError ( \"3D IoU is not impemented yet.\" ) # TODO implement 3D IoU # starting point for proper implementation: https://github.com/google-research-datasets/Objectron/blob/c06a65165a18396e1e00091981fd1652875c97b5/objectron/dataset/iou.py#L6 pass if fscore_threshold is not None : fscore = reconstruction_fscore ( points_gt , points_prediction , 0.01 ) if fscore < fscore_threshold : return 0 return 1","title":"correct_thresh()"},{"location":"reference/estimation/metrics/#sdfest.estimation.metrics.mean_accuracy","text":"mean_accuracy ( points_gt : np . ndarray , points_rec : np . ndarray , p_norm : int = 2 , normalize : bool = False , ) -> float Compute accuracy metric. Accuracy metric is the same as asymmetric chamfer distance from rec to gt. See, for example, Occupancy Networks Learning 3D Reconstruction in Function Space, Mescheder et al., 2019. PARAMETER DESCRIPTION points_gt set of true points, expected shape (N,3) TYPE: np . ndarray points_rec set of reconstructed points, expected shape (M,3) TYPE: np . ndarray p_norm which Minkowski p-norm is used for distance and nearest neighbor query TYPE: int DEFAULT: 2 normalize whether to divide result by Euclidean extent of points_gt TYPE: bool DEFAULT: False RETURNS DESCRIPTION float Arithmetic mean of p-norm from reconstructed points to closest (in p-norm) float ground truth points. Source code in sdfest/estimation/metrics.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def mean_accuracy ( points_gt : np . ndarray , points_rec : np . ndarray , p_norm : int = 2 , normalize : bool = False , ) -> float : \"\"\"Compute accuracy metric. Accuracy metric is the same as asymmetric chamfer distance from rec to gt. See, for example, Occupancy Networks Learning 3D Reconstruction in Function Space, Mescheder et al., 2019. Args: points_gt: set of true points, expected shape (N,3) points_rec: set of reconstructed points, expected shape (M,3) p_norm: which Minkowski p-norm is used for distance and nearest neighbor query normalize: whether to divide result by Euclidean extent of points_gt Returns: Arithmetic mean of p-norm from reconstructed points to closest (in p-norm) ground truth points. \"\"\" kd_tree = scipy . spatial . KDTree ( points_gt ) d , _ = kd_tree . query ( points_rec , p = p_norm ) if normalize : return np . mean ( d ) / extent ( points_gt ) else : return np . mean ( d )","title":"mean_accuracy()"},{"location":"reference/estimation/metrics/#sdfest.estimation.metrics.mean_completeness","text":"mean_completeness ( points_gt : np . ndarray , points_rec : np . ndarray , p_norm : int = 2 , normalize : bool = False , ) -> float Compute completeness metric. Completeness metric is the same as asymmetric chamfer distance from gt to rec. See, for example, Occupancy Networks Learning 3D Reconstruction in Function Space, Mescheder et al., 2019. PARAMETER DESCRIPTION points_gt set of true points, expected shape (N,3) TYPE: np . ndarray points_rec set of reconstructed points, expected shape (M,3) TYPE: np . ndarray p_norm which Minkowski p-norm is used for distance and nearest neighbor query TYPE: int DEFAULT: 2 normalize whether to divide result by Euclidean extent of points_gt TYPE: bool DEFAULT: False RETURNS DESCRIPTION float Arithmetic mean of p-norm from ground truth points to closest (in p-norm) float reconstructed points. Source code in sdfest/estimation/metrics.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def mean_completeness ( points_gt : np . ndarray , points_rec : np . ndarray , p_norm : int = 2 , normalize : bool = False , ) -> float : \"\"\"Compute completeness metric. Completeness metric is the same as asymmetric chamfer distance from gt to rec. See, for example, Occupancy Networks Learning 3D Reconstruction in Function Space, Mescheder et al., 2019. Args: points_gt: set of true points, expected shape (N,3) points_rec: set of reconstructed points, expected shape (M,3) p_norm: which Minkowski p-norm is used for distance and nearest neighbor query normalize: whether to divide result by Euclidean extent of points_gt Returns: Arithmetic mean of p-norm from ground truth points to closest (in p-norm) reconstructed points. \"\"\" kd_tree = scipy . spatial . KDTree ( points_rec ) d , _ = kd_tree . query ( points_gt , p = p_norm ) if normalize : return np . mean ( d ) / extent ( points_gt ) else : return np . mean ( d )","title":"mean_completeness()"},{"location":"reference/estimation/metrics/#sdfest.estimation.metrics.symmetric_chamfer","text":"symmetric_chamfer ( points_gt : np . ndarray , points_rec : np . ndarray , p_norm : int = 2 , normalize : bool = False , ) -> float Compute symmetric chamfer distance. There are various slightly different definitions for the chamfer distance. Note that completeness and accuracy are themselves sometimes referred to as chamfer distances, with symmetric chamfer distance being the combination of the two. Chamfer L1 in the literature (see, for example, Occupancy Networks Learning 3D Reconstruction in Function Space, Mescheder et al., 2019) refers to using arithmetic mean (note that this is actually differently scaled from L1) when combining accuracy and completeness. PARAMETER DESCRIPTION points_gt set of true points, expected shape (N,3) TYPE: np . ndarray points_rec set of reconstructed points, expected shape (M,3) TYPE: np . ndarray p_norm which Minkowski p-norm is used for distance and nearest neighbor query TYPE: int DEFAULT: 2 normalize whether to divide result by Euclidean extent of points_gt TYPE: bool DEFAULT: False RETURNS DESCRIPTION float Arithmetic mean of accuracy and completeness metrics using the specified p-norm. Source code in sdfest/estimation/metrics.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 def symmetric_chamfer ( points_gt : np . ndarray , points_rec : np . ndarray , p_norm : int = 2 , normalize : bool = False , ) -> float : \"\"\"Compute symmetric chamfer distance. There are various slightly different definitions for the chamfer distance. Note that completeness and accuracy are themselves sometimes referred to as chamfer distances, with symmetric chamfer distance being the combination of the two. Chamfer L1 in the literature (see, for example, Occupancy Networks Learning 3D Reconstruction in Function Space, Mescheder et al., 2019) refers to using arithmetic mean (note that this is actually differently scaled from L1) when combining accuracy and completeness. Args: points_gt: set of true points, expected shape (N,3) points_rec: set of reconstructed points, expected shape (M,3) p_norm: which Minkowski p-norm is used for distance and nearest neighbor query normalize: whether to divide result by Euclidean extent of points_gt Returns: Arithmetic mean of accuracy and completeness metrics using the specified p-norm. \"\"\" return ( mean_completeness ( points_gt , points_rec , p_norm = p_norm , normalize = normalize ) + mean_accuracy ( points_gt , points_rec , p_norm = p_norm , normalize = normalize ) ) / 2","title":"symmetric_chamfer()"},{"location":"reference/estimation/metrics/#sdfest.estimation.metrics.completeness_thresh","text":"completeness_thresh ( points_gt : np . ndarray , points_rec : np . ndarray , threshold : float , p_norm : int = 2 , normalize : bool = False , ) -> float Compute thresholded completion metric. See FroDO: From Detections to 3D Objects, Ru\u0308nz et al., 2020. PARAMETER DESCRIPTION points_gt set of true points, expected shape (N,3) TYPE: np . ndarray points_rec set of reconstructed points, expected shape (M,3) TYPE: np . ndarray threshold distance threshold to count a point as correct TYPE: float p_norm which Minkowski p-norm is used for distance and nearest neighbor query TYPE: int DEFAULT: 2 normalize whether to divide distances by Euclidean extent of points_gt TYPE: bool DEFAULT: False RETURNS DESCRIPTION float Ratio of ground truth points with closest reconstructed point closer than float threshold (in p-norm). Source code in sdfest/estimation/metrics.py 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 def completeness_thresh ( points_gt : np . ndarray , points_rec : np . ndarray , threshold : float , p_norm : int = 2 , normalize : bool = False , ) -> float : \"\"\"Compute thresholded completion metric. See FroDO: From Detections to 3D Objects, Ru\u0308nz et al., 2020. Args: points_gt: set of true points, expected shape (N,3) points_rec: set of reconstructed points, expected shape (M,3) threshold: distance threshold to count a point as correct p_norm: which Minkowski p-norm is used for distance and nearest neighbor query normalize: whether to divide distances by Euclidean extent of points_gt Returns: Ratio of ground truth points with closest reconstructed point closer than threshold (in p-norm). \"\"\" kd_tree = scipy . spatial . KDTree ( points_rec ) d , _ = kd_tree . query ( points_gt , p = p_norm ) if normalize : return np . sum ( d / extent ( points_gt ) < threshold ) / points_gt . shape [ 0 ] else : return np . sum ( d < threshold ) / points_gt . shape [ 0 ]","title":"completeness_thresh()"},{"location":"reference/estimation/metrics/#sdfest.estimation.metrics.accuracy_thresh","text":"accuracy_thresh ( points_gt : np . ndarray , points_rec : np . ndarray , threshold : float , p_norm : int = 2 , normalize : bool = False , ) -> float Compute thresholded accuracy metric. See FroDO: From Detections to 3D Objects, Ru\u0308nz et al., 2020. PARAMETER DESCRIPTION points_gt set of true points, expected shape (N,3) TYPE: np . ndarray points_rec set of reconstructed points, expected shape (M,3) TYPE: np . ndarray threshold distance threshold to count a point as correct TYPE: float p_norm which Minkowski p-norm is used for distance and nearest neighbor query TYPE: int DEFAULT: 2 normalize whether to divide distances by Euclidean extent of points_gt TYPE: bool DEFAULT: False RETURNS DESCRIPTION float Ratio of reconstructed points with closest ground truth point closer than float threshold (in p-norm). Source code in sdfest/estimation/metrics.py 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 def accuracy_thresh ( points_gt : np . ndarray , points_rec : np . ndarray , threshold : float , p_norm : int = 2 , normalize : bool = False , ) -> float : \"\"\"Compute thresholded accuracy metric. See FroDO: From Detections to 3D Objects, Ru\u0308nz et al., 2020. Args: points_gt: set of true points, expected shape (N,3) points_rec: set of reconstructed points, expected shape (M,3) threshold: distance threshold to count a point as correct p_norm: which Minkowski p-norm is used for distance and nearest neighbor query normalize: whether to divide distances by Euclidean extent of points_gt Returns: Ratio of reconstructed points with closest ground truth point closer than threshold (in p-norm). \"\"\" kd_tree = scipy . spatial . KDTree ( points_gt ) d , _ = kd_tree . query ( points_rec , p = p_norm ) if normalize : return np . sum ( d / extent ( points_gt ) < threshold ) / points_rec . shape [ 0 ] else : return np . sum ( d < threshold ) / points_rec . shape [ 0 ]","title":"accuracy_thresh()"},{"location":"reference/estimation/metrics/#sdfest.estimation.metrics.reconstruction_fscore","text":"reconstruction_fscore ( points_gt : np . ndarray , points_rec : np . ndarray , threshold : float , p_norm : int = 2 , normalize : bool = False , ) -> float Compute reconstruction fscore. See What Do Single-View 3D Reconstruction Networks Learn, Tatarchenko, 2019 PARAMETER DESCRIPTION points_gt set of true points, expected shape (N,3) TYPE: np . ndarray points_rec set of reconstructed points, expected shape (M,3) TYPE: np . ndarray threshold distance threshold to count a point as correct TYPE: float p_norm which Minkowski p-norm is used for distance and nearest neighbor query TYPE: int DEFAULT: 2 normalize whether to divide distances by Euclidean extent of points_gt TYPE: bool DEFAULT: False RETURNS DESCRIPTION float Harmonic mean of precision (thresholded accuracy) and recall (thresholded float completeness). Source code in sdfest/estimation/metrics.py 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 def reconstruction_fscore ( points_gt : np . ndarray , points_rec : np . ndarray , threshold : float , p_norm : int = 2 , normalize : bool = False , ) -> float : \"\"\"Compute reconstruction fscore. See What Do Single-View 3D Reconstruction Networks Learn, Tatarchenko, 2019 Args: points_gt: set of true points, expected shape (N,3) points_rec: set of reconstructed points, expected shape (M,3) threshold: distance threshold to count a point as correct p_norm: which Minkowski p-norm is used for distance and nearest neighbor query normalize: whether to divide distances by Euclidean extent of points_gt Returns: Harmonic mean of precision (thresholded accuracy) and recall (thresholded completeness). \"\"\" recall = completeness_thresh ( points_gt , points_rec , threshold , p_norm = p_norm , normalize = normalize ) precision = accuracy_thresh ( points_gt , points_rec , threshold , p_norm = p_norm , normalize = normalize ) if recall < 1e-7 or precision < 1e-7 : return 0 return 2 / ( 1 / recall + 1 / precision )","title":"reconstruction_fscore()"},{"location":"reference/estimation/metrics/#sdfest.estimation.metrics.extent","text":"extent ( points : np . ndarray ) -> float Compute largest Euclidean distance between any two points. PARAMETER DESCRIPTION points_gt set of true p_norm which Minkowski p-norm is used for distance and nearest neighbor query RETURNS DESCRIPTION float Ratio of reconstructed points with closest ground truth point closer than float threshold (in p-norm). Source code in sdfest/estimation/metrics.py 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 def extent ( points : np . ndarray ) -> float : \"\"\"Compute largest Euclidean distance between any two points. Args: points_gt: set of true p_norm: which Minkowski p-norm is used for distance and nearest neighbor query Returns: Ratio of reconstructed points with closest ground truth point closer than threshold (in p-norm). \"\"\" try : hull = scipy . spatial . ConvexHull ( points ) except scipy . spatial . qhull . QhullError : # fallback to brute force distance matrix return np . max ( scipy . spatial . distance_matrix ( points , points )) # this is wasteful, if too slow implement rotating caliper method return np . max ( scipy . spatial . distance_matrix ( points [ hull . vertices ], points [ hull . vertices ]) )","title":"extent()"},{"location":"reference/estimation/simple_setup/","text":"sdfest.estimation.simple_setup Modular SDF pose and shape estimation in depth images. NoDepthError Bases: ValueError Raised when there is no depth data left after preprocessing. Source code in sdfest/estimation/simple_setup.py 30 31 32 class NoDepthError ( ValueError ): \"\"\"Raised when there is no depth data left after preprocessing.\"\"\" pass SDFPipeline SDF pose and shape estimation pipeline. Source code in sdfest/estimation/simple_setup.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 class SDFPipeline : \"\"\"SDF pose and shape estimation pipeline.\"\"\" def __init__ ( self , config : dict ) -> None : \"\"\"Load and initialize the pipeline. Args: config: Configuration dictionary. \"\"\" self . _parse_config ( config ) self . init_network = SDFPoseNet ( INIT_MODULE_DICT [ self . init_config [ \"backbone_type\" ]]( ** self . init_config [ \"backbone\" ] ), INIT_MODULE_DICT [ self . init_config [ \"head_type\" ]]( shape_dimension = self . vae_config [ \"latent_size\" ], ** self . init_config [ \"head\" ], ), ) . to ( self . device ) load_model_weights ( self . init_config [ \"model\" ], self . init_network , self . device , self . init_config . get ( \"model_url\" ), ) self . init_network . eval () self . resolution = 64 self . vae = SDFVAE ( sdf_size = 64 , latent_size = self . vae_config [ \"latent_size\" ], encoder_dict = self . vae_config [ \"encoder\" ], decoder_dict = self . vae_config [ \"decoder\" ], device = self . device , ) . to ( self . device ) load_model_weights ( self . vae_config [ \"model\" ], self . vae , self . device , self . vae_config . get ( \"model_url\" ), ) self . vae . eval () self . cam = Camera ( ** self . camera_config ) self . render = lambda sdf , pos , quat , i_s : render_depth_gpu ( sdf , pos , quat , i_s , None , None , None , config [ \"threshold\" ], self . cam ) self . config = config self . log_data = [] def _parse_config ( self , config : dict ) -> None : \"\"\"Parse config dict. This function makes sure that all required keys are available. \"\"\" self . device = config [ \"device\" ] self . init_config = config [ \"init\" ] self . vae_config = config [ \"vae\" ] if \"vae\" in config else self . init_config [ \"vae\" ] self . camera_config = config [ \"camera\" ] self . result_selection_strategy = config . get ( \"result_selection_strategy\" , \"last_iteration\" ) # last_iteration | best_inlier_ratio self . _relative_inlier_threshold = config . get ( \"relative_inlier_threshold\" , 0.03 ) # relative depth error threshold for pixel to be considered inlier if \"far_field\" in config : self . _far_field = config [ \"far_field\" ] if \"far_field\" in config else None self . config = config @staticmethod def _compute_gradients ( loss : torch . Tensor ) -> None : loss . backward () def _compute_view_losses ( self , depth_input : torch . Tensor , depth_estimate : torch . Tensor , position : torch . Tensor , orientation : torch . Tensor , scale : torch . Tensor , sdf : torch . Tensor , ) -> Tuple [ torch . Tensor ]: # depth l1 overlap_mask = ( depth_input > 0 ) & ( depth_estimate > 0 ) depth_error = torch . abs ( depth_estimate - depth_input ) # max_depth_error = 0.05 # depth_outlier_mask = depth_error > max_depth_error # depth_mask = overlap_mask & ~depth_outlier_mask # depth_error[~overlap_mask] = 0 loss_depth = torch . mean ( depth_error [ overlap_mask ]) # pointcloud l1 pointcloud_obs = pointset_utils . depth_to_pointcloud ( depth_input , self . cam , normalize = False ) pointcloud_error = losses . pc_loss ( pointcloud_obs , position , orientation , scale , sdf , ) loss_pc = torch . mean ( torch . abs ( pointcloud_error )) # nearest neighbor l1 # pointcloud_outliers = pointset_utils.depth_to_pointcloud( # depth_estimate, self.cam, normalize=False, mask=depth_outlier_mask # ) loss_nn = 0 # if pointcloud_outliers.shape[0] != 0: # pass # loss_nn += 0 # # TODO different gradients for point cloud (not derived by renderer) # outlier_nn_d = losses.nn_loss(pointcloud_outliers, pointcloud_obs) # # only use positive, because sqrt is not differentiable at 0 # outlier_nn_d = outlier_nn_d[outlier_nn_d > 0] # loss_nn = loss_nn + torch.mean(torch.sqrt(outlier_nn_d)) return loss_depth , loss_pc , loss_nn def _compute_point_constraint_loss ( self , orientation : torch . Tensor ) -> torch . Tensor : \"\"\"Compute loss for point constraint if specified.\"\"\" if self . _point_constraint is not None : loss_point_constraint = losses . point_constraint_loss ( orientation_q = orientation [ 0 ], source = self . _point_constraint [ 0 ], target = self . _point_constraint [ 1 ], ) weight = self . _point_constraint [ 2 ] return weight * loss_point_constraint else : return orientation . new_tensor ( 0.0 ) def _compute_inlier_ratio ( self , depth_input : torch . Tensor , depth_estimate : torch . Tensor , ) -> None : \"\"\"Compute ratio of pixels with small relative depth error.\"\"\" rel_depth_error = torch . abs ( depth_input - depth_estimate ) / depth_input inlier_mask = rel_depth_error < self . _relative_inlier_threshold inliers = torch . count_nonzero ( inlier_mask ) valid_depth_pixels = torch . count_nonzero ( depth_input ) inlier_ratio = inliers / valid_depth_pixels return inlier_ratio def _update_best_estimate ( self , depth_input : torch . Tensor , depth_estimate : torch . Tensor , position : torch . Tensor , orientation : torch . Tensor , scale : torch . Tensor , latent_shape : torch . Tensor , ) -> None : \"\"\"Update the best current estimate by keeping track of inlier ratio. Returns: Inlier ratio of this configuration. \"\"\" inlier_ratio = self . _compute_inlier_ratio ( depth_input , depth_estimate ) if self . _best_inlier_ratio is None or inlier_ratio > self . _best_inlier_ratio : self . _best_inlier_ratio = inlier_ratio self . _best_position = position self . _best_orientation = orientation self . _best_scale = scale self . _best_latent_shape = latent_shape return inlier_ratio def __call__ ( self , depth_images : torch . Tensor , masks : torch . Tensor , color_images : torch . Tensor , visualize : bool = False , camera_positions : Optional [ torch . Tensor ] = None , camera_orientations : Optional [ torch . Tensor ] = None , log_path : Optional [ str ] = None , shape_optimization : bool = True , animation_path : Optional [ str ] = None , point_constraint : Optional [ Tuple [ torch . Tensor ]] = None , prior_orientation_distribution : Optional [ torch . Tensor ] = None , training_orientation_distribution : Optional [ torch . Tensor ] = None , ) -> tuple : \"\"\"Infer pose, size and latent representation from depth and mask. If multiple images are passed the cameras are assumed to be fixed. All tensors should be on the same device as the pipeline. Batch dimension N must be provided either for all or none of the arguments. Args: depth_images: The depth map containing the distance along the camera's z-axis. Does not have to be masked, necessary preprocessing is done by pipeline. Will be masked and preprocessed in-place (pass copy if full depth is used afterwards). Shape (N, H, W) or (H, W) for a single depth image. masks: binary mask of the object to estimate, same shape as depth_images color_images: the color image (currently only used in visualization), shape (N, H, W, 3) or (H, W, 3), RGB, float, 0-1. visualize: Whether to visualize the intermediate steps and final result. camera_positions: position of camera in world coordinates for each image, if None, (0,0,0) will be assumed for all images, shape (N, 3) or (3,) camera_orientations: orientation of camera in world-frame as normalized quaternion, quaternion is in scalar-last convention, note, that this is the quaternion that transforms a point from camera to world-frame if None, (0,0,0,1) will be assumed for all images, shape (N, 4) or (4,) log_path: file path to write timestamps and intermediate steps to, no logging is performed if None shape_optimization: enable or disable shape optimization during iterative optimization animation_path: file path to write rendering and error visualizations to point_constraint: tuple of source point and rotated target point and weight a loss will be added that penalizes weight * || rotation @ source - target ||_2 prior_orientation_distribution: Prior distribution of orientations used for initialization. If None, distribution of initialization network will not be modified. Only supported for initialization network with discretized orientation representation. Output distribution of initialization network will be adjusted by multiplying with prior_orientation_distribution / training_orientation_distribution and renormalizing. Tensor of shape (N,C,) or (C,) for single image. C being the number of grid cells in the SO3Grid used by the initialization network. training_orientation_distribution: Distribution of orientations used for training initialization network. If None, equal probability for each cell will be assumed. Note this is only approximately the same as a uniform distribution. Only used if prior_orientation_distribution is provided. Tensor of shape (C,). C being the number of grid cells in the SO3Grid used by the initialization network. N not supported, since same training network (and hence distribution) is used independent of view. Returns: - 3D pose of SDF center in world frame, shape (1,3,) - Orientation as normalized quaternion, scalar-last convention, shape (1,4,) - Size of SDF as length of half-width, shape (1,) - Latent shape representation of the object, shape (1,latent_size,). \"\"\" # initialize optimization self . _best_inlier_ratio = None self . _point_constraint = point_constraint if animation_path is not None : self . _create_animation_folders ( animation_path ) start_time = time . time () # for logging # Add batch dimension if necessary if depth_images . dim () == 2 : depth_images = depth_images . unsqueeze ( 0 ) masks = masks . unsqueeze ( 0 ) color_images = color_images . unsqueeze ( 0 ) if camera_positions is not None : camera_positions = camera_positions . unsqueeze ( 0 ) if camera_orientations is not None : camera_orientations = camera_orientations . unsqueeze ( 0 ) if prior_orientation_distribution is not None : prior_orientation_distribution = ( prior_orientation_distribution . unsqueeze ( 0 ) ) # TODO assert all tensors have expected dimension if animation_path is not None : self . _save_inputs ( animation_path , depth_images , color_images , masks ) n_imgs = depth_images . shape [ 0 ] if camera_positions is None : camera_positions = torch . zeros ( n_imgs , 3 , device = self . device ) if camera_orientations is None : camera_orientations = torch . zeros ( n_imgs , 4 , device = self . device ) camera_orientations [:, 3 ] = 1.0 with torch . no_grad (): self . _preprocess_depth ( depth_images , masks ) # store pointcloud without reconstruction if log_path is not None : torch . cuda . synchronize () self . _log_data ( { \"timestamp\" : time . time () - start_time , \"depth_images\" : depth_images , \"color_images\" : color_images , \"masks\" : masks , \"color_images\" : color_images , \"camera_positions\" : camera_positions , \"camera_orientations\" : camera_orientations , }, ) # Initialization with torch . no_grad (): latent_shape , position , scale , orientation = self . _nn_init ( depth_images , camera_positions , camera_orientations , prior_orientation_distribution , training_orientation_distribution , ) if log_path is not None : torch . cuda . synchronize () self . _log_data ( { \"timestamp\" : time . time () - start_time , \"camera_positions\" : camera_positions , \"camera_orientations\" : camera_orientations , \"latent_shape\" : latent_shape , \"position\" : position , \"scale_inv\" : 1 / scale , \"orientation\" : orientation , }, ) if animation_path is not None : self . _save_preprocessed_inputs ( animation_path , depth_images ) # Iterative optimization self . _current_iteration = 1 position . requires_grad_ () scale . requires_grad_ () orientation . requires_grad_ () latent_shape . requires_grad_ () if visualize : fig_vis , axes = plt . subplots ( 2 , 3 , sharex = True , sharey = True , figsize = ( 12 , 8 ) ) fig_loss , ( loss_ax , inlier_ax ) = plt . subplots ( 1 , 2 ) vmin , vmax = None , None depth_losses = [] pointcloud_losses = [] nn_losses = [] point_constraint_losses = [] inlier_ratios = [] total_losses = [] opt_vars = [ { \"params\" : position , \"lr\" : 1e-3 }, { \"params\" : orientation , \"lr\" : 1e-2 }, { \"params\" : scale , \"lr\" : 1e-3 }, { \"params\" : latent_shape , \"lr\" : 1e-2 }, ] optimizer = torch . optim . Adam ( opt_vars ) while self . _current_iteration <= self . config [ \"max_iterations\" ]: optimizer . zero_grad () norm_orientation = orientation / torch . sqrt ( torch . sum ( orientation ** 2 )) with torch . set_grad_enabled ( shape_optimization ): sdf = self . vae . decode ( latent_shape ) loss_depth = torch . tensor ( 0.0 , device = self . device , requires_grad = True ) loss_pc = torch . tensor ( 0.0 , device = self . device , requires_grad = True ) loss_nn = torch . tensor ( 0.0 , device = self . device , requires_grad = True ) for depth_image , camera_position , camera_orientation in zip ( depth_images , camera_positions , camera_orientations ): # transform object to camera frame q_w2c = quaternion_utils . quaternion_invert ( camera_orientation ) position_c = quaternion_utils . quaternion_apply ( q_w2c , position - camera_position ) orientation_c = quaternion_utils . quaternion_multiply ( q_w2c , norm_orientation ) depth_estimate = self . render ( sdf [ 0 , 0 ], position_c [ 0 ], orientation_c [ 0 ], 1 / scale [ 0 ] ) view_loss_depth , view_loss_pc , view_loss_nn = self . _compute_view_losses ( depth_image , depth_estimate , position_c [ 0 ], orientation_c [ 0 ], scale [ 0 ], sdf [ 0 , 0 ], ) loss_depth = loss_depth + view_loss_depth loss_pc = loss_pc + view_loss_pc loss_nn = loss_nn + view_loss_nn loss_point_constraint = self . _compute_point_constraint_loss ( orientation ) loss = ( self . config [ \"depth_weight\" ] * loss_depth + self . config [ \"pc_weight\" ] * loss_pc + self . config [ \"nn_weight\" ] * loss_nn + loss_point_constraint ) self . _compute_gradients ( loss ) optimizer . step () optimizer . zero_grad () with torch . no_grad (): orientation /= torch . sqrt ( torch . sum ( orientation ** 2 )) inlier_ratio = self . _update_best_estimate ( depth_image , depth_estimate , position , orientation , scale , latent_shape , ) if visualize : depth_losses . append ( loss_depth . item ()) pointcloud_losses . append ( loss_pc . item ()) nn_losses . append ( loss_nn . item ()) point_constraint_losses . append ( loss_point_constraint . item ()) inlier_ratios . append ( inlier_ratio . item ()) total_losses . append ( loss . item ()) if log_path is not None : torch . cuda . synchronize () self . _log_data ( { \"timestamp\" : time . time () - start_time , \"latent_shape\" : latent_shape , \"position\" : position , \"scale_inv\" : 1 / scale , \"orientation\" : orientation , }, ) with torch . no_grad (): if animation_path is not None : self . _save_current_state ( depth_images , animation_path , camera_positions , camera_orientations , position , orientation , 1 / scale , sdf , ) if visualize and ( self . _current_iteration % 10 == 1 or self . _current_iteration == self . config [ \"max_iterations\" ] ): q_w2c = quaternion_utils . quaternion_invert ( camera_orientations [ 0 ]) position_c = quaternion_utils . quaternion_apply ( q_w2c , position - camera_positions [ 0 ] ) orientation_c = quaternion_utils . quaternion_multiply ( q_w2c , orientation ) current_depth = self . render ( sdf [ 0 , 0 ], position_c , orientation_c , 1 / scale ) depth_image = depth_images [ 0 ] color_image = color_images [ 0 ] if self . _current_iteration == 1 : vmin = depth_image [ depth_image != 0 ] . min () * 0.9 vmax = depth_image [ depth_image != 0 ] . max () # show input image axes [ 0 , 0 ] . clear () axes [ 0 , 0 ] . imshow ( depth_image . cpu (), vmin = vmin , vmax = vmax ) axes [ 0 , 1 ] . imshow ( color_image . cpu ()) # show initial estimate axes [ 1 , 0 ] . clear () axes [ 1 , 0 ] . imshow ( current_depth . detach () . cpu (), vmin = vmin , vmax = vmax ) axes [ 1 , 0 ] . set_title ( f \"loss { loss . item () } \" ) # update iterative estimate # axes[0, 2].clear() # axes[0, 2].imshow(rendered_error.detach().cpu()) # axes[0, 2].set_title(\"depth_loss\") # axes[1, 2].clear() # axes[1, 2].imshow(error_pc.detach().cpu()) # axes[1, 2].set_title(\"pointcloud_loss\") loss_ax . clear () loss_ax . plot ( depth_losses , label = \"Depth\" ) loss_ax . plot ( pointcloud_losses , label = \"Pointcloud\" ) loss_ax . plot ( nn_losses , label = \"Nearest Neighbor\" ) if self . _point_constraint is not None : loss_ax . plot ( point_constraint_losses , label = \"Point constraint\" ) loss_ax . plot ( total_losses , label = \"Total\" ) loss_ax . set_yscale ( \"log\" ) loss_ax . legend () inlier_ax . clear () inlier_ax . plot ( inlier_ratios , label = \"Inlier Ratio\" ) inlier_ax . legend () axes [ 1 , 1 ] . clear () axes [ 1 , 1 ] . imshow ( current_depth . detach () . cpu (), vmin = vmin , vmax = vmax ) axes [ 1 , 1 ] . set_title ( f \"loss { loss . item () } \" ) fig_loss . canvas . draw () fig_vis . canvas . draw () plt . pause ( 0.1 ) self . _current_iteration += 1 if visualize : plt . show () plt . close ( fig_loss ) plt . close ( fig_vis ) if log_path is not None : self . _write_log_data ( log_path ) if animation_path is not None : self . _create_animations ( animation_path ) if self . result_selection_strategy == \"last_iteration\" : return position , orientation , scale , latent_shape elif self . result_selection_strategy == \"best_inlier_ratio\" : return ( self . _best_position , self . _best_orientation , self . _best_scale , self . _best_latent_shape , ) else : raise ValueError ( f \"Result selection strategy { self . result_selection_strategy } is not\" \"supported.\" ) def _log_data ( self , data : dict ) -> None : \"\"\"Add dictionary with associated timestamp to log data list.\"\"\" new_log_data = copy . deepcopy ( data ) self . log_data . append ( new_log_data ) def _write_log_data ( self , file_path : str ) -> None : \"\"\"Write current list of log data to file.\"\"\" with open ( file_path , \"wb\" ) as f : pickle . dump ({ \"config\" : self . config , \"log\" : self . log_data }, f ) self . log_data = [] # reset log def generate_depth ( self , position : torch . Tensor , orientation : torch . Tensor , scale : torch . Tensor , latent : torch . Tensor , ) -> torch . Tensor : \"\"\"Generate depth image representing positioned object.\"\"\" sdf = self . vae . decode ( latent ) depth = self . render ( sdf [ 0 , 0 ], position , orientation , 1 / scale ) return depth def generate_mesh ( self , latent : torch . tensor , scale : torch . tensor , complete_mesh : bool = False ) -> synthetic . Mesh : \"\"\"Generate mesh without pose. Currently only supports batch size 1. Args: latent: Latent shape descriptor, shape (1,L). scale: Relative scale of the signed distance field, (i.e., half-width), shape (1,). complete_mesh: If True, the SDF will be padded with positive values prior to converting it to a mesh. This ensures a watertight mesh is created. Returns: Generate mesh by decoding latent shape descriptor and scaling it. \"\"\" with torch . no_grad (): sdf = self . vae . decode ( latent ) if complete_mesh : inc = 2 sdf = torch . nn . functional . pad ( sdf , ( 1 , 1 , 1 , 1 , 1 , 1 ), value = 1.0 ) else : inc = 0 try : sdf = sdf . cpu () . numpy () s = 2.0 / ( self . resolution - 1 ) vertices , faces , _ , _ = marching_cubes ( sdf [ 0 , 0 ], spacing = ( s , s , s , ), level = self . config [ \"iso_threshold\" ], ) c = s * ( self . resolution + inc - 1 ) / 2.0 # move origin to center vertices -= np . array ([[ c , c , c ]]) mesh = o3d . geometry . TriangleMesh ( vertices = o3d . utility . Vector3dVector ( vertices ), triangles = o3d . utility . Vector3iVector ( faces ), ) except KeyError : return None return synthetic . Mesh ( mesh = mesh , scale = scale . item (), rel_scale = True ) def _preprocess_depth ( self , depth_images : torch . Tensor , masks : torch . Tensor ) -> None : \"\"\"Preprocesses depth image based on segmentation mask. Args: depth_images: the depth images to preprocess, will be modified in place, shape (N, H, W) masks: the masks used for preprocessing, same shape as depth_images \"\"\" # shrink mask # masks = ( # -torch.nn.functional.max_pool2d( # -masks.double(), kernel_size=9, stride=1, padding=4 # ) # ).bool() depth_images [ ~ masks ] = 0 # set outside of depth to 0 # remove data far away (should be based on what distances ocurred in training) if self . _far_field is not None : depth_images [ depth_images > self . _far_field ] = 0 # only consider available depth values for outlier detection # masks = torch.logical_and(masks, depth_images != 0) # depth_images = # remove outliers based on median # plt.imshow(depth_images[0].cpu().numpy()) # plt.show() # for mask, depth_image in zip(masks, depth_images): # median = torch.median(depth_image[mask]) # errors = torch.abs(depth_image[mask] - median) # bins = 100 # hist = torch.histc(errors, bins=bins) # print(hist) # zero_indices = torch.nonzero(hist == 0) # if len(zero_indices): # threshold = zero_indices[0] / bins * errors.max() # print(threshold) # depth_image[torch.abs(depth_image - median) > threshold] = 0 # plt.imshow(depth_images[0].cpu().numpy()) # plt.show() def _nn_init ( self , depth_images : torch . Tensor , camera_positions : torch . Tensor , camera_orientations : torch . Tensor , prior_orientation_distribution : Optional [ torch . Tensor ] = None , training_orientation_distribution : Optional [ torch . Tensor ] = None , ) -> Tuple : \"\"\"Estimate shape, pose, scale and orientation using initialization network. Args: depth_images: the preprocessed depth images, shape (N, H, W) camera_positions: position of camera in world coordinates for each image, shape (N, 3) camera_orientations: orientation of camera in world-frame as normalized quaternion, quaternion is in scalar-last convention, shape (N, 4) prior_orientation_distribution: Prior distribution of orientations used for initialization. If None, distribution of initialization network will not be modified. Only supported for initialization network with discretized orientation representation. Output distribution of initialization network will be adjusted by multiplying with prior_orientation_distribution / training_orientation_distribution and renormalizing. Tensor of shape (N,C,). C being the number of grid cells in the SO3Grid used by the initialization network. training_orientation_distribution: Distribution of orientations used for training initialization network. If None, equal probability for each cell will be assumed. Note this is only approximately the same as a uniform distribution. Only used if prior_orientation_distribution is provided. Tensor of shape (C,). C being the number of grid cells in the SO3Grid used by the initialization network. Returns: Tuple comprised of: - Latent shape representation of the object, shape (1, latent_size) - 3D pose of SDF center in camera frame, shape (1, 3) - Size of SDF as length of half-width, (1,) - Orientation of SDF as normalized quaternion (1,4) \"\"\" if ( prior_orientation_distribution is not None and self . init_config [ \"head\" ][ \"orientation_repr\" ] != \"discretized\" ): raise ValueError ( \"prior_orientation_distribution only supported for discretized \" \"orientation representation.\" ) best = 0 best_result = None for i , ( depth_image , camera_orientation , camera_position ) in enumerate ( zip ( depth_images , camera_orientations , camera_positions ) ): centroid = None if self . init_config [ \"backbone_type\" ] == \"VanillaPointNet\" : inp = pointset_utils . depth_to_pointcloud ( depth_image , self . cam , normalize = False ) if len ( inp ) == 0 : raise NoDepthError if self . init_config [ \"normalize_pose\" ]: inp , centroid = pointset_utils . normalize_points ( inp ) else : inp = depth_image inp = inp . unsqueeze ( 0 ) latent_shape , position , scale , orientation_repr = self . init_network ( inp ) if self . config [ \"mean_shape\" ]: latent_shape = latent_shape . new_zeros ( latent_shape . shape ) if centroid is not None : position += centroid if self . init_config [ \"head\" ][ \"orientation_repr\" ] == \"discretized\" : posterior_orientation_dist = torch . softmax ( orientation_repr , - 1 ) if prior_orientation_distribution is not None : posterior_orientation_dist = self . _adjust_categorical_posterior ( posterior = posterior_orientation_dist , prior = prior_orientation_distribution [ i ], train_prior = training_orientation_distribution , ) orientation_camera = torch . tensor ( self . init_network . _head . _grid . index_to_quat ( posterior_orientation_dist . argmax () . item () ), dtype = torch . float , device = self . device , ) . unsqueeze ( 0 ) elif self . init_config [ \"head\" ][ \"orientation_repr\" ] == \"quaternion\" : orientation_camera = orientation_repr else : raise NotImplementedError ( \"Orientation representation is not supported\" ) # output are in camera frame, transform to world frame position_world = ( quaternion_utils . quaternion_apply ( camera_orientation , position ) + camera_position ) orientation_world = quaternion_utils . quaternion_multiply ( camera_orientation , orientation_camera ) if self . config [ \"init_view\" ] == \"first\" : return latent_shape , position_world , scale , orientation_world elif self . config [ \"init_view\" ] == \"best\" : if self . init_config [ \"head\" ][ \"orientation_repr\" ] != \"discretized\" : raise NotImplementedError ( '\"best\" init strategy only supported with discretized ' \"orientation representation\" ) maximum = posterior_orientation_dist . max () if maximum > best : best = maximum best_result = latent_shape , position_world , scale , orientation_world else : raise NotImplementedError ( 'Only \"first\" and \"best\" strategies are currently supported' ) return best_result def _generate_uniform_quaternion ( self ) -> torch . tensor : \"\"\"Generate a uniform quaternion. Following the method from K. Shoemake, Uniform Random Rotations, 1992. See: http://planning.cs.uiuc.edu/node198.html Returns: Uniformly distributed unit quaternion on the estimator's device. \"\"\" u1 , u2 , u3 = random . random (), random . random (), random . random () return ( torch . tensor ( [ math . sqrt ( 1 - u1 ) * math . sin ( 2 * math . pi * u2 ), math . sqrt ( 1 - u1 ) * math . cos ( 2 * math . pi * u2 ), math . sqrt ( u1 ) * math . sin ( 2 * math . pi * u3 ), math . sqrt ( u1 ) * math . cos ( 2 * math . pi * u3 ), ] ) . unsqueeze ( 0 ) . to ( self . device ) ) def _create_animation_folders ( self , animation_path : str ) -> None : \"\"\"Create subfolders to store animation frames.\"\"\" os . makedirs ( animation_path ) depth_path = os . path . join ( animation_path , \"depth\" ) os . makedirs ( depth_path ) error_path = os . path . join ( animation_path , \"depth_error\" ) os . makedirs ( error_path ) sdf_path = os . path . join ( animation_path , \"sdf\" ) os . makedirs ( sdf_path ) def _save_inputs ( self , animation_path : str , color_images : torch . Tensor , depth_images : torch . Tensor , instance_masks : torch . Tensor , ) -> None : color_path = os . path . join ( animation_path , \"color_input.png\" ) fig , ax = plt . subplots () ax . imshow ( color_images [ 0 ] . cpu () . numpy ()) fig . savefig ( color_path ) plt . close ( fig ) depth_path = os . path . join ( animation_path , \"depth_input.png\" ) fig , ax = plt . subplots () ax . imshow ( depth_images [ 0 ] . cpu () . numpy ()) fig . savefig ( depth_path ) plt . close ( fig ) mask_path = os . path . join ( animation_path , \"mask.png\" ) fig , ax = plt . subplots () ax . imshow ( instance_masks [ 0 ] . cpu () . numpy ()) fig . savefig ( mask_path ) plt . close ( fig ) def _save_preprocessed_inputs ( self , animation_path : str , depth_images : torch . Tensor , ) -> None : depth_path = os . path . join ( animation_path , \"preprocessed_depth_input.png\" ) fig , ax = plt . subplots () ax . imshow ( depth_images [ 0 ] . cpu () . numpy ()) fig . savefig ( depth_path ) plt . close ( fig ) def _save_current_state ( self , depth_images : torch . Tensor , animation_path : str , camera_positions : torch . Tensor , camera_orientations : torch . Tensor , position : torch . Tensor , orientation : torch . Tensor , scale_inv : torch . Tensor , sdf : torch . Tensor , ) -> None : q_w2c = quaternion_utils . quaternion_invert ( camera_orientations [ 0 ]) position_c = quaternion_utils . quaternion_apply ( q_w2c , position - camera_positions [ 0 ] ) orientation_c = quaternion_utils . quaternion_multiply ( q_w2c , orientation ) current_depth = self . render ( sdf [ 0 , 0 ], position_c , orientation_c , scale_inv ) depth_path = os . path . join ( animation_path , \"depth\" , f \" { self . _current_iteration : 06 } .png\" ) fig , ax = plt . subplots () ax . imshow ( current_depth . cpu () . numpy (), interpolation = \"none\" ) fig . savefig ( depth_path ) plt . close ( fig ) error_image = torch . abs ( current_depth - depth_images [ 0 ]) error_image [ depth_images [ 0 ] == 0 ] = 0 error_image [ current_depth == 0 ] = 0 error_path = os . path . join ( animation_path , \"depth_error\" , f \" { self . _current_iteration : 06 } .png\" ) fig , ax = plt . subplots () ax . imshow ( error_image . cpu () . numpy (), interpolation = \"none\" ) fig . savefig ( error_path ) plt . close ( fig ) unscaled_threshold = self . config [ \"threshold\" ] * scale_inv . item () mesh = sdf_utils . mesh_from_sdf ( sdf [ 0 , 0 ] . cpu () . numpy (), unscaled_threshold , complete_mesh = True , ) sdf_path = os . path . join ( animation_path , \"sdf\" , f \" { self . _current_iteration : 06 } .png\" ) # map y -> z; z -> y transform = np . eye ( 4 ) transform [ 0 : 3 , 0 : 3 ] = np . array ([[ - 1 , 0 , 0 ], [ 0 , 0 , 1 ], [ 0 , 1 , 0 ]]) fig , ax = plt . subplots () sdf_utils . plot_mesh ( mesh , transform = transform , plot_object = ax ) fig . savefig ( sdf_path ) plt . close ( fig ) def _create_animations ( self , animation_path : str ) -> None : names = [ \"sdf\" , \"depth\" , \"depth_error\" ] for name in names : frame_folder = os . path . join ( animation_path , name ) video_name = os . path . join ( animation_path , f \" { name } .mp4\" ) ffmpeg . input ( os . path . join ( frame_folder , \"*.png\" ), pattern_type = \"glob\" , framerate = 30 ) . output ( video_name ) . run () @staticmethod def _adjust_categorical_posterior ( posterior : torch . Tensor , prior : torch . Tensor , train_prior : torch . Tensor ) -> torch . Tensor : \"\"\"Adjust categorical posterior distribution. Posterior is calculated with a train_prior Args: posterior: Posterior distribution computed assuming train_prior. Shape (..., K). K being number of categories. prior: The desired new prior distribution. Same shape as posterior. train_prior: The prior distribution used to compute the posterior. If None, equal probability for each category will be assumed. Same shape as posterior. Returns: The categorical posterior, adjusted such that prior is prior, instead of train_prior. Same shape as posterior. \"\"\" adjusted_posterior = posterior . clone () # adjust if prior different from training adjusted_posterior *= prior if train_prior is not None : adjusted_posterior /= train_prior adjusted_posterior = torch . nn . functional . normalize ( adjusted_posterior , p = 1 , dim =- 1 ) return adjusted_posterior __init__ __init__ ( config : dict ) -> None Load and initialize the pipeline. PARAMETER DESCRIPTION config Configuration dictionary. TYPE: dict Source code in sdfest/estimation/simple_setup.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def __init__ ( self , config : dict ) -> None : \"\"\"Load and initialize the pipeline. Args: config: Configuration dictionary. \"\"\" self . _parse_config ( config ) self . init_network = SDFPoseNet ( INIT_MODULE_DICT [ self . init_config [ \"backbone_type\" ]]( ** self . init_config [ \"backbone\" ] ), INIT_MODULE_DICT [ self . init_config [ \"head_type\" ]]( shape_dimension = self . vae_config [ \"latent_size\" ], ** self . init_config [ \"head\" ], ), ) . to ( self . device ) load_model_weights ( self . init_config [ \"model\" ], self . init_network , self . device , self . init_config . get ( \"model_url\" ), ) self . init_network . eval () self . resolution = 64 self . vae = SDFVAE ( sdf_size = 64 , latent_size = self . vae_config [ \"latent_size\" ], encoder_dict = self . vae_config [ \"encoder\" ], decoder_dict = self . vae_config [ \"decoder\" ], device = self . device , ) . to ( self . device ) load_model_weights ( self . vae_config [ \"model\" ], self . vae , self . device , self . vae_config . get ( \"model_url\" ), ) self . vae . eval () self . cam = Camera ( ** self . camera_config ) self . render = lambda sdf , pos , quat , i_s : render_depth_gpu ( sdf , pos , quat , i_s , None , None , None , config [ \"threshold\" ], self . cam ) self . config = config self . log_data = [] __call__ __call__ ( depth_images : torch . Tensor , masks : torch . Tensor , color_images : torch . Tensor , visualize : bool = False , camera_positions : Optional [ torch . Tensor ] = None , camera_orientations : Optional [ torch . Tensor ] = None , log_path : Optional [ str ] = None , shape_optimization : bool = True , animation_path : Optional [ str ] = None , point_constraint : Optional [ Tuple [ torch . Tensor ]] = None , prior_orientation_distribution : Optional [ torch . Tensor ] = None , training_orientation_distribution : Optional [ torch . Tensor ] = None , ) -> tuple Infer pose, size and latent representation from depth and mask. If multiple images are passed the cameras are assumed to be fixed. All tensors should be on the same device as the pipeline. Batch dimension N must be provided either for all or none of the arguments. PARAMETER DESCRIPTION depth_images The depth map containing the distance along the camera's z-axis. Does not have to be masked, necessary preprocessing is done by pipeline. Will be masked and preprocessed in-place (pass copy if full depth is used afterwards). Shape (N, H, W) or (H, W) for a single depth image. TYPE: torch . Tensor masks binary mask of the object to estimate, same shape as depth_images TYPE: torch . Tensor color_images the color image (currently only used in visualization), shape (N, H, W, 3) or (H, W, 3), RGB, float, 0-1. TYPE: torch . Tensor visualize Whether to visualize the intermediate steps and final result. TYPE: bool DEFAULT: False camera_positions position of camera in world coordinates for each image, if None, (0,0,0) will be assumed for all images, shape (N, 3) or (3,) TYPE: Optional [ torch . Tensor ] DEFAULT: None camera_orientations orientation of camera in world-frame as normalized quaternion, quaternion is in scalar-last convention, note, that this is the quaternion that transforms a point from camera to world-frame if None, (0,0,0,1) will be assumed for all images, shape (N, 4) or (4,) TYPE: Optional [ torch . Tensor ] DEFAULT: None log_path file path to write timestamps and intermediate steps to, no logging is performed if None TYPE: Optional [ str ] DEFAULT: None shape_optimization enable or disable shape optimization during iterative optimization TYPE: bool DEFAULT: True animation_path file path to write rendering and error visualizations to TYPE: Optional [ str ] DEFAULT: None point_constraint tuple of source point and rotated target point and weight a loss will be added that penalizes weight * || rotation @ source - target ||_2 TYPE: Optional [ Tuple [ torch . Tensor ]] DEFAULT: None prior_orientation_distribution Prior distribution of orientations used for initialization. If None, distribution of initialization network will not be modified. Only supported for initialization network with discretized orientation representation. Output distribution of initialization network will be adjusted by multiplying with prior_orientation_distribution / training_orientation_distribution and renormalizing. Tensor of shape (N,C,) or (C,) for single image. C being the number of grid cells in the SO3Grid used by the initialization network. TYPE: Optional [ torch . Tensor ] DEFAULT: None training_orientation_distribution Distribution of orientations used for training initialization network. If None, equal probability for each cell will be assumed. Note this is only approximately the same as a uniform distribution. Only used if prior_orientation_distribution is provided. Tensor of shape (C,). C being the number of grid cells in the SO3Grid used by the initialization network. N not supported, since same training network (and hence distribution) is used independent of view. TYPE: Optional [ torch . Tensor ] DEFAULT: None RETURNS DESCRIPTION tuple 3D pose of SDF center in world frame, shape (1,3,) tuple Orientation as normalized quaternion, scalar-last convention, shape (1,4,) tuple Size of SDF as length of half-width, shape (1,) tuple Latent shape representation of the object, shape (1,latent_size,). Source code in sdfest/estimation/simple_setup.py 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 def __call__ ( self , depth_images : torch . Tensor , masks : torch . Tensor , color_images : torch . Tensor , visualize : bool = False , camera_positions : Optional [ torch . Tensor ] = None , camera_orientations : Optional [ torch . Tensor ] = None , log_path : Optional [ str ] = None , shape_optimization : bool = True , animation_path : Optional [ str ] = None , point_constraint : Optional [ Tuple [ torch . Tensor ]] = None , prior_orientation_distribution : Optional [ torch . Tensor ] = None , training_orientation_distribution : Optional [ torch . Tensor ] = None , ) -> tuple : \"\"\"Infer pose, size and latent representation from depth and mask. If multiple images are passed the cameras are assumed to be fixed. All tensors should be on the same device as the pipeline. Batch dimension N must be provided either for all or none of the arguments. Args: depth_images: The depth map containing the distance along the camera's z-axis. Does not have to be masked, necessary preprocessing is done by pipeline. Will be masked and preprocessed in-place (pass copy if full depth is used afterwards). Shape (N, H, W) or (H, W) for a single depth image. masks: binary mask of the object to estimate, same shape as depth_images color_images: the color image (currently only used in visualization), shape (N, H, W, 3) or (H, W, 3), RGB, float, 0-1. visualize: Whether to visualize the intermediate steps and final result. camera_positions: position of camera in world coordinates for each image, if None, (0,0,0) will be assumed for all images, shape (N, 3) or (3,) camera_orientations: orientation of camera in world-frame as normalized quaternion, quaternion is in scalar-last convention, note, that this is the quaternion that transforms a point from camera to world-frame if None, (0,0,0,1) will be assumed for all images, shape (N, 4) or (4,) log_path: file path to write timestamps and intermediate steps to, no logging is performed if None shape_optimization: enable or disable shape optimization during iterative optimization animation_path: file path to write rendering and error visualizations to point_constraint: tuple of source point and rotated target point and weight a loss will be added that penalizes weight * || rotation @ source - target ||_2 prior_orientation_distribution: Prior distribution of orientations used for initialization. If None, distribution of initialization network will not be modified. Only supported for initialization network with discretized orientation representation. Output distribution of initialization network will be adjusted by multiplying with prior_orientation_distribution / training_orientation_distribution and renormalizing. Tensor of shape (N,C,) or (C,) for single image. C being the number of grid cells in the SO3Grid used by the initialization network. training_orientation_distribution: Distribution of orientations used for training initialization network. If None, equal probability for each cell will be assumed. Note this is only approximately the same as a uniform distribution. Only used if prior_orientation_distribution is provided. Tensor of shape (C,). C being the number of grid cells in the SO3Grid used by the initialization network. N not supported, since same training network (and hence distribution) is used independent of view. Returns: - 3D pose of SDF center in world frame, shape (1,3,) - Orientation as normalized quaternion, scalar-last convention, shape (1,4,) - Size of SDF as length of half-width, shape (1,) - Latent shape representation of the object, shape (1,latent_size,). \"\"\" # initialize optimization self . _best_inlier_ratio = None self . _point_constraint = point_constraint if animation_path is not None : self . _create_animation_folders ( animation_path ) start_time = time . time () # for logging # Add batch dimension if necessary if depth_images . dim () == 2 : depth_images = depth_images . unsqueeze ( 0 ) masks = masks . unsqueeze ( 0 ) color_images = color_images . unsqueeze ( 0 ) if camera_positions is not None : camera_positions = camera_positions . unsqueeze ( 0 ) if camera_orientations is not None : camera_orientations = camera_orientations . unsqueeze ( 0 ) if prior_orientation_distribution is not None : prior_orientation_distribution = ( prior_orientation_distribution . unsqueeze ( 0 ) ) # TODO assert all tensors have expected dimension if animation_path is not None : self . _save_inputs ( animation_path , depth_images , color_images , masks ) n_imgs = depth_images . shape [ 0 ] if camera_positions is None : camera_positions = torch . zeros ( n_imgs , 3 , device = self . device ) if camera_orientations is None : camera_orientations = torch . zeros ( n_imgs , 4 , device = self . device ) camera_orientations [:, 3 ] = 1.0 with torch . no_grad (): self . _preprocess_depth ( depth_images , masks ) # store pointcloud without reconstruction if log_path is not None : torch . cuda . synchronize () self . _log_data ( { \"timestamp\" : time . time () - start_time , \"depth_images\" : depth_images , \"color_images\" : color_images , \"masks\" : masks , \"color_images\" : color_images , \"camera_positions\" : camera_positions , \"camera_orientations\" : camera_orientations , }, ) # Initialization with torch . no_grad (): latent_shape , position , scale , orientation = self . _nn_init ( depth_images , camera_positions , camera_orientations , prior_orientation_distribution , training_orientation_distribution , ) if log_path is not None : torch . cuda . synchronize () self . _log_data ( { \"timestamp\" : time . time () - start_time , \"camera_positions\" : camera_positions , \"camera_orientations\" : camera_orientations , \"latent_shape\" : latent_shape , \"position\" : position , \"scale_inv\" : 1 / scale , \"orientation\" : orientation , }, ) if animation_path is not None : self . _save_preprocessed_inputs ( animation_path , depth_images ) # Iterative optimization self . _current_iteration = 1 position . requires_grad_ () scale . requires_grad_ () orientation . requires_grad_ () latent_shape . requires_grad_ () if visualize : fig_vis , axes = plt . subplots ( 2 , 3 , sharex = True , sharey = True , figsize = ( 12 , 8 ) ) fig_loss , ( loss_ax , inlier_ax ) = plt . subplots ( 1 , 2 ) vmin , vmax = None , None depth_losses = [] pointcloud_losses = [] nn_losses = [] point_constraint_losses = [] inlier_ratios = [] total_losses = [] opt_vars = [ { \"params\" : position , \"lr\" : 1e-3 }, { \"params\" : orientation , \"lr\" : 1e-2 }, { \"params\" : scale , \"lr\" : 1e-3 }, { \"params\" : latent_shape , \"lr\" : 1e-2 }, ] optimizer = torch . optim . Adam ( opt_vars ) while self . _current_iteration <= self . config [ \"max_iterations\" ]: optimizer . zero_grad () norm_orientation = orientation / torch . sqrt ( torch . sum ( orientation ** 2 )) with torch . set_grad_enabled ( shape_optimization ): sdf = self . vae . decode ( latent_shape ) loss_depth = torch . tensor ( 0.0 , device = self . device , requires_grad = True ) loss_pc = torch . tensor ( 0.0 , device = self . device , requires_grad = True ) loss_nn = torch . tensor ( 0.0 , device = self . device , requires_grad = True ) for depth_image , camera_position , camera_orientation in zip ( depth_images , camera_positions , camera_orientations ): # transform object to camera frame q_w2c = quaternion_utils . quaternion_invert ( camera_orientation ) position_c = quaternion_utils . quaternion_apply ( q_w2c , position - camera_position ) orientation_c = quaternion_utils . quaternion_multiply ( q_w2c , norm_orientation ) depth_estimate = self . render ( sdf [ 0 , 0 ], position_c [ 0 ], orientation_c [ 0 ], 1 / scale [ 0 ] ) view_loss_depth , view_loss_pc , view_loss_nn = self . _compute_view_losses ( depth_image , depth_estimate , position_c [ 0 ], orientation_c [ 0 ], scale [ 0 ], sdf [ 0 , 0 ], ) loss_depth = loss_depth + view_loss_depth loss_pc = loss_pc + view_loss_pc loss_nn = loss_nn + view_loss_nn loss_point_constraint = self . _compute_point_constraint_loss ( orientation ) loss = ( self . config [ \"depth_weight\" ] * loss_depth + self . config [ \"pc_weight\" ] * loss_pc + self . config [ \"nn_weight\" ] * loss_nn + loss_point_constraint ) self . _compute_gradients ( loss ) optimizer . step () optimizer . zero_grad () with torch . no_grad (): orientation /= torch . sqrt ( torch . sum ( orientation ** 2 )) inlier_ratio = self . _update_best_estimate ( depth_image , depth_estimate , position , orientation , scale , latent_shape , ) if visualize : depth_losses . append ( loss_depth . item ()) pointcloud_losses . append ( loss_pc . item ()) nn_losses . append ( loss_nn . item ()) point_constraint_losses . append ( loss_point_constraint . item ()) inlier_ratios . append ( inlier_ratio . item ()) total_losses . append ( loss . item ()) if log_path is not None : torch . cuda . synchronize () self . _log_data ( { \"timestamp\" : time . time () - start_time , \"latent_shape\" : latent_shape , \"position\" : position , \"scale_inv\" : 1 / scale , \"orientation\" : orientation , }, ) with torch . no_grad (): if animation_path is not None : self . _save_current_state ( depth_images , animation_path , camera_positions , camera_orientations , position , orientation , 1 / scale , sdf , ) if visualize and ( self . _current_iteration % 10 == 1 or self . _current_iteration == self . config [ \"max_iterations\" ] ): q_w2c = quaternion_utils . quaternion_invert ( camera_orientations [ 0 ]) position_c = quaternion_utils . quaternion_apply ( q_w2c , position - camera_positions [ 0 ] ) orientation_c = quaternion_utils . quaternion_multiply ( q_w2c , orientation ) current_depth = self . render ( sdf [ 0 , 0 ], position_c , orientation_c , 1 / scale ) depth_image = depth_images [ 0 ] color_image = color_images [ 0 ] if self . _current_iteration == 1 : vmin = depth_image [ depth_image != 0 ] . min () * 0.9 vmax = depth_image [ depth_image != 0 ] . max () # show input image axes [ 0 , 0 ] . clear () axes [ 0 , 0 ] . imshow ( depth_image . cpu (), vmin = vmin , vmax = vmax ) axes [ 0 , 1 ] . imshow ( color_image . cpu ()) # show initial estimate axes [ 1 , 0 ] . clear () axes [ 1 , 0 ] . imshow ( current_depth . detach () . cpu (), vmin = vmin , vmax = vmax ) axes [ 1 , 0 ] . set_title ( f \"loss { loss . item () } \" ) # update iterative estimate # axes[0, 2].clear() # axes[0, 2].imshow(rendered_error.detach().cpu()) # axes[0, 2].set_title(\"depth_loss\") # axes[1, 2].clear() # axes[1, 2].imshow(error_pc.detach().cpu()) # axes[1, 2].set_title(\"pointcloud_loss\") loss_ax . clear () loss_ax . plot ( depth_losses , label = \"Depth\" ) loss_ax . plot ( pointcloud_losses , label = \"Pointcloud\" ) loss_ax . plot ( nn_losses , label = \"Nearest Neighbor\" ) if self . _point_constraint is not None : loss_ax . plot ( point_constraint_losses , label = \"Point constraint\" ) loss_ax . plot ( total_losses , label = \"Total\" ) loss_ax . set_yscale ( \"log\" ) loss_ax . legend () inlier_ax . clear () inlier_ax . plot ( inlier_ratios , label = \"Inlier Ratio\" ) inlier_ax . legend () axes [ 1 , 1 ] . clear () axes [ 1 , 1 ] . imshow ( current_depth . detach () . cpu (), vmin = vmin , vmax = vmax ) axes [ 1 , 1 ] . set_title ( f \"loss { loss . item () } \" ) fig_loss . canvas . draw () fig_vis . canvas . draw () plt . pause ( 0.1 ) self . _current_iteration += 1 if visualize : plt . show () plt . close ( fig_loss ) plt . close ( fig_vis ) if log_path is not None : self . _write_log_data ( log_path ) if animation_path is not None : self . _create_animations ( animation_path ) if self . result_selection_strategy == \"last_iteration\" : return position , orientation , scale , latent_shape elif self . result_selection_strategy == \"best_inlier_ratio\" : return ( self . _best_position , self . _best_orientation , self . _best_scale , self . _best_latent_shape , ) else : raise ValueError ( f \"Result selection strategy { self . result_selection_strategy } is not\" \"supported.\" ) generate_depth generate_depth ( position : torch . Tensor , orientation : torch . Tensor , scale : torch . Tensor , latent : torch . Tensor , ) -> torch . Tensor Generate depth image representing positioned object. Source code in sdfest/estimation/simple_setup.py 605 606 607 608 609 610 611 612 613 614 615 def generate_depth ( self , position : torch . Tensor , orientation : torch . Tensor , scale : torch . Tensor , latent : torch . Tensor , ) -> torch . Tensor : \"\"\"Generate depth image representing positioned object.\"\"\" sdf = self . vae . decode ( latent ) depth = self . render ( sdf [ 0 , 0 ], position , orientation , 1 / scale ) return depth generate_mesh generate_mesh ( latent : torch . tensor , scale : torch . tensor , complete_mesh : bool = False ) -> synthetic . Mesh Generate mesh without pose. Currently only supports batch size 1. PARAMETER DESCRIPTION latent Latent shape descriptor, shape (1,L). TYPE: torch . tensor scale Relative scale of the signed distance field, (i.e., half-width), shape (1,). TYPE: torch . tensor complete_mesh If True, the SDF will be padded with positive values prior to converting it to a mesh. This ensures a watertight mesh is created. TYPE: bool DEFAULT: False RETURNS DESCRIPTION synthetic . Mesh Generate mesh by decoding latent shape descriptor and scaling it. Source code in sdfest/estimation/simple_setup.py 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 def generate_mesh ( self , latent : torch . tensor , scale : torch . tensor , complete_mesh : bool = False ) -> synthetic . Mesh : \"\"\"Generate mesh without pose. Currently only supports batch size 1. Args: latent: Latent shape descriptor, shape (1,L). scale: Relative scale of the signed distance field, (i.e., half-width), shape (1,). complete_mesh: If True, the SDF will be padded with positive values prior to converting it to a mesh. This ensures a watertight mesh is created. Returns: Generate mesh by decoding latent shape descriptor and scaling it. \"\"\" with torch . no_grad (): sdf = self . vae . decode ( latent ) if complete_mesh : inc = 2 sdf = torch . nn . functional . pad ( sdf , ( 1 , 1 , 1 , 1 , 1 , 1 ), value = 1.0 ) else : inc = 0 try : sdf = sdf . cpu () . numpy () s = 2.0 / ( self . resolution - 1 ) vertices , faces , _ , _ = marching_cubes ( sdf [ 0 , 0 ], spacing = ( s , s , s , ), level = self . config [ \"iso_threshold\" ], ) c = s * ( self . resolution + inc - 1 ) / 2.0 # move origin to center vertices -= np . array ([[ c , c , c ]]) mesh = o3d . geometry . TriangleMesh ( vertices = o3d . utility . Vector3dVector ( vertices ), triangles = o3d . utility . Vector3iVector ( faces ), ) except KeyError : return None return synthetic . Mesh ( mesh = mesh , scale = scale . item (), rel_scale = True )","title":"simple_setup"},{"location":"reference/estimation/simple_setup/#sdfest.estimation.simple_setup","text":"Modular SDF pose and shape estimation in depth images.","title":"simple_setup"},{"location":"reference/estimation/simple_setup/#sdfest.estimation.simple_setup.NoDepthError","text":"Bases: ValueError Raised when there is no depth data left after preprocessing. Source code in sdfest/estimation/simple_setup.py 30 31 32 class NoDepthError ( ValueError ): \"\"\"Raised when there is no depth data left after preprocessing.\"\"\" pass","title":"NoDepthError"},{"location":"reference/estimation/simple_setup/#sdfest.estimation.simple_setup.SDFPipeline","text":"SDF pose and shape estimation pipeline. Source code in sdfest/estimation/simple_setup.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 class SDFPipeline : \"\"\"SDF pose and shape estimation pipeline.\"\"\" def __init__ ( self , config : dict ) -> None : \"\"\"Load and initialize the pipeline. Args: config: Configuration dictionary. \"\"\" self . _parse_config ( config ) self . init_network = SDFPoseNet ( INIT_MODULE_DICT [ self . init_config [ \"backbone_type\" ]]( ** self . init_config [ \"backbone\" ] ), INIT_MODULE_DICT [ self . init_config [ \"head_type\" ]]( shape_dimension = self . vae_config [ \"latent_size\" ], ** self . init_config [ \"head\" ], ), ) . to ( self . device ) load_model_weights ( self . init_config [ \"model\" ], self . init_network , self . device , self . init_config . get ( \"model_url\" ), ) self . init_network . eval () self . resolution = 64 self . vae = SDFVAE ( sdf_size = 64 , latent_size = self . vae_config [ \"latent_size\" ], encoder_dict = self . vae_config [ \"encoder\" ], decoder_dict = self . vae_config [ \"decoder\" ], device = self . device , ) . to ( self . device ) load_model_weights ( self . vae_config [ \"model\" ], self . vae , self . device , self . vae_config . get ( \"model_url\" ), ) self . vae . eval () self . cam = Camera ( ** self . camera_config ) self . render = lambda sdf , pos , quat , i_s : render_depth_gpu ( sdf , pos , quat , i_s , None , None , None , config [ \"threshold\" ], self . cam ) self . config = config self . log_data = [] def _parse_config ( self , config : dict ) -> None : \"\"\"Parse config dict. This function makes sure that all required keys are available. \"\"\" self . device = config [ \"device\" ] self . init_config = config [ \"init\" ] self . vae_config = config [ \"vae\" ] if \"vae\" in config else self . init_config [ \"vae\" ] self . camera_config = config [ \"camera\" ] self . result_selection_strategy = config . get ( \"result_selection_strategy\" , \"last_iteration\" ) # last_iteration | best_inlier_ratio self . _relative_inlier_threshold = config . get ( \"relative_inlier_threshold\" , 0.03 ) # relative depth error threshold for pixel to be considered inlier if \"far_field\" in config : self . _far_field = config [ \"far_field\" ] if \"far_field\" in config else None self . config = config @staticmethod def _compute_gradients ( loss : torch . Tensor ) -> None : loss . backward () def _compute_view_losses ( self , depth_input : torch . Tensor , depth_estimate : torch . Tensor , position : torch . Tensor , orientation : torch . Tensor , scale : torch . Tensor , sdf : torch . Tensor , ) -> Tuple [ torch . Tensor ]: # depth l1 overlap_mask = ( depth_input > 0 ) & ( depth_estimate > 0 ) depth_error = torch . abs ( depth_estimate - depth_input ) # max_depth_error = 0.05 # depth_outlier_mask = depth_error > max_depth_error # depth_mask = overlap_mask & ~depth_outlier_mask # depth_error[~overlap_mask] = 0 loss_depth = torch . mean ( depth_error [ overlap_mask ]) # pointcloud l1 pointcloud_obs = pointset_utils . depth_to_pointcloud ( depth_input , self . cam , normalize = False ) pointcloud_error = losses . pc_loss ( pointcloud_obs , position , orientation , scale , sdf , ) loss_pc = torch . mean ( torch . abs ( pointcloud_error )) # nearest neighbor l1 # pointcloud_outliers = pointset_utils.depth_to_pointcloud( # depth_estimate, self.cam, normalize=False, mask=depth_outlier_mask # ) loss_nn = 0 # if pointcloud_outliers.shape[0] != 0: # pass # loss_nn += 0 # # TODO different gradients for point cloud (not derived by renderer) # outlier_nn_d = losses.nn_loss(pointcloud_outliers, pointcloud_obs) # # only use positive, because sqrt is not differentiable at 0 # outlier_nn_d = outlier_nn_d[outlier_nn_d > 0] # loss_nn = loss_nn + torch.mean(torch.sqrt(outlier_nn_d)) return loss_depth , loss_pc , loss_nn def _compute_point_constraint_loss ( self , orientation : torch . Tensor ) -> torch . Tensor : \"\"\"Compute loss for point constraint if specified.\"\"\" if self . _point_constraint is not None : loss_point_constraint = losses . point_constraint_loss ( orientation_q = orientation [ 0 ], source = self . _point_constraint [ 0 ], target = self . _point_constraint [ 1 ], ) weight = self . _point_constraint [ 2 ] return weight * loss_point_constraint else : return orientation . new_tensor ( 0.0 ) def _compute_inlier_ratio ( self , depth_input : torch . Tensor , depth_estimate : torch . Tensor , ) -> None : \"\"\"Compute ratio of pixels with small relative depth error.\"\"\" rel_depth_error = torch . abs ( depth_input - depth_estimate ) / depth_input inlier_mask = rel_depth_error < self . _relative_inlier_threshold inliers = torch . count_nonzero ( inlier_mask ) valid_depth_pixels = torch . count_nonzero ( depth_input ) inlier_ratio = inliers / valid_depth_pixels return inlier_ratio def _update_best_estimate ( self , depth_input : torch . Tensor , depth_estimate : torch . Tensor , position : torch . Tensor , orientation : torch . Tensor , scale : torch . Tensor , latent_shape : torch . Tensor , ) -> None : \"\"\"Update the best current estimate by keeping track of inlier ratio. Returns: Inlier ratio of this configuration. \"\"\" inlier_ratio = self . _compute_inlier_ratio ( depth_input , depth_estimate ) if self . _best_inlier_ratio is None or inlier_ratio > self . _best_inlier_ratio : self . _best_inlier_ratio = inlier_ratio self . _best_position = position self . _best_orientation = orientation self . _best_scale = scale self . _best_latent_shape = latent_shape return inlier_ratio def __call__ ( self , depth_images : torch . Tensor , masks : torch . Tensor , color_images : torch . Tensor , visualize : bool = False , camera_positions : Optional [ torch . Tensor ] = None , camera_orientations : Optional [ torch . Tensor ] = None , log_path : Optional [ str ] = None , shape_optimization : bool = True , animation_path : Optional [ str ] = None , point_constraint : Optional [ Tuple [ torch . Tensor ]] = None , prior_orientation_distribution : Optional [ torch . Tensor ] = None , training_orientation_distribution : Optional [ torch . Tensor ] = None , ) -> tuple : \"\"\"Infer pose, size and latent representation from depth and mask. If multiple images are passed the cameras are assumed to be fixed. All tensors should be on the same device as the pipeline. Batch dimension N must be provided either for all or none of the arguments. Args: depth_images: The depth map containing the distance along the camera's z-axis. Does not have to be masked, necessary preprocessing is done by pipeline. Will be masked and preprocessed in-place (pass copy if full depth is used afterwards). Shape (N, H, W) or (H, W) for a single depth image. masks: binary mask of the object to estimate, same shape as depth_images color_images: the color image (currently only used in visualization), shape (N, H, W, 3) or (H, W, 3), RGB, float, 0-1. visualize: Whether to visualize the intermediate steps and final result. camera_positions: position of camera in world coordinates for each image, if None, (0,0,0) will be assumed for all images, shape (N, 3) or (3,) camera_orientations: orientation of camera in world-frame as normalized quaternion, quaternion is in scalar-last convention, note, that this is the quaternion that transforms a point from camera to world-frame if None, (0,0,0,1) will be assumed for all images, shape (N, 4) or (4,) log_path: file path to write timestamps and intermediate steps to, no logging is performed if None shape_optimization: enable or disable shape optimization during iterative optimization animation_path: file path to write rendering and error visualizations to point_constraint: tuple of source point and rotated target point and weight a loss will be added that penalizes weight * || rotation @ source - target ||_2 prior_orientation_distribution: Prior distribution of orientations used for initialization. If None, distribution of initialization network will not be modified. Only supported for initialization network with discretized orientation representation. Output distribution of initialization network will be adjusted by multiplying with prior_orientation_distribution / training_orientation_distribution and renormalizing. Tensor of shape (N,C,) or (C,) for single image. C being the number of grid cells in the SO3Grid used by the initialization network. training_orientation_distribution: Distribution of orientations used for training initialization network. If None, equal probability for each cell will be assumed. Note this is only approximately the same as a uniform distribution. Only used if prior_orientation_distribution is provided. Tensor of shape (C,). C being the number of grid cells in the SO3Grid used by the initialization network. N not supported, since same training network (and hence distribution) is used independent of view. Returns: - 3D pose of SDF center in world frame, shape (1,3,) - Orientation as normalized quaternion, scalar-last convention, shape (1,4,) - Size of SDF as length of half-width, shape (1,) - Latent shape representation of the object, shape (1,latent_size,). \"\"\" # initialize optimization self . _best_inlier_ratio = None self . _point_constraint = point_constraint if animation_path is not None : self . _create_animation_folders ( animation_path ) start_time = time . time () # for logging # Add batch dimension if necessary if depth_images . dim () == 2 : depth_images = depth_images . unsqueeze ( 0 ) masks = masks . unsqueeze ( 0 ) color_images = color_images . unsqueeze ( 0 ) if camera_positions is not None : camera_positions = camera_positions . unsqueeze ( 0 ) if camera_orientations is not None : camera_orientations = camera_orientations . unsqueeze ( 0 ) if prior_orientation_distribution is not None : prior_orientation_distribution = ( prior_orientation_distribution . unsqueeze ( 0 ) ) # TODO assert all tensors have expected dimension if animation_path is not None : self . _save_inputs ( animation_path , depth_images , color_images , masks ) n_imgs = depth_images . shape [ 0 ] if camera_positions is None : camera_positions = torch . zeros ( n_imgs , 3 , device = self . device ) if camera_orientations is None : camera_orientations = torch . zeros ( n_imgs , 4 , device = self . device ) camera_orientations [:, 3 ] = 1.0 with torch . no_grad (): self . _preprocess_depth ( depth_images , masks ) # store pointcloud without reconstruction if log_path is not None : torch . cuda . synchronize () self . _log_data ( { \"timestamp\" : time . time () - start_time , \"depth_images\" : depth_images , \"color_images\" : color_images , \"masks\" : masks , \"color_images\" : color_images , \"camera_positions\" : camera_positions , \"camera_orientations\" : camera_orientations , }, ) # Initialization with torch . no_grad (): latent_shape , position , scale , orientation = self . _nn_init ( depth_images , camera_positions , camera_orientations , prior_orientation_distribution , training_orientation_distribution , ) if log_path is not None : torch . cuda . synchronize () self . _log_data ( { \"timestamp\" : time . time () - start_time , \"camera_positions\" : camera_positions , \"camera_orientations\" : camera_orientations , \"latent_shape\" : latent_shape , \"position\" : position , \"scale_inv\" : 1 / scale , \"orientation\" : orientation , }, ) if animation_path is not None : self . _save_preprocessed_inputs ( animation_path , depth_images ) # Iterative optimization self . _current_iteration = 1 position . requires_grad_ () scale . requires_grad_ () orientation . requires_grad_ () latent_shape . requires_grad_ () if visualize : fig_vis , axes = plt . subplots ( 2 , 3 , sharex = True , sharey = True , figsize = ( 12 , 8 ) ) fig_loss , ( loss_ax , inlier_ax ) = plt . subplots ( 1 , 2 ) vmin , vmax = None , None depth_losses = [] pointcloud_losses = [] nn_losses = [] point_constraint_losses = [] inlier_ratios = [] total_losses = [] opt_vars = [ { \"params\" : position , \"lr\" : 1e-3 }, { \"params\" : orientation , \"lr\" : 1e-2 }, { \"params\" : scale , \"lr\" : 1e-3 }, { \"params\" : latent_shape , \"lr\" : 1e-2 }, ] optimizer = torch . optim . Adam ( opt_vars ) while self . _current_iteration <= self . config [ \"max_iterations\" ]: optimizer . zero_grad () norm_orientation = orientation / torch . sqrt ( torch . sum ( orientation ** 2 )) with torch . set_grad_enabled ( shape_optimization ): sdf = self . vae . decode ( latent_shape ) loss_depth = torch . tensor ( 0.0 , device = self . device , requires_grad = True ) loss_pc = torch . tensor ( 0.0 , device = self . device , requires_grad = True ) loss_nn = torch . tensor ( 0.0 , device = self . device , requires_grad = True ) for depth_image , camera_position , camera_orientation in zip ( depth_images , camera_positions , camera_orientations ): # transform object to camera frame q_w2c = quaternion_utils . quaternion_invert ( camera_orientation ) position_c = quaternion_utils . quaternion_apply ( q_w2c , position - camera_position ) orientation_c = quaternion_utils . quaternion_multiply ( q_w2c , norm_orientation ) depth_estimate = self . render ( sdf [ 0 , 0 ], position_c [ 0 ], orientation_c [ 0 ], 1 / scale [ 0 ] ) view_loss_depth , view_loss_pc , view_loss_nn = self . _compute_view_losses ( depth_image , depth_estimate , position_c [ 0 ], orientation_c [ 0 ], scale [ 0 ], sdf [ 0 , 0 ], ) loss_depth = loss_depth + view_loss_depth loss_pc = loss_pc + view_loss_pc loss_nn = loss_nn + view_loss_nn loss_point_constraint = self . _compute_point_constraint_loss ( orientation ) loss = ( self . config [ \"depth_weight\" ] * loss_depth + self . config [ \"pc_weight\" ] * loss_pc + self . config [ \"nn_weight\" ] * loss_nn + loss_point_constraint ) self . _compute_gradients ( loss ) optimizer . step () optimizer . zero_grad () with torch . no_grad (): orientation /= torch . sqrt ( torch . sum ( orientation ** 2 )) inlier_ratio = self . _update_best_estimate ( depth_image , depth_estimate , position , orientation , scale , latent_shape , ) if visualize : depth_losses . append ( loss_depth . item ()) pointcloud_losses . append ( loss_pc . item ()) nn_losses . append ( loss_nn . item ()) point_constraint_losses . append ( loss_point_constraint . item ()) inlier_ratios . append ( inlier_ratio . item ()) total_losses . append ( loss . item ()) if log_path is not None : torch . cuda . synchronize () self . _log_data ( { \"timestamp\" : time . time () - start_time , \"latent_shape\" : latent_shape , \"position\" : position , \"scale_inv\" : 1 / scale , \"orientation\" : orientation , }, ) with torch . no_grad (): if animation_path is not None : self . _save_current_state ( depth_images , animation_path , camera_positions , camera_orientations , position , orientation , 1 / scale , sdf , ) if visualize and ( self . _current_iteration % 10 == 1 or self . _current_iteration == self . config [ \"max_iterations\" ] ): q_w2c = quaternion_utils . quaternion_invert ( camera_orientations [ 0 ]) position_c = quaternion_utils . quaternion_apply ( q_w2c , position - camera_positions [ 0 ] ) orientation_c = quaternion_utils . quaternion_multiply ( q_w2c , orientation ) current_depth = self . render ( sdf [ 0 , 0 ], position_c , orientation_c , 1 / scale ) depth_image = depth_images [ 0 ] color_image = color_images [ 0 ] if self . _current_iteration == 1 : vmin = depth_image [ depth_image != 0 ] . min () * 0.9 vmax = depth_image [ depth_image != 0 ] . max () # show input image axes [ 0 , 0 ] . clear () axes [ 0 , 0 ] . imshow ( depth_image . cpu (), vmin = vmin , vmax = vmax ) axes [ 0 , 1 ] . imshow ( color_image . cpu ()) # show initial estimate axes [ 1 , 0 ] . clear () axes [ 1 , 0 ] . imshow ( current_depth . detach () . cpu (), vmin = vmin , vmax = vmax ) axes [ 1 , 0 ] . set_title ( f \"loss { loss . item () } \" ) # update iterative estimate # axes[0, 2].clear() # axes[0, 2].imshow(rendered_error.detach().cpu()) # axes[0, 2].set_title(\"depth_loss\") # axes[1, 2].clear() # axes[1, 2].imshow(error_pc.detach().cpu()) # axes[1, 2].set_title(\"pointcloud_loss\") loss_ax . clear () loss_ax . plot ( depth_losses , label = \"Depth\" ) loss_ax . plot ( pointcloud_losses , label = \"Pointcloud\" ) loss_ax . plot ( nn_losses , label = \"Nearest Neighbor\" ) if self . _point_constraint is not None : loss_ax . plot ( point_constraint_losses , label = \"Point constraint\" ) loss_ax . plot ( total_losses , label = \"Total\" ) loss_ax . set_yscale ( \"log\" ) loss_ax . legend () inlier_ax . clear () inlier_ax . plot ( inlier_ratios , label = \"Inlier Ratio\" ) inlier_ax . legend () axes [ 1 , 1 ] . clear () axes [ 1 , 1 ] . imshow ( current_depth . detach () . cpu (), vmin = vmin , vmax = vmax ) axes [ 1 , 1 ] . set_title ( f \"loss { loss . item () } \" ) fig_loss . canvas . draw () fig_vis . canvas . draw () plt . pause ( 0.1 ) self . _current_iteration += 1 if visualize : plt . show () plt . close ( fig_loss ) plt . close ( fig_vis ) if log_path is not None : self . _write_log_data ( log_path ) if animation_path is not None : self . _create_animations ( animation_path ) if self . result_selection_strategy == \"last_iteration\" : return position , orientation , scale , latent_shape elif self . result_selection_strategy == \"best_inlier_ratio\" : return ( self . _best_position , self . _best_orientation , self . _best_scale , self . _best_latent_shape , ) else : raise ValueError ( f \"Result selection strategy { self . result_selection_strategy } is not\" \"supported.\" ) def _log_data ( self , data : dict ) -> None : \"\"\"Add dictionary with associated timestamp to log data list.\"\"\" new_log_data = copy . deepcopy ( data ) self . log_data . append ( new_log_data ) def _write_log_data ( self , file_path : str ) -> None : \"\"\"Write current list of log data to file.\"\"\" with open ( file_path , \"wb\" ) as f : pickle . dump ({ \"config\" : self . config , \"log\" : self . log_data }, f ) self . log_data = [] # reset log def generate_depth ( self , position : torch . Tensor , orientation : torch . Tensor , scale : torch . Tensor , latent : torch . Tensor , ) -> torch . Tensor : \"\"\"Generate depth image representing positioned object.\"\"\" sdf = self . vae . decode ( latent ) depth = self . render ( sdf [ 0 , 0 ], position , orientation , 1 / scale ) return depth def generate_mesh ( self , latent : torch . tensor , scale : torch . tensor , complete_mesh : bool = False ) -> synthetic . Mesh : \"\"\"Generate mesh without pose. Currently only supports batch size 1. Args: latent: Latent shape descriptor, shape (1,L). scale: Relative scale of the signed distance field, (i.e., half-width), shape (1,). complete_mesh: If True, the SDF will be padded with positive values prior to converting it to a mesh. This ensures a watertight mesh is created. Returns: Generate mesh by decoding latent shape descriptor and scaling it. \"\"\" with torch . no_grad (): sdf = self . vae . decode ( latent ) if complete_mesh : inc = 2 sdf = torch . nn . functional . pad ( sdf , ( 1 , 1 , 1 , 1 , 1 , 1 ), value = 1.0 ) else : inc = 0 try : sdf = sdf . cpu () . numpy () s = 2.0 / ( self . resolution - 1 ) vertices , faces , _ , _ = marching_cubes ( sdf [ 0 , 0 ], spacing = ( s , s , s , ), level = self . config [ \"iso_threshold\" ], ) c = s * ( self . resolution + inc - 1 ) / 2.0 # move origin to center vertices -= np . array ([[ c , c , c ]]) mesh = o3d . geometry . TriangleMesh ( vertices = o3d . utility . Vector3dVector ( vertices ), triangles = o3d . utility . Vector3iVector ( faces ), ) except KeyError : return None return synthetic . Mesh ( mesh = mesh , scale = scale . item (), rel_scale = True ) def _preprocess_depth ( self , depth_images : torch . Tensor , masks : torch . Tensor ) -> None : \"\"\"Preprocesses depth image based on segmentation mask. Args: depth_images: the depth images to preprocess, will be modified in place, shape (N, H, W) masks: the masks used for preprocessing, same shape as depth_images \"\"\" # shrink mask # masks = ( # -torch.nn.functional.max_pool2d( # -masks.double(), kernel_size=9, stride=1, padding=4 # ) # ).bool() depth_images [ ~ masks ] = 0 # set outside of depth to 0 # remove data far away (should be based on what distances ocurred in training) if self . _far_field is not None : depth_images [ depth_images > self . _far_field ] = 0 # only consider available depth values for outlier detection # masks = torch.logical_and(masks, depth_images != 0) # depth_images = # remove outliers based on median # plt.imshow(depth_images[0].cpu().numpy()) # plt.show() # for mask, depth_image in zip(masks, depth_images): # median = torch.median(depth_image[mask]) # errors = torch.abs(depth_image[mask] - median) # bins = 100 # hist = torch.histc(errors, bins=bins) # print(hist) # zero_indices = torch.nonzero(hist == 0) # if len(zero_indices): # threshold = zero_indices[0] / bins * errors.max() # print(threshold) # depth_image[torch.abs(depth_image - median) > threshold] = 0 # plt.imshow(depth_images[0].cpu().numpy()) # plt.show() def _nn_init ( self , depth_images : torch . Tensor , camera_positions : torch . Tensor , camera_orientations : torch . Tensor , prior_orientation_distribution : Optional [ torch . Tensor ] = None , training_orientation_distribution : Optional [ torch . Tensor ] = None , ) -> Tuple : \"\"\"Estimate shape, pose, scale and orientation using initialization network. Args: depth_images: the preprocessed depth images, shape (N, H, W) camera_positions: position of camera in world coordinates for each image, shape (N, 3) camera_orientations: orientation of camera in world-frame as normalized quaternion, quaternion is in scalar-last convention, shape (N, 4) prior_orientation_distribution: Prior distribution of orientations used for initialization. If None, distribution of initialization network will not be modified. Only supported for initialization network with discretized orientation representation. Output distribution of initialization network will be adjusted by multiplying with prior_orientation_distribution / training_orientation_distribution and renormalizing. Tensor of shape (N,C,). C being the number of grid cells in the SO3Grid used by the initialization network. training_orientation_distribution: Distribution of orientations used for training initialization network. If None, equal probability for each cell will be assumed. Note this is only approximately the same as a uniform distribution. Only used if prior_orientation_distribution is provided. Tensor of shape (C,). C being the number of grid cells in the SO3Grid used by the initialization network. Returns: Tuple comprised of: - Latent shape representation of the object, shape (1, latent_size) - 3D pose of SDF center in camera frame, shape (1, 3) - Size of SDF as length of half-width, (1,) - Orientation of SDF as normalized quaternion (1,4) \"\"\" if ( prior_orientation_distribution is not None and self . init_config [ \"head\" ][ \"orientation_repr\" ] != \"discretized\" ): raise ValueError ( \"prior_orientation_distribution only supported for discretized \" \"orientation representation.\" ) best = 0 best_result = None for i , ( depth_image , camera_orientation , camera_position ) in enumerate ( zip ( depth_images , camera_orientations , camera_positions ) ): centroid = None if self . init_config [ \"backbone_type\" ] == \"VanillaPointNet\" : inp = pointset_utils . depth_to_pointcloud ( depth_image , self . cam , normalize = False ) if len ( inp ) == 0 : raise NoDepthError if self . init_config [ \"normalize_pose\" ]: inp , centroid = pointset_utils . normalize_points ( inp ) else : inp = depth_image inp = inp . unsqueeze ( 0 ) latent_shape , position , scale , orientation_repr = self . init_network ( inp ) if self . config [ \"mean_shape\" ]: latent_shape = latent_shape . new_zeros ( latent_shape . shape ) if centroid is not None : position += centroid if self . init_config [ \"head\" ][ \"orientation_repr\" ] == \"discretized\" : posterior_orientation_dist = torch . softmax ( orientation_repr , - 1 ) if prior_orientation_distribution is not None : posterior_orientation_dist = self . _adjust_categorical_posterior ( posterior = posterior_orientation_dist , prior = prior_orientation_distribution [ i ], train_prior = training_orientation_distribution , ) orientation_camera = torch . tensor ( self . init_network . _head . _grid . index_to_quat ( posterior_orientation_dist . argmax () . item () ), dtype = torch . float , device = self . device , ) . unsqueeze ( 0 ) elif self . init_config [ \"head\" ][ \"orientation_repr\" ] == \"quaternion\" : orientation_camera = orientation_repr else : raise NotImplementedError ( \"Orientation representation is not supported\" ) # output are in camera frame, transform to world frame position_world = ( quaternion_utils . quaternion_apply ( camera_orientation , position ) + camera_position ) orientation_world = quaternion_utils . quaternion_multiply ( camera_orientation , orientation_camera ) if self . config [ \"init_view\" ] == \"first\" : return latent_shape , position_world , scale , orientation_world elif self . config [ \"init_view\" ] == \"best\" : if self . init_config [ \"head\" ][ \"orientation_repr\" ] != \"discretized\" : raise NotImplementedError ( '\"best\" init strategy only supported with discretized ' \"orientation representation\" ) maximum = posterior_orientation_dist . max () if maximum > best : best = maximum best_result = latent_shape , position_world , scale , orientation_world else : raise NotImplementedError ( 'Only \"first\" and \"best\" strategies are currently supported' ) return best_result def _generate_uniform_quaternion ( self ) -> torch . tensor : \"\"\"Generate a uniform quaternion. Following the method from K. Shoemake, Uniform Random Rotations, 1992. See: http://planning.cs.uiuc.edu/node198.html Returns: Uniformly distributed unit quaternion on the estimator's device. \"\"\" u1 , u2 , u3 = random . random (), random . random (), random . random () return ( torch . tensor ( [ math . sqrt ( 1 - u1 ) * math . sin ( 2 * math . pi * u2 ), math . sqrt ( 1 - u1 ) * math . cos ( 2 * math . pi * u2 ), math . sqrt ( u1 ) * math . sin ( 2 * math . pi * u3 ), math . sqrt ( u1 ) * math . cos ( 2 * math . pi * u3 ), ] ) . unsqueeze ( 0 ) . to ( self . device ) ) def _create_animation_folders ( self , animation_path : str ) -> None : \"\"\"Create subfolders to store animation frames.\"\"\" os . makedirs ( animation_path ) depth_path = os . path . join ( animation_path , \"depth\" ) os . makedirs ( depth_path ) error_path = os . path . join ( animation_path , \"depth_error\" ) os . makedirs ( error_path ) sdf_path = os . path . join ( animation_path , \"sdf\" ) os . makedirs ( sdf_path ) def _save_inputs ( self , animation_path : str , color_images : torch . Tensor , depth_images : torch . Tensor , instance_masks : torch . Tensor , ) -> None : color_path = os . path . join ( animation_path , \"color_input.png\" ) fig , ax = plt . subplots () ax . imshow ( color_images [ 0 ] . cpu () . numpy ()) fig . savefig ( color_path ) plt . close ( fig ) depth_path = os . path . join ( animation_path , \"depth_input.png\" ) fig , ax = plt . subplots () ax . imshow ( depth_images [ 0 ] . cpu () . numpy ()) fig . savefig ( depth_path ) plt . close ( fig ) mask_path = os . path . join ( animation_path , \"mask.png\" ) fig , ax = plt . subplots () ax . imshow ( instance_masks [ 0 ] . cpu () . numpy ()) fig . savefig ( mask_path ) plt . close ( fig ) def _save_preprocessed_inputs ( self , animation_path : str , depth_images : torch . Tensor , ) -> None : depth_path = os . path . join ( animation_path , \"preprocessed_depth_input.png\" ) fig , ax = plt . subplots () ax . imshow ( depth_images [ 0 ] . cpu () . numpy ()) fig . savefig ( depth_path ) plt . close ( fig ) def _save_current_state ( self , depth_images : torch . Tensor , animation_path : str , camera_positions : torch . Tensor , camera_orientations : torch . Tensor , position : torch . Tensor , orientation : torch . Tensor , scale_inv : torch . Tensor , sdf : torch . Tensor , ) -> None : q_w2c = quaternion_utils . quaternion_invert ( camera_orientations [ 0 ]) position_c = quaternion_utils . quaternion_apply ( q_w2c , position - camera_positions [ 0 ] ) orientation_c = quaternion_utils . quaternion_multiply ( q_w2c , orientation ) current_depth = self . render ( sdf [ 0 , 0 ], position_c , orientation_c , scale_inv ) depth_path = os . path . join ( animation_path , \"depth\" , f \" { self . _current_iteration : 06 } .png\" ) fig , ax = plt . subplots () ax . imshow ( current_depth . cpu () . numpy (), interpolation = \"none\" ) fig . savefig ( depth_path ) plt . close ( fig ) error_image = torch . abs ( current_depth - depth_images [ 0 ]) error_image [ depth_images [ 0 ] == 0 ] = 0 error_image [ current_depth == 0 ] = 0 error_path = os . path . join ( animation_path , \"depth_error\" , f \" { self . _current_iteration : 06 } .png\" ) fig , ax = plt . subplots () ax . imshow ( error_image . cpu () . numpy (), interpolation = \"none\" ) fig . savefig ( error_path ) plt . close ( fig ) unscaled_threshold = self . config [ \"threshold\" ] * scale_inv . item () mesh = sdf_utils . mesh_from_sdf ( sdf [ 0 , 0 ] . cpu () . numpy (), unscaled_threshold , complete_mesh = True , ) sdf_path = os . path . join ( animation_path , \"sdf\" , f \" { self . _current_iteration : 06 } .png\" ) # map y -> z; z -> y transform = np . eye ( 4 ) transform [ 0 : 3 , 0 : 3 ] = np . array ([[ - 1 , 0 , 0 ], [ 0 , 0 , 1 ], [ 0 , 1 , 0 ]]) fig , ax = plt . subplots () sdf_utils . plot_mesh ( mesh , transform = transform , plot_object = ax ) fig . savefig ( sdf_path ) plt . close ( fig ) def _create_animations ( self , animation_path : str ) -> None : names = [ \"sdf\" , \"depth\" , \"depth_error\" ] for name in names : frame_folder = os . path . join ( animation_path , name ) video_name = os . path . join ( animation_path , f \" { name } .mp4\" ) ffmpeg . input ( os . path . join ( frame_folder , \"*.png\" ), pattern_type = \"glob\" , framerate = 30 ) . output ( video_name ) . run () @staticmethod def _adjust_categorical_posterior ( posterior : torch . Tensor , prior : torch . Tensor , train_prior : torch . Tensor ) -> torch . Tensor : \"\"\"Adjust categorical posterior distribution. Posterior is calculated with a train_prior Args: posterior: Posterior distribution computed assuming train_prior. Shape (..., K). K being number of categories. prior: The desired new prior distribution. Same shape as posterior. train_prior: The prior distribution used to compute the posterior. If None, equal probability for each category will be assumed. Same shape as posterior. Returns: The categorical posterior, adjusted such that prior is prior, instead of train_prior. Same shape as posterior. \"\"\" adjusted_posterior = posterior . clone () # adjust if prior different from training adjusted_posterior *= prior if train_prior is not None : adjusted_posterior /= train_prior adjusted_posterior = torch . nn . functional . normalize ( adjusted_posterior , p = 1 , dim =- 1 ) return adjusted_posterior","title":"SDFPipeline"},{"location":"reference/estimation/simple_setup/#sdfest.estimation.simple_setup.SDFPipeline.__init__","text":"__init__ ( config : dict ) -> None Load and initialize the pipeline. PARAMETER DESCRIPTION config Configuration dictionary. TYPE: dict Source code in sdfest/estimation/simple_setup.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def __init__ ( self , config : dict ) -> None : \"\"\"Load and initialize the pipeline. Args: config: Configuration dictionary. \"\"\" self . _parse_config ( config ) self . init_network = SDFPoseNet ( INIT_MODULE_DICT [ self . init_config [ \"backbone_type\" ]]( ** self . init_config [ \"backbone\" ] ), INIT_MODULE_DICT [ self . init_config [ \"head_type\" ]]( shape_dimension = self . vae_config [ \"latent_size\" ], ** self . init_config [ \"head\" ], ), ) . to ( self . device ) load_model_weights ( self . init_config [ \"model\" ], self . init_network , self . device , self . init_config . get ( \"model_url\" ), ) self . init_network . eval () self . resolution = 64 self . vae = SDFVAE ( sdf_size = 64 , latent_size = self . vae_config [ \"latent_size\" ], encoder_dict = self . vae_config [ \"encoder\" ], decoder_dict = self . vae_config [ \"decoder\" ], device = self . device , ) . to ( self . device ) load_model_weights ( self . vae_config [ \"model\" ], self . vae , self . device , self . vae_config . get ( \"model_url\" ), ) self . vae . eval () self . cam = Camera ( ** self . camera_config ) self . render = lambda sdf , pos , quat , i_s : render_depth_gpu ( sdf , pos , quat , i_s , None , None , None , config [ \"threshold\" ], self . cam ) self . config = config self . log_data = []","title":"__init__()"},{"location":"reference/estimation/simple_setup/#sdfest.estimation.simple_setup.SDFPipeline.__call__","text":"__call__ ( depth_images : torch . Tensor , masks : torch . Tensor , color_images : torch . Tensor , visualize : bool = False , camera_positions : Optional [ torch . Tensor ] = None , camera_orientations : Optional [ torch . Tensor ] = None , log_path : Optional [ str ] = None , shape_optimization : bool = True , animation_path : Optional [ str ] = None , point_constraint : Optional [ Tuple [ torch . Tensor ]] = None , prior_orientation_distribution : Optional [ torch . Tensor ] = None , training_orientation_distribution : Optional [ torch . Tensor ] = None , ) -> tuple Infer pose, size and latent representation from depth and mask. If multiple images are passed the cameras are assumed to be fixed. All tensors should be on the same device as the pipeline. Batch dimension N must be provided either for all or none of the arguments. PARAMETER DESCRIPTION depth_images The depth map containing the distance along the camera's z-axis. Does not have to be masked, necessary preprocessing is done by pipeline. Will be masked and preprocessed in-place (pass copy if full depth is used afterwards). Shape (N, H, W) or (H, W) for a single depth image. TYPE: torch . Tensor masks binary mask of the object to estimate, same shape as depth_images TYPE: torch . Tensor color_images the color image (currently only used in visualization), shape (N, H, W, 3) or (H, W, 3), RGB, float, 0-1. TYPE: torch . Tensor visualize Whether to visualize the intermediate steps and final result. TYPE: bool DEFAULT: False camera_positions position of camera in world coordinates for each image, if None, (0,0,0) will be assumed for all images, shape (N, 3) or (3,) TYPE: Optional [ torch . Tensor ] DEFAULT: None camera_orientations orientation of camera in world-frame as normalized quaternion, quaternion is in scalar-last convention, note, that this is the quaternion that transforms a point from camera to world-frame if None, (0,0,0,1) will be assumed for all images, shape (N, 4) or (4,) TYPE: Optional [ torch . Tensor ] DEFAULT: None log_path file path to write timestamps and intermediate steps to, no logging is performed if None TYPE: Optional [ str ] DEFAULT: None shape_optimization enable or disable shape optimization during iterative optimization TYPE: bool DEFAULT: True animation_path file path to write rendering and error visualizations to TYPE: Optional [ str ] DEFAULT: None point_constraint tuple of source point and rotated target point and weight a loss will be added that penalizes weight * || rotation @ source - target ||_2 TYPE: Optional [ Tuple [ torch . Tensor ]] DEFAULT: None prior_orientation_distribution Prior distribution of orientations used for initialization. If None, distribution of initialization network will not be modified. Only supported for initialization network with discretized orientation representation. Output distribution of initialization network will be adjusted by multiplying with prior_orientation_distribution / training_orientation_distribution and renormalizing. Tensor of shape (N,C,) or (C,) for single image. C being the number of grid cells in the SO3Grid used by the initialization network. TYPE: Optional [ torch . Tensor ] DEFAULT: None training_orientation_distribution Distribution of orientations used for training initialization network. If None, equal probability for each cell will be assumed. Note this is only approximately the same as a uniform distribution. Only used if prior_orientation_distribution is provided. Tensor of shape (C,). C being the number of grid cells in the SO3Grid used by the initialization network. N not supported, since same training network (and hence distribution) is used independent of view. TYPE: Optional [ torch . Tensor ] DEFAULT: None RETURNS DESCRIPTION tuple 3D pose of SDF center in world frame, shape (1,3,) tuple Orientation as normalized quaternion, scalar-last convention, shape (1,4,) tuple Size of SDF as length of half-width, shape (1,) tuple Latent shape representation of the object, shape (1,latent_size,). Source code in sdfest/estimation/simple_setup.py 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 def __call__ ( self , depth_images : torch . Tensor , masks : torch . Tensor , color_images : torch . Tensor , visualize : bool = False , camera_positions : Optional [ torch . Tensor ] = None , camera_orientations : Optional [ torch . Tensor ] = None , log_path : Optional [ str ] = None , shape_optimization : bool = True , animation_path : Optional [ str ] = None , point_constraint : Optional [ Tuple [ torch . Tensor ]] = None , prior_orientation_distribution : Optional [ torch . Tensor ] = None , training_orientation_distribution : Optional [ torch . Tensor ] = None , ) -> tuple : \"\"\"Infer pose, size and latent representation from depth and mask. If multiple images are passed the cameras are assumed to be fixed. All tensors should be on the same device as the pipeline. Batch dimension N must be provided either for all or none of the arguments. Args: depth_images: The depth map containing the distance along the camera's z-axis. Does not have to be masked, necessary preprocessing is done by pipeline. Will be masked and preprocessed in-place (pass copy if full depth is used afterwards). Shape (N, H, W) or (H, W) for a single depth image. masks: binary mask of the object to estimate, same shape as depth_images color_images: the color image (currently only used in visualization), shape (N, H, W, 3) or (H, W, 3), RGB, float, 0-1. visualize: Whether to visualize the intermediate steps and final result. camera_positions: position of camera in world coordinates for each image, if None, (0,0,0) will be assumed for all images, shape (N, 3) or (3,) camera_orientations: orientation of camera in world-frame as normalized quaternion, quaternion is in scalar-last convention, note, that this is the quaternion that transforms a point from camera to world-frame if None, (0,0,0,1) will be assumed for all images, shape (N, 4) or (4,) log_path: file path to write timestamps and intermediate steps to, no logging is performed if None shape_optimization: enable or disable shape optimization during iterative optimization animation_path: file path to write rendering and error visualizations to point_constraint: tuple of source point and rotated target point and weight a loss will be added that penalizes weight * || rotation @ source - target ||_2 prior_orientation_distribution: Prior distribution of orientations used for initialization. If None, distribution of initialization network will not be modified. Only supported for initialization network with discretized orientation representation. Output distribution of initialization network will be adjusted by multiplying with prior_orientation_distribution / training_orientation_distribution and renormalizing. Tensor of shape (N,C,) or (C,) for single image. C being the number of grid cells in the SO3Grid used by the initialization network. training_orientation_distribution: Distribution of orientations used for training initialization network. If None, equal probability for each cell will be assumed. Note this is only approximately the same as a uniform distribution. Only used if prior_orientation_distribution is provided. Tensor of shape (C,). C being the number of grid cells in the SO3Grid used by the initialization network. N not supported, since same training network (and hence distribution) is used independent of view. Returns: - 3D pose of SDF center in world frame, shape (1,3,) - Orientation as normalized quaternion, scalar-last convention, shape (1,4,) - Size of SDF as length of half-width, shape (1,) - Latent shape representation of the object, shape (1,latent_size,). \"\"\" # initialize optimization self . _best_inlier_ratio = None self . _point_constraint = point_constraint if animation_path is not None : self . _create_animation_folders ( animation_path ) start_time = time . time () # for logging # Add batch dimension if necessary if depth_images . dim () == 2 : depth_images = depth_images . unsqueeze ( 0 ) masks = masks . unsqueeze ( 0 ) color_images = color_images . unsqueeze ( 0 ) if camera_positions is not None : camera_positions = camera_positions . unsqueeze ( 0 ) if camera_orientations is not None : camera_orientations = camera_orientations . unsqueeze ( 0 ) if prior_orientation_distribution is not None : prior_orientation_distribution = ( prior_orientation_distribution . unsqueeze ( 0 ) ) # TODO assert all tensors have expected dimension if animation_path is not None : self . _save_inputs ( animation_path , depth_images , color_images , masks ) n_imgs = depth_images . shape [ 0 ] if camera_positions is None : camera_positions = torch . zeros ( n_imgs , 3 , device = self . device ) if camera_orientations is None : camera_orientations = torch . zeros ( n_imgs , 4 , device = self . device ) camera_orientations [:, 3 ] = 1.0 with torch . no_grad (): self . _preprocess_depth ( depth_images , masks ) # store pointcloud without reconstruction if log_path is not None : torch . cuda . synchronize () self . _log_data ( { \"timestamp\" : time . time () - start_time , \"depth_images\" : depth_images , \"color_images\" : color_images , \"masks\" : masks , \"color_images\" : color_images , \"camera_positions\" : camera_positions , \"camera_orientations\" : camera_orientations , }, ) # Initialization with torch . no_grad (): latent_shape , position , scale , orientation = self . _nn_init ( depth_images , camera_positions , camera_orientations , prior_orientation_distribution , training_orientation_distribution , ) if log_path is not None : torch . cuda . synchronize () self . _log_data ( { \"timestamp\" : time . time () - start_time , \"camera_positions\" : camera_positions , \"camera_orientations\" : camera_orientations , \"latent_shape\" : latent_shape , \"position\" : position , \"scale_inv\" : 1 / scale , \"orientation\" : orientation , }, ) if animation_path is not None : self . _save_preprocessed_inputs ( animation_path , depth_images ) # Iterative optimization self . _current_iteration = 1 position . requires_grad_ () scale . requires_grad_ () orientation . requires_grad_ () latent_shape . requires_grad_ () if visualize : fig_vis , axes = plt . subplots ( 2 , 3 , sharex = True , sharey = True , figsize = ( 12 , 8 ) ) fig_loss , ( loss_ax , inlier_ax ) = plt . subplots ( 1 , 2 ) vmin , vmax = None , None depth_losses = [] pointcloud_losses = [] nn_losses = [] point_constraint_losses = [] inlier_ratios = [] total_losses = [] opt_vars = [ { \"params\" : position , \"lr\" : 1e-3 }, { \"params\" : orientation , \"lr\" : 1e-2 }, { \"params\" : scale , \"lr\" : 1e-3 }, { \"params\" : latent_shape , \"lr\" : 1e-2 }, ] optimizer = torch . optim . Adam ( opt_vars ) while self . _current_iteration <= self . config [ \"max_iterations\" ]: optimizer . zero_grad () norm_orientation = orientation / torch . sqrt ( torch . sum ( orientation ** 2 )) with torch . set_grad_enabled ( shape_optimization ): sdf = self . vae . decode ( latent_shape ) loss_depth = torch . tensor ( 0.0 , device = self . device , requires_grad = True ) loss_pc = torch . tensor ( 0.0 , device = self . device , requires_grad = True ) loss_nn = torch . tensor ( 0.0 , device = self . device , requires_grad = True ) for depth_image , camera_position , camera_orientation in zip ( depth_images , camera_positions , camera_orientations ): # transform object to camera frame q_w2c = quaternion_utils . quaternion_invert ( camera_orientation ) position_c = quaternion_utils . quaternion_apply ( q_w2c , position - camera_position ) orientation_c = quaternion_utils . quaternion_multiply ( q_w2c , norm_orientation ) depth_estimate = self . render ( sdf [ 0 , 0 ], position_c [ 0 ], orientation_c [ 0 ], 1 / scale [ 0 ] ) view_loss_depth , view_loss_pc , view_loss_nn = self . _compute_view_losses ( depth_image , depth_estimate , position_c [ 0 ], orientation_c [ 0 ], scale [ 0 ], sdf [ 0 , 0 ], ) loss_depth = loss_depth + view_loss_depth loss_pc = loss_pc + view_loss_pc loss_nn = loss_nn + view_loss_nn loss_point_constraint = self . _compute_point_constraint_loss ( orientation ) loss = ( self . config [ \"depth_weight\" ] * loss_depth + self . config [ \"pc_weight\" ] * loss_pc + self . config [ \"nn_weight\" ] * loss_nn + loss_point_constraint ) self . _compute_gradients ( loss ) optimizer . step () optimizer . zero_grad () with torch . no_grad (): orientation /= torch . sqrt ( torch . sum ( orientation ** 2 )) inlier_ratio = self . _update_best_estimate ( depth_image , depth_estimate , position , orientation , scale , latent_shape , ) if visualize : depth_losses . append ( loss_depth . item ()) pointcloud_losses . append ( loss_pc . item ()) nn_losses . append ( loss_nn . item ()) point_constraint_losses . append ( loss_point_constraint . item ()) inlier_ratios . append ( inlier_ratio . item ()) total_losses . append ( loss . item ()) if log_path is not None : torch . cuda . synchronize () self . _log_data ( { \"timestamp\" : time . time () - start_time , \"latent_shape\" : latent_shape , \"position\" : position , \"scale_inv\" : 1 / scale , \"orientation\" : orientation , }, ) with torch . no_grad (): if animation_path is not None : self . _save_current_state ( depth_images , animation_path , camera_positions , camera_orientations , position , orientation , 1 / scale , sdf , ) if visualize and ( self . _current_iteration % 10 == 1 or self . _current_iteration == self . config [ \"max_iterations\" ] ): q_w2c = quaternion_utils . quaternion_invert ( camera_orientations [ 0 ]) position_c = quaternion_utils . quaternion_apply ( q_w2c , position - camera_positions [ 0 ] ) orientation_c = quaternion_utils . quaternion_multiply ( q_w2c , orientation ) current_depth = self . render ( sdf [ 0 , 0 ], position_c , orientation_c , 1 / scale ) depth_image = depth_images [ 0 ] color_image = color_images [ 0 ] if self . _current_iteration == 1 : vmin = depth_image [ depth_image != 0 ] . min () * 0.9 vmax = depth_image [ depth_image != 0 ] . max () # show input image axes [ 0 , 0 ] . clear () axes [ 0 , 0 ] . imshow ( depth_image . cpu (), vmin = vmin , vmax = vmax ) axes [ 0 , 1 ] . imshow ( color_image . cpu ()) # show initial estimate axes [ 1 , 0 ] . clear () axes [ 1 , 0 ] . imshow ( current_depth . detach () . cpu (), vmin = vmin , vmax = vmax ) axes [ 1 , 0 ] . set_title ( f \"loss { loss . item () } \" ) # update iterative estimate # axes[0, 2].clear() # axes[0, 2].imshow(rendered_error.detach().cpu()) # axes[0, 2].set_title(\"depth_loss\") # axes[1, 2].clear() # axes[1, 2].imshow(error_pc.detach().cpu()) # axes[1, 2].set_title(\"pointcloud_loss\") loss_ax . clear () loss_ax . plot ( depth_losses , label = \"Depth\" ) loss_ax . plot ( pointcloud_losses , label = \"Pointcloud\" ) loss_ax . plot ( nn_losses , label = \"Nearest Neighbor\" ) if self . _point_constraint is not None : loss_ax . plot ( point_constraint_losses , label = \"Point constraint\" ) loss_ax . plot ( total_losses , label = \"Total\" ) loss_ax . set_yscale ( \"log\" ) loss_ax . legend () inlier_ax . clear () inlier_ax . plot ( inlier_ratios , label = \"Inlier Ratio\" ) inlier_ax . legend () axes [ 1 , 1 ] . clear () axes [ 1 , 1 ] . imshow ( current_depth . detach () . cpu (), vmin = vmin , vmax = vmax ) axes [ 1 , 1 ] . set_title ( f \"loss { loss . item () } \" ) fig_loss . canvas . draw () fig_vis . canvas . draw () plt . pause ( 0.1 ) self . _current_iteration += 1 if visualize : plt . show () plt . close ( fig_loss ) plt . close ( fig_vis ) if log_path is not None : self . _write_log_data ( log_path ) if animation_path is not None : self . _create_animations ( animation_path ) if self . result_selection_strategy == \"last_iteration\" : return position , orientation , scale , latent_shape elif self . result_selection_strategy == \"best_inlier_ratio\" : return ( self . _best_position , self . _best_orientation , self . _best_scale , self . _best_latent_shape , ) else : raise ValueError ( f \"Result selection strategy { self . result_selection_strategy } is not\" \"supported.\" )","title":"__call__()"},{"location":"reference/estimation/simple_setup/#sdfest.estimation.simple_setup.SDFPipeline.generate_depth","text":"generate_depth ( position : torch . Tensor , orientation : torch . Tensor , scale : torch . Tensor , latent : torch . Tensor , ) -> torch . Tensor Generate depth image representing positioned object. Source code in sdfest/estimation/simple_setup.py 605 606 607 608 609 610 611 612 613 614 615 def generate_depth ( self , position : torch . Tensor , orientation : torch . Tensor , scale : torch . Tensor , latent : torch . Tensor , ) -> torch . Tensor : \"\"\"Generate depth image representing positioned object.\"\"\" sdf = self . vae . decode ( latent ) depth = self . render ( sdf [ 0 , 0 ], position , orientation , 1 / scale ) return depth","title":"generate_depth()"},{"location":"reference/estimation/simple_setup/#sdfest.estimation.simple_setup.SDFPipeline.generate_mesh","text":"generate_mesh ( latent : torch . tensor , scale : torch . tensor , complete_mesh : bool = False ) -> synthetic . Mesh Generate mesh without pose. Currently only supports batch size 1. PARAMETER DESCRIPTION latent Latent shape descriptor, shape (1,L). TYPE: torch . tensor scale Relative scale of the signed distance field, (i.e., half-width), shape (1,). TYPE: torch . tensor complete_mesh If True, the SDF will be padded with positive values prior to converting it to a mesh. This ensures a watertight mesh is created. TYPE: bool DEFAULT: False RETURNS DESCRIPTION synthetic . Mesh Generate mesh by decoding latent shape descriptor and scaling it. Source code in sdfest/estimation/simple_setup.py 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 def generate_mesh ( self , latent : torch . tensor , scale : torch . tensor , complete_mesh : bool = False ) -> synthetic . Mesh : \"\"\"Generate mesh without pose. Currently only supports batch size 1. Args: latent: Latent shape descriptor, shape (1,L). scale: Relative scale of the signed distance field, (i.e., half-width), shape (1,). complete_mesh: If True, the SDF will be padded with positive values prior to converting it to a mesh. This ensures a watertight mesh is created. Returns: Generate mesh by decoding latent shape descriptor and scaling it. \"\"\" with torch . no_grad (): sdf = self . vae . decode ( latent ) if complete_mesh : inc = 2 sdf = torch . nn . functional . pad ( sdf , ( 1 , 1 , 1 , 1 , 1 , 1 ), value = 1.0 ) else : inc = 0 try : sdf = sdf . cpu () . numpy () s = 2.0 / ( self . resolution - 1 ) vertices , faces , _ , _ = marching_cubes ( sdf [ 0 , 0 ], spacing = ( s , s , s , ), level = self . config [ \"iso_threshold\" ], ) c = s * ( self . resolution + inc - 1 ) / 2.0 # move origin to center vertices -= np . array ([[ c , c , c ]]) mesh = o3d . geometry . TriangleMesh ( vertices = o3d . utility . Vector3dVector ( vertices ), triangles = o3d . utility . Vector3iVector ( faces ), ) except KeyError : return None return synthetic . Mesh ( mesh = mesh , scale = scale . item (), rel_scale = True )","title":"generate_mesh()"},{"location":"reference/estimation/synthetic/","text":"sdfest.estimation.synthetic Module for synthetic data generation. Object Bases: ABC Generic positioned object representation. Each object has a 6-DOF pose, stored as a 3D translation vector and a normalized quaternion representing its orientation. Source code in sdfest/estimation/synthetic.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 class Object ( ABC ): \"\"\"Generic positioned object representation. Each object has a 6-DOF pose, stored as a 3D translation vector and a normalized quaternion representing its orientation. \"\"\" def __init__ ( self , position = None , orientation = None ): \"\"\"Initialize object position and orientation.\"\"\" if position is None : position = np . array ([ 0 , 0 , 0 ]) if orientation is None : orientation = np . array ([ 0 , 0 , 0 , 1 ]) self . position = position self . orientation = orientation __init__ __init__ ( position = None , orientation = None ) Initialize object position and orientation. Source code in sdfest/estimation/synthetic.py 21 22 23 24 25 26 27 28 def __init__ ( self , position = None , orientation = None ): \"\"\"Initialize object position and orientation.\"\"\" if position is None : position = np . array ([ 0 , 0 , 0 ]) if orientation is None : orientation = np . array ([ 0 , 0 , 0 , 1 ]) self . position = position self . orientation = orientation Mesh Bases: Object Object with associated mesh. This class maintains two meshes, the original mesh and the scaled mesh. Updating the scale will always be relative to the original mesh. I.e., two times setting the relative scale by 0.1 will not yield a final scale of 0.01 as it will always be relative to the original mesh. Source code in sdfest/estimation/synthetic.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 class Mesh ( Object ): \"\"\"Object with associated mesh. This class maintains two meshes, the original mesh and the scaled mesh. Updating the scale will always be relative to the original mesh. I.e., two times setting the relative scale by 0.1 will not yield a final scale of 0.01 as it will always be relative to the original mesh. \"\"\" def __init__ ( self , mesh : Optional [ o3d . geometry . TriangleMesh ] = None , path : Optional [ str ] = None , scale : float = 1 , rel_scale : bool = False , center : bool = False , position : Optional [ np . array ] = None , orientation : Optional [ np . array ] = None , ): \"\"\"Initialize mesh. Must provide either mesh or path. If path is given, mesh will be loaded from specified file. If mesh is given, it will be used as the original mesh. Args: mesh: The original (i.e., unscaled mesh). path: The path of the mesh to load. scale: See Mesh.update_scale. rel_scale: See Mesh.update_scale. center: whether to center the mesh upon loading. position: See Object.__init__. orientation: See Object.__init__. \"\"\" super () . __init__ ( position = position , orientation = orientation ) if mesh is not None and path is not None : raise ValueError ( \"Only one of mesh or path can be specified\" ) if mesh is not None : self . _original_mesh = mesh if path is not None : self . _original_mesh = o3d . io . read_triangle_mesh ( path ) if center : self . _original_mesh . translate ([ 0 , 0 , 0 ], relative = False ) self . update_scale ( scale , rel_scale ) def load_mesh_from_file ( self , path : str , scale : float = 1 , rel_scale : bool = False ) -> None : \"\"\"Load mesh from file. Args: path: Path of the obj file. scale: See Mesh.update_scale. rel_scale: See Mesh.update_scale. \"\"\" self . _original_mesh = o3d . io . read_triangle_mesh ( path ) self . update_scale ( scale , rel_scale ) def update_scale ( self , scale : float = 1 , rel_scale : bool = False ) -> None : \"\"\"Update relative or absolute scale of mesh. Absolute scale represents half the largest extent in x, y, or z direction. Relative scale represents the scale factor from original mesh. Args: scale: The desired absolute or relative scale of the object. rel_scale: If true, scale will be relative to original mesh. Otherwise, scale will be the resulting absolute scale. \"\"\" # self._scale will always be absolute scale if rel_scale : # copy construct mesh self . _scaled_mesh = o3d . geometry . TriangleMesh ( self . _original_mesh ) original_scale = self . _get_original_scale () self . _scaled_mesh . scale ( scale , [ 0 , 0 , 0 ]) self . _scale = original_scale * scale else : # copy construct mesh self . _scaled_mesh = o3d . geometry . TriangleMesh ( self . _original_mesh ) # scale original mesh s.t. output has the provided scale original_scale = self . _get_original_scale () scale_factor = scale / original_scale self . _scaled_mesh . scale ( scale_factor , [ 0 , 0 , 0 ]) self . _scale = scale def _get_original_scale ( self ) -> float : \"\"\"Compute current scale of original mesh. Scale is largest x/y/z extent over 2. \"\"\" mins = np . amin ( self . _original_mesh . vertices , axis = 0 ) maxs = np . amax ( self . _original_mesh . vertices , axis = 0 ) ranges = maxs - mins return np . max ( ranges ) / 2 def get_transformed_o3d_geometry ( self ) -> o3d . geometry . TriangleMesh : \"\"\"Get o3d mesh at current pose.\"\"\" transformed_mesh = o3d . geometry . TriangleMesh ( self . _scaled_mesh ) R = Rotation . from_quat ( self . orientation ) . as_matrix () transformed_mesh . rotate ( R , center = np . array ([ 0 , 0 , 0 ])) transformed_mesh . translate ( self . position ) transformed_mesh . compute_vertex_normals () return transformed_mesh __init__ __init__ ( mesh : Optional [ o3d . geometry . TriangleMesh ] = None , path : Optional [ str ] = None , scale : float = 1 , rel_scale : bool = False , center : bool = False , position : Optional [ np . array ] = None , orientation : Optional [ np . array ] = None , ) Initialize mesh. Must provide either mesh or path. If path is given, mesh will be loaded from specified file. If mesh is given, it will be used as the original mesh. PARAMETER DESCRIPTION mesh The original (i.e., unscaled mesh). TYPE: Optional [ o3d . geometry . TriangleMesh ] DEFAULT: None path The path of the mesh to load. TYPE: Optional [ str ] DEFAULT: None scale See Mesh.update_scale. TYPE: float DEFAULT: 1 rel_scale See Mesh.update_scale. TYPE: bool DEFAULT: False center whether to center the mesh upon loading. TYPE: bool DEFAULT: False position See Object. init . TYPE: Optional [ np . array ] DEFAULT: None orientation See Object. init . TYPE: Optional [ np . array ] DEFAULT: None Source code in sdfest/estimation/synthetic.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def __init__ ( self , mesh : Optional [ o3d . geometry . TriangleMesh ] = None , path : Optional [ str ] = None , scale : float = 1 , rel_scale : bool = False , center : bool = False , position : Optional [ np . array ] = None , orientation : Optional [ np . array ] = None , ): \"\"\"Initialize mesh. Must provide either mesh or path. If path is given, mesh will be loaded from specified file. If mesh is given, it will be used as the original mesh. Args: mesh: The original (i.e., unscaled mesh). path: The path of the mesh to load. scale: See Mesh.update_scale. rel_scale: See Mesh.update_scale. center: whether to center the mesh upon loading. position: See Object.__init__. orientation: See Object.__init__. \"\"\" super () . __init__ ( position = position , orientation = orientation ) if mesh is not None and path is not None : raise ValueError ( \"Only one of mesh or path can be specified\" ) if mesh is not None : self . _original_mesh = mesh if path is not None : self . _original_mesh = o3d . io . read_triangle_mesh ( path ) if center : self . _original_mesh . translate ([ 0 , 0 , 0 ], relative = False ) self . update_scale ( scale , rel_scale ) load_mesh_from_file load_mesh_from_file ( path : str , scale : float = 1 , rel_scale : bool = False ) -> None Load mesh from file. PARAMETER DESCRIPTION path Path of the obj file. TYPE: str scale See Mesh.update_scale. TYPE: float DEFAULT: 1 rel_scale See Mesh.update_scale. TYPE: bool DEFAULT: False Source code in sdfest/estimation/synthetic.py 77 78 79 80 81 82 83 84 85 86 87 88 def load_mesh_from_file ( self , path : str , scale : float = 1 , rel_scale : bool = False ) -> None : \"\"\"Load mesh from file. Args: path: Path of the obj file. scale: See Mesh.update_scale. rel_scale: See Mesh.update_scale. \"\"\" self . _original_mesh = o3d . io . read_triangle_mesh ( path ) self . update_scale ( scale , rel_scale ) update_scale update_scale ( scale : float = 1 , rel_scale : bool = False ) -> None Update relative or absolute scale of mesh. Absolute scale represents half the largest extent in x, y, or z direction. Relative scale represents the scale factor from original mesh. PARAMETER DESCRIPTION scale The desired absolute or relative scale of the object. TYPE: float DEFAULT: 1 rel_scale If true, scale will be relative to original mesh. Otherwise, scale will be the resulting absolute scale. TYPE: bool DEFAULT: False Source code in sdfest/estimation/synthetic.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 def update_scale ( self , scale : float = 1 , rel_scale : bool = False ) -> None : \"\"\"Update relative or absolute scale of mesh. Absolute scale represents half the largest extent in x, y, or z direction. Relative scale represents the scale factor from original mesh. Args: scale: The desired absolute or relative scale of the object. rel_scale: If true, scale will be relative to original mesh. Otherwise, scale will be the resulting absolute scale. \"\"\" # self._scale will always be absolute scale if rel_scale : # copy construct mesh self . _scaled_mesh = o3d . geometry . TriangleMesh ( self . _original_mesh ) original_scale = self . _get_original_scale () self . _scaled_mesh . scale ( scale , [ 0 , 0 , 0 ]) self . _scale = original_scale * scale else : # copy construct mesh self . _scaled_mesh = o3d . geometry . TriangleMesh ( self . _original_mesh ) # scale original mesh s.t. output has the provided scale original_scale = self . _get_original_scale () scale_factor = scale / original_scale self . _scaled_mesh . scale ( scale_factor , [ 0 , 0 , 0 ]) self . _scale = scale get_transformed_o3d_geometry get_transformed_o3d_geometry () -> o3d . geometry . TriangleMesh Get o3d mesh at current pose. Source code in sdfest/estimation/synthetic.py 132 133 134 135 136 137 138 139 def get_transformed_o3d_geometry ( self ) -> o3d . geometry . TriangleMesh : \"\"\"Get o3d mesh at current pose.\"\"\" transformed_mesh = o3d . geometry . TriangleMesh ( self . _scaled_mesh ) R = Rotation . from_quat ( self . orientation ) . as_matrix () transformed_mesh . rotate ( R , center = np . array ([ 0 , 0 , 0 ])) transformed_mesh . translate ( self . position ) transformed_mesh . compute_vertex_normals () return transformed_mesh draw_depth_geometry draw_depth_geometry ( obj : Object , camera : Camera ) Render an object given a camera. Source code in sdfest/estimation/synthetic.py 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 def draw_depth_geometry ( obj : Object , camera : Camera ): \"\"\"Render an object given a camera.\"\"\" # see http://www.open3d.org/docs/latest/tutorial/visualization/customized_visualization.html # rend = o3d.visualization.rendering.OffscreenRenderer() # img = rend.render_to_image() # Create visualizer vis = o3d . visualization . Visualizer () vis . create_window ( width = camera . width , height = camera . height , visible = False ) # Add mesh in correct position vis . add_geometry ( obj . get_transformed_o3d_geometry (), True ) options = vis . get_render_option () options . mesh_show_back_face = True # Set camera at fixed position (i.e., at 0,0,0, looking along z axis) view_control = vis . get_view_control () o3d_cam = camera . get_o3d_pinhole_camera_parameters () o3d_cam . extrinsic = np . array ( [[ 1 , 0 , 0 , 0 ], [ 0 , 1 , 0 , 0 ], [ 0 , 0 , 1 , 0 ], [ 0 , 0 , 0 , 1 ]] ) view_control . convert_from_pinhole_camera_parameters ( o3d_cam , True ) # Generate the depth image vis . poll_events () vis . update_renderer () depth = np . asarray ( vis . capture_depth_float_buffer ( do_render = True )) return depth","title":"synthetic"},{"location":"reference/estimation/synthetic/#sdfest.estimation.synthetic","text":"Module for synthetic data generation.","title":"synthetic"},{"location":"reference/estimation/synthetic/#sdfest.estimation.synthetic.Object","text":"Bases: ABC Generic positioned object representation. Each object has a 6-DOF pose, stored as a 3D translation vector and a normalized quaternion representing its orientation. Source code in sdfest/estimation/synthetic.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 class Object ( ABC ): \"\"\"Generic positioned object representation. Each object has a 6-DOF pose, stored as a 3D translation vector and a normalized quaternion representing its orientation. \"\"\" def __init__ ( self , position = None , orientation = None ): \"\"\"Initialize object position and orientation.\"\"\" if position is None : position = np . array ([ 0 , 0 , 0 ]) if orientation is None : orientation = np . array ([ 0 , 0 , 0 , 1 ]) self . position = position self . orientation = orientation","title":"Object"},{"location":"reference/estimation/synthetic/#sdfest.estimation.synthetic.Object.__init__","text":"__init__ ( position = None , orientation = None ) Initialize object position and orientation. Source code in sdfest/estimation/synthetic.py 21 22 23 24 25 26 27 28 def __init__ ( self , position = None , orientation = None ): \"\"\"Initialize object position and orientation.\"\"\" if position is None : position = np . array ([ 0 , 0 , 0 ]) if orientation is None : orientation = np . array ([ 0 , 0 , 0 , 1 ]) self . position = position self . orientation = orientation","title":"__init__()"},{"location":"reference/estimation/synthetic/#sdfest.estimation.synthetic.Mesh","text":"Bases: Object Object with associated mesh. This class maintains two meshes, the original mesh and the scaled mesh. Updating the scale will always be relative to the original mesh. I.e., two times setting the relative scale by 0.1 will not yield a final scale of 0.01 as it will always be relative to the original mesh. Source code in sdfest/estimation/synthetic.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 class Mesh ( Object ): \"\"\"Object with associated mesh. This class maintains two meshes, the original mesh and the scaled mesh. Updating the scale will always be relative to the original mesh. I.e., two times setting the relative scale by 0.1 will not yield a final scale of 0.01 as it will always be relative to the original mesh. \"\"\" def __init__ ( self , mesh : Optional [ o3d . geometry . TriangleMesh ] = None , path : Optional [ str ] = None , scale : float = 1 , rel_scale : bool = False , center : bool = False , position : Optional [ np . array ] = None , orientation : Optional [ np . array ] = None , ): \"\"\"Initialize mesh. Must provide either mesh or path. If path is given, mesh will be loaded from specified file. If mesh is given, it will be used as the original mesh. Args: mesh: The original (i.e., unscaled mesh). path: The path of the mesh to load. scale: See Mesh.update_scale. rel_scale: See Mesh.update_scale. center: whether to center the mesh upon loading. position: See Object.__init__. orientation: See Object.__init__. \"\"\" super () . __init__ ( position = position , orientation = orientation ) if mesh is not None and path is not None : raise ValueError ( \"Only one of mesh or path can be specified\" ) if mesh is not None : self . _original_mesh = mesh if path is not None : self . _original_mesh = o3d . io . read_triangle_mesh ( path ) if center : self . _original_mesh . translate ([ 0 , 0 , 0 ], relative = False ) self . update_scale ( scale , rel_scale ) def load_mesh_from_file ( self , path : str , scale : float = 1 , rel_scale : bool = False ) -> None : \"\"\"Load mesh from file. Args: path: Path of the obj file. scale: See Mesh.update_scale. rel_scale: See Mesh.update_scale. \"\"\" self . _original_mesh = o3d . io . read_triangle_mesh ( path ) self . update_scale ( scale , rel_scale ) def update_scale ( self , scale : float = 1 , rel_scale : bool = False ) -> None : \"\"\"Update relative or absolute scale of mesh. Absolute scale represents half the largest extent in x, y, or z direction. Relative scale represents the scale factor from original mesh. Args: scale: The desired absolute or relative scale of the object. rel_scale: If true, scale will be relative to original mesh. Otherwise, scale will be the resulting absolute scale. \"\"\" # self._scale will always be absolute scale if rel_scale : # copy construct mesh self . _scaled_mesh = o3d . geometry . TriangleMesh ( self . _original_mesh ) original_scale = self . _get_original_scale () self . _scaled_mesh . scale ( scale , [ 0 , 0 , 0 ]) self . _scale = original_scale * scale else : # copy construct mesh self . _scaled_mesh = o3d . geometry . TriangleMesh ( self . _original_mesh ) # scale original mesh s.t. output has the provided scale original_scale = self . _get_original_scale () scale_factor = scale / original_scale self . _scaled_mesh . scale ( scale_factor , [ 0 , 0 , 0 ]) self . _scale = scale def _get_original_scale ( self ) -> float : \"\"\"Compute current scale of original mesh. Scale is largest x/y/z extent over 2. \"\"\" mins = np . amin ( self . _original_mesh . vertices , axis = 0 ) maxs = np . amax ( self . _original_mesh . vertices , axis = 0 ) ranges = maxs - mins return np . max ( ranges ) / 2 def get_transformed_o3d_geometry ( self ) -> o3d . geometry . TriangleMesh : \"\"\"Get o3d mesh at current pose.\"\"\" transformed_mesh = o3d . geometry . TriangleMesh ( self . _scaled_mesh ) R = Rotation . from_quat ( self . orientation ) . as_matrix () transformed_mesh . rotate ( R , center = np . array ([ 0 , 0 , 0 ])) transformed_mesh . translate ( self . position ) transformed_mesh . compute_vertex_normals () return transformed_mesh","title":"Mesh"},{"location":"reference/estimation/synthetic/#sdfest.estimation.synthetic.Mesh.__init__","text":"__init__ ( mesh : Optional [ o3d . geometry . TriangleMesh ] = None , path : Optional [ str ] = None , scale : float = 1 , rel_scale : bool = False , center : bool = False , position : Optional [ np . array ] = None , orientation : Optional [ np . array ] = None , ) Initialize mesh. Must provide either mesh or path. If path is given, mesh will be loaded from specified file. If mesh is given, it will be used as the original mesh. PARAMETER DESCRIPTION mesh The original (i.e., unscaled mesh). TYPE: Optional [ o3d . geometry . TriangleMesh ] DEFAULT: None path The path of the mesh to load. TYPE: Optional [ str ] DEFAULT: None scale See Mesh.update_scale. TYPE: float DEFAULT: 1 rel_scale See Mesh.update_scale. TYPE: bool DEFAULT: False center whether to center the mesh upon loading. TYPE: bool DEFAULT: False position See Object. init . TYPE: Optional [ np . array ] DEFAULT: None orientation See Object. init . TYPE: Optional [ np . array ] DEFAULT: None Source code in sdfest/estimation/synthetic.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def __init__ ( self , mesh : Optional [ o3d . geometry . TriangleMesh ] = None , path : Optional [ str ] = None , scale : float = 1 , rel_scale : bool = False , center : bool = False , position : Optional [ np . array ] = None , orientation : Optional [ np . array ] = None , ): \"\"\"Initialize mesh. Must provide either mesh or path. If path is given, mesh will be loaded from specified file. If mesh is given, it will be used as the original mesh. Args: mesh: The original (i.e., unscaled mesh). path: The path of the mesh to load. scale: See Mesh.update_scale. rel_scale: See Mesh.update_scale. center: whether to center the mesh upon loading. position: See Object.__init__. orientation: See Object.__init__. \"\"\" super () . __init__ ( position = position , orientation = orientation ) if mesh is not None and path is not None : raise ValueError ( \"Only one of mesh or path can be specified\" ) if mesh is not None : self . _original_mesh = mesh if path is not None : self . _original_mesh = o3d . io . read_triangle_mesh ( path ) if center : self . _original_mesh . translate ([ 0 , 0 , 0 ], relative = False ) self . update_scale ( scale , rel_scale )","title":"__init__()"},{"location":"reference/estimation/synthetic/#sdfest.estimation.synthetic.Mesh.load_mesh_from_file","text":"load_mesh_from_file ( path : str , scale : float = 1 , rel_scale : bool = False ) -> None Load mesh from file. PARAMETER DESCRIPTION path Path of the obj file. TYPE: str scale See Mesh.update_scale. TYPE: float DEFAULT: 1 rel_scale See Mesh.update_scale. TYPE: bool DEFAULT: False Source code in sdfest/estimation/synthetic.py 77 78 79 80 81 82 83 84 85 86 87 88 def load_mesh_from_file ( self , path : str , scale : float = 1 , rel_scale : bool = False ) -> None : \"\"\"Load mesh from file. Args: path: Path of the obj file. scale: See Mesh.update_scale. rel_scale: See Mesh.update_scale. \"\"\" self . _original_mesh = o3d . io . read_triangle_mesh ( path ) self . update_scale ( scale , rel_scale )","title":"load_mesh_from_file()"},{"location":"reference/estimation/synthetic/#sdfest.estimation.synthetic.Mesh.update_scale","text":"update_scale ( scale : float = 1 , rel_scale : bool = False ) -> None Update relative or absolute scale of mesh. Absolute scale represents half the largest extent in x, y, or z direction. Relative scale represents the scale factor from original mesh. PARAMETER DESCRIPTION scale The desired absolute or relative scale of the object. TYPE: float DEFAULT: 1 rel_scale If true, scale will be relative to original mesh. Otherwise, scale will be the resulting absolute scale. TYPE: bool DEFAULT: False Source code in sdfest/estimation/synthetic.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 def update_scale ( self , scale : float = 1 , rel_scale : bool = False ) -> None : \"\"\"Update relative or absolute scale of mesh. Absolute scale represents half the largest extent in x, y, or z direction. Relative scale represents the scale factor from original mesh. Args: scale: The desired absolute or relative scale of the object. rel_scale: If true, scale will be relative to original mesh. Otherwise, scale will be the resulting absolute scale. \"\"\" # self._scale will always be absolute scale if rel_scale : # copy construct mesh self . _scaled_mesh = o3d . geometry . TriangleMesh ( self . _original_mesh ) original_scale = self . _get_original_scale () self . _scaled_mesh . scale ( scale , [ 0 , 0 , 0 ]) self . _scale = original_scale * scale else : # copy construct mesh self . _scaled_mesh = o3d . geometry . TriangleMesh ( self . _original_mesh ) # scale original mesh s.t. output has the provided scale original_scale = self . _get_original_scale () scale_factor = scale / original_scale self . _scaled_mesh . scale ( scale_factor , [ 0 , 0 , 0 ]) self . _scale = scale","title":"update_scale()"},{"location":"reference/estimation/synthetic/#sdfest.estimation.synthetic.Mesh.get_transformed_o3d_geometry","text":"get_transformed_o3d_geometry () -> o3d . geometry . TriangleMesh Get o3d mesh at current pose. Source code in sdfest/estimation/synthetic.py 132 133 134 135 136 137 138 139 def get_transformed_o3d_geometry ( self ) -> o3d . geometry . TriangleMesh : \"\"\"Get o3d mesh at current pose.\"\"\" transformed_mesh = o3d . geometry . TriangleMesh ( self . _scaled_mesh ) R = Rotation . from_quat ( self . orientation ) . as_matrix () transformed_mesh . rotate ( R , center = np . array ([ 0 , 0 , 0 ])) transformed_mesh . translate ( self . position ) transformed_mesh . compute_vertex_normals () return transformed_mesh","title":"get_transformed_o3d_geometry()"},{"location":"reference/estimation/synthetic/#sdfest.estimation.synthetic.draw_depth_geometry","text":"draw_depth_geometry ( obj : Object , camera : Camera ) Render an object given a camera. Source code in sdfest/estimation/synthetic.py 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 def draw_depth_geometry ( obj : Object , camera : Camera ): \"\"\"Render an object given a camera.\"\"\" # see http://www.open3d.org/docs/latest/tutorial/visualization/customized_visualization.html # rend = o3d.visualization.rendering.OffscreenRenderer() # img = rend.render_to_image() # Create visualizer vis = o3d . visualization . Visualizer () vis . create_window ( width = camera . width , height = camera . height , visible = False ) # Add mesh in correct position vis . add_geometry ( obj . get_transformed_o3d_geometry (), True ) options = vis . get_render_option () options . mesh_show_back_face = True # Set camera at fixed position (i.e., at 0,0,0, looking along z axis) view_control = vis . get_view_control () o3d_cam = camera . get_o3d_pinhole_camera_parameters () o3d_cam . extrinsic = np . array ( [[ 1 , 0 , 0 , 0 ], [ 0 , 1 , 0 , 0 ], [ 0 , 0 , 1 , 0 ], [ 0 , 0 , 0 , 1 ]] ) view_control . convert_from_pinhole_camera_parameters ( o3d_cam , True ) # Generate the depth image vis . poll_events () vis . update_renderer () depth = np . asarray ( vis . capture_depth_float_buffer ( do_render = True )) return depth","title":"draw_depth_geometry()"},{"location":"reference/estimation/scripts/play_log/","text":"sdfest.estimation.scripts.play_log Script to play back log file and generate animation. Usage python play_log.py --log_file filename.pkl Log files in the required format is generated by render_evaluation.py. quit_program quit_program ( _ : o3d . visualization . VisualizerWithKeyCallback ) -> bool Quit program. Source code in sdfest/estimation/scripts/play_log.py 36 37 38 def quit_program ( _ : o3d . visualization . VisualizerWithKeyCallback ) -> bool : \"\"\"Quit program.\"\"\" exit () reset_bounding_box reset_bounding_box ( _ : o3d . visualization . VisualizerWithKeyCallback ) -> bool Schedule resetting of bounding box. Source code in sdfest/estimation/scripts/play_log.py 41 42 43 44 45 46 def reset_bounding_box ( _ : o3d . visualization . VisualizerWithKeyCallback ) -> bool : \"\"\"Schedule resetting of bounding box.\"\"\" global reset print ( \"Reset bounding box.\" ) reset = True return False toggle_realtime toggle_realtime ( _ : o3d . visualization . VisualizerWithKeyCallback ) -> bool Toggle realtime playback. Source code in sdfest/estimation/scripts/play_log.py 49 50 51 52 53 54 def toggle_realtime ( _ : o3d . visualization . VisualizerWithKeyCallback ) -> bool : \"\"\"Toggle realtime playback.\"\"\" global realtime realtime = not realtime print ( f \"Realtime: { realtime } \" ) return False toggle_pause toggle_pause ( _ : o3d . visualization . VisualizerWithKeyCallback ) -> bool Toggle pause. Source code in sdfest/estimation/scripts/play_log.py 57 58 59 60 61 62 def toggle_pause ( _ : o3d . visualization . VisualizerWithKeyCallback ) -> bool : \"\"\"Toggle pause.\"\"\" global pause pause = not pause print ( f \"Pause: { pause } \" ) return False toggle_color toggle_color ( _ : o3d . visualization . VisualizerWithKeyCallback ) -> bool Toggle color pointcloud. Source code in sdfest/estimation/scripts/play_log.py 65 66 67 68 69 70 def toggle_color ( _ : o3d . visualization . VisualizerWithKeyCallback ) -> bool : \"\"\"Toggle color pointcloud.\"\"\" global color color = not color print ( f \"Color: { color } \" ) return False toggle_camera_frames toggle_camera_frames ( _ : o3d . visualization . VisualizerWithKeyCallback , ) -> bool Toggle camera frame visualization. Source code in sdfest/estimation/scripts/play_log.py 73 74 75 76 77 78 def toggle_camera_frames ( _ : o3d . visualization . VisualizerWithKeyCallback ) -> bool : \"\"\"Toggle camera frame visualization.\"\"\" global camera_frames camera_frames = not camera_frames print ( f \"Camera frames: { camera_frames } \" ) return False switch_reconstruction_type switch_reconstruction_type ( _ : o3d . visualization . VisualizerWithKeyCallback , ) -> bool Switch between mesh and pointcloud reconstruction. Source code in sdfest/estimation/scripts/play_log.py 81 82 83 84 85 86 87 88 def switch_reconstruction_type ( _ : o3d . visualization . VisualizerWithKeyCallback ) -> bool : \"\"\"Switch between mesh and pointcloud reconstruction.\"\"\" global reconstruction_type cur_id = reconstruction_types . index ( reconstruction_type ) cur_id = ( cur_id + 1 ) % len ( reconstruction_types ) reconstruction_type = reconstruction_types [ cur_id ] print ( f \"Reconstruction: { reconstruction_type } \" ) return False queue_animation queue_animation ( _ : o3d . visualization . VisualizerWithKeyCallback ) -> bool Switch between mesh and pointcloud reconstruction. Source code in sdfest/estimation/scripts/play_log.py 91 92 93 94 95 96 def queue_animation ( _ : o3d . visualization . VisualizerWithKeyCallback ) -> bool : \"\"\"Switch between mesh and pointcloud reconstruction.\"\"\" global animation_queued animation_queued = True print ( \"Saving animation for next loop.\" ) return False main main () -> None Entry point of the evaluation program. Source code in sdfest/estimation/scripts/play_log.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def main () -> None : \"\"\"Entry point of the evaluation program.\"\"\" global reset global animation_queued parser = argparse . ArgumentParser ( description = \"Play log file and generate animation.\" ) parser . add_argument ( \"--log_file\" , required = True ) args = parser . parse_args () with open ( args . log_file , \"rb\" ) as f : data = pickle . load ( f ) config = data [ \"config\" ] log_entries = data [ \"log\" ] pipeline = SDFPipeline ( config ) # precompute meshes for each timestep for log_entry in tqdm ( log_entries ): # print(log_entry) if \"latent_shape\" in log_entry : out_mesh = pipeline . generate_mesh ( log_entry [ \"latent_shape\" ], 1 / log_entry [ \"scale_inv\" ], True ) out_mesh . position = log_entry [ \"position\" ][ 0 ] . detach () . cpu () . numpy () out_mesh . orientation = log_entry [ \"orientation\" ][ 0 ] . detach () . cpu () . numpy () log_entry [ \"mesh\" ] = out_mesh . get_transformed_o3d_geometry () log_entry [ \"mesh\" ] . paint_uniform_color ([ 0.2 , 0.4 , 0.7 ]) log_entry [ \"pointcloud\" ] = log_entry [ \"mesh\" ] . sample_points_uniformly ( 20000 ) # visualize log entries cam_meshes = [] pointclouds = [] KEY_ESCAPE = 256 vis = o3d . visualization . VisualizerWithKeyCallback () vis . register_key_callback ( key = ord ( \"A\" ), callback_func = reset_bounding_box ) vis . register_key_callback ( key = ord ( \"R\" ), callback_func = toggle_realtime ) vis . register_key_callback ( key = ord ( \"S\" ), callback_func = switch_reconstruction_type ) vis . register_key_callback ( key = ord ( \"N\" ), callback_func = queue_animation ) vis . register_key_callback ( key = ord ( \"C\" ), callback_func = toggle_color ) vis . register_key_callback ( key = ord ( \"F\" ), callback_func = toggle_camera_frames ) vis . register_key_callback ( key = ord ( \" \" ), callback_func = toggle_pause ) vis . register_key_callback ( key = KEY_ESCAPE , callback_func = quit_program ) vis . create_window ( width = 640 , height = 480 ) print ( \"Controls \\n\\t a: reset view point & bounding box \\n \" \" \\t r: toggle realtime \\n \" \" \\t s: switch reconstruction_type \\n \" , \" \\t n: queue animation \\n \" , \" \\t c: toggle color \\n \" , \" \\t f: toggle camera frames \\n \" , \" \\t space: pause loop \\n \" , ) first = True while True : vis . clear_geometries () animation_folder = None animation_files = [] animation_timestamps = [] if animation_queued : animation_queued = False animation_folder = ( f \"animation_ { datetime . now () . strftime ( '%Y-%m- %d _%H-%M-%S' ) } \" ) os . makedirs ( animation_folder , exist_ok = True ) start_time = time . time () for log_entry in log_entries : vis . poll_events () vis . update_renderer () if realtime and animation_folder is None : while log_entry [ \"timestamp\" ] > time . time () - start_time : vis . poll_events () vis . update_renderer () if \"camera_positions\" in log_entry : cam_meshes = [] for t_c2w , quat_c2w in zip ( log_entry [ \"camera_positions\" ], log_entry [ \"camera_orientations\" ] ): frame_mesh = synthetic . Mesh ( mesh = o3d . geometry . TriangleMesh . create_coordinate_frame ( size = 0.1 , origin = [ 0 , 0 , 0 ] ), rel_scale = True , ) frame_mesh . position = t_c2w . cpu () . numpy () frame_mesh . orientation = quaternion_utils . quaternion_multiply ( quat_c2w . cpu (), torch . tensor ([ 1.0 , 0 , 0 , 0 ]) ) . numpy () cam_meshes . append ( frame_mesh . get_transformed_o3d_geometry ()) if \"depth_images\" in log_entry : pointclouds = [] for depth_image , color_image , t_c2w , quat_c2w in zip ( log_entry [ \"depth_images\" ], log_entry [ \"color_images\" ], log_entry [ \"camera_positions\" ], log_entry [ \"camera_orientations\" ], ): points_c = pointset_utils . depth_to_pointcloud ( depth_image , pipeline . cam , normalize = False ) pointcloud_torch = ( quaternion_utils . quaternion_apply ( quat_c2w , points_c ) + t_c2w ) pointcloud_numpy = pointcloud_torch . cpu () . numpy () pointcloud_o3d = o3d . geometry . PointCloud ( o3d . utility . Vector3dVector ( pointcloud_numpy ) ) if color : pointcloud_colors_torch = color_image [ depth_image != 0 ] pointcloud_colors_numpy = pointcloud_colors_torch . cpu () . numpy () pointcloud_o3d . colors = o3d . utility . Vector3dVector ( pointcloud_colors_numpy ) else : pointcloud_o3d . colors = o3d . utility . Vector3dVector ( np . ones_like ( pointcloud_numpy ) * np . array ([ 1.0 , 0.2 , 0.2 ]) ) pointclouds . append ( pointcloud_o3d ) while True : geometries = [] + pointclouds if camera_frames : geometries += cam_meshes if \"mesh\" in log_entry and reconstruction_type != \"none\" : geometries . append ( log_entry [ reconstruction_type ]) vis . clear_geometries () for i , geometry in enumerate ( geometries ): vis . add_geometry ( geometry , reset_bounding_box = first or reset ) if i == len ( geometries ) - 1 : reset = first = False if not pause : break else : vis . poll_events () vis . update_renderer () if animation_folder is not None : vis . poll_events () vis . update_renderer () timestamp = log_entry [ \"timestamp\" ] filename = f \" { timestamp } .png\" vis . capture_screen_image ( os . path . join ( animation_folder , filename )) animation_files . append ( filename ) animation_timestamps . append ( timestamp ) # create constant framerate video if animation_folder is not None : fps = 30 frame_folder = os . path . join ( animation_folder , \"constant_framerate\" ) os . makedirs ( frame_folder , exist_ok = True ) video_name = f \" { animation_folder } .mp4\" current_frame = animation_files . pop ( 0 ) current_time = animation_timestamps . pop ( 0 ) frame_number = 0 while animation_files : if animation_timestamps [ 0 ] <= current_time : current_frame = animation_files . pop ( 0 ) animation_timestamps . pop ( 0 ) copyfile ( os . path . join ( animation_folder , current_frame ), os . path . join ( frame_folder , f \" { frame_number : 06d } .png\" ), ) current_time += 1 / fps frame_number += 1 ffmpeg . input ( os . path . join ( frame_folder , \"*.png\" ), pattern_type = \"glob\" , framerate = fps ) . output ( video_name ) . run ()","title":"play_log"},{"location":"reference/estimation/scripts/play_log/#sdfest.estimation.scripts.play_log","text":"Script to play back log file and generate animation. Usage python play_log.py --log_file filename.pkl Log files in the required format is generated by render_evaluation.py.","title":"play_log"},{"location":"reference/estimation/scripts/play_log/#sdfest.estimation.scripts.play_log.quit_program","text":"quit_program ( _ : o3d . visualization . VisualizerWithKeyCallback ) -> bool Quit program. Source code in sdfest/estimation/scripts/play_log.py 36 37 38 def quit_program ( _ : o3d . visualization . VisualizerWithKeyCallback ) -> bool : \"\"\"Quit program.\"\"\" exit ()","title":"quit_program()"},{"location":"reference/estimation/scripts/play_log/#sdfest.estimation.scripts.play_log.reset_bounding_box","text":"reset_bounding_box ( _ : o3d . visualization . VisualizerWithKeyCallback ) -> bool Schedule resetting of bounding box. Source code in sdfest/estimation/scripts/play_log.py 41 42 43 44 45 46 def reset_bounding_box ( _ : o3d . visualization . VisualizerWithKeyCallback ) -> bool : \"\"\"Schedule resetting of bounding box.\"\"\" global reset print ( \"Reset bounding box.\" ) reset = True return False","title":"reset_bounding_box()"},{"location":"reference/estimation/scripts/play_log/#sdfest.estimation.scripts.play_log.toggle_realtime","text":"toggle_realtime ( _ : o3d . visualization . VisualizerWithKeyCallback ) -> bool Toggle realtime playback. Source code in sdfest/estimation/scripts/play_log.py 49 50 51 52 53 54 def toggle_realtime ( _ : o3d . visualization . VisualizerWithKeyCallback ) -> bool : \"\"\"Toggle realtime playback.\"\"\" global realtime realtime = not realtime print ( f \"Realtime: { realtime } \" ) return False","title":"toggle_realtime()"},{"location":"reference/estimation/scripts/play_log/#sdfest.estimation.scripts.play_log.toggle_pause","text":"toggle_pause ( _ : o3d . visualization . VisualizerWithKeyCallback ) -> bool Toggle pause. Source code in sdfest/estimation/scripts/play_log.py 57 58 59 60 61 62 def toggle_pause ( _ : o3d . visualization . VisualizerWithKeyCallback ) -> bool : \"\"\"Toggle pause.\"\"\" global pause pause = not pause print ( f \"Pause: { pause } \" ) return False","title":"toggle_pause()"},{"location":"reference/estimation/scripts/play_log/#sdfest.estimation.scripts.play_log.toggle_color","text":"toggle_color ( _ : o3d . visualization . VisualizerWithKeyCallback ) -> bool Toggle color pointcloud. Source code in sdfest/estimation/scripts/play_log.py 65 66 67 68 69 70 def toggle_color ( _ : o3d . visualization . VisualizerWithKeyCallback ) -> bool : \"\"\"Toggle color pointcloud.\"\"\" global color color = not color print ( f \"Color: { color } \" ) return False","title":"toggle_color()"},{"location":"reference/estimation/scripts/play_log/#sdfest.estimation.scripts.play_log.toggle_camera_frames","text":"toggle_camera_frames ( _ : o3d . visualization . VisualizerWithKeyCallback , ) -> bool Toggle camera frame visualization. Source code in sdfest/estimation/scripts/play_log.py 73 74 75 76 77 78 def toggle_camera_frames ( _ : o3d . visualization . VisualizerWithKeyCallback ) -> bool : \"\"\"Toggle camera frame visualization.\"\"\" global camera_frames camera_frames = not camera_frames print ( f \"Camera frames: { camera_frames } \" ) return False","title":"toggle_camera_frames()"},{"location":"reference/estimation/scripts/play_log/#sdfest.estimation.scripts.play_log.switch_reconstruction_type","text":"switch_reconstruction_type ( _ : o3d . visualization . VisualizerWithKeyCallback , ) -> bool Switch between mesh and pointcloud reconstruction. Source code in sdfest/estimation/scripts/play_log.py 81 82 83 84 85 86 87 88 def switch_reconstruction_type ( _ : o3d . visualization . VisualizerWithKeyCallback ) -> bool : \"\"\"Switch between mesh and pointcloud reconstruction.\"\"\" global reconstruction_type cur_id = reconstruction_types . index ( reconstruction_type ) cur_id = ( cur_id + 1 ) % len ( reconstruction_types ) reconstruction_type = reconstruction_types [ cur_id ] print ( f \"Reconstruction: { reconstruction_type } \" ) return False","title":"switch_reconstruction_type()"},{"location":"reference/estimation/scripts/play_log/#sdfest.estimation.scripts.play_log.queue_animation","text":"queue_animation ( _ : o3d . visualization . VisualizerWithKeyCallback ) -> bool Switch between mesh and pointcloud reconstruction. Source code in sdfest/estimation/scripts/play_log.py 91 92 93 94 95 96 def queue_animation ( _ : o3d . visualization . VisualizerWithKeyCallback ) -> bool : \"\"\"Switch between mesh and pointcloud reconstruction.\"\"\" global animation_queued animation_queued = True print ( \"Saving animation for next loop.\" ) return False","title":"queue_animation()"},{"location":"reference/estimation/scripts/play_log/#sdfest.estimation.scripts.play_log.main","text":"main () -> None Entry point of the evaluation program. Source code in sdfest/estimation/scripts/play_log.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def main () -> None : \"\"\"Entry point of the evaluation program.\"\"\" global reset global animation_queued parser = argparse . ArgumentParser ( description = \"Play log file and generate animation.\" ) parser . add_argument ( \"--log_file\" , required = True ) args = parser . parse_args () with open ( args . log_file , \"rb\" ) as f : data = pickle . load ( f ) config = data [ \"config\" ] log_entries = data [ \"log\" ] pipeline = SDFPipeline ( config ) # precompute meshes for each timestep for log_entry in tqdm ( log_entries ): # print(log_entry) if \"latent_shape\" in log_entry : out_mesh = pipeline . generate_mesh ( log_entry [ \"latent_shape\" ], 1 / log_entry [ \"scale_inv\" ], True ) out_mesh . position = log_entry [ \"position\" ][ 0 ] . detach () . cpu () . numpy () out_mesh . orientation = log_entry [ \"orientation\" ][ 0 ] . detach () . cpu () . numpy () log_entry [ \"mesh\" ] = out_mesh . get_transformed_o3d_geometry () log_entry [ \"mesh\" ] . paint_uniform_color ([ 0.2 , 0.4 , 0.7 ]) log_entry [ \"pointcloud\" ] = log_entry [ \"mesh\" ] . sample_points_uniformly ( 20000 ) # visualize log entries cam_meshes = [] pointclouds = [] KEY_ESCAPE = 256 vis = o3d . visualization . VisualizerWithKeyCallback () vis . register_key_callback ( key = ord ( \"A\" ), callback_func = reset_bounding_box ) vis . register_key_callback ( key = ord ( \"R\" ), callback_func = toggle_realtime ) vis . register_key_callback ( key = ord ( \"S\" ), callback_func = switch_reconstruction_type ) vis . register_key_callback ( key = ord ( \"N\" ), callback_func = queue_animation ) vis . register_key_callback ( key = ord ( \"C\" ), callback_func = toggle_color ) vis . register_key_callback ( key = ord ( \"F\" ), callback_func = toggle_camera_frames ) vis . register_key_callback ( key = ord ( \" \" ), callback_func = toggle_pause ) vis . register_key_callback ( key = KEY_ESCAPE , callback_func = quit_program ) vis . create_window ( width = 640 , height = 480 ) print ( \"Controls \\n\\t a: reset view point & bounding box \\n \" \" \\t r: toggle realtime \\n \" \" \\t s: switch reconstruction_type \\n \" , \" \\t n: queue animation \\n \" , \" \\t c: toggle color \\n \" , \" \\t f: toggle camera frames \\n \" , \" \\t space: pause loop \\n \" , ) first = True while True : vis . clear_geometries () animation_folder = None animation_files = [] animation_timestamps = [] if animation_queued : animation_queued = False animation_folder = ( f \"animation_ { datetime . now () . strftime ( '%Y-%m- %d _%H-%M-%S' ) } \" ) os . makedirs ( animation_folder , exist_ok = True ) start_time = time . time () for log_entry in log_entries : vis . poll_events () vis . update_renderer () if realtime and animation_folder is None : while log_entry [ \"timestamp\" ] > time . time () - start_time : vis . poll_events () vis . update_renderer () if \"camera_positions\" in log_entry : cam_meshes = [] for t_c2w , quat_c2w in zip ( log_entry [ \"camera_positions\" ], log_entry [ \"camera_orientations\" ] ): frame_mesh = synthetic . Mesh ( mesh = o3d . geometry . TriangleMesh . create_coordinate_frame ( size = 0.1 , origin = [ 0 , 0 , 0 ] ), rel_scale = True , ) frame_mesh . position = t_c2w . cpu () . numpy () frame_mesh . orientation = quaternion_utils . quaternion_multiply ( quat_c2w . cpu (), torch . tensor ([ 1.0 , 0 , 0 , 0 ]) ) . numpy () cam_meshes . append ( frame_mesh . get_transformed_o3d_geometry ()) if \"depth_images\" in log_entry : pointclouds = [] for depth_image , color_image , t_c2w , quat_c2w in zip ( log_entry [ \"depth_images\" ], log_entry [ \"color_images\" ], log_entry [ \"camera_positions\" ], log_entry [ \"camera_orientations\" ], ): points_c = pointset_utils . depth_to_pointcloud ( depth_image , pipeline . cam , normalize = False ) pointcloud_torch = ( quaternion_utils . quaternion_apply ( quat_c2w , points_c ) + t_c2w ) pointcloud_numpy = pointcloud_torch . cpu () . numpy () pointcloud_o3d = o3d . geometry . PointCloud ( o3d . utility . Vector3dVector ( pointcloud_numpy ) ) if color : pointcloud_colors_torch = color_image [ depth_image != 0 ] pointcloud_colors_numpy = pointcloud_colors_torch . cpu () . numpy () pointcloud_o3d . colors = o3d . utility . Vector3dVector ( pointcloud_colors_numpy ) else : pointcloud_o3d . colors = o3d . utility . Vector3dVector ( np . ones_like ( pointcloud_numpy ) * np . array ([ 1.0 , 0.2 , 0.2 ]) ) pointclouds . append ( pointcloud_o3d ) while True : geometries = [] + pointclouds if camera_frames : geometries += cam_meshes if \"mesh\" in log_entry and reconstruction_type != \"none\" : geometries . append ( log_entry [ reconstruction_type ]) vis . clear_geometries () for i , geometry in enumerate ( geometries ): vis . add_geometry ( geometry , reset_bounding_box = first or reset ) if i == len ( geometries ) - 1 : reset = first = False if not pause : break else : vis . poll_events () vis . update_renderer () if animation_folder is not None : vis . poll_events () vis . update_renderer () timestamp = log_entry [ \"timestamp\" ] filename = f \" { timestamp } .png\" vis . capture_screen_image ( os . path . join ( animation_folder , filename )) animation_files . append ( filename ) animation_timestamps . append ( timestamp ) # create constant framerate video if animation_folder is not None : fps = 30 frame_folder = os . path . join ( animation_folder , \"constant_framerate\" ) os . makedirs ( frame_folder , exist_ok = True ) video_name = f \" { animation_folder } .mp4\" current_frame = animation_files . pop ( 0 ) current_time = animation_timestamps . pop ( 0 ) frame_number = 0 while animation_files : if animation_timestamps [ 0 ] <= current_time : current_frame = animation_files . pop ( 0 ) animation_timestamps . pop ( 0 ) copyfile ( os . path . join ( animation_folder , current_frame ), os . path . join ( frame_folder , f \" { frame_number : 06d } .png\" ), ) current_time += 1 / fps frame_number += 1 ffmpeg . input ( os . path . join ( frame_folder , \"*.png\" ), pattern_type = \"glob\" , framerate = fps ) . output ( video_name ) . run ()","title":"main()"},{"location":"reference/estimation/scripts/real_data/","text":"sdfest.estimation.scripts.real_data Simple script to run inference on real data. Usage (evaluation on random RGB-D images from folder): python -m sdfest.estimation.scripts.real_data --config estimation/configs/rgbd_objects_uw.yaml estimation/configs/mug.yaml --folder data/rgbd_objects_uw/coffee_mug/ Usage (evaluation on single RGB image from Redwood or RGB-D objects dataset): python -m sdfest.estimation.scripts.real_data --config configs/rgbd_objects_uw.yaml configs/mug.yaml --input rgbd_objects_uw/coffee_mug/coffee_mug_1/coffee_mug_1_1_103.png Specific parameters measure_runtime: if True, a breakdown of the runtime will be generated only supported for single input out_folder: if provided and measure_runtime is true, the runtime results are written to file visualize_optimization: whether to visualize optimization while at it visualize_input: whether to visualize the input create_animation: If true, three animations will be created. One for depth optimization, depth error, and mesh. load_real275_rgbd load_real275_rgbd ( rgb_path : str , ) -> Tuple [ np . ndarray , np . ndarray , str , str ] Load RGB-D image from RGB path. PARAMETER DESCRIPTION rgb_path path to RGB image TYPE: str RETURNS DESCRIPTION Tuple [ np . ndarray , np . ndarray , str , str ] Tuple containing: - The color image, float32, RGB, 0-1, shape (H,W,C). - The depth image, float32, in meters, shape (H,W). - The color path. - The depth path. Source code in sdfest/estimation/scripts/real_data.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def load_real275_rgbd ( rgb_path : str ) -> Tuple [ np . ndarray , np . ndarray , str , str ]: \"\"\"Load RGB-D image from RGB path. Args: rgb_path: path to RGB image Returns: Tuple containing: - The color image, float32, RGB, 0-1, shape (H,W,C). - The depth image, float32, in meters, shape (H,W). - The color path. - The depth path. \"\"\" depth_path = rgb_path [: - 10 ] + \"_depth.png\" color_img = np . asarray ( o3d . io . read_image ( rgb_path ), dtype = np . float32 ) / 255 depth_img = ( np . asarray ( o3d . io . read_image ( depth_path ), dtype = np . float32 , ) * 0.001 ) return color_img , depth_img , rgb_path , depth_path load_real275_sample load_real275_sample ( folder : str , ) -> Tuple [ np . ndarray , np . ndarray , str , str ] Load a sample from RGBD Object dataset. https://rgbd-dataset.cs.washington.edu/dataset/ PARAMETER DESCRIPTION folder The root folder of the dataset. TYPE: str RETURNS DESCRIPTION Tuple [ np . ndarray , np . ndarray , str , str ] See load_real275_rgbd. Source code in sdfest/estimation/scripts/real_data.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def load_real275_sample ( folder : str ) -> Tuple [ np . ndarray , np . ndarray , str , str ]: \"\"\"Load a sample from RGBD Object dataset. https://rgbd-dataset.cs.washington.edu/dataset/ Args: folder: The root folder of the dataset. Returns: See load_real275_rgbd. \"\"\" files = glob . glob ( folder + \"/**/*color.png\" , recursive = True ) rgb_path = random . choice ( files ) return load_real275_rgbd ( rgb_path ) load_rgbd_object_uw_rgbd load_rgbd_object_uw_rgbd ( rgb_path : str , ) -> Tuple [ np . ndarray , np . ndarray , str , str ] Load RGB-D image from RGB path. PARAMETER DESCRIPTION rgb_path path to RGB image TYPE: str RETURNS DESCRIPTION Tuple [ np . ndarray , np . ndarray , str , str ] Tuple containing: - The color image, float32, RGB, 0-1, shape (H,W,C). - The depth image, float32, in meters, shape (H,W). - The color path. - The depth path. Source code in sdfest/estimation/scripts/real_data.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def load_rgbd_object_uw_rgbd ( rgb_path : str ) -> Tuple [ np . ndarray , np . ndarray , str , str ]: \"\"\"Load RGB-D image from RGB path. Args: rgb_path: path to RGB image Returns: Tuple containing: - The color image, float32, RGB, 0-1, shape (H,W,C). - The depth image, float32, in meters, shape (H,W). - The color path. - The depth path. \"\"\" depth_path = rgb_path [: - 4 ] + \"_depth\" + rgb_path [ - 4 :] color_img = np . asarray ( o3d . io . read_image ( rgb_path ), dtype = np . float32 ) / 255 depth_img = ( np . asarray ( o3d . io . read_image ( depth_path ), dtype = np . float32 , ) * 0.001 ) return color_img , depth_img , rgb_path , depth_path load_rgbd_object_uw_sample load_rgbd_object_uw_sample ( folder : str , ) -> Tuple [ np . ndarray , np . ndarray , str , str ] Load a sample from RGBD Object dataset. https://rgbd-dataset.cs.washington.edu/dataset/ PARAMETER DESCRIPTION folder The root folder of the dataset. TYPE: str RETURNS DESCRIPTION Tuple [ np . ndarray , np . ndarray , str , str ] See load_rgbd_object_uw_rgbd. Source code in sdfest/estimation/scripts/real_data.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def load_rgbd_object_uw_sample ( folder : str ) -> Tuple [ np . ndarray , np . ndarray , str , str ]: \"\"\"Load a sample from RGBD Object dataset. https://rgbd-dataset.cs.washington.edu/dataset/ Args: folder: The root folder of the dataset. Returns: See load_rgbd_object_uw_rgbd. \"\"\" files = glob . glob ( folder + \"/**/*[0-9].png\" , recursive = True ) rgb_path = random . choice ( files ) return load_rgbd_object_uw_rgbd ( rgb_path ) load_redwood_rgbd load_redwood_rgbd ( rgb_path : str , ) -> Tuple [ np . ndarray , np . ndarray , str , str ] Load RGB-D image from RGB path of Redwood dataset. PARAMETER DESCRIPTION rgb_path path to RGB image TYPE: str RETURNS DESCRIPTION Tuple [ np . ndarray , np . ndarray , str , str ] Tuple containing: - The color image, float32, RGB, 0-1, shape (H,W,C). - The depth image, float32, in meters, shape (H,W). - The color path. - The depth path. Source code in sdfest/estimation/scripts/real_data.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 def load_redwood_rgbd ( rgb_path : str ) -> Tuple [ np . ndarray , np . ndarray , str , str ]: \"\"\"Load RGB-D image from RGB path of Redwood dataset. Args: rgb_path: path to RGB image Returns: Tuple containing: - The color image, float32, RGB, 0-1, shape (H,W,C). - The depth image, float32, in meters, shape (H,W). - The color path. - The depth path. \"\"\" depth_dir = os . path . join ( os . path . dirname ( rgb_path ), \"..\" , \"depth\" ) rgb_timestamp = int ( rgb_path [ - 16 : - 4 ]) # find closest depth image in time depth_paths = glob . glob ( depth_dir + \"/*.png\" ) depth_timestamps = np . array ([ int ( p [ - 16 : - 4 ]) for p in depth_paths ]) ind = np . argmin ( np . abs ( depth_timestamps - rgb_timestamp )) depth_path = depth_paths [ ind ] color_img = np . asarray ( o3d . io . read_image ( rgb_path ), dtype = np . float32 ) / 255 depth_img = ( np . asarray ( o3d . io . read_image ( depth_path ), dtype = np . float32 , ) * 0.001 ) return color_img , depth_img , rgb_path , depth_path load_redwood_sample load_redwood_sample ( folder : str , ) -> Tuple [ np . ndarray , np . ndarray , str , str ] Load a sample from Redwood dataset. PARAMETER DESCRIPTION folder The root folder of the dataset. TYPE: str RETURNS DESCRIPTION Tuple [ np . ndarray , np . ndarray , str , str ] Tuple containing: - The color image, float32, RGB, 0-1, shape (H,W,C). - The depth image, float32, in meters, shape (H,W). - The color path. - The depth path. Source code in sdfest/estimation/scripts/real_data.py 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 def load_redwood_sample ( folder : str ) -> Tuple [ np . ndarray , np . ndarray , str , str ]: \"\"\"Load a sample from Redwood dataset. Args: folder: The root folder of the dataset. Returns: Tuple containing: - The color image, float32, RGB, 0-1, shape (H,W,C). - The depth image, float32, in meters, shape (H,W). - The color path. - The depth path. \"\"\" sequence_paths = glob . glob ( folder + \"/*\" ) sequence_path = random . choice ( sequence_paths ) rgb_dir = os . path . join ( sequence_path , \"rgb\" ) rgb_paths = glob . glob ( rgb_dir + \"/*.jpg\" ) rgb_path = random . choice ( rgb_paths ) return load_redwood_rgbd ( rgb_path ) load_sample_from_folder load_sample_from_folder ( config : dict , ) -> Tuple [ np . ndarray , np . ndarray , str , str ] Load a sample from dataset specified in config. See the dataset specific load functions for more details of the expected folder structure. PARAMETER DESCRIPTION config Configuration dictionary that must contain the following keys: \"dataset\": one of \"redwood\" | \"rgbd_object_uw\" \"folder\": the root folder of the dataset TYPE: dict RETURNS DESCRIPTION Tuple [ np . ndarray , np . ndarray , str , str ] Tuple containing: - The color image, float32, RGB, 0-1, shape (H,W,C). - The depth image, float32, in meters, shape (H,W). Source code in sdfest/estimation/scripts/real_data.py 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 def load_sample_from_folder ( config : dict ) -> Tuple [ np . ndarray , np . ndarray , str , str ]: \"\"\"Load a sample from dataset specified in config. See the dataset specific load functions for more details of the expected folder structure. Params: config: Configuration dictionary that must contain the following keys: \"dataset\": one of \"redwood\" | \"rgbd_object_uw\" \"folder\": the root folder of the dataset Returns: Tuple containing: - The color image, float32, RGB, 0-1, shape (H,W,C). - The depth image, float32, in meters, shape (H,W). \"\"\" if config [ \"dataset\" ] == \"redwood\" : return load_redwood_sample ( config [ \"folder\" ]) elif config [ \"dataset\" ] == \"rgbd_object_uw\" : return load_rgbd_object_uw_sample ( config [ \"folder\" ]) elif config [ \"dataset\" ] == \"real275\" : return load_real275_sample ( config [ \"folder\" ]) else : raise NotImplementedError ( f \"Dataset { config [ 'dataset' ] } is not supported\" ) load_rgbd load_rgbd ( config : dict ) -> Tuple [ np . ndarray , np . ndarray , str , str ] Load a single RGB-D image from path and dataset specified in config. See the dataset specific load functions for more details of the expected folder structure. PARAMETER DESCRIPTION config Configuration dictionary that must contain the following keys: \"dataset\": one of \"redwood\" | \"rgbd_object_uw\" \"input\": the path to the RGB image TYPE: dict RETURNS DESCRIPTION Tuple [ np . ndarray , np . ndarray , str , str ] Tuple containing: - The color image, float32, RGB, 0-1, shape (H,W,C). - The depth image, float32, in meters, shape (H,W). - The color path. - The depth path. Source code in sdfest/estimation/scripts/real_data.py 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 def load_rgbd ( config : dict ) -> Tuple [ np . ndarray , np . ndarray , str , str ]: \"\"\"Load a single RGB-D image from path and dataset specified in config. See the dataset specific load functions for more details of the expected folder structure. Params: config: Configuration dictionary that must contain the following keys: \"dataset\": one of \"redwood\" | \"rgbd_object_uw\" \"input\": the path to the RGB image Returns: Tuple containing: - The color image, float32, RGB, 0-1, shape (H,W,C). - The depth image, float32, in meters, shape (H,W). - The color path. - The depth path. \"\"\" if config [ \"dataset\" ] == \"redwood\" : return load_redwood_rgbd ( config [ \"input\" ]) elif config [ \"dataset\" ] == \"rgbd_object_uw\" : return load_rgbd_object_uw_rgbd ( config [ \"input\" ]) elif config [ \"dataset\" ] == \"real275\" : return load_real275_rgbd ( config [ \"input\" ]) else : raise NotImplementedError ( f \"Dataset { config [ 'dataset' ] } is not supported\" ) str_to_bool str_to_bool ( v : str ) -> bool Try to convert string to boolean. From: https://stackoverflow.com/a/43357954 Source code in sdfest/estimation/scripts/real_data.py 271 272 273 274 275 276 277 278 279 280 281 282 283 def str_to_bool ( v : str ) -> bool : \"\"\"Try to convert string to boolean. From: https://stackoverflow.com/a/43357954 \"\"\" if isinstance ( v , bool ): return v if v . lower () in ( \"yes\" , \"true\" , \"t\" , \"y\" , \"1\" ): return True elif v . lower () in ( \"no\" , \"false\" , \"f\" , \"n\" , \"0\" ): return False else : raise argparse . ArgumentTypeError ( \"Boolean value expected.\" ) main main () -> None Entry point of the program. Source code in sdfest/estimation/scripts/real_data.py 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 def main () -> None : \"\"\"Entry point of the program.\"\"\" # define the arguments parser = argparse . ArgumentParser ( description = \"SDF pose estimation in real data\" ) # parse arguments parser . add_argument ( \"--device\" ) parser . add_argument ( \"--input\" ) parser . add_argument ( \"--folder\" ) parser . add_argument ( \"--measure_runtime\" , type = str_to_bool , default = False ) parser . add_argument ( \"--visualize_optimization\" , type = str_to_bool , default = False ) parser . add_argument ( \"--visualize_input\" , type = str_to_bool , default = False ) parser . add_argument ( \"--cached_segmentation\" , action = \"store_true\" ) parser . add_argument ( \"--segmentation_dir\" , default = \"./cached_segmentations/\" ) parser . add_argument ( \"--config\" , default = \"configs/default.yaml\" , nargs = \"+\" ) config = yoco . load_config_from_args ( parser , search_paths = [ \".\" , \"~/.sdfest/\" , sdfest . __path__ [ 0 ]] ) if \"input\" in config and \"folder\" in config : print ( \"Only one of input and folder can be specified.\" ) exit () if config [ \"measure_runtime\" ] and config [ \"visualize_optimization\" ]: print ( \"Visualization not supported while measuring runtime.\" ) exit () pipeline = SDFPipeline ( config ) create_animation = ( config [ \"create_animation\" ] if \"create_animation\" in config else False ) timing_dict = None timing_dicts = [] if config [ \"measure_runtime\" ]: timing_dict = add_timing_decorators ( pipeline ) # Segmentation using detectron2 print ( \"Loading segmentation model...\" ) cfg = detectron2 . config . get_cfg () cfg . merge_from_file ( model_zoo . get_config_file ( \"COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml\" ) ) cfg . MODEL . ROI_HEADS . SCORE_THRESH_TEST = 0.5 # set threshold for this model cfg . MODEL . WEIGHTS = model_zoo . get_checkpoint_url ( \"COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml\" ) predictor = detectron2 . engine . DefaultPredictor ( cfg ) print ( \"Segmentation model loaded.\" ) completed_runs = 0 shape_optimization = True while True : if timing_dict is not None : timing_dict . clear () if \"folder\" in config : color_img , depth_img , color_path , _ = load_sample_from_folder ( config ) elif \"input\" in config : color_img , depth_img , color_path , _ = load_rgbd ( config ) else : print ( \"No folder or input file specified.\" ) exit () if timing_dict is not None : timing_dict [ \"pipeline\" ] . append ([ time . time (), None ]) timing_dict [ \"segmentation\" ] . append ([ time . time (), None ]) if config [ \"cached_segmentation\" ]: # check if segmentation exists color_name , _ = os . path . splitext ( color_path ) color_dir = os . path . dirname ( color_name ) segmentation_dir = os . path . join ( config [ \"segmentation_dir\" ], color_dir ) segmentation_path = ( os . path . join ( config [ \"segmentation_dir\" ], color_name ) + \".pickle\" ) os . makedirs ( segmentation_dir , exist_ok = True ) if os . path . isfile ( segmentation_path ): with open ( segmentation_path , \"rb\" ) as f : outputs = pickle . load ( f ) else : # compute segmentation and save # detectron expects (H,C,W), BGR, 0-255 as input detectron_color_img = color_img [:, :, :: - 1 ] * 255 outputs = predictor ( detectron_color_img ) with open ( segmentation_path , \"wb\" ) as f : pickle . dump ( outputs , f ) else : # detectron expects (H,C,W), BGR, 0-255 as input detectron_color_img = color_img [:, :, :: - 1 ] * 255 outputs = predictor ( detectron_color_img ) if timing_dict is not None : torch . cuda . synchronize () timing_dict [ \"segmentation\" ][ - 1 ][ 1 ] = time . time () category_id = MetadataCatalog . get ( cfg . DATASETS . TRAIN [ 0 ]) . thing_classes . index ( config [ \"category\" ] ) matching_instances = [] for i in range ( len ( outputs [ \"instances\" ])): instance = outputs [ \"instances\" ][ i ] if instance . pred_classes != category_id : continue matching_instances . append ( instance ) matching_instances . sort ( key = lambda k : k . pred_masks . sum ()) if not matching_instances : print ( \"Warning: category not detected in input\" ) else : print ( \"Category detected\" ) for instance in matching_instances : if create_animation : animation_path = os . path . join ( os . getcwd (), f \"animation_ { datetime . now () . strftime ( '%Y-%m- %d _%H-%M-%S' ) } \" , ) else : animation_path = None if config [ \"visualize_input\" ]: v = Visualizer ( color_img * 255 , MetadataCatalog . get ( cfg . DATASETS . TRAIN [ 0 ]), scale = 1.2 , ) out = v . draw_instance_predictions ( instance . to ( \"cpu\" )) plt . imshow ( out . get_image ()) plt . show () plt . imshow ( depth_img ) plt . show () depth_tensor = torch . from_numpy ( depth_img ) . to ( config [ \"device\" ]) instance_mask = instance . pred_masks . cuda ()[ 0 ] color_img_tensor = torch . from_numpy ( color_img ) . to ( config [ \"device\" ]) try : position , orientation , scale , shape = pipeline ( depth_tensor , instance_mask , color_img_tensor , visualize = config [ \"visualize_optimization\" ], animation_path = animation_path , shape_optimization = shape_optimization , ) except NoDepthError : print ( \"No depth data, skipping\" ) break # comment to evaluate all instances, instead of largest only if timing_dict is not None : torch . cuda . synchronize () timing_dict [ \"pipeline\" ][ - 1 ][ 1 ] = time . time () if completed_runs != 0 or not config [ \"skip_first_run\" ]: timing_dicts . append ( copy . deepcopy ( timing_dict )) timing_dicts [ - 1 ][ \"shape_optimization\" ] = shape_optimization print ( f \" \\r { completed_runs + 1 } / { config [ 'runs' ] } \" , end = \"\" ) completed_runs += 1 # only run single evaluation for single file if \"input\" in config and not config [ \"measure_runtime\" ]: break elif config [ \"measure_runtime\" ] and completed_runs == config [ \"runs\" ]: if shape_optimization : shape_optimization = False completed_runs = 0 print ( \"\" ) else : print ( \"\" ) break if config [ \"measure_runtime\" ]: generate_runtime_overview ( config , timing_dicts )","title":"real_data"},{"location":"reference/estimation/scripts/real_data/#sdfest.estimation.scripts.real_data","text":"Simple script to run inference on real data. Usage (evaluation on random RGB-D images from folder): python -m sdfest.estimation.scripts.real_data --config estimation/configs/rgbd_objects_uw.yaml estimation/configs/mug.yaml --folder data/rgbd_objects_uw/coffee_mug/ Usage (evaluation on single RGB image from Redwood or RGB-D objects dataset): python -m sdfest.estimation.scripts.real_data --config configs/rgbd_objects_uw.yaml configs/mug.yaml --input rgbd_objects_uw/coffee_mug/coffee_mug_1/coffee_mug_1_1_103.png Specific parameters measure_runtime: if True, a breakdown of the runtime will be generated only supported for single input out_folder: if provided and measure_runtime is true, the runtime results are written to file visualize_optimization: whether to visualize optimization while at it visualize_input: whether to visualize the input create_animation: If true, three animations will be created. One for depth optimization, depth error, and mesh.","title":"real_data"},{"location":"reference/estimation/scripts/real_data/#sdfest.estimation.scripts.real_data.load_real275_rgbd","text":"load_real275_rgbd ( rgb_path : str , ) -> Tuple [ np . ndarray , np . ndarray , str , str ] Load RGB-D image from RGB path. PARAMETER DESCRIPTION rgb_path path to RGB image TYPE: str RETURNS DESCRIPTION Tuple [ np . ndarray , np . ndarray , str , str ] Tuple containing: - The color image, float32, RGB, 0-1, shape (H,W,C). - The depth image, float32, in meters, shape (H,W). - The color path. - The depth path. Source code in sdfest/estimation/scripts/real_data.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def load_real275_rgbd ( rgb_path : str ) -> Tuple [ np . ndarray , np . ndarray , str , str ]: \"\"\"Load RGB-D image from RGB path. Args: rgb_path: path to RGB image Returns: Tuple containing: - The color image, float32, RGB, 0-1, shape (H,W,C). - The depth image, float32, in meters, shape (H,W). - The color path. - The depth path. \"\"\" depth_path = rgb_path [: - 10 ] + \"_depth.png\" color_img = np . asarray ( o3d . io . read_image ( rgb_path ), dtype = np . float32 ) / 255 depth_img = ( np . asarray ( o3d . io . read_image ( depth_path ), dtype = np . float32 , ) * 0.001 ) return color_img , depth_img , rgb_path , depth_path","title":"load_real275_rgbd()"},{"location":"reference/estimation/scripts/real_data/#sdfest.estimation.scripts.real_data.load_real275_sample","text":"load_real275_sample ( folder : str , ) -> Tuple [ np . ndarray , np . ndarray , str , str ] Load a sample from RGBD Object dataset. https://rgbd-dataset.cs.washington.edu/dataset/ PARAMETER DESCRIPTION folder The root folder of the dataset. TYPE: str RETURNS DESCRIPTION Tuple [ np . ndarray , np . ndarray , str , str ] See load_real275_rgbd. Source code in sdfest/estimation/scripts/real_data.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def load_real275_sample ( folder : str ) -> Tuple [ np . ndarray , np . ndarray , str , str ]: \"\"\"Load a sample from RGBD Object dataset. https://rgbd-dataset.cs.washington.edu/dataset/ Args: folder: The root folder of the dataset. Returns: See load_real275_rgbd. \"\"\" files = glob . glob ( folder + \"/**/*color.png\" , recursive = True ) rgb_path = random . choice ( files ) return load_real275_rgbd ( rgb_path )","title":"load_real275_sample()"},{"location":"reference/estimation/scripts/real_data/#sdfest.estimation.scripts.real_data.load_rgbd_object_uw_rgbd","text":"load_rgbd_object_uw_rgbd ( rgb_path : str , ) -> Tuple [ np . ndarray , np . ndarray , str , str ] Load RGB-D image from RGB path. PARAMETER DESCRIPTION rgb_path path to RGB image TYPE: str RETURNS DESCRIPTION Tuple [ np . ndarray , np . ndarray , str , str ] Tuple containing: - The color image, float32, RGB, 0-1, shape (H,W,C). - The depth image, float32, in meters, shape (H,W). - The color path. - The depth path. Source code in sdfest/estimation/scripts/real_data.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def load_rgbd_object_uw_rgbd ( rgb_path : str ) -> Tuple [ np . ndarray , np . ndarray , str , str ]: \"\"\"Load RGB-D image from RGB path. Args: rgb_path: path to RGB image Returns: Tuple containing: - The color image, float32, RGB, 0-1, shape (H,W,C). - The depth image, float32, in meters, shape (H,W). - The color path. - The depth path. \"\"\" depth_path = rgb_path [: - 4 ] + \"_depth\" + rgb_path [ - 4 :] color_img = np . asarray ( o3d . io . read_image ( rgb_path ), dtype = np . float32 ) / 255 depth_img = ( np . asarray ( o3d . io . read_image ( depth_path ), dtype = np . float32 , ) * 0.001 ) return color_img , depth_img , rgb_path , depth_path","title":"load_rgbd_object_uw_rgbd()"},{"location":"reference/estimation/scripts/real_data/#sdfest.estimation.scripts.real_data.load_rgbd_object_uw_sample","text":"load_rgbd_object_uw_sample ( folder : str , ) -> Tuple [ np . ndarray , np . ndarray , str , str ] Load a sample from RGBD Object dataset. https://rgbd-dataset.cs.washington.edu/dataset/ PARAMETER DESCRIPTION folder The root folder of the dataset. TYPE: str RETURNS DESCRIPTION Tuple [ np . ndarray , np . ndarray , str , str ] See load_rgbd_object_uw_rgbd. Source code in sdfest/estimation/scripts/real_data.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def load_rgbd_object_uw_sample ( folder : str ) -> Tuple [ np . ndarray , np . ndarray , str , str ]: \"\"\"Load a sample from RGBD Object dataset. https://rgbd-dataset.cs.washington.edu/dataset/ Args: folder: The root folder of the dataset. Returns: See load_rgbd_object_uw_rgbd. \"\"\" files = glob . glob ( folder + \"/**/*[0-9].png\" , recursive = True ) rgb_path = random . choice ( files ) return load_rgbd_object_uw_rgbd ( rgb_path )","title":"load_rgbd_object_uw_sample()"},{"location":"reference/estimation/scripts/real_data/#sdfest.estimation.scripts.real_data.load_redwood_rgbd","text":"load_redwood_rgbd ( rgb_path : str , ) -> Tuple [ np . ndarray , np . ndarray , str , str ] Load RGB-D image from RGB path of Redwood dataset. PARAMETER DESCRIPTION rgb_path path to RGB image TYPE: str RETURNS DESCRIPTION Tuple [ np . ndarray , np . ndarray , str , str ] Tuple containing: - The color image, float32, RGB, 0-1, shape (H,W,C). - The depth image, float32, in meters, shape (H,W). - The color path. - The depth path. Source code in sdfest/estimation/scripts/real_data.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 def load_redwood_rgbd ( rgb_path : str ) -> Tuple [ np . ndarray , np . ndarray , str , str ]: \"\"\"Load RGB-D image from RGB path of Redwood dataset. Args: rgb_path: path to RGB image Returns: Tuple containing: - The color image, float32, RGB, 0-1, shape (H,W,C). - The depth image, float32, in meters, shape (H,W). - The color path. - The depth path. \"\"\" depth_dir = os . path . join ( os . path . dirname ( rgb_path ), \"..\" , \"depth\" ) rgb_timestamp = int ( rgb_path [ - 16 : - 4 ]) # find closest depth image in time depth_paths = glob . glob ( depth_dir + \"/*.png\" ) depth_timestamps = np . array ([ int ( p [ - 16 : - 4 ]) for p in depth_paths ]) ind = np . argmin ( np . abs ( depth_timestamps - rgb_timestamp )) depth_path = depth_paths [ ind ] color_img = np . asarray ( o3d . io . read_image ( rgb_path ), dtype = np . float32 ) / 255 depth_img = ( np . asarray ( o3d . io . read_image ( depth_path ), dtype = np . float32 , ) * 0.001 ) return color_img , depth_img , rgb_path , depth_path","title":"load_redwood_rgbd()"},{"location":"reference/estimation/scripts/real_data/#sdfest.estimation.scripts.real_data.load_redwood_sample","text":"load_redwood_sample ( folder : str , ) -> Tuple [ np . ndarray , np . ndarray , str , str ] Load a sample from Redwood dataset. PARAMETER DESCRIPTION folder The root folder of the dataset. TYPE: str RETURNS DESCRIPTION Tuple [ np . ndarray , np . ndarray , str , str ] Tuple containing: - The color image, float32, RGB, 0-1, shape (H,W,C). - The depth image, float32, in meters, shape (H,W). - The color path. - The depth path. Source code in sdfest/estimation/scripts/real_data.py 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 def load_redwood_sample ( folder : str ) -> Tuple [ np . ndarray , np . ndarray , str , str ]: \"\"\"Load a sample from Redwood dataset. Args: folder: The root folder of the dataset. Returns: Tuple containing: - The color image, float32, RGB, 0-1, shape (H,W,C). - The depth image, float32, in meters, shape (H,W). - The color path. - The depth path. \"\"\" sequence_paths = glob . glob ( folder + \"/*\" ) sequence_path = random . choice ( sequence_paths ) rgb_dir = os . path . join ( sequence_path , \"rgb\" ) rgb_paths = glob . glob ( rgb_dir + \"/*.jpg\" ) rgb_path = random . choice ( rgb_paths ) return load_redwood_rgbd ( rgb_path )","title":"load_redwood_sample()"},{"location":"reference/estimation/scripts/real_data/#sdfest.estimation.scripts.real_data.load_sample_from_folder","text":"load_sample_from_folder ( config : dict , ) -> Tuple [ np . ndarray , np . ndarray , str , str ] Load a sample from dataset specified in config. See the dataset specific load functions for more details of the expected folder structure. PARAMETER DESCRIPTION config Configuration dictionary that must contain the following keys: \"dataset\": one of \"redwood\" | \"rgbd_object_uw\" \"folder\": the root folder of the dataset TYPE: dict RETURNS DESCRIPTION Tuple [ np . ndarray , np . ndarray , str , str ] Tuple containing: - The color image, float32, RGB, 0-1, shape (H,W,C). - The depth image, float32, in meters, shape (H,W). Source code in sdfest/estimation/scripts/real_data.py 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 def load_sample_from_folder ( config : dict ) -> Tuple [ np . ndarray , np . ndarray , str , str ]: \"\"\"Load a sample from dataset specified in config. See the dataset specific load functions for more details of the expected folder structure. Params: config: Configuration dictionary that must contain the following keys: \"dataset\": one of \"redwood\" | \"rgbd_object_uw\" \"folder\": the root folder of the dataset Returns: Tuple containing: - The color image, float32, RGB, 0-1, shape (H,W,C). - The depth image, float32, in meters, shape (H,W). \"\"\" if config [ \"dataset\" ] == \"redwood\" : return load_redwood_sample ( config [ \"folder\" ]) elif config [ \"dataset\" ] == \"rgbd_object_uw\" : return load_rgbd_object_uw_sample ( config [ \"folder\" ]) elif config [ \"dataset\" ] == \"real275\" : return load_real275_sample ( config [ \"folder\" ]) else : raise NotImplementedError ( f \"Dataset { config [ 'dataset' ] } is not supported\" )","title":"load_sample_from_folder()"},{"location":"reference/estimation/scripts/real_data/#sdfest.estimation.scripts.real_data.load_rgbd","text":"load_rgbd ( config : dict ) -> Tuple [ np . ndarray , np . ndarray , str , str ] Load a single RGB-D image from path and dataset specified in config. See the dataset specific load functions for more details of the expected folder structure. PARAMETER DESCRIPTION config Configuration dictionary that must contain the following keys: \"dataset\": one of \"redwood\" | \"rgbd_object_uw\" \"input\": the path to the RGB image TYPE: dict RETURNS DESCRIPTION Tuple [ np . ndarray , np . ndarray , str , str ] Tuple containing: - The color image, float32, RGB, 0-1, shape (H,W,C). - The depth image, float32, in meters, shape (H,W). - The color path. - The depth path. Source code in sdfest/estimation/scripts/real_data.py 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 def load_rgbd ( config : dict ) -> Tuple [ np . ndarray , np . ndarray , str , str ]: \"\"\"Load a single RGB-D image from path and dataset specified in config. See the dataset specific load functions for more details of the expected folder structure. Params: config: Configuration dictionary that must contain the following keys: \"dataset\": one of \"redwood\" | \"rgbd_object_uw\" \"input\": the path to the RGB image Returns: Tuple containing: - The color image, float32, RGB, 0-1, shape (H,W,C). - The depth image, float32, in meters, shape (H,W). - The color path. - The depth path. \"\"\" if config [ \"dataset\" ] == \"redwood\" : return load_redwood_rgbd ( config [ \"input\" ]) elif config [ \"dataset\" ] == \"rgbd_object_uw\" : return load_rgbd_object_uw_rgbd ( config [ \"input\" ]) elif config [ \"dataset\" ] == \"real275\" : return load_real275_rgbd ( config [ \"input\" ]) else : raise NotImplementedError ( f \"Dataset { config [ 'dataset' ] } is not supported\" )","title":"load_rgbd()"},{"location":"reference/estimation/scripts/real_data/#sdfest.estimation.scripts.real_data.str_to_bool","text":"str_to_bool ( v : str ) -> bool Try to convert string to boolean. From: https://stackoverflow.com/a/43357954 Source code in sdfest/estimation/scripts/real_data.py 271 272 273 274 275 276 277 278 279 280 281 282 283 def str_to_bool ( v : str ) -> bool : \"\"\"Try to convert string to boolean. From: https://stackoverflow.com/a/43357954 \"\"\" if isinstance ( v , bool ): return v if v . lower () in ( \"yes\" , \"true\" , \"t\" , \"y\" , \"1\" ): return True elif v . lower () in ( \"no\" , \"false\" , \"f\" , \"n\" , \"0\" ): return False else : raise argparse . ArgumentTypeError ( \"Boolean value expected.\" )","title":"str_to_bool()"},{"location":"reference/estimation/scripts/real_data/#sdfest.estimation.scripts.real_data.main","text":"main () -> None Entry point of the program. Source code in sdfest/estimation/scripts/real_data.py 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 def main () -> None : \"\"\"Entry point of the program.\"\"\" # define the arguments parser = argparse . ArgumentParser ( description = \"SDF pose estimation in real data\" ) # parse arguments parser . add_argument ( \"--device\" ) parser . add_argument ( \"--input\" ) parser . add_argument ( \"--folder\" ) parser . add_argument ( \"--measure_runtime\" , type = str_to_bool , default = False ) parser . add_argument ( \"--visualize_optimization\" , type = str_to_bool , default = False ) parser . add_argument ( \"--visualize_input\" , type = str_to_bool , default = False ) parser . add_argument ( \"--cached_segmentation\" , action = \"store_true\" ) parser . add_argument ( \"--segmentation_dir\" , default = \"./cached_segmentations/\" ) parser . add_argument ( \"--config\" , default = \"configs/default.yaml\" , nargs = \"+\" ) config = yoco . load_config_from_args ( parser , search_paths = [ \".\" , \"~/.sdfest/\" , sdfest . __path__ [ 0 ]] ) if \"input\" in config and \"folder\" in config : print ( \"Only one of input and folder can be specified.\" ) exit () if config [ \"measure_runtime\" ] and config [ \"visualize_optimization\" ]: print ( \"Visualization not supported while measuring runtime.\" ) exit () pipeline = SDFPipeline ( config ) create_animation = ( config [ \"create_animation\" ] if \"create_animation\" in config else False ) timing_dict = None timing_dicts = [] if config [ \"measure_runtime\" ]: timing_dict = add_timing_decorators ( pipeline ) # Segmentation using detectron2 print ( \"Loading segmentation model...\" ) cfg = detectron2 . config . get_cfg () cfg . merge_from_file ( model_zoo . get_config_file ( \"COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml\" ) ) cfg . MODEL . ROI_HEADS . SCORE_THRESH_TEST = 0.5 # set threshold for this model cfg . MODEL . WEIGHTS = model_zoo . get_checkpoint_url ( \"COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml\" ) predictor = detectron2 . engine . DefaultPredictor ( cfg ) print ( \"Segmentation model loaded.\" ) completed_runs = 0 shape_optimization = True while True : if timing_dict is not None : timing_dict . clear () if \"folder\" in config : color_img , depth_img , color_path , _ = load_sample_from_folder ( config ) elif \"input\" in config : color_img , depth_img , color_path , _ = load_rgbd ( config ) else : print ( \"No folder or input file specified.\" ) exit () if timing_dict is not None : timing_dict [ \"pipeline\" ] . append ([ time . time (), None ]) timing_dict [ \"segmentation\" ] . append ([ time . time (), None ]) if config [ \"cached_segmentation\" ]: # check if segmentation exists color_name , _ = os . path . splitext ( color_path ) color_dir = os . path . dirname ( color_name ) segmentation_dir = os . path . join ( config [ \"segmentation_dir\" ], color_dir ) segmentation_path = ( os . path . join ( config [ \"segmentation_dir\" ], color_name ) + \".pickle\" ) os . makedirs ( segmentation_dir , exist_ok = True ) if os . path . isfile ( segmentation_path ): with open ( segmentation_path , \"rb\" ) as f : outputs = pickle . load ( f ) else : # compute segmentation and save # detectron expects (H,C,W), BGR, 0-255 as input detectron_color_img = color_img [:, :, :: - 1 ] * 255 outputs = predictor ( detectron_color_img ) with open ( segmentation_path , \"wb\" ) as f : pickle . dump ( outputs , f ) else : # detectron expects (H,C,W), BGR, 0-255 as input detectron_color_img = color_img [:, :, :: - 1 ] * 255 outputs = predictor ( detectron_color_img ) if timing_dict is not None : torch . cuda . synchronize () timing_dict [ \"segmentation\" ][ - 1 ][ 1 ] = time . time () category_id = MetadataCatalog . get ( cfg . DATASETS . TRAIN [ 0 ]) . thing_classes . index ( config [ \"category\" ] ) matching_instances = [] for i in range ( len ( outputs [ \"instances\" ])): instance = outputs [ \"instances\" ][ i ] if instance . pred_classes != category_id : continue matching_instances . append ( instance ) matching_instances . sort ( key = lambda k : k . pred_masks . sum ()) if not matching_instances : print ( \"Warning: category not detected in input\" ) else : print ( \"Category detected\" ) for instance in matching_instances : if create_animation : animation_path = os . path . join ( os . getcwd (), f \"animation_ { datetime . now () . strftime ( '%Y-%m- %d _%H-%M-%S' ) } \" , ) else : animation_path = None if config [ \"visualize_input\" ]: v = Visualizer ( color_img * 255 , MetadataCatalog . get ( cfg . DATASETS . TRAIN [ 0 ]), scale = 1.2 , ) out = v . draw_instance_predictions ( instance . to ( \"cpu\" )) plt . imshow ( out . get_image ()) plt . show () plt . imshow ( depth_img ) plt . show () depth_tensor = torch . from_numpy ( depth_img ) . to ( config [ \"device\" ]) instance_mask = instance . pred_masks . cuda ()[ 0 ] color_img_tensor = torch . from_numpy ( color_img ) . to ( config [ \"device\" ]) try : position , orientation , scale , shape = pipeline ( depth_tensor , instance_mask , color_img_tensor , visualize = config [ \"visualize_optimization\" ], animation_path = animation_path , shape_optimization = shape_optimization , ) except NoDepthError : print ( \"No depth data, skipping\" ) break # comment to evaluate all instances, instead of largest only if timing_dict is not None : torch . cuda . synchronize () timing_dict [ \"pipeline\" ][ - 1 ][ 1 ] = time . time () if completed_runs != 0 or not config [ \"skip_first_run\" ]: timing_dicts . append ( copy . deepcopy ( timing_dict )) timing_dicts [ - 1 ][ \"shape_optimization\" ] = shape_optimization print ( f \" \\r { completed_runs + 1 } / { config [ 'runs' ] } \" , end = \"\" ) completed_runs += 1 # only run single evaluation for single file if \"input\" in config and not config [ \"measure_runtime\" ]: break elif config [ \"measure_runtime\" ] and completed_runs == config [ \"runs\" ]: if shape_optimization : shape_optimization = False completed_runs = 0 print ( \"\" ) else : print ( \"\" ) break if config [ \"measure_runtime\" ]: generate_runtime_overview ( config , timing_dicts )","title":"main()"},{"location":"reference/estimation/scripts/rendering_evaluation/","text":"sdfest.estimation.scripts.rendering_evaluation Script to run randomized rendering evaluation on synthetic data. Usage python rendering_evaluation.py --config configs/config.yaml --data_path ./data/ --out_folder ./results See configs/rendering_evaluation.yaml for all supported arguments. See simple_setup for pipeline parameters. Specific parameters log_folder: if passed each optimization step is logged and can be played back with play_log.py visualize_optimization: whether to visualize optimization while at it visualize_points: whether to show result pointclouds after optimization visualize_meshes: whether to show result mesh after optimization camera_distance: mesh distance from the camera num_views: list of number of views to evaluate for each mesh mesh_scale: the applied scale, see rel_scale rel_scale: if True, the original mesh will be scaled by mesh_scale, if False the original mesh will be scaled such that its largest extent is mesh_scale * 2 samples: number of evaluation samples ablation_configs: used to specify specific settings for ablation study dictionary, in which each key maps to a config dictionary which will be applied on existing settings metrics: dictionary of metrics to evaluate each key, is interpreted as the name of the metric, each value has to be a dict containing f and kwargs, where f is fully qualified name of the function to evaluate and kwargs is a dictionary of keyword arguments if applicable f gets ground truth points as first, and estimated points as second parameter seed: seed used for view generation and sampling of points Evaluator Class to evaluate SDF pipeline on synthetic data. Source code in sdfest/estimation/scripts/rendering_evaluation.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 class Evaluator : \"\"\"Class to evaluate SDF pipeline on synthetic data.\"\"\" def __init__ ( self , config : dict ) -> None : \"\"\"Construct evaluator and initialize pipeline.\"\"\" self . base_config = config self . cam = Camera ( ** self . base_config [ \"camera\" ]) def run ( self ) -> None : \"\"\"Run the evaluation.\"\"\" o3d . utility . set_verbosity_level ( o3d . utility . VerbosityLevel . Warning ) if self . base_config [ \"ablation_configs\" ] is not None : ablation_results_dict = {} for name , ablation_config in self . base_config [ \"ablation_configs\" ] . items (): config = yoco . load_config ( ablation_config , copy . deepcopy ( self . base_config ) ) self . _set_seed ( config [ \"seed\" ]) ablation_results_dict [ name ] = self . _evaluate_config ( config ) self . _save_and_print_results ( ablation_results_dict ) else : self . _set_seed ( self . base_config [ \"seed\" ]) results_dict = self . _evaluate_config ( self . base_config ) self . _save_and_print_results ( results_dict ) @staticmethod def _set_seed ( seed : int = 0 ) -> None : random . seed ( seed ) def _evaluate_config ( self , config : dict ) -> dict : results_dict = {} self . pipeline = SDFPipeline ( config ) for views in config [ \"num_views\" ]: metrics_list = [] files = glob_exts ( config [ \"data_path\" ], [ \".obj\" , \".off\" ]) files . sort () for file in tqdm ( files ): metrics = self . _evaluate_file ( file , views , config ) metrics_list . append ( metrics ) results_dict [ views ] = Evaluator . _compute_metric_statistics ( metrics_list ) return results_dict def _save_and_print_results ( self , results_dict : Dict ) -> None : \"\"\"Save results and config to yaml file and print results as table. Args: results_dict: dictionary containing the results that should be saved \"\"\" os . makedirs ( self . base_config [ \"out_folder\" ], exist_ok = True ) run_name = self . base_config [ \"run_name\" ] filename = ( f \"rend_eval_ { run_name } _ { datetime . now () . strftime ( '%Y-%m- %d _%H-%M-%S' ) } .yaml\" ) out_path = os . path . join ( self . base_config [ \"out_folder\" ], filename ) combined_dict = { ** self . base_config , \"results\" : results_dict } yoco . save_config_to_file ( out_path , combined_dict ) print ( f \"Results saved to: { out_path } \" ) @staticmethod def _compute_metric_statistics ( metrics_list : List ) -> Dict : \"\"\"Compute mean and standard deviation for each metric. Args: metrics_list: metric dictionaries as returned by _evaluate_file. Returns: Statistic for each metric in the provided metrics dictionaries. The returned dictionary keys will be the name of the metrics. Each value will be another dictionary containing the keys mean, var, and std. \"\"\" metric_stats = defaultdict ( lambda : { \"mean\" : 0 , \"var\" : 0 }) for metrics in metrics_list : for name , val in metrics . items (): metric_stats [ name ][ \"mean\" ] += val for _ , stats in metric_stats . items (): stats [ \"mean\" ] /= len ( metrics_list ) for metrics in metrics_list : for name , val in metrics . items (): metric_stats [ name ][ \"var\" ] += ( val - metric_stats [ name ][ \"mean\" ]) ** 2 for _ , stats in metric_stats . items (): stats [ \"var\" ] /= len ( metrics_list ) stats [ \"std\" ] = math . sqrt ( stats [ \"var\" ]) metric_stats = dict ( metric_stats ) return metric_stats def _generate_views ( self , mesh : synthetic . Mesh , num_views : int ) -> Dict : \"\"\"Generate random views around mesh. Args: mesh: mesh to generate views of, position and orientation will be assumed to be in world coordinates num_views: number of views to generate Returns: Dictionary containing the following keys. depth_images: the depth map containing the distance along the camera's z-axis, shape (num_views, H, W) masks: binary mask of the object to estimate, same shape as depth_images color_images: color images of objects to estimate, shape (num_views, H, W, 3) note that this is currently just zero camera_positions: position of camera in world coordinates for each image, shape (num_views, 3) camera_orientations: orientation of camera in world-frame as normalized quaternion, quaternion is in scalar-last convention, this is the quaternion that transforms a point from camera to world, shape (num_views, 4) \"\"\" views_dict = defaultdict ( lambda : list ()) mesh . position = np . array ([ 0.0 , 0.0 , 0.0 ], dtype = np . float32 ) mesh_position = torch . tensor ( mesh . position ) mesh_orientation = torch . tensor ( mesh . orientation ) for _ in range ( num_views ): while True : # OpenGL convention camera camera_orientation = generate_uniform_quaternion () # ogl to world camera_position = mesh_position - quaternion_utils . quaternion_apply ( camera_orientation , torch . tensor ([ 0 , 0 , - self . base_config [ \"camera_distance\" ]]), ) # transform camera position s.t. object lies on principal axis # Transform mesh into camera frame, now with Open3D convention camera camera_orientation_o3d = quaternion_utils . quaternion_multiply ( camera_orientation , # ogl to world torch . tensor ([ 1.0 , 0 , 0 , 0 ]), # o3d to ogl ) # quaternion: o3d to world mesh_orientation_cam = quaternion_utils . quaternion_multiply ( quaternion_utils . quaternion_invert ( camera_orientation_o3d ), # world to o3d mesh_orientation , # obj to world ) # quaternion: obj to o3d mesh . position = np . array ([ 0 , 0 , self . base_config [ \"camera_distance\" ]]) mesh . orientation = mesh_orientation_cam . numpy () depth_np = synthetic . draw_depth_geometry ( mesh , self . cam ) depth = torch . tensor ( depth_np ) if ( depth != 0 ) . any (): views_dict [ \"depth_images\" ] . append ( depth ) views_dict [ \"masks\" ] . append ( depth != 0 ) views_dict [ \"color_images\" ] . append ( torch . zeros ( depth . shape + ( 3 ,))) views_dict [ \"camera_positions\" ] . append ( camera_position ) views_dict [ \"camera_orientations\" ] . append ( camera_orientation ) break print ( \"Warning: invalid depth generated, skipping this sample\" ) mesh . position = mesh_position . numpy () mesh . orientation = mesh_orientation . numpy () return { k : torch . stack ( v ) . to ( self . base_config [ \"device\" ]) for k , v in views_dict . items () } def _evaluate_file ( self , path : str , num_views : int , config : dict ) -> dict : \"\"\"Evaluate a single mesh. This will generate depth images from a few views with the mesh centered, at fixed distance, to the camera. Args: path: The path of the obj file. num_views: The number of views to generate and use in the optimization. config: Configuration dictionary. Returns: Evaluation metrics as specified in config. \"\"\" gt_mesh = synthetic . Mesh ( path = path , scale = self . base_config [ \"mesh_scale\" ], rel_scale = self . base_config [ \"rel_scale\" ], center = True , ) inputs = self . _generate_views ( gt_mesh , num_views ) log_path = self . _get_log_path () position , orientation , scale , shape = self . pipeline ( ** inputs , visualize = self . base_config [ \"visualize_optimization\" ], log_path = log_path , shape_optimization = config [ \"shape_optimization\" ], ) out_mesh = self . pipeline . generate_mesh ( shape , scale , True ) # Output and ground truth are in world frame out_mesh . position = position [ 0 ] . detach () . cpu () . numpy () out_mesh . orientation = orientation [ 0 ] . detach () . cpu () . numpy () gt_mesh = gt_mesh . get_transformed_o3d_geometry () out_mesh = out_mesh . get_transformed_o3d_geometry () gt_mesh . paint_uniform_color ([ 0.7 , 0.4 , 0.2 ]) out_mesh . paint_uniform_color ([ 0.2 , 0.4 , 0.7 ]) gt_pts = gt_mesh . sample_points_uniformly ( number_of_points = self . base_config [ \"samples\" ], seed = self . base_config [ \"seed\" ] ) out_pts = out_mesh . sample_points_uniformly ( number_of_points = self . base_config [ \"samples\" ], seed = self . base_config [ \"seed\" ] ) gt_pts_np = np . asarray ( gt_pts . points ) out_pts_np = np . asarray ( out_pts . points ) metric_dict = {} for metric_name , m in self . base_config [ \"metrics\" ] . items (): metric_dict [ metric_name ] = float ( locate ( m [ \"f\" ])( gt_pts_np , out_pts_np , ** m [ \"kwargs\" ]) ) self . visualize_result ( gt_mesh , out_mesh , gt_pts , out_pts , inputs ) return metric_dict def visualize_result ( self , mesh_1 : o3d . geometry . TriangleMesh , mesh_2 : o3d . geometry . TriangleMesh , pts_1 : o3d . geometry . PointCloud , pts_2 : o3d . geometry . PointCloud , inputs : Optional [ dict ] = None , ) -> None : \"\"\"Visualize result of a single evaluation. Args: mesh_1: the first mesh to visualize mesh_2: the second mesh to visualize pts_1: the first pointcloud to visualize pts_2: the second pointcloud to visualize inputs: the input dictionary as producted by Evaluator._generate_view \"\"\" # generate coordinate frames of cameras cam_meshes = [] if inputs is not None : # visualize OpenGL convention camera for t_c2w , quat_c2w in zip ( inputs [ \"camera_positions\" ], inputs [ \"camera_orientations\" ] ): frame_mesh = synthetic . Mesh ( mesh = o3d . geometry . TriangleMesh . create_coordinate_frame ( size = 0.1 , origin = [ 0 , 0 , 0 ] ), rel_scale = True , ) frame_mesh . position = t_c2w . cpu () . numpy () frame_mesh . orientation = quat_c2w . cpu () . numpy () cam_meshes . append ( frame_mesh . get_transformed_o3d_geometry ()) if self . base_config [ \"visualize_meshes\" ]: # Visualize result o3d . visualization . draw_geometries ( [ mesh_1 , mesh_2 , ] + cam_meshes ) time . sleep ( 0.1 ) if self . base_config [ \"visualize_points\" ]: o3d . visualization . draw_geometries ([ pts_1 , pts_2 ] + cam_meshes ) time . sleep ( 0.1 ) def _get_log_path ( self ) -> Optional [ str ]: \"\"\"Return unique filename in log folder. If log path is None, None will be returned. \"\"\" log_path = None if self . base_config [ \"log_folder\" ] is not None : os . makedirs ( self . base_config [ \"log_folder\" ], exist_ok = True ) filename = f \"log_ { datetime . now () . strftime ( '%Y-%m- %d _%H-%M-%S' ) } .pkl\" log_path = os . path . join ( self . base_config [ \"log_folder\" ], filename ) return log_path __init__ __init__ ( config : dict ) -> None Construct evaluator and initialize pipeline. Source code in sdfest/estimation/scripts/rendering_evaluation.py 86 87 88 89 def __init__ ( self , config : dict ) -> None : \"\"\"Construct evaluator and initialize pipeline.\"\"\" self . base_config = config self . cam = Camera ( ** self . base_config [ \"camera\" ]) run run () -> None Run the evaluation. Source code in sdfest/estimation/scripts/rendering_evaluation.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def run ( self ) -> None : \"\"\"Run the evaluation.\"\"\" o3d . utility . set_verbosity_level ( o3d . utility . VerbosityLevel . Warning ) if self . base_config [ \"ablation_configs\" ] is not None : ablation_results_dict = {} for name , ablation_config in self . base_config [ \"ablation_configs\" ] . items (): config = yoco . load_config ( ablation_config , copy . deepcopy ( self . base_config ) ) self . _set_seed ( config [ \"seed\" ]) ablation_results_dict [ name ] = self . _evaluate_config ( config ) self . _save_and_print_results ( ablation_results_dict ) else : self . _set_seed ( self . base_config [ \"seed\" ]) results_dict = self . _evaluate_config ( self . base_config ) self . _save_and_print_results ( results_dict ) visualize_result visualize_result ( mesh_1 : o3d . geometry . TriangleMesh , mesh_2 : o3d . geometry . TriangleMesh , pts_1 : o3d . geometry . PointCloud , pts_2 : o3d . geometry . PointCloud , inputs : Optional [ dict ] = None , ) -> None Visualize result of a single evaluation. PARAMETER DESCRIPTION mesh_1 the first mesh to visualize TYPE: o3d . geometry . TriangleMesh mesh_2 the second mesh to visualize TYPE: o3d . geometry . TriangleMesh pts_1 the first pointcloud to visualize TYPE: o3d . geometry . PointCloud pts_2 the second pointcloud to visualize TYPE: o3d . geometry . PointCloud inputs the input dictionary as producted by Evaluator._generate_view TYPE: Optional [ dict ] DEFAULT: None Source code in sdfest/estimation/scripts/rendering_evaluation.py 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 def visualize_result ( self , mesh_1 : o3d . geometry . TriangleMesh , mesh_2 : o3d . geometry . TriangleMesh , pts_1 : o3d . geometry . PointCloud , pts_2 : o3d . geometry . PointCloud , inputs : Optional [ dict ] = None , ) -> None : \"\"\"Visualize result of a single evaluation. Args: mesh_1: the first mesh to visualize mesh_2: the second mesh to visualize pts_1: the first pointcloud to visualize pts_2: the second pointcloud to visualize inputs: the input dictionary as producted by Evaluator._generate_view \"\"\" # generate coordinate frames of cameras cam_meshes = [] if inputs is not None : # visualize OpenGL convention camera for t_c2w , quat_c2w in zip ( inputs [ \"camera_positions\" ], inputs [ \"camera_orientations\" ] ): frame_mesh = synthetic . Mesh ( mesh = o3d . geometry . TriangleMesh . create_coordinate_frame ( size = 0.1 , origin = [ 0 , 0 , 0 ] ), rel_scale = True , ) frame_mesh . position = t_c2w . cpu () . numpy () frame_mesh . orientation = quat_c2w . cpu () . numpy () cam_meshes . append ( frame_mesh . get_transformed_o3d_geometry ()) if self . base_config [ \"visualize_meshes\" ]: # Visualize result o3d . visualization . draw_geometries ( [ mesh_1 , mesh_2 , ] + cam_meshes ) time . sleep ( 0.1 ) if self . base_config [ \"visualize_points\" ]: o3d . visualization . draw_geometries ([ pts_1 , pts_2 ] + cam_meshes ) time . sleep ( 0.1 ) glob_exts glob_exts ( path : str , exts : List [ str ]) -> List [ str ] Return all files in a nested directory with extensions matching. Directory is scanned recursively. PARAMETER DESCRIPTION path root path to search TYPE: str exts extensions that will be checked, must include separator (e.g., \".obj\") TYPE: List [ str ] RETURNS DESCRIPTION List [ str ] List of paths in the directory with matching extension. Source code in sdfest/estimation/scripts/rendering_evaluation.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def glob_exts ( path : str , exts : List [ str ]) -> List [ str ]: \"\"\"Return all files in a nested directory with extensions matching. Directory is scanned recursively. Args: path: root path to search exts: extensions that will be checked, must include separator (e.g., \".obj\") Returns: List of paths in the directory with matching extension. \"\"\" files = [] for ext in exts : files . extend ( glob . glob ( os . path . join ( path , f \"**/* { ext } \" ), recursive = True )) return files generate_uniform_quaternion generate_uniform_quaternion () -> torch . Tensor Generate a normalized uniform quaternion. Following the method from K. Shoemake, Uniform Random Rotations, 1992. See: http://planning.cs.uiuc.edu/node198.html RETURNS DESCRIPTION torch . Tensor Uniformly distributed unit quaternion on the estimator's device. Source code in sdfest/estimation/scripts/rendering_evaluation.py 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 def generate_uniform_quaternion () -> torch . Tensor : \"\"\"Generate a normalized uniform quaternion. Following the method from K. Shoemake, Uniform Random Rotations, 1992. See: http://planning.cs.uiuc.edu/node198.html Returns: Uniformly distributed unit quaternion on the estimator's device. \"\"\" u1 , u2 , u3 = random . random (), random . random (), random . random () return torch . tensor ( [ math . sqrt ( 1 - u1 ) * math . sin ( 2 * math . pi * u2 ), math . sqrt ( 1 - u1 ) * math . cos ( 2 * math . pi * u2 ), math . sqrt ( u1 ) * math . sin ( 2 * math . pi * u3 ), math . sqrt ( u1 ) * math . cos ( 2 * math . pi * u3 ), ] ) main main () -> None Entry point of the evaluation program. Source code in sdfest/estimation/scripts/rendering_evaluation.py 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 def main () -> None : \"\"\"Entry point of the evaluation program.\"\"\" parser = argparse . ArgumentParser ( description = \"SDF shape and pose estimation evaluation on synthetic data\" ) parser . add_argument ( \"--config\" , default = \"configs/rendering_evaluation.yaml\" , nargs = \"+\" ) parser . add_argument ( \"--data_path\" , required = True ) parser . add_argument ( \"--out_folder\" , required = True ) config = yoco . load_config_from_args ( parser , search_paths = [ \".\" , \"~/.sdfest/\" , sdfest . __path__ [ 0 ]] ) evaluator = Evaluator ( config ) evaluator . run ()","title":"rendering_evaluation"},{"location":"reference/estimation/scripts/rendering_evaluation/#sdfest.estimation.scripts.rendering_evaluation","text":"Script to run randomized rendering evaluation on synthetic data. Usage python rendering_evaluation.py --config configs/config.yaml --data_path ./data/ --out_folder ./results See configs/rendering_evaluation.yaml for all supported arguments. See simple_setup for pipeline parameters. Specific parameters log_folder: if passed each optimization step is logged and can be played back with play_log.py visualize_optimization: whether to visualize optimization while at it visualize_points: whether to show result pointclouds after optimization visualize_meshes: whether to show result mesh after optimization camera_distance: mesh distance from the camera num_views: list of number of views to evaluate for each mesh mesh_scale: the applied scale, see rel_scale rel_scale: if True, the original mesh will be scaled by mesh_scale, if False the original mesh will be scaled such that its largest extent is mesh_scale * 2 samples: number of evaluation samples ablation_configs: used to specify specific settings for ablation study dictionary, in which each key maps to a config dictionary which will be applied on existing settings metrics: dictionary of metrics to evaluate each key, is interpreted as the name of the metric, each value has to be a dict containing f and kwargs, where f is fully qualified name of the function to evaluate and kwargs is a dictionary of keyword arguments if applicable f gets ground truth points as first, and estimated points as second parameter seed: seed used for view generation and sampling of points","title":"rendering_evaluation"},{"location":"reference/estimation/scripts/rendering_evaluation/#sdfest.estimation.scripts.rendering_evaluation.Evaluator","text":"Class to evaluate SDF pipeline on synthetic data. Source code in sdfest/estimation/scripts/rendering_evaluation.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 class Evaluator : \"\"\"Class to evaluate SDF pipeline on synthetic data.\"\"\" def __init__ ( self , config : dict ) -> None : \"\"\"Construct evaluator and initialize pipeline.\"\"\" self . base_config = config self . cam = Camera ( ** self . base_config [ \"camera\" ]) def run ( self ) -> None : \"\"\"Run the evaluation.\"\"\" o3d . utility . set_verbosity_level ( o3d . utility . VerbosityLevel . Warning ) if self . base_config [ \"ablation_configs\" ] is not None : ablation_results_dict = {} for name , ablation_config in self . base_config [ \"ablation_configs\" ] . items (): config = yoco . load_config ( ablation_config , copy . deepcopy ( self . base_config ) ) self . _set_seed ( config [ \"seed\" ]) ablation_results_dict [ name ] = self . _evaluate_config ( config ) self . _save_and_print_results ( ablation_results_dict ) else : self . _set_seed ( self . base_config [ \"seed\" ]) results_dict = self . _evaluate_config ( self . base_config ) self . _save_and_print_results ( results_dict ) @staticmethod def _set_seed ( seed : int = 0 ) -> None : random . seed ( seed ) def _evaluate_config ( self , config : dict ) -> dict : results_dict = {} self . pipeline = SDFPipeline ( config ) for views in config [ \"num_views\" ]: metrics_list = [] files = glob_exts ( config [ \"data_path\" ], [ \".obj\" , \".off\" ]) files . sort () for file in tqdm ( files ): metrics = self . _evaluate_file ( file , views , config ) metrics_list . append ( metrics ) results_dict [ views ] = Evaluator . _compute_metric_statistics ( metrics_list ) return results_dict def _save_and_print_results ( self , results_dict : Dict ) -> None : \"\"\"Save results and config to yaml file and print results as table. Args: results_dict: dictionary containing the results that should be saved \"\"\" os . makedirs ( self . base_config [ \"out_folder\" ], exist_ok = True ) run_name = self . base_config [ \"run_name\" ] filename = ( f \"rend_eval_ { run_name } _ { datetime . now () . strftime ( '%Y-%m- %d _%H-%M-%S' ) } .yaml\" ) out_path = os . path . join ( self . base_config [ \"out_folder\" ], filename ) combined_dict = { ** self . base_config , \"results\" : results_dict } yoco . save_config_to_file ( out_path , combined_dict ) print ( f \"Results saved to: { out_path } \" ) @staticmethod def _compute_metric_statistics ( metrics_list : List ) -> Dict : \"\"\"Compute mean and standard deviation for each metric. Args: metrics_list: metric dictionaries as returned by _evaluate_file. Returns: Statistic for each metric in the provided metrics dictionaries. The returned dictionary keys will be the name of the metrics. Each value will be another dictionary containing the keys mean, var, and std. \"\"\" metric_stats = defaultdict ( lambda : { \"mean\" : 0 , \"var\" : 0 }) for metrics in metrics_list : for name , val in metrics . items (): metric_stats [ name ][ \"mean\" ] += val for _ , stats in metric_stats . items (): stats [ \"mean\" ] /= len ( metrics_list ) for metrics in metrics_list : for name , val in metrics . items (): metric_stats [ name ][ \"var\" ] += ( val - metric_stats [ name ][ \"mean\" ]) ** 2 for _ , stats in metric_stats . items (): stats [ \"var\" ] /= len ( metrics_list ) stats [ \"std\" ] = math . sqrt ( stats [ \"var\" ]) metric_stats = dict ( metric_stats ) return metric_stats def _generate_views ( self , mesh : synthetic . Mesh , num_views : int ) -> Dict : \"\"\"Generate random views around mesh. Args: mesh: mesh to generate views of, position and orientation will be assumed to be in world coordinates num_views: number of views to generate Returns: Dictionary containing the following keys. depth_images: the depth map containing the distance along the camera's z-axis, shape (num_views, H, W) masks: binary mask of the object to estimate, same shape as depth_images color_images: color images of objects to estimate, shape (num_views, H, W, 3) note that this is currently just zero camera_positions: position of camera in world coordinates for each image, shape (num_views, 3) camera_orientations: orientation of camera in world-frame as normalized quaternion, quaternion is in scalar-last convention, this is the quaternion that transforms a point from camera to world, shape (num_views, 4) \"\"\" views_dict = defaultdict ( lambda : list ()) mesh . position = np . array ([ 0.0 , 0.0 , 0.0 ], dtype = np . float32 ) mesh_position = torch . tensor ( mesh . position ) mesh_orientation = torch . tensor ( mesh . orientation ) for _ in range ( num_views ): while True : # OpenGL convention camera camera_orientation = generate_uniform_quaternion () # ogl to world camera_position = mesh_position - quaternion_utils . quaternion_apply ( camera_orientation , torch . tensor ([ 0 , 0 , - self . base_config [ \"camera_distance\" ]]), ) # transform camera position s.t. object lies on principal axis # Transform mesh into camera frame, now with Open3D convention camera camera_orientation_o3d = quaternion_utils . quaternion_multiply ( camera_orientation , # ogl to world torch . tensor ([ 1.0 , 0 , 0 , 0 ]), # o3d to ogl ) # quaternion: o3d to world mesh_orientation_cam = quaternion_utils . quaternion_multiply ( quaternion_utils . quaternion_invert ( camera_orientation_o3d ), # world to o3d mesh_orientation , # obj to world ) # quaternion: obj to o3d mesh . position = np . array ([ 0 , 0 , self . base_config [ \"camera_distance\" ]]) mesh . orientation = mesh_orientation_cam . numpy () depth_np = synthetic . draw_depth_geometry ( mesh , self . cam ) depth = torch . tensor ( depth_np ) if ( depth != 0 ) . any (): views_dict [ \"depth_images\" ] . append ( depth ) views_dict [ \"masks\" ] . append ( depth != 0 ) views_dict [ \"color_images\" ] . append ( torch . zeros ( depth . shape + ( 3 ,))) views_dict [ \"camera_positions\" ] . append ( camera_position ) views_dict [ \"camera_orientations\" ] . append ( camera_orientation ) break print ( \"Warning: invalid depth generated, skipping this sample\" ) mesh . position = mesh_position . numpy () mesh . orientation = mesh_orientation . numpy () return { k : torch . stack ( v ) . to ( self . base_config [ \"device\" ]) for k , v in views_dict . items () } def _evaluate_file ( self , path : str , num_views : int , config : dict ) -> dict : \"\"\"Evaluate a single mesh. This will generate depth images from a few views with the mesh centered, at fixed distance, to the camera. Args: path: The path of the obj file. num_views: The number of views to generate and use in the optimization. config: Configuration dictionary. Returns: Evaluation metrics as specified in config. \"\"\" gt_mesh = synthetic . Mesh ( path = path , scale = self . base_config [ \"mesh_scale\" ], rel_scale = self . base_config [ \"rel_scale\" ], center = True , ) inputs = self . _generate_views ( gt_mesh , num_views ) log_path = self . _get_log_path () position , orientation , scale , shape = self . pipeline ( ** inputs , visualize = self . base_config [ \"visualize_optimization\" ], log_path = log_path , shape_optimization = config [ \"shape_optimization\" ], ) out_mesh = self . pipeline . generate_mesh ( shape , scale , True ) # Output and ground truth are in world frame out_mesh . position = position [ 0 ] . detach () . cpu () . numpy () out_mesh . orientation = orientation [ 0 ] . detach () . cpu () . numpy () gt_mesh = gt_mesh . get_transformed_o3d_geometry () out_mesh = out_mesh . get_transformed_o3d_geometry () gt_mesh . paint_uniform_color ([ 0.7 , 0.4 , 0.2 ]) out_mesh . paint_uniform_color ([ 0.2 , 0.4 , 0.7 ]) gt_pts = gt_mesh . sample_points_uniformly ( number_of_points = self . base_config [ \"samples\" ], seed = self . base_config [ \"seed\" ] ) out_pts = out_mesh . sample_points_uniformly ( number_of_points = self . base_config [ \"samples\" ], seed = self . base_config [ \"seed\" ] ) gt_pts_np = np . asarray ( gt_pts . points ) out_pts_np = np . asarray ( out_pts . points ) metric_dict = {} for metric_name , m in self . base_config [ \"metrics\" ] . items (): metric_dict [ metric_name ] = float ( locate ( m [ \"f\" ])( gt_pts_np , out_pts_np , ** m [ \"kwargs\" ]) ) self . visualize_result ( gt_mesh , out_mesh , gt_pts , out_pts , inputs ) return metric_dict def visualize_result ( self , mesh_1 : o3d . geometry . TriangleMesh , mesh_2 : o3d . geometry . TriangleMesh , pts_1 : o3d . geometry . PointCloud , pts_2 : o3d . geometry . PointCloud , inputs : Optional [ dict ] = None , ) -> None : \"\"\"Visualize result of a single evaluation. Args: mesh_1: the first mesh to visualize mesh_2: the second mesh to visualize pts_1: the first pointcloud to visualize pts_2: the second pointcloud to visualize inputs: the input dictionary as producted by Evaluator._generate_view \"\"\" # generate coordinate frames of cameras cam_meshes = [] if inputs is not None : # visualize OpenGL convention camera for t_c2w , quat_c2w in zip ( inputs [ \"camera_positions\" ], inputs [ \"camera_orientations\" ] ): frame_mesh = synthetic . Mesh ( mesh = o3d . geometry . TriangleMesh . create_coordinate_frame ( size = 0.1 , origin = [ 0 , 0 , 0 ] ), rel_scale = True , ) frame_mesh . position = t_c2w . cpu () . numpy () frame_mesh . orientation = quat_c2w . cpu () . numpy () cam_meshes . append ( frame_mesh . get_transformed_o3d_geometry ()) if self . base_config [ \"visualize_meshes\" ]: # Visualize result o3d . visualization . draw_geometries ( [ mesh_1 , mesh_2 , ] + cam_meshes ) time . sleep ( 0.1 ) if self . base_config [ \"visualize_points\" ]: o3d . visualization . draw_geometries ([ pts_1 , pts_2 ] + cam_meshes ) time . sleep ( 0.1 ) def _get_log_path ( self ) -> Optional [ str ]: \"\"\"Return unique filename in log folder. If log path is None, None will be returned. \"\"\" log_path = None if self . base_config [ \"log_folder\" ] is not None : os . makedirs ( self . base_config [ \"log_folder\" ], exist_ok = True ) filename = f \"log_ { datetime . now () . strftime ( '%Y-%m- %d _%H-%M-%S' ) } .pkl\" log_path = os . path . join ( self . base_config [ \"log_folder\" ], filename ) return log_path","title":"Evaluator"},{"location":"reference/estimation/scripts/rendering_evaluation/#sdfest.estimation.scripts.rendering_evaluation.Evaluator.__init__","text":"__init__ ( config : dict ) -> None Construct evaluator and initialize pipeline. Source code in sdfest/estimation/scripts/rendering_evaluation.py 86 87 88 89 def __init__ ( self , config : dict ) -> None : \"\"\"Construct evaluator and initialize pipeline.\"\"\" self . base_config = config self . cam = Camera ( ** self . base_config [ \"camera\" ])","title":"__init__()"},{"location":"reference/estimation/scripts/rendering_evaluation/#sdfest.estimation.scripts.rendering_evaluation.Evaluator.run","text":"run () -> None Run the evaluation. Source code in sdfest/estimation/scripts/rendering_evaluation.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def run ( self ) -> None : \"\"\"Run the evaluation.\"\"\" o3d . utility . set_verbosity_level ( o3d . utility . VerbosityLevel . Warning ) if self . base_config [ \"ablation_configs\" ] is not None : ablation_results_dict = {} for name , ablation_config in self . base_config [ \"ablation_configs\" ] . items (): config = yoco . load_config ( ablation_config , copy . deepcopy ( self . base_config ) ) self . _set_seed ( config [ \"seed\" ]) ablation_results_dict [ name ] = self . _evaluate_config ( config ) self . _save_and_print_results ( ablation_results_dict ) else : self . _set_seed ( self . base_config [ \"seed\" ]) results_dict = self . _evaluate_config ( self . base_config ) self . _save_and_print_results ( results_dict )","title":"run()"},{"location":"reference/estimation/scripts/rendering_evaluation/#sdfest.estimation.scripts.rendering_evaluation.Evaluator.visualize_result","text":"visualize_result ( mesh_1 : o3d . geometry . TriangleMesh , mesh_2 : o3d . geometry . TriangleMesh , pts_1 : o3d . geometry . PointCloud , pts_2 : o3d . geometry . PointCloud , inputs : Optional [ dict ] = None , ) -> None Visualize result of a single evaluation. PARAMETER DESCRIPTION mesh_1 the first mesh to visualize TYPE: o3d . geometry . TriangleMesh mesh_2 the second mesh to visualize TYPE: o3d . geometry . TriangleMesh pts_1 the first pointcloud to visualize TYPE: o3d . geometry . PointCloud pts_2 the second pointcloud to visualize TYPE: o3d . geometry . PointCloud inputs the input dictionary as producted by Evaluator._generate_view TYPE: Optional [ dict ] DEFAULT: None Source code in sdfest/estimation/scripts/rendering_evaluation.py 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 def visualize_result ( self , mesh_1 : o3d . geometry . TriangleMesh , mesh_2 : o3d . geometry . TriangleMesh , pts_1 : o3d . geometry . PointCloud , pts_2 : o3d . geometry . PointCloud , inputs : Optional [ dict ] = None , ) -> None : \"\"\"Visualize result of a single evaluation. Args: mesh_1: the first mesh to visualize mesh_2: the second mesh to visualize pts_1: the first pointcloud to visualize pts_2: the second pointcloud to visualize inputs: the input dictionary as producted by Evaluator._generate_view \"\"\" # generate coordinate frames of cameras cam_meshes = [] if inputs is not None : # visualize OpenGL convention camera for t_c2w , quat_c2w in zip ( inputs [ \"camera_positions\" ], inputs [ \"camera_orientations\" ] ): frame_mesh = synthetic . Mesh ( mesh = o3d . geometry . TriangleMesh . create_coordinate_frame ( size = 0.1 , origin = [ 0 , 0 , 0 ] ), rel_scale = True , ) frame_mesh . position = t_c2w . cpu () . numpy () frame_mesh . orientation = quat_c2w . cpu () . numpy () cam_meshes . append ( frame_mesh . get_transformed_o3d_geometry ()) if self . base_config [ \"visualize_meshes\" ]: # Visualize result o3d . visualization . draw_geometries ( [ mesh_1 , mesh_2 , ] + cam_meshes ) time . sleep ( 0.1 ) if self . base_config [ \"visualize_points\" ]: o3d . visualization . draw_geometries ([ pts_1 , pts_2 ] + cam_meshes ) time . sleep ( 0.1 )","title":"visualize_result()"},{"location":"reference/estimation/scripts/rendering_evaluation/#sdfest.estimation.scripts.rendering_evaluation.glob_exts","text":"glob_exts ( path : str , exts : List [ str ]) -> List [ str ] Return all files in a nested directory with extensions matching. Directory is scanned recursively. PARAMETER DESCRIPTION path root path to search TYPE: str exts extensions that will be checked, must include separator (e.g., \".obj\") TYPE: List [ str ] RETURNS DESCRIPTION List [ str ] List of paths in the directory with matching extension. Source code in sdfest/estimation/scripts/rendering_evaluation.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def glob_exts ( path : str , exts : List [ str ]) -> List [ str ]: \"\"\"Return all files in a nested directory with extensions matching. Directory is scanned recursively. Args: path: root path to search exts: extensions that will be checked, must include separator (e.g., \".obj\") Returns: List of paths in the directory with matching extension. \"\"\" files = [] for ext in exts : files . extend ( glob . glob ( os . path . join ( path , f \"**/* { ext } \" ), recursive = True )) return files","title":"glob_exts()"},{"location":"reference/estimation/scripts/rendering_evaluation/#sdfest.estimation.scripts.rendering_evaluation.generate_uniform_quaternion","text":"generate_uniform_quaternion () -> torch . Tensor Generate a normalized uniform quaternion. Following the method from K. Shoemake, Uniform Random Rotations, 1992. See: http://planning.cs.uiuc.edu/node198.html RETURNS DESCRIPTION torch . Tensor Uniformly distributed unit quaternion on the estimator's device. Source code in sdfest/estimation/scripts/rendering_evaluation.py 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 def generate_uniform_quaternion () -> torch . Tensor : \"\"\"Generate a normalized uniform quaternion. Following the method from K. Shoemake, Uniform Random Rotations, 1992. See: http://planning.cs.uiuc.edu/node198.html Returns: Uniformly distributed unit quaternion on the estimator's device. \"\"\" u1 , u2 , u3 = random . random (), random . random (), random . random () return torch . tensor ( [ math . sqrt ( 1 - u1 ) * math . sin ( 2 * math . pi * u2 ), math . sqrt ( 1 - u1 ) * math . cos ( 2 * math . pi * u2 ), math . sqrt ( u1 ) * math . sin ( 2 * math . pi * u3 ), math . sqrt ( u1 ) * math . cos ( 2 * math . pi * u3 ), ] )","title":"generate_uniform_quaternion()"},{"location":"reference/estimation/scripts/rendering_evaluation/#sdfest.estimation.scripts.rendering_evaluation.main","text":"main () -> None Entry point of the evaluation program. Source code in sdfest/estimation/scripts/rendering_evaluation.py 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 def main () -> None : \"\"\"Entry point of the evaluation program.\"\"\" parser = argparse . ArgumentParser ( description = \"SDF shape and pose estimation evaluation on synthetic data\" ) parser . add_argument ( \"--config\" , default = \"configs/rendering_evaluation.yaml\" , nargs = \"+\" ) parser . add_argument ( \"--data_path\" , required = True ) parser . add_argument ( \"--out_folder\" , required = True ) config = yoco . load_config_from_args ( parser , search_paths = [ \".\" , \"~/.sdfest/\" , sdfest . __path__ [ 0 ]] ) evaluator = Evaluator ( config ) evaluator . run ()","title":"main()"},{"location":"reference/initialization/pointnet/","text":"sdfest.initialization.pointnet Parametrized PointNet. VanillaPointNet Bases: nn . Module Parametrized PointNet without transformation layers (no T-nets). Generally following PointNet Deep Learning on Point Sets for 3D Classification and Segmentation Qi, 2017 Source code in sdfest/initialization/pointnet.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 class VanillaPointNet ( nn . Module ): \"\"\"Parametrized PointNet without transformation layers (no T-nets). Generally following: PointNet Deep Learning on Point Sets for 3D Classification and Segmentation Qi, 2017 \"\"\" def __init__ ( self , in_size : int , mlp_out_sizes : List , batchnorm : bool , residual : bool = False , dense : bool = False , ) -> None : \"\"\"Initialize the VanillaPointNet module. This module will only implements the MLP + MaxPooling part of the pointnet. It still requires a task specific head. Args: in_size: dimension of the input points mlp_out_sizes: output sizes of each linear layer batchnorm: whether to use batchnorm or not \"\"\" super () . __init__ () self . _in_size = in_size self . _mlp_out_sizes = mlp_out_sizes self . _batchnorm = batchnorm self . _residual = residual self . _dense = dense # define layers self . _linear_layers = torch . nn . ModuleList ([]) for i , out_size in enumerate ( mlp_out_sizes ): if i == 0 : self . _linear_layers . append ( nn . Linear ( self . _in_size , out_size )) else : if dense : self . _linear_layers . append ( nn . Linear ( 2 * mlp_out_sizes [ i - 1 ], out_size ) ) else : self . _linear_layers . append ( nn . Linear ( mlp_out_sizes [ i - 1 ], out_size ) ) self . _bn_layers = torch . nn . ModuleList ([]) if self . _batchnorm : for out_size in mlp_out_sizes : self . _bn_layers . append ( nn . BatchNorm1d ( out_size )) def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\"Forward pass of the module. Input has dimension NxMxC, where N is the batch size, M the number of points per set, and C the number of channels per point. Args: x: batch of point sets \"\"\" set_size = x . shape [ 1 ] out = prev_out = x for i , linear_layer in enumerate ( self . _linear_layers ): out = linear_layer ( out ) if self . _batchnorm : # BN over channels across all points and sets pts_per_set = out . shape [ 1 ] out_view = out . view ( - 1 , self . _mlp_out_sizes [ i ]) out = self . _bn_layers [ i ]( out_view ) out = out . view ( - 1 , pts_per_set , self . _mlp_out_sizes [ i ]) out = nn . functional . relu ( out ) if self . _dense : out_max , _ = torch . max ( out , 1 , keepdim = True ) if i != len ( self . _linear_layers ) - 1 : out = torch . cat (( out , out_max . expand ( - 1 , set_size , - 1 )), dim = 2 ) if self . _residual : if prev_out . shape == out . shape : out = prev_out + out prev_out = out # Maximum over points in same set out , _ = torch . max ( out , 1 ) return out __init__ __init__ ( in_size : int , mlp_out_sizes : List , batchnorm : bool , residual : bool = False , dense : bool = False , ) -> None Initialize the VanillaPointNet module. This module will only implements the MLP + MaxPooling part of the pointnet. It still requires a task specific head. PARAMETER DESCRIPTION in_size dimension of the input points TYPE: int mlp_out_sizes output sizes of each linear layer TYPE: List batchnorm whether to use batchnorm or not TYPE: bool Source code in sdfest/initialization/pointnet.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def __init__ ( self , in_size : int , mlp_out_sizes : List , batchnorm : bool , residual : bool = False , dense : bool = False , ) -> None : \"\"\"Initialize the VanillaPointNet module. This module will only implements the MLP + MaxPooling part of the pointnet. It still requires a task specific head. Args: in_size: dimension of the input points mlp_out_sizes: output sizes of each linear layer batchnorm: whether to use batchnorm or not \"\"\" super () . __init__ () self . _in_size = in_size self . _mlp_out_sizes = mlp_out_sizes self . _batchnorm = batchnorm self . _residual = residual self . _dense = dense # define layers self . _linear_layers = torch . nn . ModuleList ([]) for i , out_size in enumerate ( mlp_out_sizes ): if i == 0 : self . _linear_layers . append ( nn . Linear ( self . _in_size , out_size )) else : if dense : self . _linear_layers . append ( nn . Linear ( 2 * mlp_out_sizes [ i - 1 ], out_size ) ) else : self . _linear_layers . append ( nn . Linear ( mlp_out_sizes [ i - 1 ], out_size ) ) self . _bn_layers = torch . nn . ModuleList ([]) if self . _batchnorm : for out_size in mlp_out_sizes : self . _bn_layers . append ( nn . BatchNorm1d ( out_size )) forward forward ( x : torch . Tensor ) -> torch . Tensor Forward pass of the module. Input has dimension NxMxC, where N is the batch size, M the number of points per set, and C the number of channels per point. PARAMETER DESCRIPTION x batch of point sets TYPE: torch . Tensor Source code in sdfest/initialization/pointnet.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\"Forward pass of the module. Input has dimension NxMxC, where N is the batch size, M the number of points per set, and C the number of channels per point. Args: x: batch of point sets \"\"\" set_size = x . shape [ 1 ] out = prev_out = x for i , linear_layer in enumerate ( self . _linear_layers ): out = linear_layer ( out ) if self . _batchnorm : # BN over channels across all points and sets pts_per_set = out . shape [ 1 ] out_view = out . view ( - 1 , self . _mlp_out_sizes [ i ]) out = self . _bn_layers [ i ]( out_view ) out = out . view ( - 1 , pts_per_set , self . _mlp_out_sizes [ i ]) out = nn . functional . relu ( out ) if self . _dense : out_max , _ = torch . max ( out , 1 , keepdim = True ) if i != len ( self . _linear_layers ) - 1 : out = torch . cat (( out , out_max . expand ( - 1 , set_size , - 1 )), dim = 2 ) if self . _residual : if prev_out . shape == out . shape : out = prev_out + out prev_out = out # Maximum over points in same set out , _ = torch . max ( out , 1 ) return out IterativePointNet Bases: nn . Module Iterative PointNet which concatenates input. This is composed of 2 PointNets, where the first PointNet is applied once, the second PointNet a number of times, i.e., out = PointNet1(in) for i in range(num_concat): out = PointNet2( concat( out, in ) ) Source code in sdfest/initialization/pointnet.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 class IterativePointNet ( nn . Module ): \"\"\"Iterative PointNet which concatenates input. This is composed of 2 PointNets, where the first PointNet is applied once, the second PointNet a number of times, i.e., out = PointNet1(in) for i in range(num_concat): out = PointNet2( concat( out, in ) ) \"\"\" def __init__ ( self , num_concat : int , in_size : int , mlp_out_sizes : List , batchnorm : bool ) -> None : \"\"\"Initialize the IterativePointNet module. Args: num_concat: Number of concatenations of input and previous iteration. If 0 this module is the same as VanillaPointNet. in_size: Dimension of the input points. mlp_out_sizes: Output sizes of each linear layer. batchnorm: Whether to use batchnorm or not. \"\"\" super () . __init__ () self . num_concat = num_concat # create 1st pointnet for taking points of channel = in_size self . pointnet_1 = VanillaPointNet ( in_size , mlp_out_sizes , batchnorm ) # create 2nd pointnet for taking points of channel = size of concatenated vector self . pointnet_2 = VanillaPointNet ( in_size + mlp_out_sizes [ - 1 ], mlp_out_sizes , batchnorm ) def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\"Forward pass. Input has dimension NxMxC, where N is the batch size, M the number of points per set, and C the number of channels per point. Args: x: batch of point sets \"\"\" # apply 1st pointnet to input out = self . pointnet_1 ( x ) # shape (batch_size, num_outputs) set_size = x . shape [ 1 ] for _ in range ( self . num_concat ): # repeat output vector across 2nd dimension repeated_out = out . unsqueeze ( 1 ) . repeat ( 1 , set_size , 1 ) # concatenate input vector and repeated_out modified_x = torch . cat (( repeated_out , x ), 2 ) out = self . pointnet_2 ( modified_x ) return out __init__ __init__ ( num_concat : int , in_size : int , mlp_out_sizes : List , batchnorm : bool ) -> None Initialize the IterativePointNet module. PARAMETER DESCRIPTION num_concat Number of concatenations of input and previous iteration. If 0 this module is the same as VanillaPointNet. TYPE: int in_size Dimension of the input points. TYPE: int mlp_out_sizes Output sizes of each linear layer. TYPE: List batchnorm Whether to use batchnorm or not. TYPE: bool Source code in sdfest/initialization/pointnet.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 def __init__ ( self , num_concat : int , in_size : int , mlp_out_sizes : List , batchnorm : bool ) -> None : \"\"\"Initialize the IterativePointNet module. Args: num_concat: Number of concatenations of input and previous iteration. If 0 this module is the same as VanillaPointNet. in_size: Dimension of the input points. mlp_out_sizes: Output sizes of each linear layer. batchnorm: Whether to use batchnorm or not. \"\"\" super () . __init__ () self . num_concat = num_concat # create 1st pointnet for taking points of channel = in_size self . pointnet_1 = VanillaPointNet ( in_size , mlp_out_sizes , batchnorm ) # create 2nd pointnet for taking points of channel = size of concatenated vector self . pointnet_2 = VanillaPointNet ( in_size + mlp_out_sizes [ - 1 ], mlp_out_sizes , batchnorm ) forward forward ( x : torch . Tensor ) -> torch . Tensor Forward pass. Input has dimension NxMxC, where N is the batch size, M the number of points per set, and C the number of channels per point. PARAMETER DESCRIPTION x batch of point sets TYPE: torch . Tensor Source code in sdfest/initialization/pointnet.py 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\"Forward pass. Input has dimension NxMxC, where N is the batch size, M the number of points per set, and C the number of channels per point. Args: x: batch of point sets \"\"\" # apply 1st pointnet to input out = self . pointnet_1 ( x ) # shape (batch_size, num_outputs) set_size = x . shape [ 1 ] for _ in range ( self . num_concat ): # repeat output vector across 2nd dimension repeated_out = out . unsqueeze ( 1 ) . repeat ( 1 , set_size , 1 ) # concatenate input vector and repeated_out modified_x = torch . cat (( repeated_out , x ), 2 ) out = self . pointnet_2 ( modified_x ) return out GeneralizedIterativePointNet Bases: nn . Module Generalized Iterative PointNet composed of multiple IterativePointNet instances. This is a sequence of iterative pointnets, where the initial input will be concatenated to each input, e.g., out = IterativePointNet1(in) out = IterativePointNet2(concat(out, in)) out = IterativePointNet3(concat(out, in)) ... Source code in sdfest/initialization/pointnet.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 class GeneralizedIterativePointNet ( nn . Module ): \"\"\"Generalized Iterative PointNet composed of multiple IterativePointNet instances. This is a sequence of iterative pointnets, where the initial input will be concatenated to each input, e.g., out = IterativePointNet1(in) out = IterativePointNet2(concat(out, in)) out = IterativePointNet3(concat(out, in)) ... \"\"\" def __init__ ( self , list_concat : list , in_size : int , list_mlp_out_sizes : list , batchnorm : bool ) -> None : \"\"\"Initialize GeneralizedIterativePointnet module. Args: list_concat: List of concatenations for each MLP. in_size: Dimension of the input points. list_mlp_out_sizes: List of Output sizes of each linear layer. It is a List of Lists. batchnorm: Whether to use batchnorm or not. \"\"\" super () . __init__ () init_in_size = in_size self . iterative_pointnet_list = torch . nn . ModuleList ([]) temp_iterative_pointnet = IterativePointNet ( list_concat [ 0 ], in_size , list_mlp_out_sizes [ 0 ], batchnorm ) self . iterative_pointnet_list . append ( temp_iterative_pointnet ) for iterative_pointnet_num in range ( 1 , len ( list_mlp_out_sizes )): # the input size to new MLP should be the output size of the previous MLP # plus previous input size in_size = list_mlp_out_sizes [ iterative_pointnet_num - 1 ][ - 1 ] + init_in_size temp_iterative_pointnet = IterativePointNet ( list_concat [ iterative_pointnet_num ], in_size , list_mlp_out_sizes [ iterative_pointnet_num ], batchnorm , ) self . iterative_pointnet_list . append ( temp_iterative_pointnet ) def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\"Forward pass. Input has dimension NxMxC, where N is the batch size, M the number of points per set, and C the number of channels per point. Args: x: batch of point sets \"\"\" set_size = x . shape [ 1 ] init_x = x for iterative_pointnet in self . iterative_pointnet_list : out = iterative_pointnet ( x ) # shape (batch_size, num_outputs) # repeat output vector across 2nd dimension x = out . unsqueeze ( 1 ) . repeat ( 1 , set_size , 1 ) x = torch . cat (( x , init_x ), 2 ) return out __init__ __init__ ( list_concat : list , in_size : int , list_mlp_out_sizes : list , batchnorm : bool , ) -> None Initialize GeneralizedIterativePointnet module. PARAMETER DESCRIPTION list_concat List of concatenations for each MLP. TYPE: list in_size Dimension of the input points. TYPE: int list_mlp_out_sizes List of Output sizes of each linear layer. It is a List of Lists. TYPE: list batchnorm Whether to use batchnorm or not. TYPE: bool Source code in sdfest/initialization/pointnet.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 def __init__ ( self , list_concat : list , in_size : int , list_mlp_out_sizes : list , batchnorm : bool ) -> None : \"\"\"Initialize GeneralizedIterativePointnet module. Args: list_concat: List of concatenations for each MLP. in_size: Dimension of the input points. list_mlp_out_sizes: List of Output sizes of each linear layer. It is a List of Lists. batchnorm: Whether to use batchnorm or not. \"\"\" super () . __init__ () init_in_size = in_size self . iterative_pointnet_list = torch . nn . ModuleList ([]) temp_iterative_pointnet = IterativePointNet ( list_concat [ 0 ], in_size , list_mlp_out_sizes [ 0 ], batchnorm ) self . iterative_pointnet_list . append ( temp_iterative_pointnet ) for iterative_pointnet_num in range ( 1 , len ( list_mlp_out_sizes )): # the input size to new MLP should be the output size of the previous MLP # plus previous input size in_size = list_mlp_out_sizes [ iterative_pointnet_num - 1 ][ - 1 ] + init_in_size temp_iterative_pointnet = IterativePointNet ( list_concat [ iterative_pointnet_num ], in_size , list_mlp_out_sizes [ iterative_pointnet_num ], batchnorm , ) self . iterative_pointnet_list . append ( temp_iterative_pointnet ) forward forward ( x : torch . Tensor ) -> torch . Tensor Forward pass. Input has dimension NxMxC, where N is the batch size, M the number of points per set, and C the number of channels per point. PARAMETER DESCRIPTION x batch of point sets TYPE: torch . Tensor Source code in sdfest/initialization/pointnet.py 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\"Forward pass. Input has dimension NxMxC, where N is the batch size, M the number of points per set, and C the number of channels per point. Args: x: batch of point sets \"\"\" set_size = x . shape [ 1 ] init_x = x for iterative_pointnet in self . iterative_pointnet_list : out = iterative_pointnet ( x ) # shape (batch_size, num_outputs) # repeat output vector across 2nd dimension x = out . unsqueeze ( 1 ) . repeat ( 1 , set_size , 1 ) x = torch . cat (( x , init_x ), 2 ) return out","title":"pointnet"},{"location":"reference/initialization/pointnet/#sdfest.initialization.pointnet","text":"Parametrized PointNet.","title":"pointnet"},{"location":"reference/initialization/pointnet/#sdfest.initialization.pointnet.VanillaPointNet","text":"Bases: nn . Module Parametrized PointNet without transformation layers (no T-nets). Generally following PointNet Deep Learning on Point Sets for 3D Classification and Segmentation Qi, 2017 Source code in sdfest/initialization/pointnet.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 class VanillaPointNet ( nn . Module ): \"\"\"Parametrized PointNet without transformation layers (no T-nets). Generally following: PointNet Deep Learning on Point Sets for 3D Classification and Segmentation Qi, 2017 \"\"\" def __init__ ( self , in_size : int , mlp_out_sizes : List , batchnorm : bool , residual : bool = False , dense : bool = False , ) -> None : \"\"\"Initialize the VanillaPointNet module. This module will only implements the MLP + MaxPooling part of the pointnet. It still requires a task specific head. Args: in_size: dimension of the input points mlp_out_sizes: output sizes of each linear layer batchnorm: whether to use batchnorm or not \"\"\" super () . __init__ () self . _in_size = in_size self . _mlp_out_sizes = mlp_out_sizes self . _batchnorm = batchnorm self . _residual = residual self . _dense = dense # define layers self . _linear_layers = torch . nn . ModuleList ([]) for i , out_size in enumerate ( mlp_out_sizes ): if i == 0 : self . _linear_layers . append ( nn . Linear ( self . _in_size , out_size )) else : if dense : self . _linear_layers . append ( nn . Linear ( 2 * mlp_out_sizes [ i - 1 ], out_size ) ) else : self . _linear_layers . append ( nn . Linear ( mlp_out_sizes [ i - 1 ], out_size ) ) self . _bn_layers = torch . nn . ModuleList ([]) if self . _batchnorm : for out_size in mlp_out_sizes : self . _bn_layers . append ( nn . BatchNorm1d ( out_size )) def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\"Forward pass of the module. Input has dimension NxMxC, where N is the batch size, M the number of points per set, and C the number of channels per point. Args: x: batch of point sets \"\"\" set_size = x . shape [ 1 ] out = prev_out = x for i , linear_layer in enumerate ( self . _linear_layers ): out = linear_layer ( out ) if self . _batchnorm : # BN over channels across all points and sets pts_per_set = out . shape [ 1 ] out_view = out . view ( - 1 , self . _mlp_out_sizes [ i ]) out = self . _bn_layers [ i ]( out_view ) out = out . view ( - 1 , pts_per_set , self . _mlp_out_sizes [ i ]) out = nn . functional . relu ( out ) if self . _dense : out_max , _ = torch . max ( out , 1 , keepdim = True ) if i != len ( self . _linear_layers ) - 1 : out = torch . cat (( out , out_max . expand ( - 1 , set_size , - 1 )), dim = 2 ) if self . _residual : if prev_out . shape == out . shape : out = prev_out + out prev_out = out # Maximum over points in same set out , _ = torch . max ( out , 1 ) return out","title":"VanillaPointNet"},{"location":"reference/initialization/pointnet/#sdfest.initialization.pointnet.VanillaPointNet.__init__","text":"__init__ ( in_size : int , mlp_out_sizes : List , batchnorm : bool , residual : bool = False , dense : bool = False , ) -> None Initialize the VanillaPointNet module. This module will only implements the MLP + MaxPooling part of the pointnet. It still requires a task specific head. PARAMETER DESCRIPTION in_size dimension of the input points TYPE: int mlp_out_sizes output sizes of each linear layer TYPE: List batchnorm whether to use batchnorm or not TYPE: bool Source code in sdfest/initialization/pointnet.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def __init__ ( self , in_size : int , mlp_out_sizes : List , batchnorm : bool , residual : bool = False , dense : bool = False , ) -> None : \"\"\"Initialize the VanillaPointNet module. This module will only implements the MLP + MaxPooling part of the pointnet. It still requires a task specific head. Args: in_size: dimension of the input points mlp_out_sizes: output sizes of each linear layer batchnorm: whether to use batchnorm or not \"\"\" super () . __init__ () self . _in_size = in_size self . _mlp_out_sizes = mlp_out_sizes self . _batchnorm = batchnorm self . _residual = residual self . _dense = dense # define layers self . _linear_layers = torch . nn . ModuleList ([]) for i , out_size in enumerate ( mlp_out_sizes ): if i == 0 : self . _linear_layers . append ( nn . Linear ( self . _in_size , out_size )) else : if dense : self . _linear_layers . append ( nn . Linear ( 2 * mlp_out_sizes [ i - 1 ], out_size ) ) else : self . _linear_layers . append ( nn . Linear ( mlp_out_sizes [ i - 1 ], out_size ) ) self . _bn_layers = torch . nn . ModuleList ([]) if self . _batchnorm : for out_size in mlp_out_sizes : self . _bn_layers . append ( nn . BatchNorm1d ( out_size ))","title":"__init__()"},{"location":"reference/initialization/pointnet/#sdfest.initialization.pointnet.VanillaPointNet.forward","text":"forward ( x : torch . Tensor ) -> torch . Tensor Forward pass of the module. Input has dimension NxMxC, where N is the batch size, M the number of points per set, and C the number of channels per point. PARAMETER DESCRIPTION x batch of point sets TYPE: torch . Tensor Source code in sdfest/initialization/pointnet.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\"Forward pass of the module. Input has dimension NxMxC, where N is the batch size, M the number of points per set, and C the number of channels per point. Args: x: batch of point sets \"\"\" set_size = x . shape [ 1 ] out = prev_out = x for i , linear_layer in enumerate ( self . _linear_layers ): out = linear_layer ( out ) if self . _batchnorm : # BN over channels across all points and sets pts_per_set = out . shape [ 1 ] out_view = out . view ( - 1 , self . _mlp_out_sizes [ i ]) out = self . _bn_layers [ i ]( out_view ) out = out . view ( - 1 , pts_per_set , self . _mlp_out_sizes [ i ]) out = nn . functional . relu ( out ) if self . _dense : out_max , _ = torch . max ( out , 1 , keepdim = True ) if i != len ( self . _linear_layers ) - 1 : out = torch . cat (( out , out_max . expand ( - 1 , set_size , - 1 )), dim = 2 ) if self . _residual : if prev_out . shape == out . shape : out = prev_out + out prev_out = out # Maximum over points in same set out , _ = torch . max ( out , 1 ) return out","title":"forward()"},{"location":"reference/initialization/pointnet/#sdfest.initialization.pointnet.IterativePointNet","text":"Bases: nn . Module Iterative PointNet which concatenates input. This is composed of 2 PointNets, where the first PointNet is applied once, the second PointNet a number of times, i.e., out = PointNet1(in) for i in range(num_concat): out = PointNet2( concat( out, in ) ) Source code in sdfest/initialization/pointnet.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 class IterativePointNet ( nn . Module ): \"\"\"Iterative PointNet which concatenates input. This is composed of 2 PointNets, where the first PointNet is applied once, the second PointNet a number of times, i.e., out = PointNet1(in) for i in range(num_concat): out = PointNet2( concat( out, in ) ) \"\"\" def __init__ ( self , num_concat : int , in_size : int , mlp_out_sizes : List , batchnorm : bool ) -> None : \"\"\"Initialize the IterativePointNet module. Args: num_concat: Number of concatenations of input and previous iteration. If 0 this module is the same as VanillaPointNet. in_size: Dimension of the input points. mlp_out_sizes: Output sizes of each linear layer. batchnorm: Whether to use batchnorm or not. \"\"\" super () . __init__ () self . num_concat = num_concat # create 1st pointnet for taking points of channel = in_size self . pointnet_1 = VanillaPointNet ( in_size , mlp_out_sizes , batchnorm ) # create 2nd pointnet for taking points of channel = size of concatenated vector self . pointnet_2 = VanillaPointNet ( in_size + mlp_out_sizes [ - 1 ], mlp_out_sizes , batchnorm ) def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\"Forward pass. Input has dimension NxMxC, where N is the batch size, M the number of points per set, and C the number of channels per point. Args: x: batch of point sets \"\"\" # apply 1st pointnet to input out = self . pointnet_1 ( x ) # shape (batch_size, num_outputs) set_size = x . shape [ 1 ] for _ in range ( self . num_concat ): # repeat output vector across 2nd dimension repeated_out = out . unsqueeze ( 1 ) . repeat ( 1 , set_size , 1 ) # concatenate input vector and repeated_out modified_x = torch . cat (( repeated_out , x ), 2 ) out = self . pointnet_2 ( modified_x ) return out","title":"IterativePointNet"},{"location":"reference/initialization/pointnet/#sdfest.initialization.pointnet.IterativePointNet.__init__","text":"__init__ ( num_concat : int , in_size : int , mlp_out_sizes : List , batchnorm : bool ) -> None Initialize the IterativePointNet module. PARAMETER DESCRIPTION num_concat Number of concatenations of input and previous iteration. If 0 this module is the same as VanillaPointNet. TYPE: int in_size Dimension of the input points. TYPE: int mlp_out_sizes Output sizes of each linear layer. TYPE: List batchnorm Whether to use batchnorm or not. TYPE: bool Source code in sdfest/initialization/pointnet.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 def __init__ ( self , num_concat : int , in_size : int , mlp_out_sizes : List , batchnorm : bool ) -> None : \"\"\"Initialize the IterativePointNet module. Args: num_concat: Number of concatenations of input and previous iteration. If 0 this module is the same as VanillaPointNet. in_size: Dimension of the input points. mlp_out_sizes: Output sizes of each linear layer. batchnorm: Whether to use batchnorm or not. \"\"\" super () . __init__ () self . num_concat = num_concat # create 1st pointnet for taking points of channel = in_size self . pointnet_1 = VanillaPointNet ( in_size , mlp_out_sizes , batchnorm ) # create 2nd pointnet for taking points of channel = size of concatenated vector self . pointnet_2 = VanillaPointNet ( in_size + mlp_out_sizes [ - 1 ], mlp_out_sizes , batchnorm )","title":"__init__()"},{"location":"reference/initialization/pointnet/#sdfest.initialization.pointnet.IterativePointNet.forward","text":"forward ( x : torch . Tensor ) -> torch . Tensor Forward pass. Input has dimension NxMxC, where N is the batch size, M the number of points per set, and C the number of channels per point. PARAMETER DESCRIPTION x batch of point sets TYPE: torch . Tensor Source code in sdfest/initialization/pointnet.py 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\"Forward pass. Input has dimension NxMxC, where N is the batch size, M the number of points per set, and C the number of channels per point. Args: x: batch of point sets \"\"\" # apply 1st pointnet to input out = self . pointnet_1 ( x ) # shape (batch_size, num_outputs) set_size = x . shape [ 1 ] for _ in range ( self . num_concat ): # repeat output vector across 2nd dimension repeated_out = out . unsqueeze ( 1 ) . repeat ( 1 , set_size , 1 ) # concatenate input vector and repeated_out modified_x = torch . cat (( repeated_out , x ), 2 ) out = self . pointnet_2 ( modified_x ) return out","title":"forward()"},{"location":"reference/initialization/pointnet/#sdfest.initialization.pointnet.GeneralizedIterativePointNet","text":"Bases: nn . Module Generalized Iterative PointNet composed of multiple IterativePointNet instances. This is a sequence of iterative pointnets, where the initial input will be concatenated to each input, e.g., out = IterativePointNet1(in) out = IterativePointNet2(concat(out, in)) out = IterativePointNet3(concat(out, in)) ... Source code in sdfest/initialization/pointnet.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 class GeneralizedIterativePointNet ( nn . Module ): \"\"\"Generalized Iterative PointNet composed of multiple IterativePointNet instances. This is a sequence of iterative pointnets, where the initial input will be concatenated to each input, e.g., out = IterativePointNet1(in) out = IterativePointNet2(concat(out, in)) out = IterativePointNet3(concat(out, in)) ... \"\"\" def __init__ ( self , list_concat : list , in_size : int , list_mlp_out_sizes : list , batchnorm : bool ) -> None : \"\"\"Initialize GeneralizedIterativePointnet module. Args: list_concat: List of concatenations for each MLP. in_size: Dimension of the input points. list_mlp_out_sizes: List of Output sizes of each linear layer. It is a List of Lists. batchnorm: Whether to use batchnorm or not. \"\"\" super () . __init__ () init_in_size = in_size self . iterative_pointnet_list = torch . nn . ModuleList ([]) temp_iterative_pointnet = IterativePointNet ( list_concat [ 0 ], in_size , list_mlp_out_sizes [ 0 ], batchnorm ) self . iterative_pointnet_list . append ( temp_iterative_pointnet ) for iterative_pointnet_num in range ( 1 , len ( list_mlp_out_sizes )): # the input size to new MLP should be the output size of the previous MLP # plus previous input size in_size = list_mlp_out_sizes [ iterative_pointnet_num - 1 ][ - 1 ] + init_in_size temp_iterative_pointnet = IterativePointNet ( list_concat [ iterative_pointnet_num ], in_size , list_mlp_out_sizes [ iterative_pointnet_num ], batchnorm , ) self . iterative_pointnet_list . append ( temp_iterative_pointnet ) def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\"Forward pass. Input has dimension NxMxC, where N is the batch size, M the number of points per set, and C the number of channels per point. Args: x: batch of point sets \"\"\" set_size = x . shape [ 1 ] init_x = x for iterative_pointnet in self . iterative_pointnet_list : out = iterative_pointnet ( x ) # shape (batch_size, num_outputs) # repeat output vector across 2nd dimension x = out . unsqueeze ( 1 ) . repeat ( 1 , set_size , 1 ) x = torch . cat (( x , init_x ), 2 ) return out","title":"GeneralizedIterativePointNet"},{"location":"reference/initialization/pointnet/#sdfest.initialization.pointnet.GeneralizedIterativePointNet.__init__","text":"__init__ ( list_concat : list , in_size : int , list_mlp_out_sizes : list , batchnorm : bool , ) -> None Initialize GeneralizedIterativePointnet module. PARAMETER DESCRIPTION list_concat List of concatenations for each MLP. TYPE: list in_size Dimension of the input points. TYPE: int list_mlp_out_sizes List of Output sizes of each linear layer. It is a List of Lists. TYPE: list batchnorm Whether to use batchnorm or not. TYPE: bool Source code in sdfest/initialization/pointnet.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 def __init__ ( self , list_concat : list , in_size : int , list_mlp_out_sizes : list , batchnorm : bool ) -> None : \"\"\"Initialize GeneralizedIterativePointnet module. Args: list_concat: List of concatenations for each MLP. in_size: Dimension of the input points. list_mlp_out_sizes: List of Output sizes of each linear layer. It is a List of Lists. batchnorm: Whether to use batchnorm or not. \"\"\" super () . __init__ () init_in_size = in_size self . iterative_pointnet_list = torch . nn . ModuleList ([]) temp_iterative_pointnet = IterativePointNet ( list_concat [ 0 ], in_size , list_mlp_out_sizes [ 0 ], batchnorm ) self . iterative_pointnet_list . append ( temp_iterative_pointnet ) for iterative_pointnet_num in range ( 1 , len ( list_mlp_out_sizes )): # the input size to new MLP should be the output size of the previous MLP # plus previous input size in_size = list_mlp_out_sizes [ iterative_pointnet_num - 1 ][ - 1 ] + init_in_size temp_iterative_pointnet = IterativePointNet ( list_concat [ iterative_pointnet_num ], in_size , list_mlp_out_sizes [ iterative_pointnet_num ], batchnorm , ) self . iterative_pointnet_list . append ( temp_iterative_pointnet )","title":"__init__()"},{"location":"reference/initialization/pointnet/#sdfest.initialization.pointnet.GeneralizedIterativePointNet.forward","text":"forward ( x : torch . Tensor ) -> torch . Tensor Forward pass. Input has dimension NxMxC, where N is the batch size, M the number of points per set, and C the number of channels per point. PARAMETER DESCRIPTION x batch of point sets TYPE: torch . Tensor Source code in sdfest/initialization/pointnet.py 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\"Forward pass. Input has dimension NxMxC, where N is the batch size, M the number of points per set, and C the number of channels per point. Args: x: batch of point sets \"\"\" set_size = x . shape [ 1 ] init_x = x for iterative_pointnet in self . iterative_pointnet_list : out = iterative_pointnet ( x ) # shape (batch_size, num_outputs) # repeat output vector across 2nd dimension x = out . unsqueeze ( 1 ) . repeat ( 1 , set_size , 1 ) x = torch . cat (( x , init_x ), 2 ) return out","title":"forward()"},{"location":"reference/initialization/pointset_utils/","text":"sdfest.initialization.pointset_utils Utility functions to handle pointsets. normalize_points normalize_points ( points : torch . Tensor ) -> torch . Tensor Normalize pointset to have zero mean. Normalization will be performed along second last dimension. PARAMETER DESCRIPTION points The pointsets which will be normalized, shape (N, M, D) or shape (M, D), N pointsets with M points of dimension D. TYPE: torch . Tensor Return normalized_points: The normalized pointset, same shape as points. centroids: The means of the pointclouds used to normalize points. Shape (N, D) or (D,), for (N, M, D) and (M, D) inputs, respectively. Source code in sdfest/initialization/pointset_utils.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def normalize_points ( points : torch . Tensor ) -> torch . Tensor : \"\"\"Normalize pointset to have zero mean. Normalization will be performed along second last dimension. Args: points: The pointsets which will be normalized, shape (N, M, D) or shape (M, D), N pointsets with M points of dimension D. Return: normalized_points: The normalized pointset, same shape as points. centroids: The means of the pointclouds used to normalize points. Shape (N, D) or (D,), for (N, M, D) and (M, D) inputs, respectively. \"\"\" centroids = torch . mean ( points , dim =- 2 , keepdim = True ) normalized_points = points - centroids return normalized_points , centroids . squeeze () depth_to_pointcloud depth_to_pointcloud ( depth_image : torch . Tensor , camera : Camera , normalize : bool = False , mask : Optional [ torch . Tensor ] = None , convention : str = \"opengl\" , ) -> torch . Tensor Convert depth image to pointcloud. PARAMETER DESCRIPTION depth_image The depth image to convert to pointcloud, shape (H,W). TYPE: torch . Tensor camera The camera used to lift the points. TYPE: Camera normalize Whether to normalize the pointcloud with 0 centroid. TYPE: bool DEFAULT: False mask Only points with mask != 0 will be added to pointcloud. No masking will be performed if None. TYPE: Optional [ torch . Tensor ] DEFAULT: None convention The camera frame convention to use. One of: \"opengl\": x right, y up, z back \"opencv\": x right, y down, z forward TYPE: str DEFAULT: 'opengl' RETURNS DESCRIPTION torch . Tensor The pointcloud in the camera frame, in OpenGL convention, shape (N,3). Source code in sdfest/initialization/pointset_utils.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def depth_to_pointcloud ( depth_image : torch . Tensor , camera : Camera , normalize : bool = False , mask : Optional [ torch . Tensor ] = None , convention : str = \"opengl\" , ) -> torch . Tensor : \"\"\"Convert depth image to pointcloud. Args: depth_image: The depth image to convert to pointcloud, shape (H,W). camera: The camera used to lift the points. normalize: Whether to normalize the pointcloud with 0 centroid. mask: Only points with mask != 0 will be added to pointcloud. No masking will be performed if None. convention: The camera frame convention to use. One of: \"opengl\": x right, y up, z back \"opencv\": x right, y down, z forward Returns: The pointcloud in the camera frame, in OpenGL convention, shape (N,3). \"\"\" fx , fy , cx , cy , _ = camera . get_pinhole_camera_parameters ( 0.0 ) if mask is None : indices = torch . nonzero ( depth_image , as_tuple = True ) else : indices = torch . nonzero ( depth_image * mask , as_tuple = True ) depth_values = depth_image [ indices ] points = torch . cat ( ( indices [ 1 ][:, None ] . float (), indices [ 0 ][:, None ] . float (), depth_values [:, None ], ), dim = 1 , ) if convention == \"opengl\" : final_points = torch . empty_like ( points ) final_points [:, 0 ] = ( points [:, 0 ] - cx ) * points [:, 2 ] / fx final_points [:, 1 ] = - ( points [:, 1 ] - cy ) * points [:, 2 ] / fy final_points [:, 2 ] = - points [:, 2 ] elif convention == \"opencv\" : final_points = torch . empty_like ( points ) final_points [:, 0 ] = ( points [:, 0 ] - cx ) * points [:, 2 ] / fx final_points [:, 1 ] = ( points [:, 1 ] - cy ) * points [:, 2 ] / fy final_points [:, 2 ] = points [:, 2 ] else : raise ValueError ( f \"Unsupported camera convention { convention } .\" ) if normalize : final_points , _ = normalize_points ( final_points ) return final_points change_transform_camera_convention change_transform_camera_convention ( in_transform : torch . Tensor , in_convention : str , out_convention : str ) -> torch . Tensor Change the camera convention for a frame A -> camera frame transform. PARAMETER DESCRIPTION in_transform Transformtion matrix(es) from coordinate frame A to in_convention camera frame. Shape (...,4,4). TYPE: torch . Tensor in_convention Camera convention for the in_transform. One of \"opengl\", \"opencv\". TYPE: str out_convention Camera convention for the returned transform. One of \"opengl\", \"opencv\". TYPE: str RETURNS DESCRIPTION torch . Tensor Transformtion matrix(es) from coordinate frame A to out_convention camera frame. torch . Tensor Same shape as in_transform. Source code in sdfest/initialization/pointset_utils.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def change_transform_camera_convention ( in_transform : torch . Tensor , in_convention : str , out_convention : str ) -> torch . Tensor : \"\"\"Change the camera convention for a frame A -> camera frame transform. Args: in_transform: Transformtion matrix(es) from coordinate frame A to in_convention camera frame. Shape (...,4,4). in_convention: Camera convention for the in_transform. One of \"opengl\", \"opencv\". out_convention: Camera convention for the returned transform. One of \"opengl\", \"opencv\". Returns: Transformtion matrix(es) from coordinate frame A to out_convention camera frame. Same shape as in_transform. \"\"\" # check whether valild convention was provided if in_convention not in [ \"opengl\" , \"opencv\" ]: raise ValueError ( f \"In camera convention { in_convention } not supported.\" ) if out_convention not in [ \"opengl\" , \"opencv\" ]: raise ValueError ( f \"Out camera convention { in_convention } not supported.\" ) if in_convention == out_convention : return in_transform else : gl2cv_transform = torch . diag ( in_transform . new_tensor ([ 1.0 , - 1.0 , - 1.0 , 1.0 ]) ) # == cv2gl_transform return gl2cv_transform @ in_transform change_position_camera_convention change_position_camera_convention ( in_position : torch . Tensor , in_convention : str , out_convention : str ) -> tuple Change the camera convention for a position in a camera frame. PARAMETER DESCRIPTION in_position Position(s) of coordinate frame A in in_convention camera frame. Shape (...,3). TYPE: torch . Tensor in_convention Camera convention for the in_position. One of \"opengl\", \"opencv\". TYPE: str out_convention Camera convention for the returned transform. One of \"opengl\", \"opencv\". TYPE: str RETURNS DESCRIPTION tuple Position(s) of coordinate frame A in out_convention camera frame. Shape (...,3). Source code in sdfest/initialization/pointset_utils.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 def change_position_camera_convention ( in_position : torch . Tensor , in_convention : str , out_convention : str , ) -> tuple : \"\"\"Change the camera convention for a position in a camera frame. Args: in_position: Position(s) of coordinate frame A in in_convention camera frame. Shape (...,3). in_convention: Camera convention for the in_position. One of \"opengl\", \"opencv\". out_convention: Camera convention for the returned transform. One of \"opengl\", \"opencv\". Returns: Position(s) of coordinate frame A in out_convention camera frame. Shape (...,3). \"\"\" # check whether valild convention was provided if in_convention not in [ \"opengl\" , \"opencv\" ]: raise ValueError ( f \"In camera convention { in_convention } not supported.\" ) if out_convention not in [ \"opengl\" , \"opencv\" ]: raise ValueError ( f \"Out camera convention { in_convention } not supported.\" ) if in_convention == out_convention : return in_position else : gl2cv = in_position . new_tensor ([ 1.0 , - 1.0 , - 1.0 ]) # == cv2gl return gl2cv * in_position change_orientation_camera_convention change_orientation_camera_convention ( in_orientation_q : torch . Tensor , in_convention : str , out_convention : str ) -> tuple Change the camera convention for an orientation in a camera frame. Orientation is represented as a quaternion, that rotates points from a coordinate frame A to a camera frame (if those frames had the same origin). PARAMETER DESCRIPTION in_orientation_q Quaternion(s) which transforms from coordinate frame A to in_convention camera frame. Scalar-last convention. Shape (...,4). TYPE: torch . Tensor in_convention Camera convention for the in_transform. One of \"opengl\", \"opencv\". TYPE: str out_convention Camera convention for the returned transform. One of \"opengl\", \"opencv\". TYPE: str RETURNS DESCRIPTION tuple Quaternion(s) which transforms from coordinate frame A to in_convention camera tuple frame. Scalar-last convention. Same shape as in_orientation_q. Source code in sdfest/initialization/pointset_utils.py 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 def change_orientation_camera_convention ( in_orientation_q : torch . Tensor , in_convention : str , out_convention : str , ) -> tuple : \"\"\"Change the camera convention for an orientation in a camera frame. Orientation is represented as a quaternion, that rotates points from a coordinate frame A to a camera frame (if those frames had the same origin). Args: in_orientation_q: Quaternion(s) which transforms from coordinate frame A to in_convention camera frame. Scalar-last convention. Shape (...,4). in_convention: Camera convention for the in_transform. One of \"opengl\", \"opencv\". out_convention: Camera convention for the returned transform. One of \"opengl\", \"opencv\". Returns: Quaternion(s) which transforms from coordinate frame A to in_convention camera frame. Scalar-last convention. Same shape as in_orientation_q. \"\"\" # check whether valild convention was provided if in_convention not in [ \"opengl\" , \"opencv\" ]: raise ValueError ( f \"In camera convention { in_convention } not supported.\" ) if out_convention not in [ \"opengl\" , \"opencv\" ]: raise ValueError ( f \"Out camera convention { in_convention } not supported.\" ) if in_convention == out_convention : return in_orientation_q else : # rotate 180deg around x direction gl2cv_q = in_orientation_q . new_tensor ([ 1.0 , 0 , 0 , 0 ]) # == cv2gl return quaternion_utils . quaternion_multiply ( gl2cv_q , in_orientation_q ) visualize_pointset visualize_pointset ( pointset : torch . Tensor , max_points : int = 1000 ) -> None Visualize pointset as 3D scatter plot. PARAMETER DESCRIPTION pointset The pointset to visualize. Either shape (N,3), xyz, or shape (N,6), xyzrgb. TYPE: torch . Tensor max_points Maximum number of points. If N>max_points only a random subset will be shown. TYPE: int DEFAULT: 1000 Source code in sdfest/initialization/pointset_utils.py 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 def visualize_pointset ( pointset : torch . Tensor , max_points : int = 1000 ) -> None : \"\"\"Visualize pointset as 3D scatter plot. Args: pointset: The pointset to visualize. Either shape (N,3), xyz, or shape (N,6), xyzrgb. max_points: Maximum number of points. If N>max_points only a random subset will be shown. \"\"\" pointset_np = pointset . cpu () . detach () . numpy () fig = plt . figure () ax = fig . add_subplot ( projection = \"3d\" ) ax . set_box_aspect (( 1 , 1 , 1 )) if len ( pointset_np ) > max_points : indices = np . random . choice ( len ( pointset_np ), replace = False , size = max_points ) pointset_np = pointset_np [ indices ] if pointset_np . shape [ 1 ] == 6 : colors = pointset_np [:, 3 :] else : colors = None ax . scatter ( pointset_np [:, 0 ], pointset_np [:, 1 ], pointset_np [:, 2 ], c = colors ) ax . set_xlabel ( \"x\" ) ax . set_ylabel ( \"y\" ) ax . set_zlabel ( \"z\" ) utils . set_axes_equal ( ax ) plt . show ()","title":"pointset_utils"},{"location":"reference/initialization/pointset_utils/#sdfest.initialization.pointset_utils","text":"Utility functions to handle pointsets.","title":"pointset_utils"},{"location":"reference/initialization/pointset_utils/#sdfest.initialization.pointset_utils.normalize_points","text":"normalize_points ( points : torch . Tensor ) -> torch . Tensor Normalize pointset to have zero mean. Normalization will be performed along second last dimension. PARAMETER DESCRIPTION points The pointsets which will be normalized, shape (N, M, D) or shape (M, D), N pointsets with M points of dimension D. TYPE: torch . Tensor Return normalized_points: The normalized pointset, same shape as points. centroids: The means of the pointclouds used to normalize points. Shape (N, D) or (D,), for (N, M, D) and (M, D) inputs, respectively. Source code in sdfest/initialization/pointset_utils.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def normalize_points ( points : torch . Tensor ) -> torch . Tensor : \"\"\"Normalize pointset to have zero mean. Normalization will be performed along second last dimension. Args: points: The pointsets which will be normalized, shape (N, M, D) or shape (M, D), N pointsets with M points of dimension D. Return: normalized_points: The normalized pointset, same shape as points. centroids: The means of the pointclouds used to normalize points. Shape (N, D) or (D,), for (N, M, D) and (M, D) inputs, respectively. \"\"\" centroids = torch . mean ( points , dim =- 2 , keepdim = True ) normalized_points = points - centroids return normalized_points , centroids . squeeze ()","title":"normalize_points()"},{"location":"reference/initialization/pointset_utils/#sdfest.initialization.pointset_utils.depth_to_pointcloud","text":"depth_to_pointcloud ( depth_image : torch . Tensor , camera : Camera , normalize : bool = False , mask : Optional [ torch . Tensor ] = None , convention : str = \"opengl\" , ) -> torch . Tensor Convert depth image to pointcloud. PARAMETER DESCRIPTION depth_image The depth image to convert to pointcloud, shape (H,W). TYPE: torch . Tensor camera The camera used to lift the points. TYPE: Camera normalize Whether to normalize the pointcloud with 0 centroid. TYPE: bool DEFAULT: False mask Only points with mask != 0 will be added to pointcloud. No masking will be performed if None. TYPE: Optional [ torch . Tensor ] DEFAULT: None convention The camera frame convention to use. One of: \"opengl\": x right, y up, z back \"opencv\": x right, y down, z forward TYPE: str DEFAULT: 'opengl' RETURNS DESCRIPTION torch . Tensor The pointcloud in the camera frame, in OpenGL convention, shape (N,3). Source code in sdfest/initialization/pointset_utils.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def depth_to_pointcloud ( depth_image : torch . Tensor , camera : Camera , normalize : bool = False , mask : Optional [ torch . Tensor ] = None , convention : str = \"opengl\" , ) -> torch . Tensor : \"\"\"Convert depth image to pointcloud. Args: depth_image: The depth image to convert to pointcloud, shape (H,W). camera: The camera used to lift the points. normalize: Whether to normalize the pointcloud with 0 centroid. mask: Only points with mask != 0 will be added to pointcloud. No masking will be performed if None. convention: The camera frame convention to use. One of: \"opengl\": x right, y up, z back \"opencv\": x right, y down, z forward Returns: The pointcloud in the camera frame, in OpenGL convention, shape (N,3). \"\"\" fx , fy , cx , cy , _ = camera . get_pinhole_camera_parameters ( 0.0 ) if mask is None : indices = torch . nonzero ( depth_image , as_tuple = True ) else : indices = torch . nonzero ( depth_image * mask , as_tuple = True ) depth_values = depth_image [ indices ] points = torch . cat ( ( indices [ 1 ][:, None ] . float (), indices [ 0 ][:, None ] . float (), depth_values [:, None ], ), dim = 1 , ) if convention == \"opengl\" : final_points = torch . empty_like ( points ) final_points [:, 0 ] = ( points [:, 0 ] - cx ) * points [:, 2 ] / fx final_points [:, 1 ] = - ( points [:, 1 ] - cy ) * points [:, 2 ] / fy final_points [:, 2 ] = - points [:, 2 ] elif convention == \"opencv\" : final_points = torch . empty_like ( points ) final_points [:, 0 ] = ( points [:, 0 ] - cx ) * points [:, 2 ] / fx final_points [:, 1 ] = ( points [:, 1 ] - cy ) * points [:, 2 ] / fy final_points [:, 2 ] = points [:, 2 ] else : raise ValueError ( f \"Unsupported camera convention { convention } .\" ) if normalize : final_points , _ = normalize_points ( final_points ) return final_points","title":"depth_to_pointcloud()"},{"location":"reference/initialization/pointset_utils/#sdfest.initialization.pointset_utils.change_transform_camera_convention","text":"change_transform_camera_convention ( in_transform : torch . Tensor , in_convention : str , out_convention : str ) -> torch . Tensor Change the camera convention for a frame A -> camera frame transform. PARAMETER DESCRIPTION in_transform Transformtion matrix(es) from coordinate frame A to in_convention camera frame. Shape (...,4,4). TYPE: torch . Tensor in_convention Camera convention for the in_transform. One of \"opengl\", \"opencv\". TYPE: str out_convention Camera convention for the returned transform. One of \"opengl\", \"opencv\". TYPE: str RETURNS DESCRIPTION torch . Tensor Transformtion matrix(es) from coordinate frame A to out_convention camera frame. torch . Tensor Same shape as in_transform. Source code in sdfest/initialization/pointset_utils.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def change_transform_camera_convention ( in_transform : torch . Tensor , in_convention : str , out_convention : str ) -> torch . Tensor : \"\"\"Change the camera convention for a frame A -> camera frame transform. Args: in_transform: Transformtion matrix(es) from coordinate frame A to in_convention camera frame. Shape (...,4,4). in_convention: Camera convention for the in_transform. One of \"opengl\", \"opencv\". out_convention: Camera convention for the returned transform. One of \"opengl\", \"opencv\". Returns: Transformtion matrix(es) from coordinate frame A to out_convention camera frame. Same shape as in_transform. \"\"\" # check whether valild convention was provided if in_convention not in [ \"opengl\" , \"opencv\" ]: raise ValueError ( f \"In camera convention { in_convention } not supported.\" ) if out_convention not in [ \"opengl\" , \"opencv\" ]: raise ValueError ( f \"Out camera convention { in_convention } not supported.\" ) if in_convention == out_convention : return in_transform else : gl2cv_transform = torch . diag ( in_transform . new_tensor ([ 1.0 , - 1.0 , - 1.0 , 1.0 ]) ) # == cv2gl_transform return gl2cv_transform @ in_transform","title":"change_transform_camera_convention()"},{"location":"reference/initialization/pointset_utils/#sdfest.initialization.pointset_utils.change_position_camera_convention","text":"change_position_camera_convention ( in_position : torch . Tensor , in_convention : str , out_convention : str ) -> tuple Change the camera convention for a position in a camera frame. PARAMETER DESCRIPTION in_position Position(s) of coordinate frame A in in_convention camera frame. Shape (...,3). TYPE: torch . Tensor in_convention Camera convention for the in_position. One of \"opengl\", \"opencv\". TYPE: str out_convention Camera convention for the returned transform. One of \"opengl\", \"opencv\". TYPE: str RETURNS DESCRIPTION tuple Position(s) of coordinate frame A in out_convention camera frame. Shape (...,3). Source code in sdfest/initialization/pointset_utils.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 def change_position_camera_convention ( in_position : torch . Tensor , in_convention : str , out_convention : str , ) -> tuple : \"\"\"Change the camera convention for a position in a camera frame. Args: in_position: Position(s) of coordinate frame A in in_convention camera frame. Shape (...,3). in_convention: Camera convention for the in_position. One of \"opengl\", \"opencv\". out_convention: Camera convention for the returned transform. One of \"opengl\", \"opencv\". Returns: Position(s) of coordinate frame A in out_convention camera frame. Shape (...,3). \"\"\" # check whether valild convention was provided if in_convention not in [ \"opengl\" , \"opencv\" ]: raise ValueError ( f \"In camera convention { in_convention } not supported.\" ) if out_convention not in [ \"opengl\" , \"opencv\" ]: raise ValueError ( f \"Out camera convention { in_convention } not supported.\" ) if in_convention == out_convention : return in_position else : gl2cv = in_position . new_tensor ([ 1.0 , - 1.0 , - 1.0 ]) # == cv2gl return gl2cv * in_position","title":"change_position_camera_convention()"},{"location":"reference/initialization/pointset_utils/#sdfest.initialization.pointset_utils.change_orientation_camera_convention","text":"change_orientation_camera_convention ( in_orientation_q : torch . Tensor , in_convention : str , out_convention : str ) -> tuple Change the camera convention for an orientation in a camera frame. Orientation is represented as a quaternion, that rotates points from a coordinate frame A to a camera frame (if those frames had the same origin). PARAMETER DESCRIPTION in_orientation_q Quaternion(s) which transforms from coordinate frame A to in_convention camera frame. Scalar-last convention. Shape (...,4). TYPE: torch . Tensor in_convention Camera convention for the in_transform. One of \"opengl\", \"opencv\". TYPE: str out_convention Camera convention for the returned transform. One of \"opengl\", \"opencv\". TYPE: str RETURNS DESCRIPTION tuple Quaternion(s) which transforms from coordinate frame A to in_convention camera tuple frame. Scalar-last convention. Same shape as in_orientation_q. Source code in sdfest/initialization/pointset_utils.py 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 def change_orientation_camera_convention ( in_orientation_q : torch . Tensor , in_convention : str , out_convention : str , ) -> tuple : \"\"\"Change the camera convention for an orientation in a camera frame. Orientation is represented as a quaternion, that rotates points from a coordinate frame A to a camera frame (if those frames had the same origin). Args: in_orientation_q: Quaternion(s) which transforms from coordinate frame A to in_convention camera frame. Scalar-last convention. Shape (...,4). in_convention: Camera convention for the in_transform. One of \"opengl\", \"opencv\". out_convention: Camera convention for the returned transform. One of \"opengl\", \"opencv\". Returns: Quaternion(s) which transforms from coordinate frame A to in_convention camera frame. Scalar-last convention. Same shape as in_orientation_q. \"\"\" # check whether valild convention was provided if in_convention not in [ \"opengl\" , \"opencv\" ]: raise ValueError ( f \"In camera convention { in_convention } not supported.\" ) if out_convention not in [ \"opengl\" , \"opencv\" ]: raise ValueError ( f \"Out camera convention { in_convention } not supported.\" ) if in_convention == out_convention : return in_orientation_q else : # rotate 180deg around x direction gl2cv_q = in_orientation_q . new_tensor ([ 1.0 , 0 , 0 , 0 ]) # == cv2gl return quaternion_utils . quaternion_multiply ( gl2cv_q , in_orientation_q )","title":"change_orientation_camera_convention()"},{"location":"reference/initialization/pointset_utils/#sdfest.initialization.pointset_utils.visualize_pointset","text":"visualize_pointset ( pointset : torch . Tensor , max_points : int = 1000 ) -> None Visualize pointset as 3D scatter plot. PARAMETER DESCRIPTION pointset The pointset to visualize. Either shape (N,3), xyz, or shape (N,6), xyzrgb. TYPE: torch . Tensor max_points Maximum number of points. If N>max_points only a random subset will be shown. TYPE: int DEFAULT: 1000 Source code in sdfest/initialization/pointset_utils.py 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 def visualize_pointset ( pointset : torch . Tensor , max_points : int = 1000 ) -> None : \"\"\"Visualize pointset as 3D scatter plot. Args: pointset: The pointset to visualize. Either shape (N,3), xyz, or shape (N,6), xyzrgb. max_points: Maximum number of points. If N>max_points only a random subset will be shown. \"\"\" pointset_np = pointset . cpu () . detach () . numpy () fig = plt . figure () ax = fig . add_subplot ( projection = \"3d\" ) ax . set_box_aspect (( 1 , 1 , 1 )) if len ( pointset_np ) > max_points : indices = np . random . choice ( len ( pointset_np ), replace = False , size = max_points ) pointset_np = pointset_np [ indices ] if pointset_np . shape [ 1 ] == 6 : colors = pointset_np [:, 3 :] else : colors = None ax . scatter ( pointset_np [:, 0 ], pointset_np [:, 1 ], pointset_np [:, 2 ], c = colors ) ax . set_xlabel ( \"x\" ) ax . set_ylabel ( \"y\" ) ax . set_zlabel ( \"z\" ) utils . set_axes_equal ( ax ) plt . show ()","title":"visualize_pointset()"},{"location":"reference/initialization/quaternion_utils/","text":"sdfest.initialization.quaternion_utils Functions to handle transformations with quaternions. Inspired by PyTorch3D, but using scalar-last convention and not enforcing scalar > 0. https://github.com/facebookresearch/pytorch3d quaternion_multiply quaternion_multiply ( quaternions_1 : torch . Tensor , quaternions_2 : torch . Tensor ) -> torch . Tensor Multiply two quaternions representing rotations. Normal broadcasting rules apply. PARAMETER DESCRIPTION quaternions_1 normalized quaternions of shape (..., 4), scalar-last convention TYPE: torch . Tensor quaternions_2 normalized quaternions of shape (..., 4), scalar-last convention TYPE: torch . Tensor RETURNS DESCRIPTION torch . Tensor Composition of passed quaternions. Source code in sdfest/initialization/quaternion_utils.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def quaternion_multiply ( quaternions_1 : torch . Tensor , quaternions_2 : torch . Tensor ) -> torch . Tensor : \"\"\"Multiply two quaternions representing rotations. Normal broadcasting rules apply. Args: quaternions_1: normalized quaternions of shape (..., 4), scalar-last convention quaternions_2: normalized quaternions of shape (..., 4), scalar-last convention Returns: Composition of passed quaternions. \"\"\" ax , ay , az , aw = torch . unbind ( quaternions_1 , - 1 ) bx , by , bz , bw = torch . unbind ( quaternions_2 , - 1 ) ox = aw * bx + ax * bw + ay * bz - az * by oy = aw * by - ax * bz + ay * bw + az * bx oz = aw * bz + ax * by - ay * bx + az * bw ow = aw * bw - ax * bx - ay * by - az * bz return torch . stack (( ox , oy , oz , ow ), - 1 ) quaternion_apply quaternion_apply ( quaternions : torch . Tensor , points : torch . Tensor ) -> torch . Tensor Rotate points by quaternions representing rotations. Normal broadcasting rules apply. PARAMETER DESCRIPTION quaternions normalized quaternions of shape (..., 4), scalar-last convention TYPE: torch . Tensor points points of shape (..., 3) TYPE: torch . Tensor RETURNS DESCRIPTION torch . Tensor Points rotated by the rotations representing quaternions. Source code in sdfest/initialization/quaternion_utils.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def quaternion_apply ( quaternions : torch . Tensor , points : torch . Tensor ) -> torch . Tensor : \"\"\"Rotate points by quaternions representing rotations. Normal broadcasting rules apply. Args: quaternions: normalized quaternions of shape (..., 4), scalar-last convention points: points of shape (..., 3) Returns: Points rotated by the rotations representing quaternions. \"\"\" points_as_quaternions = points . new_zeros ( points . shape [: - 1 ] + ( 4 ,)) points_as_quaternions [ ... , : - 1 ] = points return quaternion_multiply ( quaternion_multiply ( quaternions , points_as_quaternions ), quaternion_invert ( quaternions ), )[ ... , : - 1 ] quaternion_invert quaternion_invert ( quaternions : torch . Tensor ) -> torch . Tensor Invert quaternions representing orientations. PARAMETER DESCRIPTION quaternions The quaternions to invert, shape (..., 4), scalar-last convention. TYPE: torch . Tensor RETURNS DESCRIPTION torch . Tensor Inverted quaternions, same shape as quaternions. Source code in sdfest/initialization/quaternion_utils.py 57 58 59 60 61 62 63 64 65 66 def quaternion_invert ( quaternions : torch . Tensor ) -> torch . Tensor : \"\"\"Invert quaternions representing orientations. Args: quaternions: The quaternions to invert, shape (..., 4), scalar-last convention. Returns: Inverted quaternions, same shape as quaternions. \"\"\" return quaternions * quaternions . new_tensor ([ - 1 , - 1 , - 1 , 1 ]) geodesic_distance geodesic_distance ( q1 : torch . Tensor , q2 : torch . Tensor ) -> torch . Tensor Compute geodesic distances between quaternions. PARAMETER DESCRIPTION q1 First set of quaterions, shape (N,4). TYPE: torch . Tensor q2 Second set of quaternions, shape (N,4). TYPE: torch . Tensor RETURNS DESCRIPTION torch . Tensor Mean distance between the quaternions, scalar. Source code in sdfest/initialization/quaternion_utils.py 69 70 71 72 73 74 75 76 77 78 79 80 def geodesic_distance ( q1 : torch . Tensor , q2 : torch . Tensor ) -> torch . Tensor : \"\"\"Compute geodesic distances between quaternions. Args: q1: First set of quaterions, shape (N,4). q2: Second set of quaternions, shape (N,4). Returns: Mean distance between the quaternions, scalar. \"\"\" abs_q1q2 = torch . clip ( torch . abs ( torch . sum ( q1 * q2 , dim = 1 )), 0 , 1 ) geodesic_distances = 2 * torch . acos ( abs_q1q2 ) return geodesic_distances simple_quaternion_loss simple_quaternion_loss ( q1 : torch . Tensor , q2 : torch . Tensor ) -> torch . Tensor Compute distance measure between quaternions not involving trig functions. From https://math.stackexchange.com/a/90098 PARAMETER DESCRIPTION q1 First set of quaterions, shape (N,4). TYPE: torch . Tensor q2 Second set of quaternions, shape (N,4). TYPE: torch . Tensor RETURNS DESCRIPTION torch . Tensor Mean distance between the quaternions, scalar. Source code in sdfest/initialization/quaternion_utils.py 83 84 85 86 87 88 89 90 91 92 93 94 95 def simple_quaternion_loss ( q1 : torch . Tensor , q2 : torch . Tensor ) -> torch . Tensor : \"\"\"Compute distance measure between quaternions not involving trig functions. From: https://math.stackexchange.com/a/90098 Args: q1: First set of quaterions, shape (N,4). q2: Second set of quaternions, shape (N,4). Returns: Mean distance between the quaternions, scalar. \"\"\" return torch . mean ( 1 - torch . sum ( q1 * q2 , 1 ) ** 2 ) generate_uniform_quaternion generate_uniform_quaternion () -> torch . Tensor Generate a normalized uniform quaternion. Following the method from K. Shoemake, Uniform Random Rotations, 1992. See: http://planning.cs.uiuc.edu/node198.html RETURNS DESCRIPTION torch . Tensor Uniformly distributed unit quaternion on the estimator's device. Source code in sdfest/initialization/quaternion_utils.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def generate_uniform_quaternion () -> torch . Tensor : \"\"\"Generate a normalized uniform quaternion. Following the method from K. Shoemake, Uniform Random Rotations, 1992. See: http://planning.cs.uiuc.edu/node198.html Returns: Uniformly distributed unit quaternion on the estimator's device. \"\"\" u1 , u2 , u3 = random . random (), random . random (), random . random () return torch . tensor ( [ math . sqrt ( 1 - u1 ) * math . sin ( 2 * math . pi * u2 ), math . sqrt ( 1 - u1 ) * math . cos ( 2 * math . pi * u2 ), math . sqrt ( u1 ) * math . sin ( 2 * math . pi * u3 ), math . sqrt ( u1 ) * math . cos ( 2 * math . pi * u3 ), ] )","title":"quaternion_utils"},{"location":"reference/initialization/quaternion_utils/#sdfest.initialization.quaternion_utils","text":"Functions to handle transformations with quaternions. Inspired by PyTorch3D, but using scalar-last convention and not enforcing scalar > 0. https://github.com/facebookresearch/pytorch3d","title":"quaternion_utils"},{"location":"reference/initialization/quaternion_utils/#sdfest.initialization.quaternion_utils.quaternion_multiply","text":"quaternion_multiply ( quaternions_1 : torch . Tensor , quaternions_2 : torch . Tensor ) -> torch . Tensor Multiply two quaternions representing rotations. Normal broadcasting rules apply. PARAMETER DESCRIPTION quaternions_1 normalized quaternions of shape (..., 4), scalar-last convention TYPE: torch . Tensor quaternions_2 normalized quaternions of shape (..., 4), scalar-last convention TYPE: torch . Tensor RETURNS DESCRIPTION torch . Tensor Composition of passed quaternions. Source code in sdfest/initialization/quaternion_utils.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def quaternion_multiply ( quaternions_1 : torch . Tensor , quaternions_2 : torch . Tensor ) -> torch . Tensor : \"\"\"Multiply two quaternions representing rotations. Normal broadcasting rules apply. Args: quaternions_1: normalized quaternions of shape (..., 4), scalar-last convention quaternions_2: normalized quaternions of shape (..., 4), scalar-last convention Returns: Composition of passed quaternions. \"\"\" ax , ay , az , aw = torch . unbind ( quaternions_1 , - 1 ) bx , by , bz , bw = torch . unbind ( quaternions_2 , - 1 ) ox = aw * bx + ax * bw + ay * bz - az * by oy = aw * by - ax * bz + ay * bw + az * bx oz = aw * bz + ax * by - ay * bx + az * bw ow = aw * bw - ax * bx - ay * by - az * bz return torch . stack (( ox , oy , oz , ow ), - 1 )","title":"quaternion_multiply()"},{"location":"reference/initialization/quaternion_utils/#sdfest.initialization.quaternion_utils.quaternion_apply","text":"quaternion_apply ( quaternions : torch . Tensor , points : torch . Tensor ) -> torch . Tensor Rotate points by quaternions representing rotations. Normal broadcasting rules apply. PARAMETER DESCRIPTION quaternions normalized quaternions of shape (..., 4), scalar-last convention TYPE: torch . Tensor points points of shape (..., 3) TYPE: torch . Tensor RETURNS DESCRIPTION torch . Tensor Points rotated by the rotations representing quaternions. Source code in sdfest/initialization/quaternion_utils.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def quaternion_apply ( quaternions : torch . Tensor , points : torch . Tensor ) -> torch . Tensor : \"\"\"Rotate points by quaternions representing rotations. Normal broadcasting rules apply. Args: quaternions: normalized quaternions of shape (..., 4), scalar-last convention points: points of shape (..., 3) Returns: Points rotated by the rotations representing quaternions. \"\"\" points_as_quaternions = points . new_zeros ( points . shape [: - 1 ] + ( 4 ,)) points_as_quaternions [ ... , : - 1 ] = points return quaternion_multiply ( quaternion_multiply ( quaternions , points_as_quaternions ), quaternion_invert ( quaternions ), )[ ... , : - 1 ]","title":"quaternion_apply()"},{"location":"reference/initialization/quaternion_utils/#sdfest.initialization.quaternion_utils.quaternion_invert","text":"quaternion_invert ( quaternions : torch . Tensor ) -> torch . Tensor Invert quaternions representing orientations. PARAMETER DESCRIPTION quaternions The quaternions to invert, shape (..., 4), scalar-last convention. TYPE: torch . Tensor RETURNS DESCRIPTION torch . Tensor Inverted quaternions, same shape as quaternions. Source code in sdfest/initialization/quaternion_utils.py 57 58 59 60 61 62 63 64 65 66 def quaternion_invert ( quaternions : torch . Tensor ) -> torch . Tensor : \"\"\"Invert quaternions representing orientations. Args: quaternions: The quaternions to invert, shape (..., 4), scalar-last convention. Returns: Inverted quaternions, same shape as quaternions. \"\"\" return quaternions * quaternions . new_tensor ([ - 1 , - 1 , - 1 , 1 ])","title":"quaternion_invert()"},{"location":"reference/initialization/quaternion_utils/#sdfest.initialization.quaternion_utils.geodesic_distance","text":"geodesic_distance ( q1 : torch . Tensor , q2 : torch . Tensor ) -> torch . Tensor Compute geodesic distances between quaternions. PARAMETER DESCRIPTION q1 First set of quaterions, shape (N,4). TYPE: torch . Tensor q2 Second set of quaternions, shape (N,4). TYPE: torch . Tensor RETURNS DESCRIPTION torch . Tensor Mean distance between the quaternions, scalar. Source code in sdfest/initialization/quaternion_utils.py 69 70 71 72 73 74 75 76 77 78 79 80 def geodesic_distance ( q1 : torch . Tensor , q2 : torch . Tensor ) -> torch . Tensor : \"\"\"Compute geodesic distances between quaternions. Args: q1: First set of quaterions, shape (N,4). q2: Second set of quaternions, shape (N,4). Returns: Mean distance between the quaternions, scalar. \"\"\" abs_q1q2 = torch . clip ( torch . abs ( torch . sum ( q1 * q2 , dim = 1 )), 0 , 1 ) geodesic_distances = 2 * torch . acos ( abs_q1q2 ) return geodesic_distances","title":"geodesic_distance()"},{"location":"reference/initialization/quaternion_utils/#sdfest.initialization.quaternion_utils.simple_quaternion_loss","text":"simple_quaternion_loss ( q1 : torch . Tensor , q2 : torch . Tensor ) -> torch . Tensor Compute distance measure between quaternions not involving trig functions. From https://math.stackexchange.com/a/90098 PARAMETER DESCRIPTION q1 First set of quaterions, shape (N,4). TYPE: torch . Tensor q2 Second set of quaternions, shape (N,4). TYPE: torch . Tensor RETURNS DESCRIPTION torch . Tensor Mean distance between the quaternions, scalar. Source code in sdfest/initialization/quaternion_utils.py 83 84 85 86 87 88 89 90 91 92 93 94 95 def simple_quaternion_loss ( q1 : torch . Tensor , q2 : torch . Tensor ) -> torch . Tensor : \"\"\"Compute distance measure between quaternions not involving trig functions. From: https://math.stackexchange.com/a/90098 Args: q1: First set of quaterions, shape (N,4). q2: Second set of quaternions, shape (N,4). Returns: Mean distance between the quaternions, scalar. \"\"\" return torch . mean ( 1 - torch . sum ( q1 * q2 , 1 ) ** 2 )","title":"simple_quaternion_loss()"},{"location":"reference/initialization/quaternion_utils/#sdfest.initialization.quaternion_utils.generate_uniform_quaternion","text":"generate_uniform_quaternion () -> torch . Tensor Generate a normalized uniform quaternion. Following the method from K. Shoemake, Uniform Random Rotations, 1992. See: http://planning.cs.uiuc.edu/node198.html RETURNS DESCRIPTION torch . Tensor Uniformly distributed unit quaternion on the estimator's device. Source code in sdfest/initialization/quaternion_utils.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def generate_uniform_quaternion () -> torch . Tensor : \"\"\"Generate a normalized uniform quaternion. Following the method from K. Shoemake, Uniform Random Rotations, 1992. See: http://planning.cs.uiuc.edu/node198.html Returns: Uniformly distributed unit quaternion on the estimator's device. \"\"\" u1 , u2 , u3 = random . random (), random . random (), random . random () return torch . tensor ( [ math . sqrt ( 1 - u1 ) * math . sin ( 2 * math . pi * u2 ), math . sqrt ( 1 - u1 ) * math . cos ( 2 * math . pi * u2 ), math . sqrt ( u1 ) * math . sin ( 2 * math . pi * u3 ), math . sqrt ( u1 ) * math . cos ( 2 * math . pi * u3 ), ] )","title":"generate_uniform_quaternion()"},{"location":"reference/initialization/sdf_pose_network/","text":"sdfest.initialization.sdf_pose_network Parametrized networks for pose and shape estimation. SDFPoseHead Bases: nn . Module Parametrized head to estimate pose and shape from feature vector. Source code in sdfest/initialization/sdf_pose_network.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 class SDFPoseHead ( nn . Module ): \"\"\"Parametrized head to estimate pose and shape from feature vector.\"\"\" def __init__ ( self , in_size : int , mlp_out_sizes : List , shape_dimension : int , batchnorm : bool , orientation_repr : Optional [ str ] = \"quaternion\" , orientation_grid_resolution : Optional [ int ] = None , ): \"\"\"Initialize the SDFPoseHead. Args: in_size: number of input features mlp_out_sizes: output sizes of each linear layer shape_dimension: dimension of shape description batchnorm: whether to use batchnorm or not orientation_repr: The orientation represention. One of \"quaternion\"|\"discretized\". orientation_grid_resolution: The resolution of the SO3 grid. Only used when orientation_repr == \"discretized\". \"\"\" super () . __init__ () self . _in_size = in_size self . _mlp_out_sizes = mlp_out_sizes self . _batchnorm = batchnorm self . _shape_dimension = shape_dimension self . _orientation_repr = orientation_repr # define layers self . _linear_layers = torch . nn . ModuleList ([]) for i , out_size in enumerate ( mlp_out_sizes ): if i == 0 : self . _linear_layers . append ( nn . Linear ( self . _in_size , out_size )) else : self . _linear_layers . append ( nn . Linear ( mlp_out_sizes [ i - 1 ], out_size )) self . _bn_layers = torch . nn . ModuleList ([]) if self . _batchnorm : for out_size in mlp_out_sizes : self . _bn_layers . append ( nn . BatchNorm1d ( out_size )) if orientation_repr == \"quaternion\" : self . _grid = None self . _final_layer = nn . Linear ( mlp_out_sizes [ - 1 ], self . _shape_dimension + 8 ) elif orientation_repr == \"discretized\" : self . _grid = SO3Grid ( orientation_grid_resolution ) self . _final_layer = nn . Linear ( mlp_out_sizes [ - 1 ], self . _shape_dimension + 4 + self . _grid . num_cells () ) else : raise NotImplementedError ( f \"orientation_repr { orientation_repr } is not supported.\" ) def forward ( self , x ): \"\"\"Forward pass of the module. Input represents set of input features used to compute pose. Args: x: batch of input vectors Returns: Tuple with the following entries: The predicted shape vector. The predicted pose. The predicted scale. The predicted orientation in the specified orientation representation. For \"quaternion\" this will be of shape (N,4) with each quaternion having the order (x, y, z, w), i.e., scalar-last, and normalized. For \"discretized\" this will be of shape (N,M) based on the grid resolution. No activation function is applied. I.e., softmax has to be used to get probabilities, and cross_entropy_loss should be used during training. \"\"\" out = x for i , linear_layer in enumerate ( self . _linear_layers ): out = linear_layer ( out ) if self . _batchnorm : out = self . _bn_layers [ i ]( out ) out = nn . functional . relu ( out ) # Normalize quaternion if self . _orientation_repr == \"quaternion\" : out = self . _final_layer ( out ) orientation = out [:, self . _shape_dimension + 4 :] orientation = orientation / torch . sqrt ( torch . sum ( orientation ** 2 , 1 , keepdim = True ) ) elif self . _orientation_repr == \"discretized\" : out = self . _final_layer ( out ) orientation = out [:, self . _shape_dimension + 4 :] else : raise NotImplementedError ( f \"orientation_repr { self . orientation_repr } is not supported.\" ) return ( out [:, 0 : self . _shape_dimension ], out [:, self . _shape_dimension : self . _shape_dimension + 3 ], out [:, self . _shape_dimension + 3 ], orientation , ) __init__ __init__ ( in_size : int , mlp_out_sizes : List , shape_dimension : int , batchnorm : bool , orientation_repr : Optional [ str ] = \"quaternion\" , orientation_grid_resolution : Optional [ int ] = None , ) Initialize the SDFPoseHead. PARAMETER DESCRIPTION in_size number of input features TYPE: int mlp_out_sizes output sizes of each linear layer TYPE: List shape_dimension dimension of shape description TYPE: int batchnorm whether to use batchnorm or not TYPE: bool orientation_repr The orientation represention. One of \"quaternion\"|\"discretized\". TYPE: Optional [ str ] DEFAULT: 'quaternion' orientation_grid_resolution The resolution of the SO3 grid. Only used when orientation_repr == \"discretized\". TYPE: Optional [ int ] DEFAULT: None Source code in sdfest/initialization/sdf_pose_network.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def __init__ ( self , in_size : int , mlp_out_sizes : List , shape_dimension : int , batchnorm : bool , orientation_repr : Optional [ str ] = \"quaternion\" , orientation_grid_resolution : Optional [ int ] = None , ): \"\"\"Initialize the SDFPoseHead. Args: in_size: number of input features mlp_out_sizes: output sizes of each linear layer shape_dimension: dimension of shape description batchnorm: whether to use batchnorm or not orientation_repr: The orientation represention. One of \"quaternion\"|\"discretized\". orientation_grid_resolution: The resolution of the SO3 grid. Only used when orientation_repr == \"discretized\". \"\"\" super () . __init__ () self . _in_size = in_size self . _mlp_out_sizes = mlp_out_sizes self . _batchnorm = batchnorm self . _shape_dimension = shape_dimension self . _orientation_repr = orientation_repr # define layers self . _linear_layers = torch . nn . ModuleList ([]) for i , out_size in enumerate ( mlp_out_sizes ): if i == 0 : self . _linear_layers . append ( nn . Linear ( self . _in_size , out_size )) else : self . _linear_layers . append ( nn . Linear ( mlp_out_sizes [ i - 1 ], out_size )) self . _bn_layers = torch . nn . ModuleList ([]) if self . _batchnorm : for out_size in mlp_out_sizes : self . _bn_layers . append ( nn . BatchNorm1d ( out_size )) if orientation_repr == \"quaternion\" : self . _grid = None self . _final_layer = nn . Linear ( mlp_out_sizes [ - 1 ], self . _shape_dimension + 8 ) elif orientation_repr == \"discretized\" : self . _grid = SO3Grid ( orientation_grid_resolution ) self . _final_layer = nn . Linear ( mlp_out_sizes [ - 1 ], self . _shape_dimension + 4 + self . _grid . num_cells () ) else : raise NotImplementedError ( f \"orientation_repr { orientation_repr } is not supported.\" ) forward forward ( x ) Forward pass of the module. Input represents set of input features used to compute pose. PARAMETER DESCRIPTION x batch of input vectors RETURNS DESCRIPTION Tuple with the following entries: The predicted shape vector. The predicted pose. The predicted scale. The predicted orientation in the specified orientation representation. For \"quaternion\" this will be of shape (N,4) with each quaternion having the order (x, y, z, w), i.e., scalar-last, and normalized. For \"discretized\" this will be of shape (N,M) based on the grid resolution. No activation function is applied. I.e., softmax has to be used to get probabilities, and cross_entropy_loss should be used during training. Source code in sdfest/initialization/sdf_pose_network.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def forward ( self , x ): \"\"\"Forward pass of the module. Input represents set of input features used to compute pose. Args: x: batch of input vectors Returns: Tuple with the following entries: The predicted shape vector. The predicted pose. The predicted scale. The predicted orientation in the specified orientation representation. For \"quaternion\" this will be of shape (N,4) with each quaternion having the order (x, y, z, w), i.e., scalar-last, and normalized. For \"discretized\" this will be of shape (N,M) based on the grid resolution. No activation function is applied. I.e., softmax has to be used to get probabilities, and cross_entropy_loss should be used during training. \"\"\" out = x for i , linear_layer in enumerate ( self . _linear_layers ): out = linear_layer ( out ) if self . _batchnorm : out = self . _bn_layers [ i ]( out ) out = nn . functional . relu ( out ) # Normalize quaternion if self . _orientation_repr == \"quaternion\" : out = self . _final_layer ( out ) orientation = out [:, self . _shape_dimension + 4 :] orientation = orientation / torch . sqrt ( torch . sum ( orientation ** 2 , 1 , keepdim = True ) ) elif self . _orientation_repr == \"discretized\" : out = self . _final_layer ( out ) orientation = out [:, self . _shape_dimension + 4 :] else : raise NotImplementedError ( f \"orientation_repr { self . orientation_repr } is not supported.\" ) return ( out [:, 0 : self . _shape_dimension ], out [:, self . _shape_dimension : self . _shape_dimension + 3 ], out [:, self . _shape_dimension + 3 ], orientation , ) SDFPoseNet Bases: nn . Module Pose and shape estimation from sensor data. Composed of feature extraction backbone and shape/pose head. Source code in sdfest/initialization/sdf_pose_network.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 class SDFPoseNet ( nn . Module ): \"\"\"Pose and shape estimation from sensor data. Composed of feature extraction backbone and shape/pose head. \"\"\" def __init__ ( self , backbone : nn . Module , head : nn . Module ): \"\"\"Construct SDF pose and shape network. Args: backbone: function or class representing the backbone backbone_dict: parameters passed to backbone on construction head: function or class representing the head head_dict: parameters passed to head on construction \"\"\" super () . __init__ () self . _backbone = backbone self . _head = head def forward ( self , x ): \"\"\"Forward pass. Args: x: input compatible with backbone. Returns: output from head \"\"\" out = self . _backbone ( x ) out = self . _head ( out ) return out __init__ __init__ ( backbone : nn . Module , head : nn . Module ) Construct SDF pose and shape network. PARAMETER DESCRIPTION backbone function or class representing the backbone TYPE: nn . Module backbone_dict parameters passed to backbone on construction head function or class representing the head TYPE: nn . Module head_dict parameters passed to head on construction Source code in sdfest/initialization/sdf_pose_network.py 124 125 126 127 128 129 130 131 132 133 134 135 def __init__ ( self , backbone : nn . Module , head : nn . Module ): \"\"\"Construct SDF pose and shape network. Args: backbone: function or class representing the backbone backbone_dict: parameters passed to backbone on construction head: function or class representing the head head_dict: parameters passed to head on construction \"\"\" super () . __init__ () self . _backbone = backbone self . _head = head forward forward ( x ) Forward pass. PARAMETER DESCRIPTION x input compatible with backbone. RETURNS DESCRIPTION output from head Source code in sdfest/initialization/sdf_pose_network.py 137 138 139 140 141 142 143 144 145 146 147 def forward ( self , x ): \"\"\"Forward pass. Args: x: input compatible with backbone. Returns: output from head \"\"\" out = self . _backbone ( x ) out = self . _head ( out ) return out","title":"sdf_pose_network"},{"location":"reference/initialization/sdf_pose_network/#sdfest.initialization.sdf_pose_network","text":"Parametrized networks for pose and shape estimation.","title":"sdf_pose_network"},{"location":"reference/initialization/sdf_pose_network/#sdfest.initialization.sdf_pose_network.SDFPoseHead","text":"Bases: nn . Module Parametrized head to estimate pose and shape from feature vector. Source code in sdfest/initialization/sdf_pose_network.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 class SDFPoseHead ( nn . Module ): \"\"\"Parametrized head to estimate pose and shape from feature vector.\"\"\" def __init__ ( self , in_size : int , mlp_out_sizes : List , shape_dimension : int , batchnorm : bool , orientation_repr : Optional [ str ] = \"quaternion\" , orientation_grid_resolution : Optional [ int ] = None , ): \"\"\"Initialize the SDFPoseHead. Args: in_size: number of input features mlp_out_sizes: output sizes of each linear layer shape_dimension: dimension of shape description batchnorm: whether to use batchnorm or not orientation_repr: The orientation represention. One of \"quaternion\"|\"discretized\". orientation_grid_resolution: The resolution of the SO3 grid. Only used when orientation_repr == \"discretized\". \"\"\" super () . __init__ () self . _in_size = in_size self . _mlp_out_sizes = mlp_out_sizes self . _batchnorm = batchnorm self . _shape_dimension = shape_dimension self . _orientation_repr = orientation_repr # define layers self . _linear_layers = torch . nn . ModuleList ([]) for i , out_size in enumerate ( mlp_out_sizes ): if i == 0 : self . _linear_layers . append ( nn . Linear ( self . _in_size , out_size )) else : self . _linear_layers . append ( nn . Linear ( mlp_out_sizes [ i - 1 ], out_size )) self . _bn_layers = torch . nn . ModuleList ([]) if self . _batchnorm : for out_size in mlp_out_sizes : self . _bn_layers . append ( nn . BatchNorm1d ( out_size )) if orientation_repr == \"quaternion\" : self . _grid = None self . _final_layer = nn . Linear ( mlp_out_sizes [ - 1 ], self . _shape_dimension + 8 ) elif orientation_repr == \"discretized\" : self . _grid = SO3Grid ( orientation_grid_resolution ) self . _final_layer = nn . Linear ( mlp_out_sizes [ - 1 ], self . _shape_dimension + 4 + self . _grid . num_cells () ) else : raise NotImplementedError ( f \"orientation_repr { orientation_repr } is not supported.\" ) def forward ( self , x ): \"\"\"Forward pass of the module. Input represents set of input features used to compute pose. Args: x: batch of input vectors Returns: Tuple with the following entries: The predicted shape vector. The predicted pose. The predicted scale. The predicted orientation in the specified orientation representation. For \"quaternion\" this will be of shape (N,4) with each quaternion having the order (x, y, z, w), i.e., scalar-last, and normalized. For \"discretized\" this will be of shape (N,M) based on the grid resolution. No activation function is applied. I.e., softmax has to be used to get probabilities, and cross_entropy_loss should be used during training. \"\"\" out = x for i , linear_layer in enumerate ( self . _linear_layers ): out = linear_layer ( out ) if self . _batchnorm : out = self . _bn_layers [ i ]( out ) out = nn . functional . relu ( out ) # Normalize quaternion if self . _orientation_repr == \"quaternion\" : out = self . _final_layer ( out ) orientation = out [:, self . _shape_dimension + 4 :] orientation = orientation / torch . sqrt ( torch . sum ( orientation ** 2 , 1 , keepdim = True ) ) elif self . _orientation_repr == \"discretized\" : out = self . _final_layer ( out ) orientation = out [:, self . _shape_dimension + 4 :] else : raise NotImplementedError ( f \"orientation_repr { self . orientation_repr } is not supported.\" ) return ( out [:, 0 : self . _shape_dimension ], out [:, self . _shape_dimension : self . _shape_dimension + 3 ], out [:, self . _shape_dimension + 3 ], orientation , )","title":"SDFPoseHead"},{"location":"reference/initialization/sdf_pose_network/#sdfest.initialization.sdf_pose_network.SDFPoseHead.__init__","text":"__init__ ( in_size : int , mlp_out_sizes : List , shape_dimension : int , batchnorm : bool , orientation_repr : Optional [ str ] = \"quaternion\" , orientation_grid_resolution : Optional [ int ] = None , ) Initialize the SDFPoseHead. PARAMETER DESCRIPTION in_size number of input features TYPE: int mlp_out_sizes output sizes of each linear layer TYPE: List shape_dimension dimension of shape description TYPE: int batchnorm whether to use batchnorm or not TYPE: bool orientation_repr The orientation represention. One of \"quaternion\"|\"discretized\". TYPE: Optional [ str ] DEFAULT: 'quaternion' orientation_grid_resolution The resolution of the SO3 grid. Only used when orientation_repr == \"discretized\". TYPE: Optional [ int ] DEFAULT: None Source code in sdfest/initialization/sdf_pose_network.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def __init__ ( self , in_size : int , mlp_out_sizes : List , shape_dimension : int , batchnorm : bool , orientation_repr : Optional [ str ] = \"quaternion\" , orientation_grid_resolution : Optional [ int ] = None , ): \"\"\"Initialize the SDFPoseHead. Args: in_size: number of input features mlp_out_sizes: output sizes of each linear layer shape_dimension: dimension of shape description batchnorm: whether to use batchnorm or not orientation_repr: The orientation represention. One of \"quaternion\"|\"discretized\". orientation_grid_resolution: The resolution of the SO3 grid. Only used when orientation_repr == \"discretized\". \"\"\" super () . __init__ () self . _in_size = in_size self . _mlp_out_sizes = mlp_out_sizes self . _batchnorm = batchnorm self . _shape_dimension = shape_dimension self . _orientation_repr = orientation_repr # define layers self . _linear_layers = torch . nn . ModuleList ([]) for i , out_size in enumerate ( mlp_out_sizes ): if i == 0 : self . _linear_layers . append ( nn . Linear ( self . _in_size , out_size )) else : self . _linear_layers . append ( nn . Linear ( mlp_out_sizes [ i - 1 ], out_size )) self . _bn_layers = torch . nn . ModuleList ([]) if self . _batchnorm : for out_size in mlp_out_sizes : self . _bn_layers . append ( nn . BatchNorm1d ( out_size )) if orientation_repr == \"quaternion\" : self . _grid = None self . _final_layer = nn . Linear ( mlp_out_sizes [ - 1 ], self . _shape_dimension + 8 ) elif orientation_repr == \"discretized\" : self . _grid = SO3Grid ( orientation_grid_resolution ) self . _final_layer = nn . Linear ( mlp_out_sizes [ - 1 ], self . _shape_dimension + 4 + self . _grid . num_cells () ) else : raise NotImplementedError ( f \"orientation_repr { orientation_repr } is not supported.\" )","title":"__init__()"},{"location":"reference/initialization/sdf_pose_network/#sdfest.initialization.sdf_pose_network.SDFPoseHead.forward","text":"forward ( x ) Forward pass of the module. Input represents set of input features used to compute pose. PARAMETER DESCRIPTION x batch of input vectors RETURNS DESCRIPTION Tuple with the following entries: The predicted shape vector. The predicted pose. The predicted scale. The predicted orientation in the specified orientation representation. For \"quaternion\" this will be of shape (N,4) with each quaternion having the order (x, y, z, w), i.e., scalar-last, and normalized. For \"discretized\" this will be of shape (N,M) based on the grid resolution. No activation function is applied. I.e., softmax has to be used to get probabilities, and cross_entropy_loss should be used during training. Source code in sdfest/initialization/sdf_pose_network.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def forward ( self , x ): \"\"\"Forward pass of the module. Input represents set of input features used to compute pose. Args: x: batch of input vectors Returns: Tuple with the following entries: The predicted shape vector. The predicted pose. The predicted scale. The predicted orientation in the specified orientation representation. For \"quaternion\" this will be of shape (N,4) with each quaternion having the order (x, y, z, w), i.e., scalar-last, and normalized. For \"discretized\" this will be of shape (N,M) based on the grid resolution. No activation function is applied. I.e., softmax has to be used to get probabilities, and cross_entropy_loss should be used during training. \"\"\" out = x for i , linear_layer in enumerate ( self . _linear_layers ): out = linear_layer ( out ) if self . _batchnorm : out = self . _bn_layers [ i ]( out ) out = nn . functional . relu ( out ) # Normalize quaternion if self . _orientation_repr == \"quaternion\" : out = self . _final_layer ( out ) orientation = out [:, self . _shape_dimension + 4 :] orientation = orientation / torch . sqrt ( torch . sum ( orientation ** 2 , 1 , keepdim = True ) ) elif self . _orientation_repr == \"discretized\" : out = self . _final_layer ( out ) orientation = out [:, self . _shape_dimension + 4 :] else : raise NotImplementedError ( f \"orientation_repr { self . orientation_repr } is not supported.\" ) return ( out [:, 0 : self . _shape_dimension ], out [:, self . _shape_dimension : self . _shape_dimension + 3 ], out [:, self . _shape_dimension + 3 ], orientation , )","title":"forward()"},{"location":"reference/initialization/sdf_pose_network/#sdfest.initialization.sdf_pose_network.SDFPoseNet","text":"Bases: nn . Module Pose and shape estimation from sensor data. Composed of feature extraction backbone and shape/pose head. Source code in sdfest/initialization/sdf_pose_network.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 class SDFPoseNet ( nn . Module ): \"\"\"Pose and shape estimation from sensor data. Composed of feature extraction backbone and shape/pose head. \"\"\" def __init__ ( self , backbone : nn . Module , head : nn . Module ): \"\"\"Construct SDF pose and shape network. Args: backbone: function or class representing the backbone backbone_dict: parameters passed to backbone on construction head: function or class representing the head head_dict: parameters passed to head on construction \"\"\" super () . __init__ () self . _backbone = backbone self . _head = head def forward ( self , x ): \"\"\"Forward pass. Args: x: input compatible with backbone. Returns: output from head \"\"\" out = self . _backbone ( x ) out = self . _head ( out ) return out","title":"SDFPoseNet"},{"location":"reference/initialization/sdf_pose_network/#sdfest.initialization.sdf_pose_network.SDFPoseNet.__init__","text":"__init__ ( backbone : nn . Module , head : nn . Module ) Construct SDF pose and shape network. PARAMETER DESCRIPTION backbone function or class representing the backbone TYPE: nn . Module backbone_dict parameters passed to backbone on construction head function or class representing the head TYPE: nn . Module head_dict parameters passed to head on construction Source code in sdfest/initialization/sdf_pose_network.py 124 125 126 127 128 129 130 131 132 133 134 135 def __init__ ( self , backbone : nn . Module , head : nn . Module ): \"\"\"Construct SDF pose and shape network. Args: backbone: function or class representing the backbone backbone_dict: parameters passed to backbone on construction head: function or class representing the head head_dict: parameters passed to head on construction \"\"\" super () . __init__ () self . _backbone = backbone self . _head = head","title":"__init__()"},{"location":"reference/initialization/sdf_pose_network/#sdfest.initialization.sdf_pose_network.SDFPoseNet.forward","text":"forward ( x ) Forward pass. PARAMETER DESCRIPTION x input compatible with backbone. RETURNS DESCRIPTION output from head Source code in sdfest/initialization/sdf_pose_network.py 137 138 139 140 141 142 143 144 145 146 147 def forward ( self , x ): \"\"\"Forward pass. Args: x: input compatible with backbone. Returns: output from head \"\"\" out = self . _backbone ( x ) out = self . _head ( out ) return out","title":"forward()"},{"location":"reference/initialization/sdf_utils/","text":"sdfest.initialization.sdf_utils Utility functions to handle voxel-based signed distance fields. sdf_to_pointcloud sdf_to_pointcloud ( sdf : np . array , position : np . array , orientation : np . array , scale : float , max_points : Optional [ int ] = None , threshold : float = 0 , ) Convert SDF to pointcloud. Puts a point onto each cell vertex with a value < threshold. PARAMETER DESCRIPTION sdf The values of the voxelized SDF. Shape (D, D, D). TYPE: np . array position The position of the SDF center. Shape (3,). TYPE: np . array orientation The orientation of the SDF as a normalized quaternion. This is the quaternion that will be applied to each point. Scalar-last convention, shape (4,). TYPE: np . array scale The half-length of the SDF. TYPE: float max_points Maximum number of points in the pointcloud. TYPE: Optional [ int ] DEFAULT: None threshold The threshold below which a voxel will be included in the pointcloud. TYPE: float DEFAULT: 0 RETURNS DESCRIPTION The pointcloud as a Nx3 array. Source code in sdfest/initialization/sdf_utils.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def sdf_to_pointcloud ( sdf : np . array , position : np . array , orientation : np . array , scale : float , max_points : Optional [ int ] = None , threshold : float = 0 , ): \"\"\"Convert SDF to pointcloud. Puts a point onto each cell vertex with a value < threshold. Args: sdf: The values of the voxelized SDF. Shape (D, D, D). position: The position of the SDF center. Shape (3,). orientation: The orientation of the SDF as a normalized quaternion. This is the quaternion that will be applied to each point. Scalar-last convention, shape (4,). scale: The half-length of the SDF. max_points: Maximum number of points in the pointcloud. threshold: The threshold below which a voxel will be included in the pointcloud. Returns: The pointcloud as a Nx3 array. \"\"\" grid_size = 2.0 / ( sdf . shape [ 0 ] - 1.0 ) indices = np . argwhere ( sdf <= threshold ) points = ( indices * grid_size - 1.0 ) * scale rot_matrix = Rotation . from_quat ( orientation ) . as_matrix () points = ( rot_matrix @ points . T ) . T points += position if max_points is not None and max_points < points . shape [ 0 ]: points = points [ np . random . choice ( points . shape [ 0 ], 2 , replace = False ), :] return points","title":"sdf_utils"},{"location":"reference/initialization/sdf_utils/#sdfest.initialization.sdf_utils","text":"Utility functions to handle voxel-based signed distance fields.","title":"sdf_utils"},{"location":"reference/initialization/sdf_utils/#sdfest.initialization.sdf_utils.sdf_to_pointcloud","text":"sdf_to_pointcloud ( sdf : np . array , position : np . array , orientation : np . array , scale : float , max_points : Optional [ int ] = None , threshold : float = 0 , ) Convert SDF to pointcloud. Puts a point onto each cell vertex with a value < threshold. PARAMETER DESCRIPTION sdf The values of the voxelized SDF. Shape (D, D, D). TYPE: np . array position The position of the SDF center. Shape (3,). TYPE: np . array orientation The orientation of the SDF as a normalized quaternion. This is the quaternion that will be applied to each point. Scalar-last convention, shape (4,). TYPE: np . array scale The half-length of the SDF. TYPE: float max_points Maximum number of points in the pointcloud. TYPE: Optional [ int ] DEFAULT: None threshold The threshold below which a voxel will be included in the pointcloud. TYPE: float DEFAULT: 0 RETURNS DESCRIPTION The pointcloud as a Nx3 array. Source code in sdfest/initialization/sdf_utils.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def sdf_to_pointcloud ( sdf : np . array , position : np . array , orientation : np . array , scale : float , max_points : Optional [ int ] = None , threshold : float = 0 , ): \"\"\"Convert SDF to pointcloud. Puts a point onto each cell vertex with a value < threshold. Args: sdf: The values of the voxelized SDF. Shape (D, D, D). position: The position of the SDF center. Shape (3,). orientation: The orientation of the SDF as a normalized quaternion. This is the quaternion that will be applied to each point. Scalar-last convention, shape (4,). scale: The half-length of the SDF. max_points: Maximum number of points in the pointcloud. threshold: The threshold below which a voxel will be included in the pointcloud. Returns: The pointcloud as a Nx3 array. \"\"\" grid_size = 2.0 / ( sdf . shape [ 0 ] - 1.0 ) indices = np . argwhere ( sdf <= threshold ) points = ( indices * grid_size - 1.0 ) * scale rot_matrix = Rotation . from_quat ( orientation ) . as_matrix () points = ( rot_matrix @ points . T ) . T points += position if max_points is not None and max_points < points . shape [ 0 ]: points = points [ np . random . choice ( points . shape [ 0 ], 2 , replace = False ), :] return points","title":"sdf_to_pointcloud()"},{"location":"reference/initialization/so3grid/","text":"sdfest.initialization.so3grid This module provides a deterministic low-dispersion grid on SO3. SO3Grid Low-dispersion SO3 grid. This approach was introduced by Generating Uniform Incremental Grids on SO3 Using the Hopf Fibration, Yershova, 2010. We only generate the base grid (i.e., up to and including Section 5.2 of the paper), since we only need a fixed size set. Implementation roughly based on https://github.com/zhonge/cryodrgn Source code in sdfest/initialization/so3grid.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 class SO3Grid : \"\"\"Low-dispersion SO3 grid. This approach was introduced by Generating Uniform Incremental Grids on SO3 Using the Hopf Fibration, Yershova, 2010. We only generate the base grid (i.e., up to and including Section 5.2 of the paper), since we only need a fixed size set. Implementation roughly based on https://github.com/zhonge/cryodrgn \"\"\" def __init__ ( self , resol : int ): \"\"\"Construct the SO3 grid. Args: resol: The resolution of the grid. Coarsest possible grid for 0. \"\"\" self . _resol = resol self . _s1 = self . _grid_s1 ( resol ) self . _s2_theta , self . _s2_phi = self . _grid_s2 ( resol ) def num_cells ( self ) -> int : \"\"\"Return the number of points in the grid.\"\"\" return len ( self . _s1 ) * len ( self . _s2_theta ) def hopf_to_index ( self , psi , theta , phi ): \"\"\"Convert hopf coordinate to index. Args: phi: [0, 2pi) theta: [0, pi] psi: [0, 2pi) Returns: Grid index of closest point in grid. \"\"\" s1_index = int ( psi // ( 2 * np . pi / len ( self . _s1 ))) s2_index = hp . ang2pix ( 2 ** self . _resol , theta , phi , nest = True ) return s1_index * len ( self . _s2_theta ) + s2_index def index_to_hopf ( self , index : int ) -> Tuple [ float , float , float ]: \"\"\"Convert index to hopf coordinates. Psi: [0,2*pi) Theta: [0, pi] Phi: [0, 2*pi) Args: index: The index of the grid point. Returns: Tuple of psi, theta, phi. \"\"\" s1_index = index // len ( self . _s2_theta ) s2_index = index % len ( self . _s2_theta ) psi = self . _s1 [ s1_index ] theta = self . _s2_theta [ s2_index ] phi = self . _s2_phi [ s2_index ] return psi , theta , phi def quat_to_index ( self , quaternion : np . array ) -> int : \"\"\"Convert quaternion to index. Will convert quaternion to Hopf coordinates and look up closest Hopf coordinate. Closest means, closest in Hopf coordinates. Args: quaternion: Array of shape (4,), containing a normalized quaternion. The order of the quaternion is (x, y, z, w). Returns: The index of the closest (in Hopf coordinates) point. \"\"\" hopf = SO3Grid . _quat_to_hopf ( quaternion ) return self . hopf_to_index ( * hopf ) def index_to_quat ( self , index : int ) -> np . array : \"\"\"Convert index to quaternion. Returns: Array of shape (4,), containing the normalized quaternion corresponding to the index. \"\"\" hopf = self . index_to_hopf ( index ) return SO3Grid . _hopf_to_quat ( * hopf ) @staticmethod def _quat_to_hopf ( quaternion : np . array ) -> Tuple [ float , float , float ]: \"\"\"Convert quaternion to hopf coordinates. Args: quaternion: Array of shape (4,), containing a normalized quaternion. The order of the quaternion is (x, y, z, w). Returns: Tuple of psi, theta, phi. With psi, theta, phi in [0,2pi), [0,pi], [0,2pi) respectively. \"\"\" x , y , z , w = quaternion psi = 2 * np . arctan2 ( x , w ) theta = 2 * np . arctan2 ( np . sqrt ( z ** 2 + y ** 2 ), np . sqrt ( w ** 2 + x ** 2 )) phi = np . arctan2 ( z * w - x * y , y * w + x * z ) # Note for the following correction use while instead of if, to support # float32, because atan2 range for float32 ([-np.float32(np.pi), # np.float32(np.pi)]) is larger than for float64 ([-np.pi,np.pi]). # Psi must be [0, 2pi) and wraps around at 4*pi, so this correction changes the # the half-sphere while psi < 0 : psi += 2 * np . pi while psi >= 2 * np . pi : psi -= 2 * np . pi # Phi must be [0, 2pi) and wraps around at 2*pi, so this correction just makes # sure the angle is in the expected range while phi < 0 : phi += 2 * np . pi while phi >= 2 * np . pi : phi -= 2 * np . pi return psi , theta , phi @staticmethod def _hopf_to_quat ( psi , theta , phi ): \"\"\"Convert quaternion to hopf coordinates. Args: phi: [0, 2pi) theta: [0, pi] psi: [0, 2pi) Returns: Array of shape (4,), containing the normalized quaternion corresponding to the index. \"\"\" quaternion = np . array ( [ np . cos ( theta / 2 ) * np . sin ( psi / 2 ), # x np . sin ( theta / 2 ) * np . cos ( phi + psi / 2 ), # y np . sin ( theta / 2 ) * np . sin ( phi + psi / 2 ), # z np . cos ( theta / 2 ) * np . cos ( psi / 2 ), # w ] ) if quaternion [ 0 ] < 0 : quaternion *= - 1 return quaternion @staticmethod def _grid_s1 ( resol ): \"\"\"Compute equidistant grid on 1-sphere. Args: resol: Resolution of grid. Returns: Center points of the grid cells.\"\"\" points = 6 * 2 ** resol grid = np . linspace ( 0 , 2 * np . pi , points , endpoint = False ) + np . pi / points return grid @staticmethod def _grid_s2 ( resol ): \"\"\"Compute HEALpix coordinates of 2-sphere. Args: resol: Resolution of grid. Returns: Center points of the grid cells.\"\"\" points_per_side = 2 ** resol points = 12 * points_per_side * points_per_side theta , phi = hp . pix2ang ( points_per_side , np . arange ( points ), nest = True ) return theta , phi __init__ __init__ ( resol : int ) Construct the SO3 grid. PARAMETER DESCRIPTION resol The resolution of the grid. Coarsest possible grid for 0. TYPE: int Source code in sdfest/initialization/so3grid.py 18 19 20 21 22 23 24 25 26 def __init__ ( self , resol : int ): \"\"\"Construct the SO3 grid. Args: resol: The resolution of the grid. Coarsest possible grid for 0. \"\"\" self . _resol = resol self . _s1 = self . _grid_s1 ( resol ) self . _s2_theta , self . _s2_phi = self . _grid_s2 ( resol ) num_cells num_cells () -> int Return the number of points in the grid. Source code in sdfest/initialization/so3grid.py 28 29 30 def num_cells ( self ) -> int : \"\"\"Return the number of points in the grid.\"\"\" return len ( self . _s1 ) * len ( self . _s2_theta ) hopf_to_index hopf_to_index ( psi , theta , phi ) Convert hopf coordinate to index. PARAMETER DESCRIPTION phi [0, 2pi) theta [0, pi] psi [0, 2pi) RETURNS DESCRIPTION Grid index of closest point in grid. Source code in sdfest/initialization/so3grid.py 32 33 34 35 36 37 38 39 40 41 42 43 44 def hopf_to_index ( self , psi , theta , phi ): \"\"\"Convert hopf coordinate to index. Args: phi: [0, 2pi) theta: [0, pi] psi: [0, 2pi) Returns: Grid index of closest point in grid. \"\"\" s1_index = int ( psi // ( 2 * np . pi / len ( self . _s1 ))) s2_index = hp . ang2pix ( 2 ** self . _resol , theta , phi , nest = True ) return s1_index * len ( self . _s2_theta ) + s2_index index_to_hopf index_to_hopf ( index : int ) -> Tuple [ float , float , float ] Convert index to hopf coordinates. Psi: [0,2 pi) Theta: [0, pi] Phi: [0, 2 pi) PARAMETER DESCRIPTION index The index of the grid point. TYPE: int RETURNS DESCRIPTION Tuple [ float , float , float ] Tuple of psi, theta, phi. Source code in sdfest/initialization/so3grid.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def index_to_hopf ( self , index : int ) -> Tuple [ float , float , float ]: \"\"\"Convert index to hopf coordinates. Psi: [0,2*pi) Theta: [0, pi] Phi: [0, 2*pi) Args: index: The index of the grid point. Returns: Tuple of psi, theta, phi. \"\"\" s1_index = index // len ( self . _s2_theta ) s2_index = index % len ( self . _s2_theta ) psi = self . _s1 [ s1_index ] theta = self . _s2_theta [ s2_index ] phi = self . _s2_phi [ s2_index ] return psi , theta , phi quat_to_index quat_to_index ( quaternion : np . array ) -> int Convert quaternion to index. Will convert quaternion to Hopf coordinates and look up closest Hopf coordinate. Closest means, closest in Hopf coordinates. PARAMETER DESCRIPTION quaternion Array of shape (4,), containing a normalized quaternion. The order of the quaternion is (x, y, z, w). TYPE: np . array RETURNS DESCRIPTION int The index of the closest (in Hopf coordinates) point. Source code in sdfest/initialization/so3grid.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def quat_to_index ( self , quaternion : np . array ) -> int : \"\"\"Convert quaternion to index. Will convert quaternion to Hopf coordinates and look up closest Hopf coordinate. Closest means, closest in Hopf coordinates. Args: quaternion: Array of shape (4,), containing a normalized quaternion. The order of the quaternion is (x, y, z, w). Returns: The index of the closest (in Hopf coordinates) point. \"\"\" hopf = SO3Grid . _quat_to_hopf ( quaternion ) return self . hopf_to_index ( * hopf ) index_to_quat index_to_quat ( index : int ) -> np . array Convert index to quaternion. RETURNS DESCRIPTION np . array Array of shape (4,), containing the normalized quaternion corresponding np . array to the index. Source code in sdfest/initialization/so3grid.py 81 82 83 84 85 86 87 88 89 def index_to_quat ( self , index : int ) -> np . array : \"\"\"Convert index to quaternion. Returns: Array of shape (4,), containing the normalized quaternion corresponding to the index. \"\"\" hopf = self . index_to_hopf ( index ) return SO3Grid . _hopf_to_quat ( * hopf )","title":"so3grid"},{"location":"reference/initialization/so3grid/#sdfest.initialization.so3grid","text":"This module provides a deterministic low-dispersion grid on SO3.","title":"so3grid"},{"location":"reference/initialization/so3grid/#sdfest.initialization.so3grid.SO3Grid","text":"Low-dispersion SO3 grid. This approach was introduced by Generating Uniform Incremental Grids on SO3 Using the Hopf Fibration, Yershova, 2010. We only generate the base grid (i.e., up to and including Section 5.2 of the paper), since we only need a fixed size set. Implementation roughly based on https://github.com/zhonge/cryodrgn Source code in sdfest/initialization/so3grid.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 class SO3Grid : \"\"\"Low-dispersion SO3 grid. This approach was introduced by Generating Uniform Incremental Grids on SO3 Using the Hopf Fibration, Yershova, 2010. We only generate the base grid (i.e., up to and including Section 5.2 of the paper), since we only need a fixed size set. Implementation roughly based on https://github.com/zhonge/cryodrgn \"\"\" def __init__ ( self , resol : int ): \"\"\"Construct the SO3 grid. Args: resol: The resolution of the grid. Coarsest possible grid for 0. \"\"\" self . _resol = resol self . _s1 = self . _grid_s1 ( resol ) self . _s2_theta , self . _s2_phi = self . _grid_s2 ( resol ) def num_cells ( self ) -> int : \"\"\"Return the number of points in the grid.\"\"\" return len ( self . _s1 ) * len ( self . _s2_theta ) def hopf_to_index ( self , psi , theta , phi ): \"\"\"Convert hopf coordinate to index. Args: phi: [0, 2pi) theta: [0, pi] psi: [0, 2pi) Returns: Grid index of closest point in grid. \"\"\" s1_index = int ( psi // ( 2 * np . pi / len ( self . _s1 ))) s2_index = hp . ang2pix ( 2 ** self . _resol , theta , phi , nest = True ) return s1_index * len ( self . _s2_theta ) + s2_index def index_to_hopf ( self , index : int ) -> Tuple [ float , float , float ]: \"\"\"Convert index to hopf coordinates. Psi: [0,2*pi) Theta: [0, pi] Phi: [0, 2*pi) Args: index: The index of the grid point. Returns: Tuple of psi, theta, phi. \"\"\" s1_index = index // len ( self . _s2_theta ) s2_index = index % len ( self . _s2_theta ) psi = self . _s1 [ s1_index ] theta = self . _s2_theta [ s2_index ] phi = self . _s2_phi [ s2_index ] return psi , theta , phi def quat_to_index ( self , quaternion : np . array ) -> int : \"\"\"Convert quaternion to index. Will convert quaternion to Hopf coordinates and look up closest Hopf coordinate. Closest means, closest in Hopf coordinates. Args: quaternion: Array of shape (4,), containing a normalized quaternion. The order of the quaternion is (x, y, z, w). Returns: The index of the closest (in Hopf coordinates) point. \"\"\" hopf = SO3Grid . _quat_to_hopf ( quaternion ) return self . hopf_to_index ( * hopf ) def index_to_quat ( self , index : int ) -> np . array : \"\"\"Convert index to quaternion. Returns: Array of shape (4,), containing the normalized quaternion corresponding to the index. \"\"\" hopf = self . index_to_hopf ( index ) return SO3Grid . _hopf_to_quat ( * hopf ) @staticmethod def _quat_to_hopf ( quaternion : np . array ) -> Tuple [ float , float , float ]: \"\"\"Convert quaternion to hopf coordinates. Args: quaternion: Array of shape (4,), containing a normalized quaternion. The order of the quaternion is (x, y, z, w). Returns: Tuple of psi, theta, phi. With psi, theta, phi in [0,2pi), [0,pi], [0,2pi) respectively. \"\"\" x , y , z , w = quaternion psi = 2 * np . arctan2 ( x , w ) theta = 2 * np . arctan2 ( np . sqrt ( z ** 2 + y ** 2 ), np . sqrt ( w ** 2 + x ** 2 )) phi = np . arctan2 ( z * w - x * y , y * w + x * z ) # Note for the following correction use while instead of if, to support # float32, because atan2 range for float32 ([-np.float32(np.pi), # np.float32(np.pi)]) is larger than for float64 ([-np.pi,np.pi]). # Psi must be [0, 2pi) and wraps around at 4*pi, so this correction changes the # the half-sphere while psi < 0 : psi += 2 * np . pi while psi >= 2 * np . pi : psi -= 2 * np . pi # Phi must be [0, 2pi) and wraps around at 2*pi, so this correction just makes # sure the angle is in the expected range while phi < 0 : phi += 2 * np . pi while phi >= 2 * np . pi : phi -= 2 * np . pi return psi , theta , phi @staticmethod def _hopf_to_quat ( psi , theta , phi ): \"\"\"Convert quaternion to hopf coordinates. Args: phi: [0, 2pi) theta: [0, pi] psi: [0, 2pi) Returns: Array of shape (4,), containing the normalized quaternion corresponding to the index. \"\"\" quaternion = np . array ( [ np . cos ( theta / 2 ) * np . sin ( psi / 2 ), # x np . sin ( theta / 2 ) * np . cos ( phi + psi / 2 ), # y np . sin ( theta / 2 ) * np . sin ( phi + psi / 2 ), # z np . cos ( theta / 2 ) * np . cos ( psi / 2 ), # w ] ) if quaternion [ 0 ] < 0 : quaternion *= - 1 return quaternion @staticmethod def _grid_s1 ( resol ): \"\"\"Compute equidistant grid on 1-sphere. Args: resol: Resolution of grid. Returns: Center points of the grid cells.\"\"\" points = 6 * 2 ** resol grid = np . linspace ( 0 , 2 * np . pi , points , endpoint = False ) + np . pi / points return grid @staticmethod def _grid_s2 ( resol ): \"\"\"Compute HEALpix coordinates of 2-sphere. Args: resol: Resolution of grid. Returns: Center points of the grid cells.\"\"\" points_per_side = 2 ** resol points = 12 * points_per_side * points_per_side theta , phi = hp . pix2ang ( points_per_side , np . arange ( points ), nest = True ) return theta , phi","title":"SO3Grid"},{"location":"reference/initialization/so3grid/#sdfest.initialization.so3grid.SO3Grid.__init__","text":"__init__ ( resol : int ) Construct the SO3 grid. PARAMETER DESCRIPTION resol The resolution of the grid. Coarsest possible grid for 0. TYPE: int Source code in sdfest/initialization/so3grid.py 18 19 20 21 22 23 24 25 26 def __init__ ( self , resol : int ): \"\"\"Construct the SO3 grid. Args: resol: The resolution of the grid. Coarsest possible grid for 0. \"\"\" self . _resol = resol self . _s1 = self . _grid_s1 ( resol ) self . _s2_theta , self . _s2_phi = self . _grid_s2 ( resol )","title":"__init__()"},{"location":"reference/initialization/so3grid/#sdfest.initialization.so3grid.SO3Grid.num_cells","text":"num_cells () -> int Return the number of points in the grid. Source code in sdfest/initialization/so3grid.py 28 29 30 def num_cells ( self ) -> int : \"\"\"Return the number of points in the grid.\"\"\" return len ( self . _s1 ) * len ( self . _s2_theta )","title":"num_cells()"},{"location":"reference/initialization/so3grid/#sdfest.initialization.so3grid.SO3Grid.hopf_to_index","text":"hopf_to_index ( psi , theta , phi ) Convert hopf coordinate to index. PARAMETER DESCRIPTION phi [0, 2pi) theta [0, pi] psi [0, 2pi) RETURNS DESCRIPTION Grid index of closest point in grid. Source code in sdfest/initialization/so3grid.py 32 33 34 35 36 37 38 39 40 41 42 43 44 def hopf_to_index ( self , psi , theta , phi ): \"\"\"Convert hopf coordinate to index. Args: phi: [0, 2pi) theta: [0, pi] psi: [0, 2pi) Returns: Grid index of closest point in grid. \"\"\" s1_index = int ( psi // ( 2 * np . pi / len ( self . _s1 ))) s2_index = hp . ang2pix ( 2 ** self . _resol , theta , phi , nest = True ) return s1_index * len ( self . _s2_theta ) + s2_index","title":"hopf_to_index()"},{"location":"reference/initialization/so3grid/#sdfest.initialization.so3grid.SO3Grid.index_to_hopf","text":"index_to_hopf ( index : int ) -> Tuple [ float , float , float ] Convert index to hopf coordinates. Psi: [0,2 pi) Theta: [0, pi] Phi: [0, 2 pi) PARAMETER DESCRIPTION index The index of the grid point. TYPE: int RETURNS DESCRIPTION Tuple [ float , float , float ] Tuple of psi, theta, phi. Source code in sdfest/initialization/so3grid.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def index_to_hopf ( self , index : int ) -> Tuple [ float , float , float ]: \"\"\"Convert index to hopf coordinates. Psi: [0,2*pi) Theta: [0, pi] Phi: [0, 2*pi) Args: index: The index of the grid point. Returns: Tuple of psi, theta, phi. \"\"\" s1_index = index // len ( self . _s2_theta ) s2_index = index % len ( self . _s2_theta ) psi = self . _s1 [ s1_index ] theta = self . _s2_theta [ s2_index ] phi = self . _s2_phi [ s2_index ] return psi , theta , phi","title":"index_to_hopf()"},{"location":"reference/initialization/so3grid/#sdfest.initialization.so3grid.SO3Grid.quat_to_index","text":"quat_to_index ( quaternion : np . array ) -> int Convert quaternion to index. Will convert quaternion to Hopf coordinates and look up closest Hopf coordinate. Closest means, closest in Hopf coordinates. PARAMETER DESCRIPTION quaternion Array of shape (4,), containing a normalized quaternion. The order of the quaternion is (x, y, z, w). TYPE: np . array RETURNS DESCRIPTION int The index of the closest (in Hopf coordinates) point. Source code in sdfest/initialization/so3grid.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def quat_to_index ( self , quaternion : np . array ) -> int : \"\"\"Convert quaternion to index. Will convert quaternion to Hopf coordinates and look up closest Hopf coordinate. Closest means, closest in Hopf coordinates. Args: quaternion: Array of shape (4,), containing a normalized quaternion. The order of the quaternion is (x, y, z, w). Returns: The index of the closest (in Hopf coordinates) point. \"\"\" hopf = SO3Grid . _quat_to_hopf ( quaternion ) return self . hopf_to_index ( * hopf )","title":"quat_to_index()"},{"location":"reference/initialization/so3grid/#sdfest.initialization.so3grid.SO3Grid.index_to_quat","text":"index_to_quat ( index : int ) -> np . array Convert index to quaternion. RETURNS DESCRIPTION np . array Array of shape (4,), containing the normalized quaternion corresponding np . array to the index. Source code in sdfest/initialization/so3grid.py 81 82 83 84 85 86 87 88 89 def index_to_quat ( self , index : int ) -> np . array : \"\"\"Convert index to quaternion. Returns: Array of shape (4,), containing the normalized quaternion corresponding to the index. \"\"\" hopf = self . index_to_hopf ( index ) return SO3Grid . _hopf_to_quat ( * hopf )","title":"index_to_quat()"},{"location":"reference/initialization/utils/","text":"sdfest.initialization.utils General functions for experiments and pytorch. str_to_object str_to_object ( name : str ) -> Any Try to find object with a given name. First scope of calling function is checked for the name, then current environment (in which case name has to be a fully qualified name). In the second case, the object is imported if found. PARAMETER DESCRIPTION name Name of the object to resolve. TYPE: str RETURNS DESCRIPTION Any The object which the provided name refers to. None if no object was found. Source code in sdfest/initialization/utils.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def str_to_object ( name : str ) -> Any : \"\"\"Try to find object with a given name. First scope of calling function is checked for the name, then current environment (in which case name has to be a fully qualified name). In the second case, the object is imported if found. Args: name: Name of the object to resolve. Returns: The object which the provided name refers to. None if no object was found. \"\"\" # check callers local variables caller_locals = inspect . currentframe () . f_back . f_locals if name in caller_locals : return caller_locals [ name ] # check callers global variables (i.e., imported modules etc.) caller_globals = inspect . currentframe () . f_back . f_globals if name in caller_globals : return caller_globals [ name ] # check environment return locate ( name ) visualize_sample visualize_sample ( sample : Optional [ dict ] = None , prediction : Optional [ dict ] = None ) Visualize sample and prediction. Assumes the following conventions and keys \"scale\": Half maximum side length of bounding box. \"quaternion: Scalar-last orientation of object. Source code in sdfest/initialization/utils.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def visualize_sample ( sample : Optional [ dict ] = None , prediction : Optional [ dict ] = None ): \"\"\"Visualize sample and prediction. Assumes the following conventions and keys \"scale\": Half maximum side length of bounding box. \"quaternion: Scalar-last orientation of object. \"\"\" pointset = sample [ \"pointset\" ] . cpu () . numpy () plt . imshow ( sample [ \"mask\" ] . cpu () . numpy ()) plt . show () plt . imshow ( sample [ \"depth\" ] . cpu () . numpy ()) plt . show () fig = plt . figure () ax = fig . add_subplot ( projection = \"3d\" ) ax . set_box_aspect (( 1 , 1 , 1 )) max_points = 500 if len ( pointset ) > max_points : indices = np . random . choice ( len ( pointset ), replace = False , size = max_points ) ax . scatter ( pointset [ indices , 0 ], pointset [ indices , 1 ], pointset [ indices , 2 ]) else : ax . scatter ( pointset [:, 0 ], pointset [:, 1 ], pointset [:, 2 ]) _plot_coordinate_frame ( ax , sample ) _plot_bounding_box ( ax , sample ) set_axes_equal ( ax ) plt . show () set_axes_equal set_axes_equal ( ax ) -> None Make axes of 3D plot have equal scale. This ensures that spheres appear as spheres, cubes as cubes, ... This is needed since Matplotlib's ax.set_aspect('equal') and and ax.axis('equal') are not supported for 3D. From: https://stackoverflow.com/a/31364297 PARAMETER DESCRIPTION ax A Matplotlib axis, e.g., as output from plt.gca(). Source code in sdfest/initialization/utils.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def set_axes_equal ( ax ) -> None : \"\"\"Make axes of 3D plot have equal scale. This ensures that spheres appear as spheres, cubes as cubes, ... This is needed since Matplotlib's ax.set_aspect('equal') and and ax.axis('equal') are not supported for 3D. From: https://stackoverflow.com/a/31364297 Args: ax: A Matplotlib axis, e.g., as output from plt.gca(). \"\"\" x_limits = ax . get_xlim3d () y_limits = ax . get_ylim3d () z_limits = ax . get_zlim3d () x_range = abs ( x_limits [ 1 ] - x_limits [ 0 ]) x_middle = np . mean ( x_limits ) y_range = abs ( y_limits [ 1 ] - y_limits [ 0 ]) y_middle = np . mean ( y_limits ) z_range = abs ( z_limits [ 1 ] - z_limits [ 0 ]) z_middle = np . mean ( z_limits ) # The plot bounding box is a sphere in the sense of the infinity # norm, hence I call half the max range the plot radius. plot_radius = 0.5 * max ([ x_range , y_range , z_range ]) ax . set_xlim3d ([ x_middle - plot_radius , x_middle + plot_radius ]) ax . set_ylim3d ([ y_middle - plot_radius , y_middle + plot_radius ]) ax . set_zlim3d ([ z_middle - plot_radius , z_middle + plot_radius ]) dict_to dict_to ( data_dict : dict , device : torch . device ) -> dict Move values in dictionary of type torch.Tensor to a specfied device. PARAMETER DESCRIPTION data_dict Dictionary to be iterated over. TYPE: dict device Device to move objects of type torch.Tensor to. TYPE: torch . device RETURNS DESCRIPTION dict Dictionary containing the same keys and values as data_dict, but with all dict objects of type torch.Tensor moved to the specified device. Source code in sdfest/initialization/utils.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 def dict_to ( data_dict : dict , device : torch . device ) -> dict : \"\"\"Move values in dictionary of type torch.Tensor to a specfied device. Args: data_dict: Dictionary to be iterated over. device: Device to move objects of type torch.Tensor to. Returns: Dictionary containing the same keys and values as data_dict, but with all objects of type torch.Tensor moved to the specified device. \"\"\" new_data_dict = {} for k , v in data_dict . items (): if isinstance ( v , torch . Tensor ): new_data_dict [ k ] = v . to ( device ) else : new_data_dict [ k ] = v return new_data_dict","title":"utils"},{"location":"reference/initialization/utils/#sdfest.initialization.utils","text":"General functions for experiments and pytorch.","title":"utils"},{"location":"reference/initialization/utils/#sdfest.initialization.utils.str_to_object","text":"str_to_object ( name : str ) -> Any Try to find object with a given name. First scope of calling function is checked for the name, then current environment (in which case name has to be a fully qualified name). In the second case, the object is imported if found. PARAMETER DESCRIPTION name Name of the object to resolve. TYPE: str RETURNS DESCRIPTION Any The object which the provided name refers to. None if no object was found. Source code in sdfest/initialization/utils.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def str_to_object ( name : str ) -> Any : \"\"\"Try to find object with a given name. First scope of calling function is checked for the name, then current environment (in which case name has to be a fully qualified name). In the second case, the object is imported if found. Args: name: Name of the object to resolve. Returns: The object which the provided name refers to. None if no object was found. \"\"\" # check callers local variables caller_locals = inspect . currentframe () . f_back . f_locals if name in caller_locals : return caller_locals [ name ] # check callers global variables (i.e., imported modules etc.) caller_globals = inspect . currentframe () . f_back . f_globals if name in caller_globals : return caller_globals [ name ] # check environment return locate ( name )","title":"str_to_object()"},{"location":"reference/initialization/utils/#sdfest.initialization.utils.visualize_sample","text":"visualize_sample ( sample : Optional [ dict ] = None , prediction : Optional [ dict ] = None ) Visualize sample and prediction. Assumes the following conventions and keys \"scale\": Half maximum side length of bounding box. \"quaternion: Scalar-last orientation of object. Source code in sdfest/initialization/utils.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def visualize_sample ( sample : Optional [ dict ] = None , prediction : Optional [ dict ] = None ): \"\"\"Visualize sample and prediction. Assumes the following conventions and keys \"scale\": Half maximum side length of bounding box. \"quaternion: Scalar-last orientation of object. \"\"\" pointset = sample [ \"pointset\" ] . cpu () . numpy () plt . imshow ( sample [ \"mask\" ] . cpu () . numpy ()) plt . show () plt . imshow ( sample [ \"depth\" ] . cpu () . numpy ()) plt . show () fig = plt . figure () ax = fig . add_subplot ( projection = \"3d\" ) ax . set_box_aspect (( 1 , 1 , 1 )) max_points = 500 if len ( pointset ) > max_points : indices = np . random . choice ( len ( pointset ), replace = False , size = max_points ) ax . scatter ( pointset [ indices , 0 ], pointset [ indices , 1 ], pointset [ indices , 2 ]) else : ax . scatter ( pointset [:, 0 ], pointset [:, 1 ], pointset [:, 2 ]) _plot_coordinate_frame ( ax , sample ) _plot_bounding_box ( ax , sample ) set_axes_equal ( ax ) plt . show ()","title":"visualize_sample()"},{"location":"reference/initialization/utils/#sdfest.initialization.utils.set_axes_equal","text":"set_axes_equal ( ax ) -> None Make axes of 3D plot have equal scale. This ensures that spheres appear as spheres, cubes as cubes, ... This is needed since Matplotlib's ax.set_aspect('equal') and and ax.axis('equal') are not supported for 3D. From: https://stackoverflow.com/a/31364297 PARAMETER DESCRIPTION ax A Matplotlib axis, e.g., as output from plt.gca(). Source code in sdfest/initialization/utils.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def set_axes_equal ( ax ) -> None : \"\"\"Make axes of 3D plot have equal scale. This ensures that spheres appear as spheres, cubes as cubes, ... This is needed since Matplotlib's ax.set_aspect('equal') and and ax.axis('equal') are not supported for 3D. From: https://stackoverflow.com/a/31364297 Args: ax: A Matplotlib axis, e.g., as output from plt.gca(). \"\"\" x_limits = ax . get_xlim3d () y_limits = ax . get_ylim3d () z_limits = ax . get_zlim3d () x_range = abs ( x_limits [ 1 ] - x_limits [ 0 ]) x_middle = np . mean ( x_limits ) y_range = abs ( y_limits [ 1 ] - y_limits [ 0 ]) y_middle = np . mean ( y_limits ) z_range = abs ( z_limits [ 1 ] - z_limits [ 0 ]) z_middle = np . mean ( z_limits ) # The plot bounding box is a sphere in the sense of the infinity # norm, hence I call half the max range the plot radius. plot_radius = 0.5 * max ([ x_range , y_range , z_range ]) ax . set_xlim3d ([ x_middle - plot_radius , x_middle + plot_radius ]) ax . set_ylim3d ([ y_middle - plot_radius , y_middle + plot_radius ]) ax . set_zlim3d ([ z_middle - plot_radius , z_middle + plot_radius ])","title":"set_axes_equal()"},{"location":"reference/initialization/utils/#sdfest.initialization.utils.dict_to","text":"dict_to ( data_dict : dict , device : torch . device ) -> dict Move values in dictionary of type torch.Tensor to a specfied device. PARAMETER DESCRIPTION data_dict Dictionary to be iterated over. TYPE: dict device Device to move objects of type torch.Tensor to. TYPE: torch . device RETURNS DESCRIPTION dict Dictionary containing the same keys and values as data_dict, but with all dict objects of type torch.Tensor moved to the specified device. Source code in sdfest/initialization/utils.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 def dict_to ( data_dict : dict , device : torch . device ) -> dict : \"\"\"Move values in dictionary of type torch.Tensor to a specfied device. Args: data_dict: Dictionary to be iterated over. device: Device to move objects of type torch.Tensor to. Returns: Dictionary containing the same keys and values as data_dict, but with all objects of type torch.Tensor moved to the specified device. \"\"\" new_data_dict = {} for k , v in data_dict . items (): if isinstance ( v , torch . Tensor ): new_data_dict [ k ] = v . to ( device ) else : new_data_dict [ k ] = v return new_data_dict","title":"dict_to()"},{"location":"reference/initialization/datasets/dataset_utils/","text":"sdfest.initialization.datasets.dataset_utils Utility functions to handle various datasets. MultiDataLoader Wrapper for multiple dataloaders. Source code in sdfest/initialization/datasets/dataset_utils.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 class MultiDataLoader : \"\"\"Wrapper for multiple dataloaders.\"\"\" def __init__ ( self , data_loaders : List [ torch . utils . data . DataLoader ], probabilities : List [ float ], ) -> None : \"\"\"Initialize the class.\"\"\" self . _data_loaders = data_loaders self . _data_loader_iterators = [ iter ( dl ) for dl in self . _data_loaders ] self . _probabilities = probabilities assert len ( self . _data_loaders ) == len ( self . _probabilities ) def __iter__ ( self ) -> Iterator : \"\"\"Return infinite iterator which returns samples from sampled data_loader.\"\"\" while True : i = np . random . choice ( np . arange ( len ( self . _probabilities )), p = self . _probabilities ) try : yield next ( self . _data_loader_iterators [ i ]) except StopIteration : self . _data_loader_iterators [ i ] = iter ( self . _data_loaders [ i ]) __init__ __init__ ( data_loaders : List [ torch . utils . data . DataLoader ], probabilities : List [ float ], ) -> None Initialize the class. Source code in sdfest/initialization/datasets/dataset_utils.py 63 64 65 66 67 68 69 70 71 72 def __init__ ( self , data_loaders : List [ torch . utils . data . DataLoader ], probabilities : List [ float ], ) -> None : \"\"\"Initialize the class.\"\"\" self . _data_loaders = data_loaders self . _data_loader_iterators = [ iter ( dl ) for dl in self . _data_loaders ] self . _probabilities = probabilities assert len ( self . _data_loaders ) == len ( self . _probabilities ) __iter__ __iter__ () -> Iterator Return infinite iterator which returns samples from sampled data_loader. Source code in sdfest/initialization/datasets/dataset_utils.py 74 75 76 77 78 79 80 81 82 83 def __iter__ ( self ) -> Iterator : \"\"\"Return infinite iterator which returns samples from sampled data_loader.\"\"\" while True : i = np . random . choice ( np . arange ( len ( self . _probabilities )), p = self . _probabilities ) try : yield next ( self . _data_loader_iterators [ i ]) except StopIteration : self . _data_loader_iterators [ i ] = iter ( self . _data_loaders [ i ]) collate_samples collate_samples ( samples : List [ dict ]) -> dict Collate sample dictionaries. Performs standard batching and additionally batches pointsets by taking subset of points. Also supports non-tensor types, which will be returned as standard lists. Reduces all pointsets to a common size based on the smallest set. PARAMETER DESCRIPTION samples Dictionary containing various types of data. All keys except \"pointset\" will use standard batching. All samples are expected to contain the same keys. TYPE: List [ dict ] RETURNS DESCRIPTION dict Dictionary containing same keys as each sample. dict For \"pointset\" key: Tensor of size (N, M_min, D) where N is the batch size, M_min the number of points in the smallest pointset and D the number of channels per point. Source code in sdfest/initialization/datasets/dataset_utils.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def collate_samples ( samples : List [ dict ]) -> dict : \"\"\"Collate sample dictionaries. Performs standard batching and additionally batches pointsets by taking subset of points. Also supports non-tensor types, which will be returned as standard lists. Reduces all pointsets to a common size based on the smallest set. Args: samples: Dictionary containing various types of data. All keys except \"pointset\" will use standard batching. All samples are expected to contain the same keys. Returns: Dictionary containing same keys as each sample. For \"pointset\" key: Tensor of size (N, M_min, D) where N is the batch size, M_min the number of points in the smallest pointset and D the number of channels per point. \"\"\" batch = {} for key in samples [ 0 ] . keys (): if key == \"pointset\" : batch_size = len ( samples ) smallest_set = min ( s [ \"pointset\" ] . shape [ 0 ] for s in samples ) # limit number of points to limit memory usage smallest_set = min ( smallest_set , 2500 ) sample_pointset = samples [ 0 ][ \"pointset\" ] channels = sample_pointset . shape [ - 1 ] device = sample_pointset . device batch [ \"pointset\" ] = torch . empty ( batch_size , smallest_set , channels , device = device ) for i , sample in enumerate ( samples ): num_points = sample [ \"pointset\" ] . shape [ 0 ] point_indices = random . sample ( range ( 0 , num_points ), smallest_set ) batch [ \"pointset\" ][ i ] = sample [ \"pointset\" ][ point_indices ] elif isinstance ( samples [ 0 ][ key ], torch . Tensor ): # standard batching for torch tensors batch [ key ] = torch . stack ([ s [ key ] for s in samples ]) else : # standard list for other data types batch [ key ] = [ s [ key ] for s in samples ] return batch","title":"dataset_utils"},{"location":"reference/initialization/datasets/dataset_utils/#sdfest.initialization.datasets.dataset_utils","text":"Utility functions to handle various datasets.","title":"dataset_utils"},{"location":"reference/initialization/datasets/dataset_utils/#sdfest.initialization.datasets.dataset_utils.MultiDataLoader","text":"Wrapper for multiple dataloaders. Source code in sdfest/initialization/datasets/dataset_utils.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 class MultiDataLoader : \"\"\"Wrapper for multiple dataloaders.\"\"\" def __init__ ( self , data_loaders : List [ torch . utils . data . DataLoader ], probabilities : List [ float ], ) -> None : \"\"\"Initialize the class.\"\"\" self . _data_loaders = data_loaders self . _data_loader_iterators = [ iter ( dl ) for dl in self . _data_loaders ] self . _probabilities = probabilities assert len ( self . _data_loaders ) == len ( self . _probabilities ) def __iter__ ( self ) -> Iterator : \"\"\"Return infinite iterator which returns samples from sampled data_loader.\"\"\" while True : i = np . random . choice ( np . arange ( len ( self . _probabilities )), p = self . _probabilities ) try : yield next ( self . _data_loader_iterators [ i ]) except StopIteration : self . _data_loader_iterators [ i ] = iter ( self . _data_loaders [ i ])","title":"MultiDataLoader"},{"location":"reference/initialization/datasets/dataset_utils/#sdfest.initialization.datasets.dataset_utils.MultiDataLoader.__init__","text":"__init__ ( data_loaders : List [ torch . utils . data . DataLoader ], probabilities : List [ float ], ) -> None Initialize the class. Source code in sdfest/initialization/datasets/dataset_utils.py 63 64 65 66 67 68 69 70 71 72 def __init__ ( self , data_loaders : List [ torch . utils . data . DataLoader ], probabilities : List [ float ], ) -> None : \"\"\"Initialize the class.\"\"\" self . _data_loaders = data_loaders self . _data_loader_iterators = [ iter ( dl ) for dl in self . _data_loaders ] self . _probabilities = probabilities assert len ( self . _data_loaders ) == len ( self . _probabilities )","title":"__init__()"},{"location":"reference/initialization/datasets/dataset_utils/#sdfest.initialization.datasets.dataset_utils.MultiDataLoader.__iter__","text":"__iter__ () -> Iterator Return infinite iterator which returns samples from sampled data_loader. Source code in sdfest/initialization/datasets/dataset_utils.py 74 75 76 77 78 79 80 81 82 83 def __iter__ ( self ) -> Iterator : \"\"\"Return infinite iterator which returns samples from sampled data_loader.\"\"\" while True : i = np . random . choice ( np . arange ( len ( self . _probabilities )), p = self . _probabilities ) try : yield next ( self . _data_loader_iterators [ i ]) except StopIteration : self . _data_loader_iterators [ i ] = iter ( self . _data_loaders [ i ])","title":"__iter__()"},{"location":"reference/initialization/datasets/dataset_utils/#sdfest.initialization.datasets.dataset_utils.collate_samples","text":"collate_samples ( samples : List [ dict ]) -> dict Collate sample dictionaries. Performs standard batching and additionally batches pointsets by taking subset of points. Also supports non-tensor types, which will be returned as standard lists. Reduces all pointsets to a common size based on the smallest set. PARAMETER DESCRIPTION samples Dictionary containing various types of data. All keys except \"pointset\" will use standard batching. All samples are expected to contain the same keys. TYPE: List [ dict ] RETURNS DESCRIPTION dict Dictionary containing same keys as each sample. dict For \"pointset\" key: Tensor of size (N, M_min, D) where N is the batch size, M_min the number of points in the smallest pointset and D the number of channels per point. Source code in sdfest/initialization/datasets/dataset_utils.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def collate_samples ( samples : List [ dict ]) -> dict : \"\"\"Collate sample dictionaries. Performs standard batching and additionally batches pointsets by taking subset of points. Also supports non-tensor types, which will be returned as standard lists. Reduces all pointsets to a common size based on the smallest set. Args: samples: Dictionary containing various types of data. All keys except \"pointset\" will use standard batching. All samples are expected to contain the same keys. Returns: Dictionary containing same keys as each sample. For \"pointset\" key: Tensor of size (N, M_min, D) where N is the batch size, M_min the number of points in the smallest pointset and D the number of channels per point. \"\"\" batch = {} for key in samples [ 0 ] . keys (): if key == \"pointset\" : batch_size = len ( samples ) smallest_set = min ( s [ \"pointset\" ] . shape [ 0 ] for s in samples ) # limit number of points to limit memory usage smallest_set = min ( smallest_set , 2500 ) sample_pointset = samples [ 0 ][ \"pointset\" ] channels = sample_pointset . shape [ - 1 ] device = sample_pointset . device batch [ \"pointset\" ] = torch . empty ( batch_size , smallest_set , channels , device = device ) for i , sample in enumerate ( samples ): num_points = sample [ \"pointset\" ] . shape [ 0 ] point_indices = random . sample ( range ( 0 , num_points ), smallest_set ) batch [ \"pointset\" ][ i ] = sample [ \"pointset\" ][ point_indices ] elif isinstance ( samples [ 0 ][ key ], torch . Tensor ): # standard batching for torch tensors batch [ key ] = torch . stack ([ s [ key ] for s in samples ]) else : # standard list for other data types batch [ key ] = [ s [ key ] for s in samples ] return batch","title":"collate_samples()"},{"location":"reference/initialization/datasets/generated_dataset/","text":"sdfest.initialization.datasets.generated_dataset Module which provides SDFDataset class. SDFVAEViewDataset Bases: torch . utils . data . IterableDataset Dataset of SDF views generated by VAE and renderer from a random view. Source code in sdfest/initialization/datasets/generated_dataset.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 class SDFVAEViewDataset ( torch . utils . data . IterableDataset ): \"\"\"Dataset of SDF views generated by VAE and renderer from a random view.\"\"\" class Config ( TypedDict , total = False ): \"\"\"Configuration dictionary for SDFVAEViewDataset. Attributes: width: The width of the generated images in px. height: The height of the generated images in px. fov_deg: The horizontal fov in deg. z_min: Minimum z value (i.e., distance from camera) for the SDF. Note that positive z means in front of the camera, hence z_sampler should in most cases return positive values. z_max: Maximum z value (i.e., distance from camera) for the SDF. extent_mean: Mean extent of the SDF. Extent is the total side length of an SDF. extent_std: Standard deviation of the SDF scale. pointcloud: Whether to generate pointcloud or depth image. normalize_pose: Whether to center the augmented pointcloud at 0,0,0. Ignored if pointcloud=False orientation_repr: Which orientation representation is used. One of: \"quaternion\" \"discretized\" orientation_grid_resolution: Resolution of the orientation grid. Only used if orientation_repr is \"discretized\". mask_noise: Whether the mask should be perturbed to simulate noisy segmentation. If True a random, small, affine transform will be applied to the correct mask. The outliers will be filled with a random value sampled between mask_noise_min, and mask_noise_max. mask_noise_min: Minimum value to fill in for noisy mask. Only used if mask_noise is True. mask_noise_max: Maximum value to fill in for noisy mask. Only used if mask_noise is True. gaussian_noise_probability: Probability to apply gaussian noise filter on depth image. gaussian_noise_kernel_size: Size of the Gaussian kernel. Only used if Gaussian noise probability > 0.0. gausian_noise_kernel_std: Standard deviation of the Gaussian kernel. Only used if Gaussian noise probability > 0.0. \"\"\" width : int height : int fov_deg : float z_min : float z_max : float extent_mean : float extent_std : float pointcloud : bool normalize_pose : Optional [ bool ] render_threshold : float orientation_repr : str orientation_grid_resolution : Optional [ int ] mask_noise : bool mask_noise_min : Optional [ float ] mask_noise_max : Optional [ float ] norm_noise : bool norm_noise_min : Optional [ float ] norm_noise_max : Optional [ float ] scale_to_unit_ball : bool gaussian_noise_probability : float gaussian_noise_kernel_size : Optional [ int ] gausian_noise_kernel_std : Optional [ float ] default_config : Config = { \"device\" : \"cuda\" , \"width\" : 640 , \"height\" : 480 , \"fov_deg\" : 90 , \"render_threshold\" : 0.004 , \"normalize_pose\" : None , \"orientation_repr\" : \"quaternion\" , \"orientation_grid_resolution\" : None , \"mask_noise\" : False , \"mask_noise_min\" : 0.1 , \"mask_noise_max\" : 2.0 , \"norm_noise\" : False , \"norm_noise_min\" : - 0.2 , \"norm_noise_max\" : 0.2 , \"scale_to_unit_ball\" : False , \"gaussian_noise_probability\" : 0.0 , \"gaussian_noise_kernel_size\" : 5 , \"gaussian_noise_kernel_std\" : 1 , } def __init__ ( self , config : dict , vae : SDFVAE , ) -> None : \"\"\"Initialize the dataset. Args: config: Configuration dictionary of dataset. Provided dictionary will be merged with default_dict. See SDFVAEViewDataset.Config for supported keys. vae: The variational autoencoder used to create training samples. \"\"\" config = yoco . load_config ( config , current_dict = SDFVAEViewDataset . default_config ) self . _vae = vae self . _vae . eval () self . _device = next ( self . _vae . parameters ()) . device f = config [ \"width\" ] / math . tan ( config [ \"fov_deg\" ] * math . pi / 180.0 / 2.0 ) / 2 self . _camera = Camera ( width = config [ \"width\" ], height = config [ \"height\" ], fx = f , fy = f , cx = config [ \"width\" ] / 2 , cy = config [ \"height\" ] / 2 , pixel_center = 0.5 , ) self . _fov_deg = config [ \"fov_deg\" ] self . _z_min = config [ \"z_min\" ] self . _z_max = config [ \"z_max\" ] self . _z_sampler = lambda : random . uniform ( self . _z_min , self . _z_max ) self . _extent_mean = config [ \"extent_mean\" ] self . _extent_std = config [ \"extent_std\" ] self . _scale_sampler = ( lambda : random . gauss ( self . _extent_mean , self . _extent_std ) / 2.0 ) self . _mask_noise = config [ \"mask_noise\" ] self . _mask_noise_min = config [ \"mask_noise_min\" ] self . _mask_noise_max = config [ \"mask_noise_max\" ] self . _mask_noise_sampler = lambda : random . uniform ( config [ \"mask_noise_min\" ], config [ \"mask_noise_max\" ] ) self . _norm_noise = config [ \"norm_noise\" ] self . _norm_noise_min = config [ \"norm_noise_min\" ] self . _norm_noise_max = config [ \"norm_noise_max\" ] self . _norm_noise_sampler = lambda : random . uniform ( config [ \"norm_noise_min\" ], config [ \"norm_noise_max\" ] ) self . _scale_to_unit_ball = config [ \"scale_to_unit_ball\" ] self . _pointcloud = config [ \"pointcloud\" ] self . _normalize_pose = config [ \"normalize_pose\" ] self . _render_threshold = config [ \"render_threshold\" ] self . _orientation_repr = config [ \"orientation_repr\" ] if self . _orientation_repr == \"discretized\" : self . _orientation_grid = so3grid . SO3Grid ( config [ \"orientation_grid_resolution\" ] ) self . _gaussian_noise_probability = config [ \"gaussian_noise_probability\" ] self . _create_gaussian_kernel ( config [ \"gaussian_noise_kernel_std\" ], config [ \"gaussian_noise_kernel_size\" ] ) def __iter__ ( self ) -> Iterator : \"\"\"Return SDF volume at a specific index. Returns: Infinite iterator, generating sample dictionaries. See SDFVAEViewDataset._generate_sample for more details about returned dictionaries. \"\"\" # this is an infinite iterator as the sentinel False will never be returned while True : yield self . _generate_valid_sample () def _generate_uniform_quaternion ( self ) -> torch . Tensor : \"\"\"Generate a uniform quaternion. Following the method from K. Shoemake, Uniform Random Rotations, 1992. See: http://planning.cs.uiuc.edu/node198.html Returns: Uniformly distributed unit quaternion on the dataset's device. \"\"\" u1 , u2 , u3 = random . random (), random . random (), random . random () return torch . tensor ( [ math . sqrt ( 1 - u1 ) * math . sin ( 2 * math . pi * u2 ), math . sqrt ( 1 - u1 ) * math . cos ( 2 * math . pi * u2 ), math . sqrt ( u1 ) * math . sin ( 2 * math . pi * u3 ), math . sqrt ( u1 ) * math . cos ( 2 * math . pi * u3 ), ] ) . to ( self . _device ) def _is_valid ( self , sample : dict ) -> bool : \"\"\"Check whether a generated sample is valid. A valid sample contains at least one valid point in the depth image and hence in the pointcloud. \"\"\" if sample [ \"depth\" ] . max () == 0 : return False return True def _generate_valid_sample ( self ) -> dict : \"\"\"Generate a single non-zero sample. Returns: See _generate_sample. \"\"\" sample = self . _generate_sample () while not self . _is_valid ( sample ): sample = self . _generate_sample () print ( \"Warning: invalid sample, this should only happen very infrequently.\" ) # if this happens often, either the SDF does not have any zero crossings # or the object pose is completely outside the frustum return sample def _perturb_mask ( self , mask : torch . Tensor ) -> torch . Tensor : \"\"\"Perturb mask by applying small random affine transform to it. Args: mask: The mask to perturb. Returns: The perturbed mask. Same shape as mask. \"\"\" affine_transfomer = T . RandomAffine ( degrees = ( 0 , 1 ), translate = ( 0.00 , 0.01 ), scale = ( 0.999 , 1.001 ) ) return affine_transfomer ( mask . unsqueeze ( 0 ))[ 0 ] def _generate_sample ( self ) -> Tuple : \"\"\"Generate a single sample. Possibly (albeit very unlikely) zero / empty. Return: Sample containing the following items: \"depth\" \"pointset\" \"latent_shape\" \"position\" \"orientation\" \"quaternion\" \"scale\" \"\"\" sample = {} latent = self . _vae . sample () # will be 1xlatent_size (batch size 1 here) with torch . no_grad (): sdf = self . _vae . decode ( latent ) # generate x, y, z s.t. center is inside frustum z = self . _z_sampler () x_pix = random . uniform ( - self . _camera . width / 2 , self . _camera . height / 2 ) x = x_pix / self . _camera . fx * z y_pix = random . uniform ( - self . _camera . height / 2 , self . _camera . height / 2 ) y = y_pix / self . _camera . fy * z position = torch . tensor ([ x , y , - z ]) . to ( self . _device ) quaternion = self . _generate_uniform_quaternion () scale = torch . tensor ( self . _scale_sampler ()) . to ( self . _device ) inv_scale = 1.0 / scale orientation = self . _quat_to_orientation_repr ( quaternion ) depth = render_depth_gpu ( sdf [ 0 , 0 ], position , quaternion , inv_scale , threshold = self . _render_threshold , camera = self . _camera , ) exact_mask = depth != 0 if self . _mask_noise : final_mask = self . _perturb_mask ( exact_mask ) depth [ ~ exact_mask ] = self . _mask_noise_sampler () else : final_mask = exact_mask if self . _gaussian_noise_probability > 0.0 : if random . random () < self . _gaussian_noise_probability : invalid_depth_mask = depth == 0 depth [ invalid_depth_mask ] = torch . nan depth_filtered = torch . nn . functional . conv2d ( depth [ None , None ], self . _gaussian_kernel , padding = \"same\" )[ 0 , 0 ] # nan might become inf be preserved # https://github.com/pytorch/pytorch/issues/12484 mask = torch . logical_or ( depth_filtered . isnan (), depth_filtered . isinf ()) depth [ ~ mask ] = depth_filtered [ ~ mask ] depth [ depth . isnan ()] = 0.0 depth [ ~ final_mask ] = 0 if self . _pointcloud : pointset = pointset_utils . depth_to_pointcloud ( depth , self . _camera , convention = \"opengl\" ) if self . _normalize_pose : pointset , centroid = pointset_utils . normalize_points ( pointset ) position -= centroid # adjust target if self . _norm_noise : noise = position . new_tensor ( [ self . _norm_noise_sampler (), self . _norm_noise_sampler (), self . _norm_noise_sampler (), ] ) position += noise pointset += noise if self . _scale_to_unit_ball : max_distance = torch . max ( torch . linalg . norm ( pointset )) pointset /= max_distance scale /= max_distance sample [ \"pointset\" ] = pointset sample [ \"depth\" ] = depth sample [ \"latent_shape\" ] = latent . squeeze () sample [ \"position\" ] = position sample [ \"orientation\" ] = orientation sample [ \"quaternion\" ] = quaternion sample [ \"scale\" ] = scale return sample # TODO: add augmentation def _quat_to_orientation_repr ( self , quaternion : torch . Tensor ) -> torch . Tensor : \"\"\"Convert quaternion to selected orientation representation. Args: quaternion: The quaternion to convert, scalar-last, shape (4,). Returns: The same orientation as represented by the quaternion in the chosen orientation representation. \"\"\" if self . _orientation_repr == \"quaternion\" : return quaternion elif self . _orientation_repr == \"discretized\" : index = self . _orientation_grid . quat_to_index ( quaternion . cpu () . numpy ()) return torch . tensor ( index , device = self . _device , dtype = torch . long , ) else : raise NotImplementedError ( f \"Orientation representation { self . _orientation_repr } is not supported.\" ) def _create_gaussian_kernel ( self , std : float , kernel_size : int ) -> None : \"\"\"Create and set Gaussian noise kernel used for smoothing the depth image.\"\"\" if kernel_size % 2 != 1 : raise ValueError ( \"Kernel size should be odd.\" ) impulse = np . zeros (( kernel_size , kernel_size )) impulse [ kernel_size // 2 , kernel_size // 2 ] = 1 kernel = gaussian_filter ( impulse , std ) self . _gaussian_kernel = torch . Tensor ( kernel [ None , None ]) . to ( self . _device ) Config Bases: TypedDict Configuration dictionary for SDFVAEViewDataset. ATTRIBUTE DESCRIPTION width The width of the generated images in px. TYPE: int height The height of the generated images in px. TYPE: int fov_deg The horizontal fov in deg. TYPE: float z_min Minimum z value (i.e., distance from camera) for the SDF. Note that positive z means in front of the camera, hence z_sampler should in most cases return positive values. TYPE: float z_max Maximum z value (i.e., distance from camera) for the SDF. TYPE: float extent_mean Mean extent of the SDF. Extent is the total side length of an SDF. TYPE: float extent_std Standard deviation of the SDF scale. TYPE: float pointcloud Whether to generate pointcloud or depth image. TYPE: bool normalize_pose Whether to center the augmented pointcloud at 0,0,0. Ignored if pointcloud=False TYPE: Optional [ bool ] orientation_repr Which orientation representation is used. One of: \"quaternion\" \"discretized\" TYPE: str orientation_grid_resolution Resolution of the orientation grid. Only used if orientation_repr is \"discretized\". TYPE: Optional [ int ] mask_noise Whether the mask should be perturbed to simulate noisy segmentation. If True a random, small, affine transform will be applied to the correct mask. The outliers will be filled with a random value sampled between mask_noise_min, and mask_noise_max. TYPE: bool mask_noise_min Minimum value to fill in for noisy mask. Only used if mask_noise is True. TYPE: Optional [ float ] mask_noise_max Maximum value to fill in for noisy mask. Only used if mask_noise is True. TYPE: Optional [ float ] gaussian_noise_probability Probability to apply gaussian noise filter on depth image. TYPE: float gaussian_noise_kernel_size Size of the Gaussian kernel. Only used if Gaussian noise probability > 0.0. TYPE: Optional [ int ] gausian_noise_kernel_std Standard deviation of the Gaussian kernel. Only used if Gaussian noise probability > 0.0. TYPE: Optional [ float ] Source code in sdfest/initialization/datasets/generated_dataset.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 class Config ( TypedDict , total = False ): \"\"\"Configuration dictionary for SDFVAEViewDataset. Attributes: width: The width of the generated images in px. height: The height of the generated images in px. fov_deg: The horizontal fov in deg. z_min: Minimum z value (i.e., distance from camera) for the SDF. Note that positive z means in front of the camera, hence z_sampler should in most cases return positive values. z_max: Maximum z value (i.e., distance from camera) for the SDF. extent_mean: Mean extent of the SDF. Extent is the total side length of an SDF. extent_std: Standard deviation of the SDF scale. pointcloud: Whether to generate pointcloud or depth image. normalize_pose: Whether to center the augmented pointcloud at 0,0,0. Ignored if pointcloud=False orientation_repr: Which orientation representation is used. One of: \"quaternion\" \"discretized\" orientation_grid_resolution: Resolution of the orientation grid. Only used if orientation_repr is \"discretized\". mask_noise: Whether the mask should be perturbed to simulate noisy segmentation. If True a random, small, affine transform will be applied to the correct mask. The outliers will be filled with a random value sampled between mask_noise_min, and mask_noise_max. mask_noise_min: Minimum value to fill in for noisy mask. Only used if mask_noise is True. mask_noise_max: Maximum value to fill in for noisy mask. Only used if mask_noise is True. gaussian_noise_probability: Probability to apply gaussian noise filter on depth image. gaussian_noise_kernel_size: Size of the Gaussian kernel. Only used if Gaussian noise probability > 0.0. gausian_noise_kernel_std: Standard deviation of the Gaussian kernel. Only used if Gaussian noise probability > 0.0. \"\"\" width : int height : int fov_deg : float z_min : float z_max : float extent_mean : float extent_std : float pointcloud : bool normalize_pose : Optional [ bool ] render_threshold : float orientation_repr : str orientation_grid_resolution : Optional [ int ] mask_noise : bool mask_noise_min : Optional [ float ] mask_noise_max : Optional [ float ] norm_noise : bool norm_noise_min : Optional [ float ] norm_noise_max : Optional [ float ] scale_to_unit_ball : bool gaussian_noise_probability : float gaussian_noise_kernel_size : Optional [ int ] gausian_noise_kernel_std : Optional [ float ] __init__ __init__ ( config : dict , vae : SDFVAE ) -> None Initialize the dataset. PARAMETER DESCRIPTION config Configuration dictionary of dataset. Provided dictionary will be merged with default_dict. See SDFVAEViewDataset.Config for supported keys. TYPE: dict vae The variational autoencoder used to create training samples. TYPE: SDFVAE Source code in sdfest/initialization/datasets/generated_dataset.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 def __init__ ( self , config : dict , vae : SDFVAE , ) -> None : \"\"\"Initialize the dataset. Args: config: Configuration dictionary of dataset. Provided dictionary will be merged with default_dict. See SDFVAEViewDataset.Config for supported keys. vae: The variational autoencoder used to create training samples. \"\"\" config = yoco . load_config ( config , current_dict = SDFVAEViewDataset . default_config ) self . _vae = vae self . _vae . eval () self . _device = next ( self . _vae . parameters ()) . device f = config [ \"width\" ] / math . tan ( config [ \"fov_deg\" ] * math . pi / 180.0 / 2.0 ) / 2 self . _camera = Camera ( width = config [ \"width\" ], height = config [ \"height\" ], fx = f , fy = f , cx = config [ \"width\" ] / 2 , cy = config [ \"height\" ] / 2 , pixel_center = 0.5 , ) self . _fov_deg = config [ \"fov_deg\" ] self . _z_min = config [ \"z_min\" ] self . _z_max = config [ \"z_max\" ] self . _z_sampler = lambda : random . uniform ( self . _z_min , self . _z_max ) self . _extent_mean = config [ \"extent_mean\" ] self . _extent_std = config [ \"extent_std\" ] self . _scale_sampler = ( lambda : random . gauss ( self . _extent_mean , self . _extent_std ) / 2.0 ) self . _mask_noise = config [ \"mask_noise\" ] self . _mask_noise_min = config [ \"mask_noise_min\" ] self . _mask_noise_max = config [ \"mask_noise_max\" ] self . _mask_noise_sampler = lambda : random . uniform ( config [ \"mask_noise_min\" ], config [ \"mask_noise_max\" ] ) self . _norm_noise = config [ \"norm_noise\" ] self . _norm_noise_min = config [ \"norm_noise_min\" ] self . _norm_noise_max = config [ \"norm_noise_max\" ] self . _norm_noise_sampler = lambda : random . uniform ( config [ \"norm_noise_min\" ], config [ \"norm_noise_max\" ] ) self . _scale_to_unit_ball = config [ \"scale_to_unit_ball\" ] self . _pointcloud = config [ \"pointcloud\" ] self . _normalize_pose = config [ \"normalize_pose\" ] self . _render_threshold = config [ \"render_threshold\" ] self . _orientation_repr = config [ \"orientation_repr\" ] if self . _orientation_repr == \"discretized\" : self . _orientation_grid = so3grid . SO3Grid ( config [ \"orientation_grid_resolution\" ] ) self . _gaussian_noise_probability = config [ \"gaussian_noise_probability\" ] self . _create_gaussian_kernel ( config [ \"gaussian_noise_kernel_std\" ], config [ \"gaussian_noise_kernel_size\" ] ) __iter__ __iter__ () -> Iterator Return SDF volume at a specific index. RETURNS DESCRIPTION Iterator Infinite iterator, generating sample dictionaries. Iterator See SDFVAEViewDataset._generate_sample for more details about returned Iterator dictionaries. Source code in sdfest/initialization/datasets/generated_dataset.py 178 179 180 181 182 183 184 185 186 187 188 def __iter__ ( self ) -> Iterator : \"\"\"Return SDF volume at a specific index. Returns: Infinite iterator, generating sample dictionaries. See SDFVAEViewDataset._generate_sample for more details about returned dictionaries. \"\"\" # this is an infinite iterator as the sentinel False will never be returned while True : yield self . _generate_valid_sample ()","title":"generated_dataset"},{"location":"reference/initialization/datasets/generated_dataset/#sdfest.initialization.datasets.generated_dataset","text":"Module which provides SDFDataset class.","title":"generated_dataset"},{"location":"reference/initialization/datasets/generated_dataset/#sdfest.initialization.datasets.generated_dataset.SDFVAEViewDataset","text":"Bases: torch . utils . data . IterableDataset Dataset of SDF views generated by VAE and renderer from a random view. Source code in sdfest/initialization/datasets/generated_dataset.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 class SDFVAEViewDataset ( torch . utils . data . IterableDataset ): \"\"\"Dataset of SDF views generated by VAE and renderer from a random view.\"\"\" class Config ( TypedDict , total = False ): \"\"\"Configuration dictionary for SDFVAEViewDataset. Attributes: width: The width of the generated images in px. height: The height of the generated images in px. fov_deg: The horizontal fov in deg. z_min: Minimum z value (i.e., distance from camera) for the SDF. Note that positive z means in front of the camera, hence z_sampler should in most cases return positive values. z_max: Maximum z value (i.e., distance from camera) for the SDF. extent_mean: Mean extent of the SDF. Extent is the total side length of an SDF. extent_std: Standard deviation of the SDF scale. pointcloud: Whether to generate pointcloud or depth image. normalize_pose: Whether to center the augmented pointcloud at 0,0,0. Ignored if pointcloud=False orientation_repr: Which orientation representation is used. One of: \"quaternion\" \"discretized\" orientation_grid_resolution: Resolution of the orientation grid. Only used if orientation_repr is \"discretized\". mask_noise: Whether the mask should be perturbed to simulate noisy segmentation. If True a random, small, affine transform will be applied to the correct mask. The outliers will be filled with a random value sampled between mask_noise_min, and mask_noise_max. mask_noise_min: Minimum value to fill in for noisy mask. Only used if mask_noise is True. mask_noise_max: Maximum value to fill in for noisy mask. Only used if mask_noise is True. gaussian_noise_probability: Probability to apply gaussian noise filter on depth image. gaussian_noise_kernel_size: Size of the Gaussian kernel. Only used if Gaussian noise probability > 0.0. gausian_noise_kernel_std: Standard deviation of the Gaussian kernel. Only used if Gaussian noise probability > 0.0. \"\"\" width : int height : int fov_deg : float z_min : float z_max : float extent_mean : float extent_std : float pointcloud : bool normalize_pose : Optional [ bool ] render_threshold : float orientation_repr : str orientation_grid_resolution : Optional [ int ] mask_noise : bool mask_noise_min : Optional [ float ] mask_noise_max : Optional [ float ] norm_noise : bool norm_noise_min : Optional [ float ] norm_noise_max : Optional [ float ] scale_to_unit_ball : bool gaussian_noise_probability : float gaussian_noise_kernel_size : Optional [ int ] gausian_noise_kernel_std : Optional [ float ] default_config : Config = { \"device\" : \"cuda\" , \"width\" : 640 , \"height\" : 480 , \"fov_deg\" : 90 , \"render_threshold\" : 0.004 , \"normalize_pose\" : None , \"orientation_repr\" : \"quaternion\" , \"orientation_grid_resolution\" : None , \"mask_noise\" : False , \"mask_noise_min\" : 0.1 , \"mask_noise_max\" : 2.0 , \"norm_noise\" : False , \"norm_noise_min\" : - 0.2 , \"norm_noise_max\" : 0.2 , \"scale_to_unit_ball\" : False , \"gaussian_noise_probability\" : 0.0 , \"gaussian_noise_kernel_size\" : 5 , \"gaussian_noise_kernel_std\" : 1 , } def __init__ ( self , config : dict , vae : SDFVAE , ) -> None : \"\"\"Initialize the dataset. Args: config: Configuration dictionary of dataset. Provided dictionary will be merged with default_dict. See SDFVAEViewDataset.Config for supported keys. vae: The variational autoencoder used to create training samples. \"\"\" config = yoco . load_config ( config , current_dict = SDFVAEViewDataset . default_config ) self . _vae = vae self . _vae . eval () self . _device = next ( self . _vae . parameters ()) . device f = config [ \"width\" ] / math . tan ( config [ \"fov_deg\" ] * math . pi / 180.0 / 2.0 ) / 2 self . _camera = Camera ( width = config [ \"width\" ], height = config [ \"height\" ], fx = f , fy = f , cx = config [ \"width\" ] / 2 , cy = config [ \"height\" ] / 2 , pixel_center = 0.5 , ) self . _fov_deg = config [ \"fov_deg\" ] self . _z_min = config [ \"z_min\" ] self . _z_max = config [ \"z_max\" ] self . _z_sampler = lambda : random . uniform ( self . _z_min , self . _z_max ) self . _extent_mean = config [ \"extent_mean\" ] self . _extent_std = config [ \"extent_std\" ] self . _scale_sampler = ( lambda : random . gauss ( self . _extent_mean , self . _extent_std ) / 2.0 ) self . _mask_noise = config [ \"mask_noise\" ] self . _mask_noise_min = config [ \"mask_noise_min\" ] self . _mask_noise_max = config [ \"mask_noise_max\" ] self . _mask_noise_sampler = lambda : random . uniform ( config [ \"mask_noise_min\" ], config [ \"mask_noise_max\" ] ) self . _norm_noise = config [ \"norm_noise\" ] self . _norm_noise_min = config [ \"norm_noise_min\" ] self . _norm_noise_max = config [ \"norm_noise_max\" ] self . _norm_noise_sampler = lambda : random . uniform ( config [ \"norm_noise_min\" ], config [ \"norm_noise_max\" ] ) self . _scale_to_unit_ball = config [ \"scale_to_unit_ball\" ] self . _pointcloud = config [ \"pointcloud\" ] self . _normalize_pose = config [ \"normalize_pose\" ] self . _render_threshold = config [ \"render_threshold\" ] self . _orientation_repr = config [ \"orientation_repr\" ] if self . _orientation_repr == \"discretized\" : self . _orientation_grid = so3grid . SO3Grid ( config [ \"orientation_grid_resolution\" ] ) self . _gaussian_noise_probability = config [ \"gaussian_noise_probability\" ] self . _create_gaussian_kernel ( config [ \"gaussian_noise_kernel_std\" ], config [ \"gaussian_noise_kernel_size\" ] ) def __iter__ ( self ) -> Iterator : \"\"\"Return SDF volume at a specific index. Returns: Infinite iterator, generating sample dictionaries. See SDFVAEViewDataset._generate_sample for more details about returned dictionaries. \"\"\" # this is an infinite iterator as the sentinel False will never be returned while True : yield self . _generate_valid_sample () def _generate_uniform_quaternion ( self ) -> torch . Tensor : \"\"\"Generate a uniform quaternion. Following the method from K. Shoemake, Uniform Random Rotations, 1992. See: http://planning.cs.uiuc.edu/node198.html Returns: Uniformly distributed unit quaternion on the dataset's device. \"\"\" u1 , u2 , u3 = random . random (), random . random (), random . random () return torch . tensor ( [ math . sqrt ( 1 - u1 ) * math . sin ( 2 * math . pi * u2 ), math . sqrt ( 1 - u1 ) * math . cos ( 2 * math . pi * u2 ), math . sqrt ( u1 ) * math . sin ( 2 * math . pi * u3 ), math . sqrt ( u1 ) * math . cos ( 2 * math . pi * u3 ), ] ) . to ( self . _device ) def _is_valid ( self , sample : dict ) -> bool : \"\"\"Check whether a generated sample is valid. A valid sample contains at least one valid point in the depth image and hence in the pointcloud. \"\"\" if sample [ \"depth\" ] . max () == 0 : return False return True def _generate_valid_sample ( self ) -> dict : \"\"\"Generate a single non-zero sample. Returns: See _generate_sample. \"\"\" sample = self . _generate_sample () while not self . _is_valid ( sample ): sample = self . _generate_sample () print ( \"Warning: invalid sample, this should only happen very infrequently.\" ) # if this happens often, either the SDF does not have any zero crossings # or the object pose is completely outside the frustum return sample def _perturb_mask ( self , mask : torch . Tensor ) -> torch . Tensor : \"\"\"Perturb mask by applying small random affine transform to it. Args: mask: The mask to perturb. Returns: The perturbed mask. Same shape as mask. \"\"\" affine_transfomer = T . RandomAffine ( degrees = ( 0 , 1 ), translate = ( 0.00 , 0.01 ), scale = ( 0.999 , 1.001 ) ) return affine_transfomer ( mask . unsqueeze ( 0 ))[ 0 ] def _generate_sample ( self ) -> Tuple : \"\"\"Generate a single sample. Possibly (albeit very unlikely) zero / empty. Return: Sample containing the following items: \"depth\" \"pointset\" \"latent_shape\" \"position\" \"orientation\" \"quaternion\" \"scale\" \"\"\" sample = {} latent = self . _vae . sample () # will be 1xlatent_size (batch size 1 here) with torch . no_grad (): sdf = self . _vae . decode ( latent ) # generate x, y, z s.t. center is inside frustum z = self . _z_sampler () x_pix = random . uniform ( - self . _camera . width / 2 , self . _camera . height / 2 ) x = x_pix / self . _camera . fx * z y_pix = random . uniform ( - self . _camera . height / 2 , self . _camera . height / 2 ) y = y_pix / self . _camera . fy * z position = torch . tensor ([ x , y , - z ]) . to ( self . _device ) quaternion = self . _generate_uniform_quaternion () scale = torch . tensor ( self . _scale_sampler ()) . to ( self . _device ) inv_scale = 1.0 / scale orientation = self . _quat_to_orientation_repr ( quaternion ) depth = render_depth_gpu ( sdf [ 0 , 0 ], position , quaternion , inv_scale , threshold = self . _render_threshold , camera = self . _camera , ) exact_mask = depth != 0 if self . _mask_noise : final_mask = self . _perturb_mask ( exact_mask ) depth [ ~ exact_mask ] = self . _mask_noise_sampler () else : final_mask = exact_mask if self . _gaussian_noise_probability > 0.0 : if random . random () < self . _gaussian_noise_probability : invalid_depth_mask = depth == 0 depth [ invalid_depth_mask ] = torch . nan depth_filtered = torch . nn . functional . conv2d ( depth [ None , None ], self . _gaussian_kernel , padding = \"same\" )[ 0 , 0 ] # nan might become inf be preserved # https://github.com/pytorch/pytorch/issues/12484 mask = torch . logical_or ( depth_filtered . isnan (), depth_filtered . isinf ()) depth [ ~ mask ] = depth_filtered [ ~ mask ] depth [ depth . isnan ()] = 0.0 depth [ ~ final_mask ] = 0 if self . _pointcloud : pointset = pointset_utils . depth_to_pointcloud ( depth , self . _camera , convention = \"opengl\" ) if self . _normalize_pose : pointset , centroid = pointset_utils . normalize_points ( pointset ) position -= centroid # adjust target if self . _norm_noise : noise = position . new_tensor ( [ self . _norm_noise_sampler (), self . _norm_noise_sampler (), self . _norm_noise_sampler (), ] ) position += noise pointset += noise if self . _scale_to_unit_ball : max_distance = torch . max ( torch . linalg . norm ( pointset )) pointset /= max_distance scale /= max_distance sample [ \"pointset\" ] = pointset sample [ \"depth\" ] = depth sample [ \"latent_shape\" ] = latent . squeeze () sample [ \"position\" ] = position sample [ \"orientation\" ] = orientation sample [ \"quaternion\" ] = quaternion sample [ \"scale\" ] = scale return sample # TODO: add augmentation def _quat_to_orientation_repr ( self , quaternion : torch . Tensor ) -> torch . Tensor : \"\"\"Convert quaternion to selected orientation representation. Args: quaternion: The quaternion to convert, scalar-last, shape (4,). Returns: The same orientation as represented by the quaternion in the chosen orientation representation. \"\"\" if self . _orientation_repr == \"quaternion\" : return quaternion elif self . _orientation_repr == \"discretized\" : index = self . _orientation_grid . quat_to_index ( quaternion . cpu () . numpy ()) return torch . tensor ( index , device = self . _device , dtype = torch . long , ) else : raise NotImplementedError ( f \"Orientation representation { self . _orientation_repr } is not supported.\" ) def _create_gaussian_kernel ( self , std : float , kernel_size : int ) -> None : \"\"\"Create and set Gaussian noise kernel used for smoothing the depth image.\"\"\" if kernel_size % 2 != 1 : raise ValueError ( \"Kernel size should be odd.\" ) impulse = np . zeros (( kernel_size , kernel_size )) impulse [ kernel_size // 2 , kernel_size // 2 ] = 1 kernel = gaussian_filter ( impulse , std ) self . _gaussian_kernel = torch . Tensor ( kernel [ None , None ]) . to ( self . _device )","title":"SDFVAEViewDataset"},{"location":"reference/initialization/datasets/generated_dataset/#sdfest.initialization.datasets.generated_dataset.SDFVAEViewDataset.Config","text":"Bases: TypedDict Configuration dictionary for SDFVAEViewDataset. ATTRIBUTE DESCRIPTION width The width of the generated images in px. TYPE: int height The height of the generated images in px. TYPE: int fov_deg The horizontal fov in deg. TYPE: float z_min Minimum z value (i.e., distance from camera) for the SDF. Note that positive z means in front of the camera, hence z_sampler should in most cases return positive values. TYPE: float z_max Maximum z value (i.e., distance from camera) for the SDF. TYPE: float extent_mean Mean extent of the SDF. Extent is the total side length of an SDF. TYPE: float extent_std Standard deviation of the SDF scale. TYPE: float pointcloud Whether to generate pointcloud or depth image. TYPE: bool normalize_pose Whether to center the augmented pointcloud at 0,0,0. Ignored if pointcloud=False TYPE: Optional [ bool ] orientation_repr Which orientation representation is used. One of: \"quaternion\" \"discretized\" TYPE: str orientation_grid_resolution Resolution of the orientation grid. Only used if orientation_repr is \"discretized\". TYPE: Optional [ int ] mask_noise Whether the mask should be perturbed to simulate noisy segmentation. If True a random, small, affine transform will be applied to the correct mask. The outliers will be filled with a random value sampled between mask_noise_min, and mask_noise_max. TYPE: bool mask_noise_min Minimum value to fill in for noisy mask. Only used if mask_noise is True. TYPE: Optional [ float ] mask_noise_max Maximum value to fill in for noisy mask. Only used if mask_noise is True. TYPE: Optional [ float ] gaussian_noise_probability Probability to apply gaussian noise filter on depth image. TYPE: float gaussian_noise_kernel_size Size of the Gaussian kernel. Only used if Gaussian noise probability > 0.0. TYPE: Optional [ int ] gausian_noise_kernel_std Standard deviation of the Gaussian kernel. Only used if Gaussian noise probability > 0.0. TYPE: Optional [ float ] Source code in sdfest/initialization/datasets/generated_dataset.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 class Config ( TypedDict , total = False ): \"\"\"Configuration dictionary for SDFVAEViewDataset. Attributes: width: The width of the generated images in px. height: The height of the generated images in px. fov_deg: The horizontal fov in deg. z_min: Minimum z value (i.e., distance from camera) for the SDF. Note that positive z means in front of the camera, hence z_sampler should in most cases return positive values. z_max: Maximum z value (i.e., distance from camera) for the SDF. extent_mean: Mean extent of the SDF. Extent is the total side length of an SDF. extent_std: Standard deviation of the SDF scale. pointcloud: Whether to generate pointcloud or depth image. normalize_pose: Whether to center the augmented pointcloud at 0,0,0. Ignored if pointcloud=False orientation_repr: Which orientation representation is used. One of: \"quaternion\" \"discretized\" orientation_grid_resolution: Resolution of the orientation grid. Only used if orientation_repr is \"discretized\". mask_noise: Whether the mask should be perturbed to simulate noisy segmentation. If True a random, small, affine transform will be applied to the correct mask. The outliers will be filled with a random value sampled between mask_noise_min, and mask_noise_max. mask_noise_min: Minimum value to fill in for noisy mask. Only used if mask_noise is True. mask_noise_max: Maximum value to fill in for noisy mask. Only used if mask_noise is True. gaussian_noise_probability: Probability to apply gaussian noise filter on depth image. gaussian_noise_kernel_size: Size of the Gaussian kernel. Only used if Gaussian noise probability > 0.0. gausian_noise_kernel_std: Standard deviation of the Gaussian kernel. Only used if Gaussian noise probability > 0.0. \"\"\" width : int height : int fov_deg : float z_min : float z_max : float extent_mean : float extent_std : float pointcloud : bool normalize_pose : Optional [ bool ] render_threshold : float orientation_repr : str orientation_grid_resolution : Optional [ int ] mask_noise : bool mask_noise_min : Optional [ float ] mask_noise_max : Optional [ float ] norm_noise : bool norm_noise_min : Optional [ float ] norm_noise_max : Optional [ float ] scale_to_unit_ball : bool gaussian_noise_probability : float gaussian_noise_kernel_size : Optional [ int ] gausian_noise_kernel_std : Optional [ float ]","title":"Config"},{"location":"reference/initialization/datasets/generated_dataset/#sdfest.initialization.datasets.generated_dataset.SDFVAEViewDataset.__init__","text":"__init__ ( config : dict , vae : SDFVAE ) -> None Initialize the dataset. PARAMETER DESCRIPTION config Configuration dictionary of dataset. Provided dictionary will be merged with default_dict. See SDFVAEViewDataset.Config for supported keys. TYPE: dict vae The variational autoencoder used to create training samples. TYPE: SDFVAE Source code in sdfest/initialization/datasets/generated_dataset.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 def __init__ ( self , config : dict , vae : SDFVAE , ) -> None : \"\"\"Initialize the dataset. Args: config: Configuration dictionary of dataset. Provided dictionary will be merged with default_dict. See SDFVAEViewDataset.Config for supported keys. vae: The variational autoencoder used to create training samples. \"\"\" config = yoco . load_config ( config , current_dict = SDFVAEViewDataset . default_config ) self . _vae = vae self . _vae . eval () self . _device = next ( self . _vae . parameters ()) . device f = config [ \"width\" ] / math . tan ( config [ \"fov_deg\" ] * math . pi / 180.0 / 2.0 ) / 2 self . _camera = Camera ( width = config [ \"width\" ], height = config [ \"height\" ], fx = f , fy = f , cx = config [ \"width\" ] / 2 , cy = config [ \"height\" ] / 2 , pixel_center = 0.5 , ) self . _fov_deg = config [ \"fov_deg\" ] self . _z_min = config [ \"z_min\" ] self . _z_max = config [ \"z_max\" ] self . _z_sampler = lambda : random . uniform ( self . _z_min , self . _z_max ) self . _extent_mean = config [ \"extent_mean\" ] self . _extent_std = config [ \"extent_std\" ] self . _scale_sampler = ( lambda : random . gauss ( self . _extent_mean , self . _extent_std ) / 2.0 ) self . _mask_noise = config [ \"mask_noise\" ] self . _mask_noise_min = config [ \"mask_noise_min\" ] self . _mask_noise_max = config [ \"mask_noise_max\" ] self . _mask_noise_sampler = lambda : random . uniform ( config [ \"mask_noise_min\" ], config [ \"mask_noise_max\" ] ) self . _norm_noise = config [ \"norm_noise\" ] self . _norm_noise_min = config [ \"norm_noise_min\" ] self . _norm_noise_max = config [ \"norm_noise_max\" ] self . _norm_noise_sampler = lambda : random . uniform ( config [ \"norm_noise_min\" ], config [ \"norm_noise_max\" ] ) self . _scale_to_unit_ball = config [ \"scale_to_unit_ball\" ] self . _pointcloud = config [ \"pointcloud\" ] self . _normalize_pose = config [ \"normalize_pose\" ] self . _render_threshold = config [ \"render_threshold\" ] self . _orientation_repr = config [ \"orientation_repr\" ] if self . _orientation_repr == \"discretized\" : self . _orientation_grid = so3grid . SO3Grid ( config [ \"orientation_grid_resolution\" ] ) self . _gaussian_noise_probability = config [ \"gaussian_noise_probability\" ] self . _create_gaussian_kernel ( config [ \"gaussian_noise_kernel_std\" ], config [ \"gaussian_noise_kernel_size\" ] )","title":"__init__()"},{"location":"reference/initialization/datasets/generated_dataset/#sdfest.initialization.datasets.generated_dataset.SDFVAEViewDataset.__iter__","text":"__iter__ () -> Iterator Return SDF volume at a specific index. RETURNS DESCRIPTION Iterator Infinite iterator, generating sample dictionaries. Iterator See SDFVAEViewDataset._generate_sample for more details about returned Iterator dictionaries. Source code in sdfest/initialization/datasets/generated_dataset.py 178 179 180 181 182 183 184 185 186 187 188 def __iter__ ( self ) -> Iterator : \"\"\"Return SDF volume at a specific index. Returns: Infinite iterator, generating sample dictionaries. See SDFVAEViewDataset._generate_sample for more details about returned dictionaries. \"\"\" # this is an infinite iterator as the sentinel False will never be returned while True : yield self . _generate_valid_sample ()","title":"__iter__()"},{"location":"reference/initialization/datasets/nocs_dataset/","text":"sdfest.initialization.datasets.nocs_dataset Module providing dataset class for NOCS datasets (CAMERA / REAL). NOCSDataset Bases: torch . utils . data . Dataset Dataset class for NOCS dataset. CAMERA and REAL are training sets. CAMERA25 and REAL275 are test data. Some papers use CAMERA25 as validation when benchmarking REAL275. Datasets can be found here: https://github.com/hughw19/NOCS_CVPR2019/tree/master Expected directory format {root_dir}/real_train/... {root_dir}/real_test/... {root_dir}/gts/... {root_dir}/obj_models/... {root_dir}/camera_composed_depth/... {root_dir}/val/... {root_dir}/train/... Which is easily obtained by downloading all the provided files and extracting them into the same directory. Necessary preprocessing of this data is performed during first initialization per and is saved to {root_dir}/sdfest_pre/... Source code in sdfest/initialization/datasets/nocs_dataset.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 class NOCSDataset ( torch . utils . data . Dataset ): \"\"\"Dataset class for NOCS dataset. CAMERA* and REAL* are training sets. CAMERA25 and REAL275 are test data. Some papers use CAMERA25 as validation when benchmarking REAL275. Datasets can be found here: https://github.com/hughw19/NOCS_CVPR2019/tree/master Expected directory format: {root_dir}/real_train/... {root_dir}/real_test/... {root_dir}/gts/... {root_dir}/obj_models/... {root_dir}/camera_composed_depth/... {root_dir}/val/... {root_dir}/train/... Which is easily obtained by downloading all the provided files and extracting them into the same directory. Necessary preprocessing of this data is performed during first initialization per and is saved to {root_dir}/sdfest_pre/... \"\"\" num_categories = 7 category_id_to_str = { 0 : \"unknown\" , 1 : \"bottle\" , 2 : \"bowl\" , 3 : \"camera\" , 4 : \"can\" , 5 : \"laptop\" , 6 : \"mug\" , } category_str_to_id = { v : k for k , v in category_id_to_str . items ()} class Config ( TypedDict , total = False ): \"\"\"Configuration dictionary for NOCSDataset. Attributes: root_dir: See NOCSDataset docstring. split: The dataset split. The following strings are supported: \"camera_train\": 275000 images, synthetic objects + real background \"camera_val\": 25000 images, synthetic objects + real background \"real_train\": 4300 images in 7 scenes, real \"real_test\": 2750 images in 6 scenes, real mask_pointcloud: Whether the returned pointcloud will be masked. normalize_pointcloud: Whether the returned pointcloud and position will be normalized, such that pointcloud centroid is at the origin. scale_convention: Which scale is returned. The following strings are supported: \"diagonal\": Length of bounding box' diagonal. This is what NOCS uses. \"max\": Maximum side length of bounding box. \"half_max\": Half maximum side length of bounding box. \"full\": Bounding box side lengths. Shape (3,). camera_convention: Which camera convention is used for position and orientation. One of: \"opengl\": x right, y up, z back \"opencv\": x right, y down, z forward Note that this does not influence how the dataset is processed, only the returned position and quaternion. orientation_repr: Which orientation representation is used. One of: \"quaternion\" \"discretized\" orientation_grid_resolution: Resolution of the orientation grid. Only used if orientation_repr is \"discretized\". remap_y_axis: If not None, the NOCS y-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. This is typically the up-axis. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: y One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\" remap_x_axis: If not None, the original x-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: -z One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\" category_str: If not None, only samples from the matching category will be returned. See NOCSDataset.category_id_to_str for admissible category strings. \"\"\" root_dir : str split : str mask_pointcloud : bool normalize_pointcloud : bool scale_convention : str camera_convention : str orientation_repr : str orientation_grid_resolution : int remap_y_axis : Optional [ str ] remap_x_axis : Optional [ str ] category_str : Optional [ str ] # TODO symmetry # TODO unify dataset code, adapt generated_dataset default_config : Config = { \"root_dir\" : None , \"split\" : None , \"mask_pointcloud\" : False , \"normalize_pointcloud\" : False , \"camera_convention\" : \"opengl\" , \"scale_convention\" : \"half_max\" , \"orientation_repr\" : \"quaternion\" , \"orientation_grid_resolution\" : None , \"category_str\" : None , \"remap_y_axis\" : None , \"remap_x_axis\" : None , } def __init__ ( self , config : Config , ) -> None : \"\"\"Initialize the dataset. Args: config: Configuration dictionary of dataset. Provided dictionary will be merged with default_dict. See NOCSDataset.Config for supported keys. \"\"\" config = yoco . load_config ( config , current_dict = NOCSDataset . default_config ) self . _root_dir = config [ \"root_dir\" ] self . _split = config [ \"split\" ] self . _camera_convention = config [ \"camera_convention\" ] self . _camera = self . _get_split_camera () self . _preprocess_path = os . path . join ( self . _root_dir , \"sdfest_pre\" , self . _split ) if not os . path . isdir ( self . _preprocess_path ): self . _preprocess_dataset () self . _mask_pointcloud = config [ \"mask_pointcloud\" ] self . _normalize_pointcloud = config [ \"normalize_pointcloud\" ] self . _scale_convention = config [ \"scale_convention\" ] self . _sample_files = self . _get_sample_files ( config [ \"category_str\" ]) self . _remap_y_axis = config [ \"remap_y_axis\" ] self . _remap_x_axis = config [ \"remap_x_axis\" ] self . _orientation_repr = config [ \"orientation_repr\" ] if self . _orientation_repr == \"discretized\" : self . _orientation_grid = so3grid . SO3Grid ( config [ \"orientation_grid_resolution\" ] ) def __len__ ( self ) -> int : \"\"\"Return number of sample in dataset.\"\"\" return len ( self . _sample_files ) def __getitem__ ( self , idx : int ) -> dict : \"\"\"Return a sample of the dataset. Args: idx: Index of the instance. Returns: Sample containing the following items: \"color\" \"depth\" \"mask\" \"pointset\" \"position\" \"orientation\" \"quaternion\" \"scale\" \"color_path\" \"\"\" sample_file = self . _sample_files [ idx ] sample_data = pickle . load ( open ( sample_file , \"rb\" )) sample = self . _sample_from_sample_data ( sample_data ) return sample def _preprocess_dataset ( self ) -> None : \"\"\"Create preprocessing files for the current split. One file per sample, which currently means per valid object mask will be created. Preprocessing will be stored on disk to {root_dir}/sdfest_pre/... This function will not store the preprocessing, so it still has to be loaded afterwards. \"\"\" os . makedirs ( self . _preprocess_path ) self . _fix_obj_models () self . _start_time = time . time () self . _color_paths = self . _get_color_files () Parallel ( n_jobs =- 1 )( ( delayed ( self . _preprocess_color_path )( i , color_path ) for i , color_path in enumerate ( self . _color_paths ) ) ) # store dictionary to map category to files sample_files = self . _get_sample_files () category_str_to_files = { category_str : [] for category_str in NOCSDataset . category_id_to_str . values () } for sample_file in tqdm ( sample_files ): sample_data = pickle . load ( open ( sample_file , \"rb\" )) category_id = sample_data [ \"category_id\" ] category_str = NOCSDataset . category_id_to_str [ category_id ] _ , file_name = os . path . split ( sample_file ) category_str_to_files [ category_str ] . append ( file_name ) category_json_path = os . path . join ( self . _preprocess_path , \"categories.json\" ) with open ( category_json_path , \"w\" ) as f : json . dump ( dict ( category_str_to_files ), f ) print ( f \"Finished preprocessing for { self . _split } .\" , end = \" \\033 [K \\n \" ) def _fix_obj_models ( self ) -> None : \"\"\"Fix issues with fileextensions. Some png files have jpg extension. This function fixes these models. \"\"\" glob_pattern = os . path . join ( self . _root_dir , \"**\" , \"*.jpg\" ) files = glob ( glob_pattern , recursive = True ) for filepath in files : what = imghdr . what ( filepath ) if what == \"png\" : print ( \"Fixing: \" , filepath ) folder , problematic_filename = os . path . split ( filepath ) name , _ = problematic_filename . split ( \".\" ) fixed_filename = f \"fixed_ { name } .png\" fixed_filepath = os . path . join ( folder , fixed_filename ) mtl_filepath = os . path . join ( folder , \"model.mtl\" ) bu_mtl_filepath = os . path . join ( folder , \"model.mtl.old\" ) copyfile ( mtl_filepath , bu_mtl_filepath ) copyfile ( filepath , fixed_filepath ) def _update_preprocess_progress ( self , image_id : int ) -> None : current_time = time . time () duration = current_time - self . _start_time imgs_per_sec = image_id / duration if image_id > 10 : remaining_imgs = len ( self . _color_paths ) - image_id remaining_secs = remaining_imgs / imgs_per_sec remaining_time_str = str ( datetime . timedelta ( seconds = round ( remaining_secs ))) else : remaining_time_str = \"N/A\" print ( f \"Preprocessing image: { image_id : >10 } / { len ( self . _color_paths ) } \" f \" { image_id / len ( self . _color_paths ) * 100 : >6.2f } %\" # progress percentage f \" Remaining time: { remaining_time_str } \" # remaining time \" \\033 [K\" , # clear until end of line end = \" \\r \" , # overwrite previous ) def _preprocess_color_path ( self , image_id : int , color_path : str ) -> None : counter = 0 self . _update_preprocess_progress ( image_id ) depth_path = self . _depth_path_from_color_path ( color_path ) if not os . path . isfile ( depth_path ): print ( f \"Missing depth file { depth_path } . Skipping.\" , end = \" \\033 [K \\n \" ) return mask_path = self . _mask_path_from_color_path ( color_path ) meta_path = self . _meta_path_from_color_path ( color_path ) meta_data = pd . read_csv ( meta_path , sep = \" \" , header = None , converters = { 2 : lambda x : str ( x )} ) instances_mask = self . _load_mask ( mask_path ) mask_ids = np . unique ( instances_mask ) . tolist () gt_id = 0 # GT only contains valid objects of interests and is 0-indexed for mask_id in mask_ids : if mask_id == 255 : # 255 is background continue match = meta_data [ meta_data . iloc [:, 0 ] == mask_id ] if match . empty : print ( f \"Warning: mask { mask_id } not found in { meta_path } \" , end = \" \\033 [K \\n \" ) elif match . shape [ 0 ] != 1 : print ( f \"Warning: mask { mask_id } not unique in { meta_path } \" , end = \" \\033 [K \\n \" ) meta_row = match . iloc [ 0 ] category_id = meta_row . iloc [ 1 ] if category_id == 0 : # unknown / distractor object continue try : ( position , orientation_q , extents , nocs_transform , ) = self . _get_pose_and_scale ( color_path , mask_id , gt_id , meta_row ) except nocs_utils . PoseEstimationError : print ( \"Insufficient data for pose estimation. \" f \"Skipping { color_path } : { mask_id } .\" , end = \" \\033 [K \\n \" , ) continue except ObjectError : print ( \"Insufficient object mesh for pose estimation. \" f \"Skipping { color_path } : { mask_id } .\" , end = \" \\033 [K \\n \" , ) continue obj_path = self . _get_obj_path ( meta_row ) sample_info = { \"color_path\" : color_path , \"depth_path\" : depth_path , \"mask_path\" : mask_path , \"mask_id\" : mask_id , \"category_id\" : category_id , \"obj_path\" : obj_path , \"nocs_transform\" : nocs_transform , \"position\" : position , \"orientation_q\" : orientation_q , \"extents\" : extents , \"nocs_scale\" : torch . linalg . norm ( extents ), \"max_extent\" : torch . max ( extents ), } out_file = os . path . join ( self . _preprocess_path , f \" { image_id : 08 } _ { counter } .pkl\" ) pickle . dump ( sample_info , open ( out_file , \"wb\" )) counter += 1 gt_id += 1 def _get_color_files ( self ) -> list : \"\"\"Return list of paths of color images of the selected split.\"\"\" if self . _split == \"camera_train\" : glob_pattern = os . path . join ( self . _root_dir , \"train\" , \"**\" , \"*_color.png\" ) return sorted ( glob ( glob_pattern , recursive = True )) elif self . _split == \"camera_val\" : glob_pattern = os . path . join ( self . _root_dir , \"val\" , \"**\" , \"*_color.png\" ) return sorted ( glob ( glob_pattern , recursive = True )) elif self . _split == \"real_train\" : glob_pattern = os . path . join ( self . _root_dir , \"real_train\" , \"**\" , \"*_color.png\" ) return sorted ( glob ( glob_pattern , recursive = True )) elif self . _split == \"real_test\" : glob_pattern = os . path . join ( self . _root_dir , \"real_test\" , \"**\" , \"*_color.png\" ) return sorted ( glob ( glob_pattern , recursive = True )) else : raise ValueError ( f \"Specified split { self . _split } is not supported.\" ) def _get_sample_files ( self , category_str : Optional [ str ] = None ) -> list : \"\"\"Return sorted list of sample file paths. Sample files are generated by NOCSDataset._preprocess_dataset. Args: category_str: If not None, only instances of the provided category will be returned. Returns: List of sample_data files. \"\"\" glob_pattern = os . path . join ( self . _preprocess_path , \"*.pkl\" ) sample_files = glob ( glob_pattern ) sample_files . sort () if category_str is None : return sample_files if category_str not in NOCSDataset . category_str_to_id : raise ValueError ( f \"Unsupported category_str { category_str } .\" ) category_json_path = os . path . join ( self . _preprocess_path , \"categories.json\" ) with open ( category_json_path , \"r\" ) as f : category_str_to_filenames = json . load ( f ) filtered_sample_files = [ os . path . join ( self . _preprocess_path , fn ) for fn in category_str_to_filenames [ category_str ] ] return filtered_sample_files def _get_split_camera ( self ) -> None : \"\"\"Return camera information for selected split.\"\"\" # from: https://github.com/hughw19/NOCS_CVPR2019/blob/master/detect_eval.py if self . _split in [ \"real_train\" , \"real_test\" ]: return Camera ( width = 640 , height = 480 , fx = 591.0125 , fy = 590.16775 , cx = 322.525 , cy = 244.11084 , pixel_center = 0.0 , ) elif self . _split in [ \"camera_train\" , \"camera_val\" ]: return Camera ( width = 640 , height = 480 , fx = 577.5 , fy = 577.5 , cx = 319.5 , cy = 239.5 , pixel_center = 0.0 , ) else : raise ValueError ( f \"Specified split { self . _split } is not supported.\" ) def _sample_from_sample_data ( self , sample_data : dict ) -> dict : \"\"\"Create dictionary containing a single sample.\"\"\" color = torch . from_numpy ( np . asarray ( Image . open ( sample_data [ \"color_path\" ]), dtype = np . float32 ) / 255 ) depth = self . _load_depth ( sample_data [ \"depth_path\" ]) instances_mask = self . _load_mask ( sample_data [ \"mask_path\" ]) instance_mask = instances_mask == sample_data [ \"mask_id\" ] pointcloud_mask = instance_mask if self . _mask_pointcloud else None pointcloud = pointset_utils . depth_to_pointcloud ( depth , self . _camera , mask = pointcloud_mask , convention = self . _camera_convention , ) # adjust camera convention for position, orientation and scale position = pointset_utils . change_position_camera_convention ( sample_data [ \"position\" ], \"opencv\" , self . _camera_convention ) # orientation / scale orientation_q , extents = self . _change_axis_convention ( sample_data [ \"orientation_q\" ], sample_data [ \"extents\" ] ) orientation_q = pointset_utils . change_orientation_camera_convention ( orientation_q , \"opencv\" , self . _camera_convention ) orientation = self . _quat_to_orientation_repr ( orientation_q ) scale = self . _get_scale ( sample_data , extents ) # normalize pointcloud & position if self . _normalize_pointcloud : pointcloud , centroid = pointset_utils . normalize_points ( pointcloud ) position = position - centroid sample = { \"color\" : color , \"depth\" : depth , \"pointset\" : pointcloud , \"mask\" : instance_mask , \"position\" : position , \"orientation\" : orientation , \"quaternion\" : orientation_q , \"scale\" : scale , \"color_path\" : sample_data [ \"color_path\" ], \"obj_path\" : sample_data [ \"obj_path\" ], \"category_id\" : sample_data [ \"category_id\" ], \"category_str\" : NOCSDataset . category_id_to_str [ sample_data [ \"category_id\" ]], } return sample def _depth_path_from_color_path ( self , color_path : str ) -> str : \"\"\"Return path to depth file from color filepath.\"\"\" if self . _split in [ \"real_train\" , \"real_test\" ]: depth_path = color_path . replace ( \"color\" , \"depth\" ) elif self . _split in [ \"camera_train\" ]: depth_path = color_path . replace ( \"color\" , \"composed\" ) depth_path = depth_path . replace ( \"/train/\" , \"/camera_full_depths/train/\" ) elif self . _split in [ \"camera_val\" ]: depth_path = color_path . replace ( \"color\" , \"composed\" ) depth_path = depth_path . replace ( \"/val/\" , \"/camera_full_depths/val/\" ) else : raise ValueError ( f \"Specified split { self . _split } is not supported.\" ) return depth_path def _mask_path_from_color_path ( self , color_path : str ) -> str : \"\"\"Return path to mask file from color filepath.\"\"\" mask_path = color_path . replace ( \"color\" , \"mask\" ) return mask_path def _meta_path_from_color_path ( self , color_path : str ) -> str : \"\"\"Return path to meta file from color filepath.\"\"\" meta_path = color_path . replace ( \"color.png\" , \"meta.txt\" ) return meta_path def _nocs_map_path_from_color_path ( self , color_path : str ) -> str : \"\"\"Return NOCS map filepath from color filepath.\"\"\" nocs_map_path = color_path . replace ( \"color.png\" , \"coord.png\" ) return nocs_map_path def _get_pose_and_scale ( self , color_path : str , mask_id : int , gt_id : int , meta_row : pd . Series ) -> tuple : \"\"\"Return position, orientation, scale and NOCS transform. All of those follow OpenCV (x right, y down, z forward) convention. Args: color_path: Path to the color file. mask_id: Instance id in the instances mask. gt_id: Ground truth id. This is 0-indexed id of valid instances in meta file. meta_row: Matching row of meta file. Contains necessary information about mesh. Returns: position (torch.Tensor): Position of object center in camera frame. Shape (3,). quaternion (torch.Tensor): Orientation of object in camera frame. Scalar-last quaternion, shape (4,). extents (torch.Tensor): Bounding box side lengths. nocs_transformation (torch.Tensor): Transformation from centered [-0.5,0.5]^3 NOCS coordinates to camera. \"\"\" gts_path = self . _get_gts_path ( color_path ) obj_path = self . _get_obj_path ( meta_row ) if self . _split == \"real_test\" : # only use gt for real test data, since there are errors in camera val gts_data = pickle . load ( open ( gts_path , \"rb\" )) nocs_transform = gts_data [ \"gt_RTs\" ][ gt_id ] position = nocs_transform [ 0 : 3 , 3 ] rot_scale = nocs_transform [ 0 : 3 , 0 : 3 ] nocs_scales = np . sqrt ( np . sum ( rot_scale ** 2 , axis = 0 )) rotation_matrix = rot_scale / nocs_scales [:, None ] nocs_scale = nocs_scales [ 0 ] else : # camera_train, camera_val, real_train # use ground truth NOCS mask to perform alignment ( position , rotation_matrix , nocs_scale , nocs_transform , ) = self . _estimate_object ( color_path , mask_id ) orientation_q = Rotation . from_matrix ( rotation_matrix ) . as_quat () mesh_extents = self . _get_mesh_extents_from_obj ( obj_path ) if \"camera\" in self . _split : # CAMERA / ShapeNet meshes are normalized s.t. diagonal == 1 # get metric extents by scaling with the diagonal extents = nocs_scale * mesh_extents elif \"real\" in self . _split : # REAL object meshes are not normalized extents = mesh_extents else : raise ValueError ( f \"Specified split { self . _split } is not supported.\" ) position = torch . Tensor ( position ) orientation_q = torch . Tensor ( orientation_q ) extents = torch . Tensor ( extents ) nocs_transform = torch . Tensor ( nocs_transform ) return position , orientation_q , extents , nocs_transform def _get_gts_path ( self , color_path : str ) -> Optional [ str ]: \"\"\"Return path to gts file from color filepath. Return None if split does not have ground truth information. \"\"\" if self . _split == \"real_test\" : gts_folder = os . path . join ( self . _root_dir , \"gts\" , \"real_test\" ) elif self . _split == \"camera_val\" : gts_folder = os . path . join ( self . _root_dir , \"gts\" , \"val\" ) else : return None path = os . path . normpath ( color_path ) split_path = path . split ( os . sep ) number = path [ - 14 : - 10 ] gts_filename = f \"results_ { split_path [ - 3 ] } _ { split_path [ - 2 ] } _ { number } .pkl\" gts_path = os . path . join ( gts_folder , gts_filename ) return gts_path def _get_obj_path ( self , meta_row : pd . Series ) -> str : \"\"\"Return path to object file from meta data row.\"\"\" if \"camera\" in self . _split : # ShapeNet mesh synset_id = meta_row . iloc [ 2 ] object_id = meta_row . iloc [ 3 ] obj_path = os . path . join ( self . _root_dir , \"obj_models\" , self . _split . replace ( \"camera_\" , \"\" ), synset_id , object_id , \"model.obj\" , ) elif \"real\" in self . _split : # REAL mesh object_id = meta_row . iloc [ 2 ] obj_path = os . path . join ( self . _root_dir , \"obj_models\" , self . _split , object_id + \".obj\" ) else : raise ValueError ( f \"Specified split { self . _split } is not supported.\" ) return obj_path def _get_mesh_extents_from_obj ( self , obj_path : str ) -> torch . Tensor : \"\"\"Return maximum extent of bounding box from obj filepath. Note that this is normalized extent (with diagonal == 1) in the case of CAMERA dataset, and unnormalized (i.e., metric) extent in the case of REAL dataset. \"\"\" mesh = o3d . io . read_triangle_mesh ( obj_path ) vertices = np . asarray ( mesh . vertices ) if len ( vertices ) == 0 : raise ObjectError () extents = np . max ( vertices , axis = 0 ) - np . min ( vertices , axis = 0 ) return torch . Tensor ( extents ) def _load_mask ( self , mask_path : str ) -> torch . Tensor : \"\"\"Load mask from mask filepath.\"\"\" mask_img = np . asarray ( Image . open ( mask_path ), dtype = np . uint8 ) if mask_img . ndim == 3 and mask_img . shape [ 2 ] == 4 : # CAMERA masks are RGBA instances_mask = mask_img [:, :, 0 ] # use first channel only else : # REAL masks are grayscale instances_mask = mask_img return torch . from_numpy ( instances_mask ) def _load_depth ( self , depth_path : str ) -> torch . Tensor : \"\"\"Load depth from depth filepath.\"\"\" depth = torch . from_numpy ( np . asarray ( Image . open ( depth_path ), dtype = np . float32 ) * 0.001 ) return depth def _load_nocs_map ( self , nocs_map_path : str ) -> torch . Tensor : \"\"\"Load NOCS map from NOCS map filepath. Returns: NOCS map where each channel corresponds to one dimension in NOCS. Coordinates are normalized to [0,1], shape (H,W,3). \"\"\" nocs_map = torch . from_numpy ( np . asarray ( Image . open ( nocs_map_path ), dtype = np . float32 ) / 255 ) # z-coordinate has to be flipped # see https://github.com/hughw19/NOCS_CVPR2019/blob/14dbce775c3c7c45bb7b19269bd53d68efb8f73f/dataset.py#L327 # noqa: E501 nocs_map [:, :, 2 ] = 1 - nocs_map [:, :, 2 ] return nocs_map [:, :, : 3 ] def _estimate_object ( self , color_path : str , mask_id : int ) -> tuple : \"\"\"Estimate pose and scale through ground truth NOCS map.\"\"\" position = rotation_matrix = scale = out_transform = None depth_path = self . _depth_path_from_color_path ( color_path ) depth = self . _load_depth ( depth_path ) mask_path = self . _mask_path_from_color_path ( color_path ) instances_mask = self . _load_mask ( mask_path ) instance_mask = instances_mask == mask_id nocs_map_path = self . _nocs_map_path_from_color_path ( color_path ) nocs_map = self . _load_nocs_map ( nocs_map_path ) valid_instance_mask = instance_mask * depth != 0 nocs_map [ ~ valid_instance_mask ] = 0 centered_nocs_points = nocs_map [ valid_instance_mask ] - 0.5 measured_points = pointset_utils . depth_to_pointcloud ( depth , self . _camera , mask = valid_instance_mask , convention = \"opencv\" ) # require at least 30 point correspondences to prevent outliers if len ( measured_points ) < 30 : raise nocs_utils . PoseEstimationError () # skip object if it cointains errorneous depth if torch . max ( depth [ valid_instance_mask ]) > 32.0 : print ( \"Erroneous depth detected.\" , end = \" \\033 [K \\n \" ) raise nocs_utils . PoseEstimationError () ( position , rotation_matrix , scale , out_transform , ) = nocs_utils . estimate_similarity_transform ( centered_nocs_points , measured_points , verbose = False ) if position is None : raise nocs_utils . PoseEstimationError () return position , rotation_matrix , scale , out_transform def _get_scale ( self , sample_data : dict , extents : torch . Tensor ) -> float : \"\"\"Return scale from stored sample data and extents.\"\"\" if self . _scale_convention == \"diagonal\" : return sample_data [ \"nocs_scale\" ] elif self . _scale_convention == \"max\" : return sample_data [ \"max_extent\" ] elif self . _scale_convention == \"half_max\" : return 0.5 * sample_data [ \"max_extent\" ] elif self . _scale_convention == \"full\" : return extents else : raise ValueError ( f \"Specified scale convention { self . _scale_convnetion } not supported.\" ) def _change_axis_convention ( self , orientation_q : torch . Tensor , extents : torch . Tensor ) -> tuple : \"\"\"Adjust up-axis for orientation and extents. Returns: Tuple of position, orienation_q and extents, with specified up-axis. \"\"\" if self . _remap_y_axis is None and self . _remap_x_axis is None : return orientation_q , extents elif self . _remap_y_axis is None or self . _remap_x_axis is None : raise ValueError ( \"Either both or none of remap_{y,x}_axis have to be None.\" ) rotation_o2n = self . _get_o2n_object_rotation_matrix () remapped_extents = torch . abs ( torch . Tensor ( rotation_o2n ) @ extents ) # quaternion so far: original -> camera # we want a quaternion: new -> camera rotation_n2o = rotation_o2n . T quaternion_n2o = torch . from_numpy ( Rotation . from_matrix ( rotation_n2o ) . as_quat ()) remapped_orientation_q = quaternion_utils . quaternion_multiply ( orientation_q , quaternion_n2o ) # new -> original -> camera return remapped_orientation_q , remapped_extents def _get_o2n_object_rotation_matrix ( self ) -> np . ndarray : \"\"\"Compute rotation matrix which rotates original to new object coordinates.\"\"\" rotation_o2n = np . zeros (( 3 , 3 )) # original to new object convention if self . _remap_y_axis == \"x\" : rotation_o2n [ 0 , 1 ] = 1 elif self . _remap_y_axis == \"-x\" : rotation_o2n [ 0 , 1 ] = - 1 elif self . _remap_y_axis == \"y\" : rotation_o2n [ 1 , 1 ] = 1 elif self . _remap_y_axis == \"-y\" : rotation_o2n [ 1 , 1 ] = - 1 elif self . _remap_y_axis == \"z\" : rotation_o2n [ 2 , 1 ] = 1 elif self . _remap_y_axis == \"-z\" : rotation_o2n [ 2 , 1 ] = - 1 else : raise ValueError ( \"Unsupported remap_y_axis {self.remap_y} \" ) if self . _remap_x_axis == \"x\" : rotation_o2n [ 0 , 0 ] = 1 elif self . _remap_x_axis == \"-x\" : rotation_o2n [ 0 , 0 ] = - 1 elif self . _remap_x_axis == \"y\" : rotation_o2n [ 1 , 0 ] = 1 elif self . _remap_x_axis == \"-y\" : rotation_o2n [ 1 , 0 ] = - 1 elif self . _remap_x_axis == \"z\" : rotation_o2n [ 2 , 0 ] = 1 elif self . _remap_x_axis == \"-z\" : rotation_o2n [ 2 , 0 ] = - 1 else : raise ValueError ( \"Unsupported remap_x_axis {self.remap_y} \" ) # infer last column rotation_o2n [:, 2 ] = 1 - np . abs ( np . sum ( rotation_o2n , 1 )) # rows must sum to +-1 rotation_o2n [:, 2 ] *= np . linalg . det ( rotation_o2n ) # make special orthogonal if np . linalg . det ( rotation_o2n ) != 1.0 : # check if special orthogonal raise ValueError ( \"Unsupported combination of remap_{y,x}_axis. det != 1\" ) return rotation_o2n def _quat_to_orientation_repr ( self , quaternion : torch . Tensor ) -> torch . Tensor : \"\"\"Convert quaternion to selected orientation representation. Args: quaternion: The quaternion to convert, scalar-last, shape (4,). Returns: The same orientation as represented by the quaternion in the chosen orientation representation. \"\"\" if self . _orientation_repr == \"quaternion\" : return quaternion elif self . _orientation_repr == \"discretized\" : index = self . _orientation_grid . quat_to_index ( quaternion . numpy ()) return torch . tensor ( index , dtype = torch . long , ) else : raise NotImplementedError ( f \"Orientation representation { self . _orientation_repr } is not supported.\" ) def load_mesh ( self , object_path : str ) -> o3d . geometry . TriangleMesh : \"\"\"Load an object mesh and adjust its object frame convention.\"\"\" mesh = o3d . io . read_triangle_mesh ( object_path ) if self . _remap_y_axis is None and self . _remap_x_axis is None : return mesh elif self . _remap_y_axis is None or self . _remap_x_axis is None : raise ValueError ( \"Either both or none of remap_{y,x}_axis have to be None.\" ) rotation_o2n = self . _get_o2n_object_rotation_matrix () mesh . rotate ( rotation_o2n , center = np . array ([ 0.0 , 0.0 , 0.0 ])[:, None ], ) return mesh Config Bases: TypedDict Configuration dictionary for NOCSDataset. ATTRIBUTE DESCRIPTION root_dir See NOCSDataset docstring. TYPE: str split The dataset split. The following strings are supported: \"camera_train\": 275000 images, synthetic objects + real background \"camera_val\": 25000 images, synthetic objects + real background \"real_train\": 4300 images in 7 scenes, real \"real_test\": 2750 images in 6 scenes, real TYPE: str mask_pointcloud Whether the returned pointcloud will be masked. TYPE: bool normalize_pointcloud Whether the returned pointcloud and position will be normalized, such that pointcloud centroid is at the origin. TYPE: bool scale_convention Which scale is returned. The following strings are supported: \"diagonal\": Length of bounding box' diagonal. This is what NOCS uses. \"max\": Maximum side length of bounding box. \"half_max\": Half maximum side length of bounding box. \"full\": Bounding box side lengths. Shape (3,). TYPE: str camera_convention Which camera convention is used for position and orientation. One of: \"opengl\": x right, y up, z back \"opencv\": x right, y down, z forward Note that this does not influence how the dataset is processed, only the returned position and quaternion. TYPE: str orientation_repr Which orientation representation is used. One of: \"quaternion\" \"discretized\" TYPE: str orientation_grid_resolution Resolution of the orientation grid. Only used if orientation_repr is \"discretized\". TYPE: int remap_y_axis If not None, the NOCS y-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. This is typically the up-axis. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: y One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\" TYPE: Optional [ str ] remap_x_axis If not None, the original x-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: -z One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\" TYPE: Optional [ str ] category_str If not None, only samples from the matching category will be returned. See NOCSDataset.category_id_to_str for admissible category strings. TYPE: Optional [ str ] Source code in sdfest/initialization/datasets/nocs_dataset.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 class Config ( TypedDict , total = False ): \"\"\"Configuration dictionary for NOCSDataset. Attributes: root_dir: See NOCSDataset docstring. split: The dataset split. The following strings are supported: \"camera_train\": 275000 images, synthetic objects + real background \"camera_val\": 25000 images, synthetic objects + real background \"real_train\": 4300 images in 7 scenes, real \"real_test\": 2750 images in 6 scenes, real mask_pointcloud: Whether the returned pointcloud will be masked. normalize_pointcloud: Whether the returned pointcloud and position will be normalized, such that pointcloud centroid is at the origin. scale_convention: Which scale is returned. The following strings are supported: \"diagonal\": Length of bounding box' diagonal. This is what NOCS uses. \"max\": Maximum side length of bounding box. \"half_max\": Half maximum side length of bounding box. \"full\": Bounding box side lengths. Shape (3,). camera_convention: Which camera convention is used for position and orientation. One of: \"opengl\": x right, y up, z back \"opencv\": x right, y down, z forward Note that this does not influence how the dataset is processed, only the returned position and quaternion. orientation_repr: Which orientation representation is used. One of: \"quaternion\" \"discretized\" orientation_grid_resolution: Resolution of the orientation grid. Only used if orientation_repr is \"discretized\". remap_y_axis: If not None, the NOCS y-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. This is typically the up-axis. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: y One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\" remap_x_axis: If not None, the original x-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: -z One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\" category_str: If not None, only samples from the matching category will be returned. See NOCSDataset.category_id_to_str for admissible category strings. \"\"\" root_dir : str split : str mask_pointcloud : bool normalize_pointcloud : bool scale_convention : str camera_convention : str orientation_repr : str orientation_grid_resolution : int remap_y_axis : Optional [ str ] remap_x_axis : Optional [ str ] category_str : Optional [ str ] __init__ __init__ ( config : Config ) -> None Initialize the dataset. PARAMETER DESCRIPTION config Configuration dictionary of dataset. Provided dictionary will be merged with default_dict. See NOCSDataset.Config for supported keys. TYPE: Config Source code in sdfest/initialization/datasets/nocs_dataset.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 def __init__ ( self , config : Config , ) -> None : \"\"\"Initialize the dataset. Args: config: Configuration dictionary of dataset. Provided dictionary will be merged with default_dict. See NOCSDataset.Config for supported keys. \"\"\" config = yoco . load_config ( config , current_dict = NOCSDataset . default_config ) self . _root_dir = config [ \"root_dir\" ] self . _split = config [ \"split\" ] self . _camera_convention = config [ \"camera_convention\" ] self . _camera = self . _get_split_camera () self . _preprocess_path = os . path . join ( self . _root_dir , \"sdfest_pre\" , self . _split ) if not os . path . isdir ( self . _preprocess_path ): self . _preprocess_dataset () self . _mask_pointcloud = config [ \"mask_pointcloud\" ] self . _normalize_pointcloud = config [ \"normalize_pointcloud\" ] self . _scale_convention = config [ \"scale_convention\" ] self . _sample_files = self . _get_sample_files ( config [ \"category_str\" ]) self . _remap_y_axis = config [ \"remap_y_axis\" ] self . _remap_x_axis = config [ \"remap_x_axis\" ] self . _orientation_repr = config [ \"orientation_repr\" ] if self . _orientation_repr == \"discretized\" : self . _orientation_grid = so3grid . SO3Grid ( config [ \"orientation_grid_resolution\" ] ) __len__ __len__ () -> int Return number of sample in dataset. Source code in sdfest/initialization/datasets/nocs_dataset.py 178 179 180 def __len__ ( self ) -> int : \"\"\"Return number of sample in dataset.\"\"\" return len ( self . _sample_files ) __getitem__ __getitem__ ( idx : int ) -> dict Return a sample of the dataset. PARAMETER DESCRIPTION idx Index of the instance. TYPE: int RETURNS DESCRIPTION dict Sample containing the following items: \"color\" \"depth\" \"mask\" \"pointset\" \"position\" \"orientation\" \"quaternion\" \"scale\" \"color_path\" Source code in sdfest/initialization/datasets/nocs_dataset.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def __getitem__ ( self , idx : int ) -> dict : \"\"\"Return a sample of the dataset. Args: idx: Index of the instance. Returns: Sample containing the following items: \"color\" \"depth\" \"mask\" \"pointset\" \"position\" \"orientation\" \"quaternion\" \"scale\" \"color_path\" \"\"\" sample_file = self . _sample_files [ idx ] sample_data = pickle . load ( open ( sample_file , \"rb\" )) sample = self . _sample_from_sample_data ( sample_data ) return sample load_mesh load_mesh ( object_path : str ) -> o3d . geometry . TriangleMesh Load an object mesh and adjust its object frame convention. Source code in sdfest/initialization/datasets/nocs_dataset.py 820 821 822 823 824 825 826 827 828 829 830 831 832 833 def load_mesh ( self , object_path : str ) -> o3d . geometry . TriangleMesh : \"\"\"Load an object mesh and adjust its object frame convention.\"\"\" mesh = o3d . io . read_triangle_mesh ( object_path ) if self . _remap_y_axis is None and self . _remap_x_axis is None : return mesh elif self . _remap_y_axis is None or self . _remap_x_axis is None : raise ValueError ( \"Either both or none of remap_{y,x}_axis have to be None.\" ) rotation_o2n = self . _get_o2n_object_rotation_matrix () mesh . rotate ( rotation_o2n , center = np . array ([ 0.0 , 0.0 , 0.0 ])[:, None ], ) return mesh ObjectError Bases: Exception Error if something with the mesh is wrong. Source code in sdfest/initialization/datasets/nocs_dataset.py 836 837 838 839 class ObjectError ( Exception ): \"\"\"Error if something with the mesh is wrong.\"\"\" pass","title":"nocs_dataset"},{"location":"reference/initialization/datasets/nocs_dataset/#sdfest.initialization.datasets.nocs_dataset","text":"Module providing dataset class for NOCS datasets (CAMERA / REAL).","title":"nocs_dataset"},{"location":"reference/initialization/datasets/nocs_dataset/#sdfest.initialization.datasets.nocs_dataset.NOCSDataset","text":"Bases: torch . utils . data . Dataset Dataset class for NOCS dataset. CAMERA and REAL are training sets. CAMERA25 and REAL275 are test data. Some papers use CAMERA25 as validation when benchmarking REAL275. Datasets can be found here: https://github.com/hughw19/NOCS_CVPR2019/tree/master Expected directory format {root_dir}/real_train/... {root_dir}/real_test/... {root_dir}/gts/... {root_dir}/obj_models/... {root_dir}/camera_composed_depth/... {root_dir}/val/... {root_dir}/train/... Which is easily obtained by downloading all the provided files and extracting them into the same directory. Necessary preprocessing of this data is performed during first initialization per and is saved to {root_dir}/sdfest_pre/... Source code in sdfest/initialization/datasets/nocs_dataset.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 class NOCSDataset ( torch . utils . data . Dataset ): \"\"\"Dataset class for NOCS dataset. CAMERA* and REAL* are training sets. CAMERA25 and REAL275 are test data. Some papers use CAMERA25 as validation when benchmarking REAL275. Datasets can be found here: https://github.com/hughw19/NOCS_CVPR2019/tree/master Expected directory format: {root_dir}/real_train/... {root_dir}/real_test/... {root_dir}/gts/... {root_dir}/obj_models/... {root_dir}/camera_composed_depth/... {root_dir}/val/... {root_dir}/train/... Which is easily obtained by downloading all the provided files and extracting them into the same directory. Necessary preprocessing of this data is performed during first initialization per and is saved to {root_dir}/sdfest_pre/... \"\"\" num_categories = 7 category_id_to_str = { 0 : \"unknown\" , 1 : \"bottle\" , 2 : \"bowl\" , 3 : \"camera\" , 4 : \"can\" , 5 : \"laptop\" , 6 : \"mug\" , } category_str_to_id = { v : k for k , v in category_id_to_str . items ()} class Config ( TypedDict , total = False ): \"\"\"Configuration dictionary for NOCSDataset. Attributes: root_dir: See NOCSDataset docstring. split: The dataset split. The following strings are supported: \"camera_train\": 275000 images, synthetic objects + real background \"camera_val\": 25000 images, synthetic objects + real background \"real_train\": 4300 images in 7 scenes, real \"real_test\": 2750 images in 6 scenes, real mask_pointcloud: Whether the returned pointcloud will be masked. normalize_pointcloud: Whether the returned pointcloud and position will be normalized, such that pointcloud centroid is at the origin. scale_convention: Which scale is returned. The following strings are supported: \"diagonal\": Length of bounding box' diagonal. This is what NOCS uses. \"max\": Maximum side length of bounding box. \"half_max\": Half maximum side length of bounding box. \"full\": Bounding box side lengths. Shape (3,). camera_convention: Which camera convention is used for position and orientation. One of: \"opengl\": x right, y up, z back \"opencv\": x right, y down, z forward Note that this does not influence how the dataset is processed, only the returned position and quaternion. orientation_repr: Which orientation representation is used. One of: \"quaternion\" \"discretized\" orientation_grid_resolution: Resolution of the orientation grid. Only used if orientation_repr is \"discretized\". remap_y_axis: If not None, the NOCS y-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. This is typically the up-axis. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: y One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\" remap_x_axis: If not None, the original x-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: -z One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\" category_str: If not None, only samples from the matching category will be returned. See NOCSDataset.category_id_to_str for admissible category strings. \"\"\" root_dir : str split : str mask_pointcloud : bool normalize_pointcloud : bool scale_convention : str camera_convention : str orientation_repr : str orientation_grid_resolution : int remap_y_axis : Optional [ str ] remap_x_axis : Optional [ str ] category_str : Optional [ str ] # TODO symmetry # TODO unify dataset code, adapt generated_dataset default_config : Config = { \"root_dir\" : None , \"split\" : None , \"mask_pointcloud\" : False , \"normalize_pointcloud\" : False , \"camera_convention\" : \"opengl\" , \"scale_convention\" : \"half_max\" , \"orientation_repr\" : \"quaternion\" , \"orientation_grid_resolution\" : None , \"category_str\" : None , \"remap_y_axis\" : None , \"remap_x_axis\" : None , } def __init__ ( self , config : Config , ) -> None : \"\"\"Initialize the dataset. Args: config: Configuration dictionary of dataset. Provided dictionary will be merged with default_dict. See NOCSDataset.Config for supported keys. \"\"\" config = yoco . load_config ( config , current_dict = NOCSDataset . default_config ) self . _root_dir = config [ \"root_dir\" ] self . _split = config [ \"split\" ] self . _camera_convention = config [ \"camera_convention\" ] self . _camera = self . _get_split_camera () self . _preprocess_path = os . path . join ( self . _root_dir , \"sdfest_pre\" , self . _split ) if not os . path . isdir ( self . _preprocess_path ): self . _preprocess_dataset () self . _mask_pointcloud = config [ \"mask_pointcloud\" ] self . _normalize_pointcloud = config [ \"normalize_pointcloud\" ] self . _scale_convention = config [ \"scale_convention\" ] self . _sample_files = self . _get_sample_files ( config [ \"category_str\" ]) self . _remap_y_axis = config [ \"remap_y_axis\" ] self . _remap_x_axis = config [ \"remap_x_axis\" ] self . _orientation_repr = config [ \"orientation_repr\" ] if self . _orientation_repr == \"discretized\" : self . _orientation_grid = so3grid . SO3Grid ( config [ \"orientation_grid_resolution\" ] ) def __len__ ( self ) -> int : \"\"\"Return number of sample in dataset.\"\"\" return len ( self . _sample_files ) def __getitem__ ( self , idx : int ) -> dict : \"\"\"Return a sample of the dataset. Args: idx: Index of the instance. Returns: Sample containing the following items: \"color\" \"depth\" \"mask\" \"pointset\" \"position\" \"orientation\" \"quaternion\" \"scale\" \"color_path\" \"\"\" sample_file = self . _sample_files [ idx ] sample_data = pickle . load ( open ( sample_file , \"rb\" )) sample = self . _sample_from_sample_data ( sample_data ) return sample def _preprocess_dataset ( self ) -> None : \"\"\"Create preprocessing files for the current split. One file per sample, which currently means per valid object mask will be created. Preprocessing will be stored on disk to {root_dir}/sdfest_pre/... This function will not store the preprocessing, so it still has to be loaded afterwards. \"\"\" os . makedirs ( self . _preprocess_path ) self . _fix_obj_models () self . _start_time = time . time () self . _color_paths = self . _get_color_files () Parallel ( n_jobs =- 1 )( ( delayed ( self . _preprocess_color_path )( i , color_path ) for i , color_path in enumerate ( self . _color_paths ) ) ) # store dictionary to map category to files sample_files = self . _get_sample_files () category_str_to_files = { category_str : [] for category_str in NOCSDataset . category_id_to_str . values () } for sample_file in tqdm ( sample_files ): sample_data = pickle . load ( open ( sample_file , \"rb\" )) category_id = sample_data [ \"category_id\" ] category_str = NOCSDataset . category_id_to_str [ category_id ] _ , file_name = os . path . split ( sample_file ) category_str_to_files [ category_str ] . append ( file_name ) category_json_path = os . path . join ( self . _preprocess_path , \"categories.json\" ) with open ( category_json_path , \"w\" ) as f : json . dump ( dict ( category_str_to_files ), f ) print ( f \"Finished preprocessing for { self . _split } .\" , end = \" \\033 [K \\n \" ) def _fix_obj_models ( self ) -> None : \"\"\"Fix issues with fileextensions. Some png files have jpg extension. This function fixes these models. \"\"\" glob_pattern = os . path . join ( self . _root_dir , \"**\" , \"*.jpg\" ) files = glob ( glob_pattern , recursive = True ) for filepath in files : what = imghdr . what ( filepath ) if what == \"png\" : print ( \"Fixing: \" , filepath ) folder , problematic_filename = os . path . split ( filepath ) name , _ = problematic_filename . split ( \".\" ) fixed_filename = f \"fixed_ { name } .png\" fixed_filepath = os . path . join ( folder , fixed_filename ) mtl_filepath = os . path . join ( folder , \"model.mtl\" ) bu_mtl_filepath = os . path . join ( folder , \"model.mtl.old\" ) copyfile ( mtl_filepath , bu_mtl_filepath ) copyfile ( filepath , fixed_filepath ) def _update_preprocess_progress ( self , image_id : int ) -> None : current_time = time . time () duration = current_time - self . _start_time imgs_per_sec = image_id / duration if image_id > 10 : remaining_imgs = len ( self . _color_paths ) - image_id remaining_secs = remaining_imgs / imgs_per_sec remaining_time_str = str ( datetime . timedelta ( seconds = round ( remaining_secs ))) else : remaining_time_str = \"N/A\" print ( f \"Preprocessing image: { image_id : >10 } / { len ( self . _color_paths ) } \" f \" { image_id / len ( self . _color_paths ) * 100 : >6.2f } %\" # progress percentage f \" Remaining time: { remaining_time_str } \" # remaining time \" \\033 [K\" , # clear until end of line end = \" \\r \" , # overwrite previous ) def _preprocess_color_path ( self , image_id : int , color_path : str ) -> None : counter = 0 self . _update_preprocess_progress ( image_id ) depth_path = self . _depth_path_from_color_path ( color_path ) if not os . path . isfile ( depth_path ): print ( f \"Missing depth file { depth_path } . Skipping.\" , end = \" \\033 [K \\n \" ) return mask_path = self . _mask_path_from_color_path ( color_path ) meta_path = self . _meta_path_from_color_path ( color_path ) meta_data = pd . read_csv ( meta_path , sep = \" \" , header = None , converters = { 2 : lambda x : str ( x )} ) instances_mask = self . _load_mask ( mask_path ) mask_ids = np . unique ( instances_mask ) . tolist () gt_id = 0 # GT only contains valid objects of interests and is 0-indexed for mask_id in mask_ids : if mask_id == 255 : # 255 is background continue match = meta_data [ meta_data . iloc [:, 0 ] == mask_id ] if match . empty : print ( f \"Warning: mask { mask_id } not found in { meta_path } \" , end = \" \\033 [K \\n \" ) elif match . shape [ 0 ] != 1 : print ( f \"Warning: mask { mask_id } not unique in { meta_path } \" , end = \" \\033 [K \\n \" ) meta_row = match . iloc [ 0 ] category_id = meta_row . iloc [ 1 ] if category_id == 0 : # unknown / distractor object continue try : ( position , orientation_q , extents , nocs_transform , ) = self . _get_pose_and_scale ( color_path , mask_id , gt_id , meta_row ) except nocs_utils . PoseEstimationError : print ( \"Insufficient data for pose estimation. \" f \"Skipping { color_path } : { mask_id } .\" , end = \" \\033 [K \\n \" , ) continue except ObjectError : print ( \"Insufficient object mesh for pose estimation. \" f \"Skipping { color_path } : { mask_id } .\" , end = \" \\033 [K \\n \" , ) continue obj_path = self . _get_obj_path ( meta_row ) sample_info = { \"color_path\" : color_path , \"depth_path\" : depth_path , \"mask_path\" : mask_path , \"mask_id\" : mask_id , \"category_id\" : category_id , \"obj_path\" : obj_path , \"nocs_transform\" : nocs_transform , \"position\" : position , \"orientation_q\" : orientation_q , \"extents\" : extents , \"nocs_scale\" : torch . linalg . norm ( extents ), \"max_extent\" : torch . max ( extents ), } out_file = os . path . join ( self . _preprocess_path , f \" { image_id : 08 } _ { counter } .pkl\" ) pickle . dump ( sample_info , open ( out_file , \"wb\" )) counter += 1 gt_id += 1 def _get_color_files ( self ) -> list : \"\"\"Return list of paths of color images of the selected split.\"\"\" if self . _split == \"camera_train\" : glob_pattern = os . path . join ( self . _root_dir , \"train\" , \"**\" , \"*_color.png\" ) return sorted ( glob ( glob_pattern , recursive = True )) elif self . _split == \"camera_val\" : glob_pattern = os . path . join ( self . _root_dir , \"val\" , \"**\" , \"*_color.png\" ) return sorted ( glob ( glob_pattern , recursive = True )) elif self . _split == \"real_train\" : glob_pattern = os . path . join ( self . _root_dir , \"real_train\" , \"**\" , \"*_color.png\" ) return sorted ( glob ( glob_pattern , recursive = True )) elif self . _split == \"real_test\" : glob_pattern = os . path . join ( self . _root_dir , \"real_test\" , \"**\" , \"*_color.png\" ) return sorted ( glob ( glob_pattern , recursive = True )) else : raise ValueError ( f \"Specified split { self . _split } is not supported.\" ) def _get_sample_files ( self , category_str : Optional [ str ] = None ) -> list : \"\"\"Return sorted list of sample file paths. Sample files are generated by NOCSDataset._preprocess_dataset. Args: category_str: If not None, only instances of the provided category will be returned. Returns: List of sample_data files. \"\"\" glob_pattern = os . path . join ( self . _preprocess_path , \"*.pkl\" ) sample_files = glob ( glob_pattern ) sample_files . sort () if category_str is None : return sample_files if category_str not in NOCSDataset . category_str_to_id : raise ValueError ( f \"Unsupported category_str { category_str } .\" ) category_json_path = os . path . join ( self . _preprocess_path , \"categories.json\" ) with open ( category_json_path , \"r\" ) as f : category_str_to_filenames = json . load ( f ) filtered_sample_files = [ os . path . join ( self . _preprocess_path , fn ) for fn in category_str_to_filenames [ category_str ] ] return filtered_sample_files def _get_split_camera ( self ) -> None : \"\"\"Return camera information for selected split.\"\"\" # from: https://github.com/hughw19/NOCS_CVPR2019/blob/master/detect_eval.py if self . _split in [ \"real_train\" , \"real_test\" ]: return Camera ( width = 640 , height = 480 , fx = 591.0125 , fy = 590.16775 , cx = 322.525 , cy = 244.11084 , pixel_center = 0.0 , ) elif self . _split in [ \"camera_train\" , \"camera_val\" ]: return Camera ( width = 640 , height = 480 , fx = 577.5 , fy = 577.5 , cx = 319.5 , cy = 239.5 , pixel_center = 0.0 , ) else : raise ValueError ( f \"Specified split { self . _split } is not supported.\" ) def _sample_from_sample_data ( self , sample_data : dict ) -> dict : \"\"\"Create dictionary containing a single sample.\"\"\" color = torch . from_numpy ( np . asarray ( Image . open ( sample_data [ \"color_path\" ]), dtype = np . float32 ) / 255 ) depth = self . _load_depth ( sample_data [ \"depth_path\" ]) instances_mask = self . _load_mask ( sample_data [ \"mask_path\" ]) instance_mask = instances_mask == sample_data [ \"mask_id\" ] pointcloud_mask = instance_mask if self . _mask_pointcloud else None pointcloud = pointset_utils . depth_to_pointcloud ( depth , self . _camera , mask = pointcloud_mask , convention = self . _camera_convention , ) # adjust camera convention for position, orientation and scale position = pointset_utils . change_position_camera_convention ( sample_data [ \"position\" ], \"opencv\" , self . _camera_convention ) # orientation / scale orientation_q , extents = self . _change_axis_convention ( sample_data [ \"orientation_q\" ], sample_data [ \"extents\" ] ) orientation_q = pointset_utils . change_orientation_camera_convention ( orientation_q , \"opencv\" , self . _camera_convention ) orientation = self . _quat_to_orientation_repr ( orientation_q ) scale = self . _get_scale ( sample_data , extents ) # normalize pointcloud & position if self . _normalize_pointcloud : pointcloud , centroid = pointset_utils . normalize_points ( pointcloud ) position = position - centroid sample = { \"color\" : color , \"depth\" : depth , \"pointset\" : pointcloud , \"mask\" : instance_mask , \"position\" : position , \"orientation\" : orientation , \"quaternion\" : orientation_q , \"scale\" : scale , \"color_path\" : sample_data [ \"color_path\" ], \"obj_path\" : sample_data [ \"obj_path\" ], \"category_id\" : sample_data [ \"category_id\" ], \"category_str\" : NOCSDataset . category_id_to_str [ sample_data [ \"category_id\" ]], } return sample def _depth_path_from_color_path ( self , color_path : str ) -> str : \"\"\"Return path to depth file from color filepath.\"\"\" if self . _split in [ \"real_train\" , \"real_test\" ]: depth_path = color_path . replace ( \"color\" , \"depth\" ) elif self . _split in [ \"camera_train\" ]: depth_path = color_path . replace ( \"color\" , \"composed\" ) depth_path = depth_path . replace ( \"/train/\" , \"/camera_full_depths/train/\" ) elif self . _split in [ \"camera_val\" ]: depth_path = color_path . replace ( \"color\" , \"composed\" ) depth_path = depth_path . replace ( \"/val/\" , \"/camera_full_depths/val/\" ) else : raise ValueError ( f \"Specified split { self . _split } is not supported.\" ) return depth_path def _mask_path_from_color_path ( self , color_path : str ) -> str : \"\"\"Return path to mask file from color filepath.\"\"\" mask_path = color_path . replace ( \"color\" , \"mask\" ) return mask_path def _meta_path_from_color_path ( self , color_path : str ) -> str : \"\"\"Return path to meta file from color filepath.\"\"\" meta_path = color_path . replace ( \"color.png\" , \"meta.txt\" ) return meta_path def _nocs_map_path_from_color_path ( self , color_path : str ) -> str : \"\"\"Return NOCS map filepath from color filepath.\"\"\" nocs_map_path = color_path . replace ( \"color.png\" , \"coord.png\" ) return nocs_map_path def _get_pose_and_scale ( self , color_path : str , mask_id : int , gt_id : int , meta_row : pd . Series ) -> tuple : \"\"\"Return position, orientation, scale and NOCS transform. All of those follow OpenCV (x right, y down, z forward) convention. Args: color_path: Path to the color file. mask_id: Instance id in the instances mask. gt_id: Ground truth id. This is 0-indexed id of valid instances in meta file. meta_row: Matching row of meta file. Contains necessary information about mesh. Returns: position (torch.Tensor): Position of object center in camera frame. Shape (3,). quaternion (torch.Tensor): Orientation of object in camera frame. Scalar-last quaternion, shape (4,). extents (torch.Tensor): Bounding box side lengths. nocs_transformation (torch.Tensor): Transformation from centered [-0.5,0.5]^3 NOCS coordinates to camera. \"\"\" gts_path = self . _get_gts_path ( color_path ) obj_path = self . _get_obj_path ( meta_row ) if self . _split == \"real_test\" : # only use gt for real test data, since there are errors in camera val gts_data = pickle . load ( open ( gts_path , \"rb\" )) nocs_transform = gts_data [ \"gt_RTs\" ][ gt_id ] position = nocs_transform [ 0 : 3 , 3 ] rot_scale = nocs_transform [ 0 : 3 , 0 : 3 ] nocs_scales = np . sqrt ( np . sum ( rot_scale ** 2 , axis = 0 )) rotation_matrix = rot_scale / nocs_scales [:, None ] nocs_scale = nocs_scales [ 0 ] else : # camera_train, camera_val, real_train # use ground truth NOCS mask to perform alignment ( position , rotation_matrix , nocs_scale , nocs_transform , ) = self . _estimate_object ( color_path , mask_id ) orientation_q = Rotation . from_matrix ( rotation_matrix ) . as_quat () mesh_extents = self . _get_mesh_extents_from_obj ( obj_path ) if \"camera\" in self . _split : # CAMERA / ShapeNet meshes are normalized s.t. diagonal == 1 # get metric extents by scaling with the diagonal extents = nocs_scale * mesh_extents elif \"real\" in self . _split : # REAL object meshes are not normalized extents = mesh_extents else : raise ValueError ( f \"Specified split { self . _split } is not supported.\" ) position = torch . Tensor ( position ) orientation_q = torch . Tensor ( orientation_q ) extents = torch . Tensor ( extents ) nocs_transform = torch . Tensor ( nocs_transform ) return position , orientation_q , extents , nocs_transform def _get_gts_path ( self , color_path : str ) -> Optional [ str ]: \"\"\"Return path to gts file from color filepath. Return None if split does not have ground truth information. \"\"\" if self . _split == \"real_test\" : gts_folder = os . path . join ( self . _root_dir , \"gts\" , \"real_test\" ) elif self . _split == \"camera_val\" : gts_folder = os . path . join ( self . _root_dir , \"gts\" , \"val\" ) else : return None path = os . path . normpath ( color_path ) split_path = path . split ( os . sep ) number = path [ - 14 : - 10 ] gts_filename = f \"results_ { split_path [ - 3 ] } _ { split_path [ - 2 ] } _ { number } .pkl\" gts_path = os . path . join ( gts_folder , gts_filename ) return gts_path def _get_obj_path ( self , meta_row : pd . Series ) -> str : \"\"\"Return path to object file from meta data row.\"\"\" if \"camera\" in self . _split : # ShapeNet mesh synset_id = meta_row . iloc [ 2 ] object_id = meta_row . iloc [ 3 ] obj_path = os . path . join ( self . _root_dir , \"obj_models\" , self . _split . replace ( \"camera_\" , \"\" ), synset_id , object_id , \"model.obj\" , ) elif \"real\" in self . _split : # REAL mesh object_id = meta_row . iloc [ 2 ] obj_path = os . path . join ( self . _root_dir , \"obj_models\" , self . _split , object_id + \".obj\" ) else : raise ValueError ( f \"Specified split { self . _split } is not supported.\" ) return obj_path def _get_mesh_extents_from_obj ( self , obj_path : str ) -> torch . Tensor : \"\"\"Return maximum extent of bounding box from obj filepath. Note that this is normalized extent (with diagonal == 1) in the case of CAMERA dataset, and unnormalized (i.e., metric) extent in the case of REAL dataset. \"\"\" mesh = o3d . io . read_triangle_mesh ( obj_path ) vertices = np . asarray ( mesh . vertices ) if len ( vertices ) == 0 : raise ObjectError () extents = np . max ( vertices , axis = 0 ) - np . min ( vertices , axis = 0 ) return torch . Tensor ( extents ) def _load_mask ( self , mask_path : str ) -> torch . Tensor : \"\"\"Load mask from mask filepath.\"\"\" mask_img = np . asarray ( Image . open ( mask_path ), dtype = np . uint8 ) if mask_img . ndim == 3 and mask_img . shape [ 2 ] == 4 : # CAMERA masks are RGBA instances_mask = mask_img [:, :, 0 ] # use first channel only else : # REAL masks are grayscale instances_mask = mask_img return torch . from_numpy ( instances_mask ) def _load_depth ( self , depth_path : str ) -> torch . Tensor : \"\"\"Load depth from depth filepath.\"\"\" depth = torch . from_numpy ( np . asarray ( Image . open ( depth_path ), dtype = np . float32 ) * 0.001 ) return depth def _load_nocs_map ( self , nocs_map_path : str ) -> torch . Tensor : \"\"\"Load NOCS map from NOCS map filepath. Returns: NOCS map where each channel corresponds to one dimension in NOCS. Coordinates are normalized to [0,1], shape (H,W,3). \"\"\" nocs_map = torch . from_numpy ( np . asarray ( Image . open ( nocs_map_path ), dtype = np . float32 ) / 255 ) # z-coordinate has to be flipped # see https://github.com/hughw19/NOCS_CVPR2019/blob/14dbce775c3c7c45bb7b19269bd53d68efb8f73f/dataset.py#L327 # noqa: E501 nocs_map [:, :, 2 ] = 1 - nocs_map [:, :, 2 ] return nocs_map [:, :, : 3 ] def _estimate_object ( self , color_path : str , mask_id : int ) -> tuple : \"\"\"Estimate pose and scale through ground truth NOCS map.\"\"\" position = rotation_matrix = scale = out_transform = None depth_path = self . _depth_path_from_color_path ( color_path ) depth = self . _load_depth ( depth_path ) mask_path = self . _mask_path_from_color_path ( color_path ) instances_mask = self . _load_mask ( mask_path ) instance_mask = instances_mask == mask_id nocs_map_path = self . _nocs_map_path_from_color_path ( color_path ) nocs_map = self . _load_nocs_map ( nocs_map_path ) valid_instance_mask = instance_mask * depth != 0 nocs_map [ ~ valid_instance_mask ] = 0 centered_nocs_points = nocs_map [ valid_instance_mask ] - 0.5 measured_points = pointset_utils . depth_to_pointcloud ( depth , self . _camera , mask = valid_instance_mask , convention = \"opencv\" ) # require at least 30 point correspondences to prevent outliers if len ( measured_points ) < 30 : raise nocs_utils . PoseEstimationError () # skip object if it cointains errorneous depth if torch . max ( depth [ valid_instance_mask ]) > 32.0 : print ( \"Erroneous depth detected.\" , end = \" \\033 [K \\n \" ) raise nocs_utils . PoseEstimationError () ( position , rotation_matrix , scale , out_transform , ) = nocs_utils . estimate_similarity_transform ( centered_nocs_points , measured_points , verbose = False ) if position is None : raise nocs_utils . PoseEstimationError () return position , rotation_matrix , scale , out_transform def _get_scale ( self , sample_data : dict , extents : torch . Tensor ) -> float : \"\"\"Return scale from stored sample data and extents.\"\"\" if self . _scale_convention == \"diagonal\" : return sample_data [ \"nocs_scale\" ] elif self . _scale_convention == \"max\" : return sample_data [ \"max_extent\" ] elif self . _scale_convention == \"half_max\" : return 0.5 * sample_data [ \"max_extent\" ] elif self . _scale_convention == \"full\" : return extents else : raise ValueError ( f \"Specified scale convention { self . _scale_convnetion } not supported.\" ) def _change_axis_convention ( self , orientation_q : torch . Tensor , extents : torch . Tensor ) -> tuple : \"\"\"Adjust up-axis for orientation and extents. Returns: Tuple of position, orienation_q and extents, with specified up-axis. \"\"\" if self . _remap_y_axis is None and self . _remap_x_axis is None : return orientation_q , extents elif self . _remap_y_axis is None or self . _remap_x_axis is None : raise ValueError ( \"Either both or none of remap_{y,x}_axis have to be None.\" ) rotation_o2n = self . _get_o2n_object_rotation_matrix () remapped_extents = torch . abs ( torch . Tensor ( rotation_o2n ) @ extents ) # quaternion so far: original -> camera # we want a quaternion: new -> camera rotation_n2o = rotation_o2n . T quaternion_n2o = torch . from_numpy ( Rotation . from_matrix ( rotation_n2o ) . as_quat ()) remapped_orientation_q = quaternion_utils . quaternion_multiply ( orientation_q , quaternion_n2o ) # new -> original -> camera return remapped_orientation_q , remapped_extents def _get_o2n_object_rotation_matrix ( self ) -> np . ndarray : \"\"\"Compute rotation matrix which rotates original to new object coordinates.\"\"\" rotation_o2n = np . zeros (( 3 , 3 )) # original to new object convention if self . _remap_y_axis == \"x\" : rotation_o2n [ 0 , 1 ] = 1 elif self . _remap_y_axis == \"-x\" : rotation_o2n [ 0 , 1 ] = - 1 elif self . _remap_y_axis == \"y\" : rotation_o2n [ 1 , 1 ] = 1 elif self . _remap_y_axis == \"-y\" : rotation_o2n [ 1 , 1 ] = - 1 elif self . _remap_y_axis == \"z\" : rotation_o2n [ 2 , 1 ] = 1 elif self . _remap_y_axis == \"-z\" : rotation_o2n [ 2 , 1 ] = - 1 else : raise ValueError ( \"Unsupported remap_y_axis {self.remap_y} \" ) if self . _remap_x_axis == \"x\" : rotation_o2n [ 0 , 0 ] = 1 elif self . _remap_x_axis == \"-x\" : rotation_o2n [ 0 , 0 ] = - 1 elif self . _remap_x_axis == \"y\" : rotation_o2n [ 1 , 0 ] = 1 elif self . _remap_x_axis == \"-y\" : rotation_o2n [ 1 , 0 ] = - 1 elif self . _remap_x_axis == \"z\" : rotation_o2n [ 2 , 0 ] = 1 elif self . _remap_x_axis == \"-z\" : rotation_o2n [ 2 , 0 ] = - 1 else : raise ValueError ( \"Unsupported remap_x_axis {self.remap_y} \" ) # infer last column rotation_o2n [:, 2 ] = 1 - np . abs ( np . sum ( rotation_o2n , 1 )) # rows must sum to +-1 rotation_o2n [:, 2 ] *= np . linalg . det ( rotation_o2n ) # make special orthogonal if np . linalg . det ( rotation_o2n ) != 1.0 : # check if special orthogonal raise ValueError ( \"Unsupported combination of remap_{y,x}_axis. det != 1\" ) return rotation_o2n def _quat_to_orientation_repr ( self , quaternion : torch . Tensor ) -> torch . Tensor : \"\"\"Convert quaternion to selected orientation representation. Args: quaternion: The quaternion to convert, scalar-last, shape (4,). Returns: The same orientation as represented by the quaternion in the chosen orientation representation. \"\"\" if self . _orientation_repr == \"quaternion\" : return quaternion elif self . _orientation_repr == \"discretized\" : index = self . _orientation_grid . quat_to_index ( quaternion . numpy ()) return torch . tensor ( index , dtype = torch . long , ) else : raise NotImplementedError ( f \"Orientation representation { self . _orientation_repr } is not supported.\" ) def load_mesh ( self , object_path : str ) -> o3d . geometry . TriangleMesh : \"\"\"Load an object mesh and adjust its object frame convention.\"\"\" mesh = o3d . io . read_triangle_mesh ( object_path ) if self . _remap_y_axis is None and self . _remap_x_axis is None : return mesh elif self . _remap_y_axis is None or self . _remap_x_axis is None : raise ValueError ( \"Either both or none of remap_{y,x}_axis have to be None.\" ) rotation_o2n = self . _get_o2n_object_rotation_matrix () mesh . rotate ( rotation_o2n , center = np . array ([ 0.0 , 0.0 , 0.0 ])[:, None ], ) return mesh","title":"NOCSDataset"},{"location":"reference/initialization/datasets/nocs_dataset/#sdfest.initialization.datasets.nocs_dataset.NOCSDataset.Config","text":"Bases: TypedDict Configuration dictionary for NOCSDataset. ATTRIBUTE DESCRIPTION root_dir See NOCSDataset docstring. TYPE: str split The dataset split. The following strings are supported: \"camera_train\": 275000 images, synthetic objects + real background \"camera_val\": 25000 images, synthetic objects + real background \"real_train\": 4300 images in 7 scenes, real \"real_test\": 2750 images in 6 scenes, real TYPE: str mask_pointcloud Whether the returned pointcloud will be masked. TYPE: bool normalize_pointcloud Whether the returned pointcloud and position will be normalized, such that pointcloud centroid is at the origin. TYPE: bool scale_convention Which scale is returned. The following strings are supported: \"diagonal\": Length of bounding box' diagonal. This is what NOCS uses. \"max\": Maximum side length of bounding box. \"half_max\": Half maximum side length of bounding box. \"full\": Bounding box side lengths. Shape (3,). TYPE: str camera_convention Which camera convention is used for position and orientation. One of: \"opengl\": x right, y up, z back \"opencv\": x right, y down, z forward Note that this does not influence how the dataset is processed, only the returned position and quaternion. TYPE: str orientation_repr Which orientation representation is used. One of: \"quaternion\" \"discretized\" TYPE: str orientation_grid_resolution Resolution of the orientation grid. Only used if orientation_repr is \"discretized\". TYPE: int remap_y_axis If not None, the NOCS y-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. This is typically the up-axis. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: y One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\" TYPE: Optional [ str ] remap_x_axis If not None, the original x-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: -z One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\" TYPE: Optional [ str ] category_str If not None, only samples from the matching category will be returned. See NOCSDataset.category_id_to_str for admissible category strings. TYPE: Optional [ str ] Source code in sdfest/initialization/datasets/nocs_dataset.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 class Config ( TypedDict , total = False ): \"\"\"Configuration dictionary for NOCSDataset. Attributes: root_dir: See NOCSDataset docstring. split: The dataset split. The following strings are supported: \"camera_train\": 275000 images, synthetic objects + real background \"camera_val\": 25000 images, synthetic objects + real background \"real_train\": 4300 images in 7 scenes, real \"real_test\": 2750 images in 6 scenes, real mask_pointcloud: Whether the returned pointcloud will be masked. normalize_pointcloud: Whether the returned pointcloud and position will be normalized, such that pointcloud centroid is at the origin. scale_convention: Which scale is returned. The following strings are supported: \"diagonal\": Length of bounding box' diagonal. This is what NOCS uses. \"max\": Maximum side length of bounding box. \"half_max\": Half maximum side length of bounding box. \"full\": Bounding box side lengths. Shape (3,). camera_convention: Which camera convention is used for position and orientation. One of: \"opengl\": x right, y up, z back \"opencv\": x right, y down, z forward Note that this does not influence how the dataset is processed, only the returned position and quaternion. orientation_repr: Which orientation representation is used. One of: \"quaternion\" \"discretized\" orientation_grid_resolution: Resolution of the orientation grid. Only used if orientation_repr is \"discretized\". remap_y_axis: If not None, the NOCS y-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. This is typically the up-axis. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: y One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\" remap_x_axis: If not None, the original x-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: -z One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\" category_str: If not None, only samples from the matching category will be returned. See NOCSDataset.category_id_to_str for admissible category strings. \"\"\" root_dir : str split : str mask_pointcloud : bool normalize_pointcloud : bool scale_convention : str camera_convention : str orientation_repr : str orientation_grid_resolution : int remap_y_axis : Optional [ str ] remap_x_axis : Optional [ str ] category_str : Optional [ str ]","title":"Config"},{"location":"reference/initialization/datasets/nocs_dataset/#sdfest.initialization.datasets.nocs_dataset.NOCSDataset.__init__","text":"__init__ ( config : Config ) -> None Initialize the dataset. PARAMETER DESCRIPTION config Configuration dictionary of dataset. Provided dictionary will be merged with default_dict. See NOCSDataset.Config for supported keys. TYPE: Config Source code in sdfest/initialization/datasets/nocs_dataset.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 def __init__ ( self , config : Config , ) -> None : \"\"\"Initialize the dataset. Args: config: Configuration dictionary of dataset. Provided dictionary will be merged with default_dict. See NOCSDataset.Config for supported keys. \"\"\" config = yoco . load_config ( config , current_dict = NOCSDataset . default_config ) self . _root_dir = config [ \"root_dir\" ] self . _split = config [ \"split\" ] self . _camera_convention = config [ \"camera_convention\" ] self . _camera = self . _get_split_camera () self . _preprocess_path = os . path . join ( self . _root_dir , \"sdfest_pre\" , self . _split ) if not os . path . isdir ( self . _preprocess_path ): self . _preprocess_dataset () self . _mask_pointcloud = config [ \"mask_pointcloud\" ] self . _normalize_pointcloud = config [ \"normalize_pointcloud\" ] self . _scale_convention = config [ \"scale_convention\" ] self . _sample_files = self . _get_sample_files ( config [ \"category_str\" ]) self . _remap_y_axis = config [ \"remap_y_axis\" ] self . _remap_x_axis = config [ \"remap_x_axis\" ] self . _orientation_repr = config [ \"orientation_repr\" ] if self . _orientation_repr == \"discretized\" : self . _orientation_grid = so3grid . SO3Grid ( config [ \"orientation_grid_resolution\" ] )","title":"__init__()"},{"location":"reference/initialization/datasets/nocs_dataset/#sdfest.initialization.datasets.nocs_dataset.NOCSDataset.__len__","text":"__len__ () -> int Return number of sample in dataset. Source code in sdfest/initialization/datasets/nocs_dataset.py 178 179 180 def __len__ ( self ) -> int : \"\"\"Return number of sample in dataset.\"\"\" return len ( self . _sample_files )","title":"__len__()"},{"location":"reference/initialization/datasets/nocs_dataset/#sdfest.initialization.datasets.nocs_dataset.NOCSDataset.__getitem__","text":"__getitem__ ( idx : int ) -> dict Return a sample of the dataset. PARAMETER DESCRIPTION idx Index of the instance. TYPE: int RETURNS DESCRIPTION dict Sample containing the following items: \"color\" \"depth\" \"mask\" \"pointset\" \"position\" \"orientation\" \"quaternion\" \"scale\" \"color_path\" Source code in sdfest/initialization/datasets/nocs_dataset.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def __getitem__ ( self , idx : int ) -> dict : \"\"\"Return a sample of the dataset. Args: idx: Index of the instance. Returns: Sample containing the following items: \"color\" \"depth\" \"mask\" \"pointset\" \"position\" \"orientation\" \"quaternion\" \"scale\" \"color_path\" \"\"\" sample_file = self . _sample_files [ idx ] sample_data = pickle . load ( open ( sample_file , \"rb\" )) sample = self . _sample_from_sample_data ( sample_data ) return sample","title":"__getitem__()"},{"location":"reference/initialization/datasets/nocs_dataset/#sdfest.initialization.datasets.nocs_dataset.NOCSDataset.load_mesh","text":"load_mesh ( object_path : str ) -> o3d . geometry . TriangleMesh Load an object mesh and adjust its object frame convention. Source code in sdfest/initialization/datasets/nocs_dataset.py 820 821 822 823 824 825 826 827 828 829 830 831 832 833 def load_mesh ( self , object_path : str ) -> o3d . geometry . TriangleMesh : \"\"\"Load an object mesh and adjust its object frame convention.\"\"\" mesh = o3d . io . read_triangle_mesh ( object_path ) if self . _remap_y_axis is None and self . _remap_x_axis is None : return mesh elif self . _remap_y_axis is None or self . _remap_x_axis is None : raise ValueError ( \"Either both or none of remap_{y,x}_axis have to be None.\" ) rotation_o2n = self . _get_o2n_object_rotation_matrix () mesh . rotate ( rotation_o2n , center = np . array ([ 0.0 , 0.0 , 0.0 ])[:, None ], ) return mesh","title":"load_mesh()"},{"location":"reference/initialization/datasets/nocs_dataset/#sdfest.initialization.datasets.nocs_dataset.ObjectError","text":"Bases: Exception Error if something with the mesh is wrong. Source code in sdfest/initialization/datasets/nocs_dataset.py 836 837 838 839 class ObjectError ( Exception ): \"\"\"Error if something with the mesh is wrong.\"\"\" pass","title":"ObjectError"},{"location":"reference/initialization/datasets/nocs_utils/","text":"sdfest.initialization.datasets.nocs_utils Module for utility function related to NOCS dataset. This module contains functions to find similarity transform from NOCS maps and evaluation function for typical metrics on the NOCS datasets. Aligning code by Srinath Sridhar https://raw.githubusercontent.com/hughw19/NOCS_CVPR2019/master/aligning.py Evaluation code by ... TODO PoseEstimationError Bases: Exception Error if pose estimation encountered an error. Source code in sdfest/initialization/datasets/nocs_utils.py 244 245 246 247 class PoseEstimationError ( Exception ): \"\"\"Error if pose estimation encountered an error.\"\"\" pass estimate_similarity_transform estimate_similarity_transform ( source : np . ndarray , target : np . ndarray , verbose : bool = False ) -> tuple Estimate similarity transform from source to target from point correspondences. Source and target are pairwise correponding pointsets, i.e., they include same number of points and the first point of source corresponds to the first point of target. RANSAC is used for outlier-robust estimation. A similarity transform is estimated (i.e., isotropic scale, rotation and translation) that transforms source points onto the target points. Note that the returned values fulfill the following equations transform @ source_points = scale * rotation_matrix @ source_points + position when ignoring homogeneous coordinate for left-hand side. PARAMETER DESCRIPTION source Source points that will be transformed, shape (N,3). TYPE: np . ndarray target Target points to which source will be aligned to, shape (N,3). TYPE: np . ndarray verbose If true additional information will be printed. TYPE: bool DEFAULT: False RETURNS DESCRIPTION position Translation to translate source to target, shape (3,). TYPE: np . ndarray rotation_matrix Rotation to rotate source to target, shape (3,3). TYPE: np . ndarray scale Scaling factor along each axis, to scale source to target. TYPE: float transform Homogeneous transformation matrix, shape (4,4). TYPE: np . ndarray Source code in sdfest/initialization/datasets/nocs_utils.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def estimate_similarity_transform ( source : np . ndarray , target : np . ndarray , verbose : bool = False ) -> tuple : \"\"\"Estimate similarity transform from source to target from point correspondences. Source and target are pairwise correponding pointsets, i.e., they include same number of points and the first point of source corresponds to the first point of target. RANSAC is used for outlier-robust estimation. A similarity transform is estimated (i.e., isotropic scale, rotation and translation) that transforms source points onto the target points. Note that the returned values fulfill the following equations transform @ source_points = scale * rotation_matrix @ source_points + position when ignoring homogeneous coordinate for left-hand side. Args: source: Source points that will be transformed, shape (N,3). target: Target points to which source will be aligned to, shape (N,3). verbose: If true additional information will be printed. Returns: position (np.ndarray): Translation to translate source to target, shape (3,). rotation_matrix (np.ndarray): Rotation to rotate source to target, shape (3,3). scale (float): Scaling factor along each axis, to scale source to target. transform (np.ndarray): Homogeneous transformation matrix, shape (4,4). \"\"\" if len ( source ) < 5 or len ( target ) < 5 : print ( \"Pose estimation failed. Not enough point correspondences: \" , len ( source )) return None , None , None , None # make points homogeneous source_hom = np . transpose ( np . hstack ([ source , np . ones ([ source . shape [ 0 ], 1 ])])) # 4,N target_hom = np . transpose ( np . hstack ([ target , np . ones ([ source . shape [ 0 ], 1 ])])) # 4,N # Auto-parameter selection based on source-target heuristics target_norm = np . mean ( np . linalg . norm ( target , axis = 1 )) # mean distance from origin source_norm = np . mean ( np . linalg . norm ( source , axis = 1 )) ratio_ts = target_norm / source_norm ratio_st = source_norm / target_norm pass_t = ratio_st if ( ratio_st > ratio_ts ) else ratio_ts pass_t *= 0.01 # tighter bound stop_t = pass_t / 100 n_iter = 100 if verbose : print ( \"Pass threshold: \" , pass_t ) print ( \"Stop threshold: \" , stop_t ) print ( \"Number of iterations: \" , n_iter ) source_inliers_hom , target_inliers_hom , best_inlier_ratio = _get_ransac_inliers ( source_hom , target_hom , max_iterations = n_iter , pass_threshold = pass_t , stop_threshold = stop_t , ) if best_inlier_ratio < 0.1 : print ( \"Pose estimation failed. Small inlier ratio: \" , best_inlier_ratio ) return None , None , None , None scales , rotation_matrix , position , out_transform = _estimate_similarity_umeyama ( source_inliers_hom , target_inliers_hom ) scale = scales [ 0 ] if verbose : print ( \"BestInlierRatio:\" , best_inlier_ratio ) print ( \"Rotation: \\n \" , rotation_matrix ) print ( \"Position: \\n \" , position ) print ( \"Scales:\" , scales ) return position , rotation_matrix , scale , out_transform","title":"nocs_utils"},{"location":"reference/initialization/datasets/nocs_utils/#sdfest.initialization.datasets.nocs_utils","text":"Module for utility function related to NOCS dataset. This module contains functions to find similarity transform from NOCS maps and evaluation function for typical metrics on the NOCS datasets. Aligning code by Srinath Sridhar https://raw.githubusercontent.com/hughw19/NOCS_CVPR2019/master/aligning.py Evaluation code by ... TODO","title":"nocs_utils"},{"location":"reference/initialization/datasets/nocs_utils/#sdfest.initialization.datasets.nocs_utils.PoseEstimationError","text":"Bases: Exception Error if pose estimation encountered an error. Source code in sdfest/initialization/datasets/nocs_utils.py 244 245 246 247 class PoseEstimationError ( Exception ): \"\"\"Error if pose estimation encountered an error.\"\"\" pass","title":"PoseEstimationError"},{"location":"reference/initialization/datasets/nocs_utils/#sdfest.initialization.datasets.nocs_utils.estimate_similarity_transform","text":"estimate_similarity_transform ( source : np . ndarray , target : np . ndarray , verbose : bool = False ) -> tuple Estimate similarity transform from source to target from point correspondences. Source and target are pairwise correponding pointsets, i.e., they include same number of points and the first point of source corresponds to the first point of target. RANSAC is used for outlier-robust estimation. A similarity transform is estimated (i.e., isotropic scale, rotation and translation) that transforms source points onto the target points. Note that the returned values fulfill the following equations transform @ source_points = scale * rotation_matrix @ source_points + position when ignoring homogeneous coordinate for left-hand side. PARAMETER DESCRIPTION source Source points that will be transformed, shape (N,3). TYPE: np . ndarray target Target points to which source will be aligned to, shape (N,3). TYPE: np . ndarray verbose If true additional information will be printed. TYPE: bool DEFAULT: False RETURNS DESCRIPTION position Translation to translate source to target, shape (3,). TYPE: np . ndarray rotation_matrix Rotation to rotate source to target, shape (3,3). TYPE: np . ndarray scale Scaling factor along each axis, to scale source to target. TYPE: float transform Homogeneous transformation matrix, shape (4,4). TYPE: np . ndarray Source code in sdfest/initialization/datasets/nocs_utils.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def estimate_similarity_transform ( source : np . ndarray , target : np . ndarray , verbose : bool = False ) -> tuple : \"\"\"Estimate similarity transform from source to target from point correspondences. Source and target are pairwise correponding pointsets, i.e., they include same number of points and the first point of source corresponds to the first point of target. RANSAC is used for outlier-robust estimation. A similarity transform is estimated (i.e., isotropic scale, rotation and translation) that transforms source points onto the target points. Note that the returned values fulfill the following equations transform @ source_points = scale * rotation_matrix @ source_points + position when ignoring homogeneous coordinate for left-hand side. Args: source: Source points that will be transformed, shape (N,3). target: Target points to which source will be aligned to, shape (N,3). verbose: If true additional information will be printed. Returns: position (np.ndarray): Translation to translate source to target, shape (3,). rotation_matrix (np.ndarray): Rotation to rotate source to target, shape (3,3). scale (float): Scaling factor along each axis, to scale source to target. transform (np.ndarray): Homogeneous transformation matrix, shape (4,4). \"\"\" if len ( source ) < 5 or len ( target ) < 5 : print ( \"Pose estimation failed. Not enough point correspondences: \" , len ( source )) return None , None , None , None # make points homogeneous source_hom = np . transpose ( np . hstack ([ source , np . ones ([ source . shape [ 0 ], 1 ])])) # 4,N target_hom = np . transpose ( np . hstack ([ target , np . ones ([ source . shape [ 0 ], 1 ])])) # 4,N # Auto-parameter selection based on source-target heuristics target_norm = np . mean ( np . linalg . norm ( target , axis = 1 )) # mean distance from origin source_norm = np . mean ( np . linalg . norm ( source , axis = 1 )) ratio_ts = target_norm / source_norm ratio_st = source_norm / target_norm pass_t = ratio_st if ( ratio_st > ratio_ts ) else ratio_ts pass_t *= 0.01 # tighter bound stop_t = pass_t / 100 n_iter = 100 if verbose : print ( \"Pass threshold: \" , pass_t ) print ( \"Stop threshold: \" , stop_t ) print ( \"Number of iterations: \" , n_iter ) source_inliers_hom , target_inliers_hom , best_inlier_ratio = _get_ransac_inliers ( source_hom , target_hom , max_iterations = n_iter , pass_threshold = pass_t , stop_threshold = stop_t , ) if best_inlier_ratio < 0.1 : print ( \"Pose estimation failed. Small inlier ratio: \" , best_inlier_ratio ) return None , None , None , None scales , rotation_matrix , position , out_transform = _estimate_similarity_umeyama ( source_inliers_hom , target_inliers_hom ) scale = scales [ 0 ] if verbose : print ( \"BestInlierRatio:\" , best_inlier_ratio ) print ( \"Rotation: \\n \" , rotation_matrix ) print ( \"Position: \\n \" , position ) print ( \"Scales:\" , scales ) return position , rotation_matrix , scale , out_transform","title":"estimate_similarity_transform()"},{"location":"reference/initialization/datasets/redwood_dataset/","text":"sdfest.initialization.datasets.redwood_dataset Module providing dataset class for annotated Redwood dataset. AnnotatedRedwoodDataset Bases: torch . utils . data . Dataset Dataset class for annotated Redwood dataset. Data can be found here: http://redwood-data.org/3dscan/index.html Annotations are part SDFEst repo. Expected directory format {root_dir}/{category_str}/rgbd/{sequence_id}/... {ann_dir}/{sequence_id}.obj {ann_dir}/annotations.json Source code in sdfest/initialization/datasets/redwood_dataset.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 class AnnotatedRedwoodDataset ( torch . utils . data . Dataset ): \"\"\"Dataset class for annotated Redwood dataset. Data can be found here: http://redwood-data.org/3dscan/index.html Annotations are part SDFEst repo. Expected directory format: {root_dir}/{category_str}/rgbd/{sequence_id}/... {ann_dir}/{sequence_id}.obj {ann_dir}/annotations.json \"\"\" num_categories = 3 category_id_to_str = { 0 : \"bottle\" , 1 : \"bowl\" , 2 : \"mug\" , } category_str_to_id = { v : k for k , v in category_id_to_str . items ()} class Config ( TypedDict , total = False ): \"\"\"Configuration dictionary for annoated Redwood dataset. Attributes: root_dir: See AnnotatedRedwoodDataset docstring. ann_dir: See AnnotatedRedwoodDataset docstring. mask_pointcloud: Whether the returned pointcloud will be masked. normalize_pointcloud: Whether the returned pointcloud and position will be normalized, such that pointcloud centroid is at the origin. scale_convention: Which scale is returned. The following strings are supported: \"diagonal\": Length of bounding box' diagonal. This is what NOCS uses. \"max\": Maximum side length of bounding box. \"half_max\": Half maximum side length of bounding box. \"full\": Bounding box side lengths. Shape (3,). camera_convention: Which camera convention is used for position and orientation. One of: \"opengl\": x right, y up, z back \"opencv\": x right, y down, z forward Note that this does not influence how the dataset is processed, only the returned position and quaternion. orientation_repr: Which orientation representation is used. One of: \"quaternion\" \"discretized\" orientation_grid_resolution: Resolution of the orientation grid. Only used if orientation_repr is \"discretized\". remap_y_axis: If not None, the Redwood y-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. This is typically the up-axis. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: -y One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\" remap_x_axis: If not None, the original x-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: z One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\" category_str: If not None, only samples from the matching category will be returned. See AnnotatedRedwoodDataset.category_id_to_str for admissible category strings. \"\"\" root_dir : str ann_dir : str split : str mask_pointcloud : bool normalize_pointcloud : bool scale_convention : str camera_convention : str orientation_repr : str orientation_grid_resolution : int remap_y_axis : Optional [ str ] remap_x_axis : Optional [ str ] category_str : Optional [ str ] default_config : Config = { \"root_dir\" : None , \"ann_dir\" : None , \"mask_pointcloud\" : False , \"normalize_pointcloud\" : False , \"camera_convention\" : \"opengl\" , \"scale_convention\" : \"half_max\" , \"orientation_repr\" : \"quaternion\" , \"orientation_grid_resolution\" : None , \"category_str\" : None , \"remap_y_axis\" : None , \"remap_x_axis\" : None , } def __init__ ( self , config : Config , ) -> None : \"\"\"Initialize the dataset. Args: config: Configuration dictionary of dataset. Provided dictionary will be merged with default_dict. See AnnotatedRedwoodDataset.Config for keys. \"\"\" config = yoco . load_config ( config , current_dict = AnnotatedRedwoodDataset . default_config ) self . _root_dir = config [ \"root_dir\" ] self . _ann_dir = config [ \"ann_dir\" ] self . _camera_convention = config [ \"camera_convention\" ] self . _mask_pointcloud = config [ \"mask_pointcloud\" ] self . _normalize_pointcloud = config [ \"normalize_pointcloud\" ] self . _scale_convention = config [ \"scale_convention\" ] self . _remap_y_axis = config [ \"remap_y_axis\" ] self . _remap_x_axis = config [ \"remap_x_axis\" ] self . _orientation_repr = config [ \"orientation_repr\" ] if self . _orientation_repr == \"discretized\" : self . _orientation_grid = so3grid . SO3Grid ( config [ \"orientation_grid_resolution\" ] ) self . _load_annotations () self . _camera = Camera ( width = 640 , height = 480 , fx = 525 , fy = 525 , cx = 319.5 , cy = 239.5 ) def _load_annotations ( self ) -> None : \"\"\"Load annotations into memory.\"\"\" ann_json = os . path . join ( self . _ann_dir , \"annotations.json\" ) with open ( ann_json , \"r\" ) as f : anns_dict = json . load ( f ) self . _raw_samples = [] for seq_id , seq_anns in anns_dict . items (): for pose_ann in seq_anns [ \"pose_anns\" ]: self . _raw_samples . append ( self . _create_raw_sample ( seq_id , seq_anns , pose_ann ) ) def _create_raw_sample ( self , seq_id : str , sequence_dict : dict , annotation_dict : dict ) -> dict : \"\"\"Create raw sample from information in annotations file.\"\"\" position = torch . tensor ( annotation_dict [ \"position\" ]) orientation_q = torch . tensor ( annotation_dict [ \"orientation\" ]) rgb_filename = annotation_dict [ \"rgb_file\" ] depth_filename = annotation_dict [ \"depth_file\" ] mesh_filename = sequence_dict [ \"mesh\" ] mesh_path = os . path . join ( self . _ann_dir , mesh_filename ) category_str = sequence_dict [ \"category\" ] color_path = os . path . join ( self . _root_dir , category_str , \"rgbd\" , seq_id , \"rgb\" , rgb_filename ) depth_path = os . path . join ( self . _root_dir , category_str , \"rgbd\" , seq_id , \"depth\" , depth_filename ) extents = torch . tensor ( sequence_dict [ \"scale\" ]) * 2 return { \"position\" : position , \"orientation_q\" : orientation_q , \"extents\" : extents , \"color_path\" : color_path , \"depth_path\" : depth_path , \"mesh_path\" : mesh_path , \"category_str\" : category_str , } def __len__ ( self ) -> int : \"\"\"Return number of sample in dataset.\"\"\" return len ( self . _raw_samples ) def __getitem__ ( self , idx : int ) -> dict : \"\"\"Return a sample of the dataset. Args: idx: Index of the instance. Returns: Sample containing the following keys: \"color\" \"depth\" \"mask\" \"pointset\" \"position\" \"orientation\" \"quaternion\" \"scale\" \"color_path\" \"obj_path\" \"category_id\" \"category_str\" \"\"\" raw_sample = self . _raw_samples [ idx ] color = torch . from_numpy ( np . asarray ( Image . open ( raw_sample [ \"color_path\" ]), dtype = np . float32 ) / 255 ) depth = self . _load_depth ( raw_sample [ \"depth_path\" ]) instance_mask = self . _compute_mask ( depth , raw_sample ) pointcloud_mask = instance_mask if self . _mask_pointcloud else None pointcloud = pointset_utils . depth_to_pointcloud ( depth , self . _camera , mask = pointcloud_mask , convention = self . _camera_convention , ) # adjust camera convention for position, orientation and scale position = pointset_utils . change_position_camera_convention ( raw_sample [ \"position\" ], \"opencv\" , self . _camera_convention ) # orientation / scale orientation_q , extents = self . _change_axis_convention ( raw_sample [ \"orientation_q\" ], raw_sample [ \"extents\" ] ) orientation_q = pointset_utils . change_orientation_camera_convention ( orientation_q , \"opencv\" , self . _camera_convention ) orientation = self . _quat_to_orientation_repr ( orientation_q ) scale = self . _get_scale ( extents ) # normalize pointcloud & position if self . _normalize_pointcloud : pointcloud , centroid = pointset_utils . normalize_points ( pointcloud ) position = position - centroid category_str = raw_sample [ \"category_str\" ] sample = { \"color\" : color , \"depth\" : depth , \"pointset\" : pointcloud , \"mask\" : instance_mask , \"position\" : position , \"orientation\" : orientation , \"quaternion\" : orientation_q , \"scale\" : scale , \"color_path\" : raw_sample [ \"color_path\" ], \"obj_path\" : raw_sample [ \"mesh_path\" ], \"category_id\" : self . category_str_to_id [ category_str ], \"category_str\" : category_str , } return sample def _compute_mask ( self , depth : torch . Tensor , raw_sample : dict ) -> torch . Tensor : mesh = synthetic . Mesh ( path = raw_sample [ \"mesh_path\" ], scale = 1.0 , # do not resize mesh, as it is already at right size rel_scale = True , center = False , ) mesh . position = raw_sample [ \"position\" ] mesh . orientation = raw_sample [ \"orientation_q\" ] gt_depth = torch . from_numpy ( synthetic . draw_depth_geometry ( mesh , self . _camera )) mask = gt_depth != 0 # exclude occluded parts from mask mask [( depth != 0 ) * ( depth < gt_depth - 0.01 )] = 0 return mask def _load_depth ( self , depth_path : str ) -> torch . Tensor : \"\"\"Load depth from depth filepath.\"\"\" depth = torch . from_numpy ( np . asarray ( Image . open ( depth_path ), dtype = np . float32 ) * 0.001 ) return depth def _get_scale ( self , extents : torch . Tensor ) -> float : \"\"\"Return scale from stored sample data and extents.\"\"\" if self . _scale_convention == \"diagonal\" : return torch . linalg . norm ( extents ) elif self . _scale_convention == \"max\" : return extents . max () elif self . _scale_convention == \"half_max\" : return 0.5 * extents . max () elif self . _scale_convention == \"full\" : return extents else : raise ValueError ( f \"Specified scale convention { self . _scale_convention } not supported.\" ) def _change_axis_convention ( self , orientation_q : torch . Tensor , extents : torch . Tensor ) -> tuple : \"\"\"Adjust up-axis for orientation and extents. Returns: Tuple of position, orienation_q and extents, with specified up-axis. \"\"\" if self . _remap_y_axis is None and self . _remap_x_axis is None : return orientation_q , extents elif self . _remap_y_axis is None or self . _remap_x_axis is None : raise ValueError ( \"Either both or none of remap_{y,x}_axis have to be None.\" ) rotation_o2n = self . _get_o2n_object_rotation_matrix () remapped_extents = torch . abs ( torch . Tensor ( rotation_o2n ) @ extents ) # quaternion so far: original -> camera # we want a quaternion: new -> camera rotation_n2o = rotation_o2n . T quaternion_n2o = torch . from_numpy ( Rotation . from_matrix ( rotation_n2o ) . as_quat ()) remapped_orientation_q = quaternion_utils . quaternion_multiply ( orientation_q , quaternion_n2o ) # new -> original -> camera return remapped_orientation_q , remapped_extents def _get_o2n_object_rotation_matrix ( self ) -> np . ndarray : \"\"\"Compute rotation matrix which rotates original to new object coordinates.\"\"\" rotation_o2n = np . zeros (( 3 , 3 )) # original to new object convention if self . _remap_y_axis == \"x\" : rotation_o2n [ 0 , 1 ] = 1 elif self . _remap_y_axis == \"-x\" : rotation_o2n [ 0 , 1 ] = - 1 elif self . _remap_y_axis == \"y\" : rotation_o2n [ 1 , 1 ] = 1 elif self . _remap_y_axis == \"-y\" : rotation_o2n [ 1 , 1 ] = - 1 elif self . _remap_y_axis == \"z\" : rotation_o2n [ 2 , 1 ] = 1 elif self . _remap_y_axis == \"-z\" : rotation_o2n [ 2 , 1 ] = - 1 else : raise ValueError ( \"Unsupported remap_y_axis {self.remap_y} \" ) if self . _remap_x_axis == \"x\" : rotation_o2n [ 0 , 0 ] = 1 elif self . _remap_x_axis == \"-x\" : rotation_o2n [ 0 , 0 ] = - 1 elif self . _remap_x_axis == \"y\" : rotation_o2n [ 1 , 0 ] = 1 elif self . _remap_x_axis == \"-y\" : rotation_o2n [ 1 , 0 ] = - 1 elif self . _remap_x_axis == \"z\" : rotation_o2n [ 2 , 0 ] = 1 elif self . _remap_x_axis == \"-z\" : rotation_o2n [ 2 , 0 ] = - 1 else : raise ValueError ( \"Unsupported remap_x_axis {self.remap_y} \" ) # infer last column rotation_o2n [:, 2 ] = 1 - np . abs ( np . sum ( rotation_o2n , 1 )) # rows must sum to +-1 rotation_o2n [:, 2 ] *= np . linalg . det ( rotation_o2n ) # make special orthogonal if np . linalg . det ( rotation_o2n ) != 1.0 : # check if special orthogonal raise ValueError ( \"Unsupported combination of remap_{y,x}_axis. det != 1\" ) return rotation_o2n def _quat_to_orientation_repr ( self , quaternion : torch . Tensor ) -> torch . Tensor : \"\"\"Convert quaternion to selected orientation representation. Args: quaternion: The quaternion to convert, scalar-last, shape (4,). Returns: The same orientation as represented by the quaternion in the chosen orientation representation. \"\"\" if self . _orientation_repr == \"quaternion\" : return quaternion elif self . _orientation_repr == \"discretized\" : index = self . _orientation_grid . quat_to_index ( quaternion . numpy ()) return torch . tensor ( index , dtype = torch . long , ) else : raise NotImplementedError ( f \"Orientation representation { self . _orientation_repr } is not supported.\" ) def load_mesh ( self , object_path : str ) -> o3d . geometry . TriangleMesh : \"\"\"Load an object mesh and adjust its object frame convention.\"\"\" mesh = o3d . io . read_triangle_mesh ( object_path ) if self . _remap_y_axis is None and self . _remap_x_axis is None : return mesh elif self . _remap_y_axis is None or self . _remap_x_axis is None : raise ValueError ( \"Either both or none of remap_{y,x}_axis have to be None.\" ) rotation_o2n = self . _get_o2n_object_rotation_matrix () mesh . rotate ( rotation_o2n , center = np . array ([ 0.0 , 0.0 , 0.0 ])[:, None ], ) return mesh Config Bases: TypedDict Configuration dictionary for annoated Redwood dataset. ATTRIBUTE DESCRIPTION root_dir See AnnotatedRedwoodDataset docstring. TYPE: str ann_dir See AnnotatedRedwoodDataset docstring. TYPE: str mask_pointcloud Whether the returned pointcloud will be masked. TYPE: bool normalize_pointcloud Whether the returned pointcloud and position will be normalized, such that pointcloud centroid is at the origin. TYPE: bool scale_convention Which scale is returned. The following strings are supported: \"diagonal\": Length of bounding box' diagonal. This is what NOCS uses. \"max\": Maximum side length of bounding box. \"half_max\": Half maximum side length of bounding box. \"full\": Bounding box side lengths. Shape (3,). TYPE: str camera_convention Which camera convention is used for position and orientation. One of: \"opengl\": x right, y up, z back \"opencv\": x right, y down, z forward Note that this does not influence how the dataset is processed, only the returned position and quaternion. TYPE: str orientation_repr Which orientation representation is used. One of: \"quaternion\" \"discretized\" TYPE: str orientation_grid_resolution Resolution of the orientation grid. Only used if orientation_repr is \"discretized\". TYPE: int remap_y_axis If not None, the Redwood y-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. This is typically the up-axis. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: -y One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\" TYPE: Optional [ str ] remap_x_axis If not None, the original x-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: z One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\" TYPE: Optional [ str ] category_str If not None, only samples from the matching category will be returned. See AnnotatedRedwoodDataset.category_id_to_str for admissible category strings. TYPE: Optional [ str ] Source code in sdfest/initialization/datasets/redwood_dataset.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 class Config ( TypedDict , total = False ): \"\"\"Configuration dictionary for annoated Redwood dataset. Attributes: root_dir: See AnnotatedRedwoodDataset docstring. ann_dir: See AnnotatedRedwoodDataset docstring. mask_pointcloud: Whether the returned pointcloud will be masked. normalize_pointcloud: Whether the returned pointcloud and position will be normalized, such that pointcloud centroid is at the origin. scale_convention: Which scale is returned. The following strings are supported: \"diagonal\": Length of bounding box' diagonal. This is what NOCS uses. \"max\": Maximum side length of bounding box. \"half_max\": Half maximum side length of bounding box. \"full\": Bounding box side lengths. Shape (3,). camera_convention: Which camera convention is used for position and orientation. One of: \"opengl\": x right, y up, z back \"opencv\": x right, y down, z forward Note that this does not influence how the dataset is processed, only the returned position and quaternion. orientation_repr: Which orientation representation is used. One of: \"quaternion\" \"discretized\" orientation_grid_resolution: Resolution of the orientation grid. Only used if orientation_repr is \"discretized\". remap_y_axis: If not None, the Redwood y-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. This is typically the up-axis. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: -y One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\" remap_x_axis: If not None, the original x-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: z One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\" category_str: If not None, only samples from the matching category will be returned. See AnnotatedRedwoodDataset.category_id_to_str for admissible category strings. \"\"\" root_dir : str ann_dir : str split : str mask_pointcloud : bool normalize_pointcloud : bool scale_convention : str camera_convention : str orientation_repr : str orientation_grid_resolution : int remap_y_axis : Optional [ str ] remap_x_axis : Optional [ str ] category_str : Optional [ str ] __init__ __init__ ( config : Config ) -> None Initialize the dataset. PARAMETER DESCRIPTION config Configuration dictionary of dataset. Provided dictionary will be merged with default_dict. See AnnotatedRedwoodDataset.Config for keys. TYPE: Config Source code in sdfest/initialization/datasets/redwood_dataset.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def __init__ ( self , config : Config , ) -> None : \"\"\"Initialize the dataset. Args: config: Configuration dictionary of dataset. Provided dictionary will be merged with default_dict. See AnnotatedRedwoodDataset.Config for keys. \"\"\" config = yoco . load_config ( config , current_dict = AnnotatedRedwoodDataset . default_config ) self . _root_dir = config [ \"root_dir\" ] self . _ann_dir = config [ \"ann_dir\" ] self . _camera_convention = config [ \"camera_convention\" ] self . _mask_pointcloud = config [ \"mask_pointcloud\" ] self . _normalize_pointcloud = config [ \"normalize_pointcloud\" ] self . _scale_convention = config [ \"scale_convention\" ] self . _remap_y_axis = config [ \"remap_y_axis\" ] self . _remap_x_axis = config [ \"remap_x_axis\" ] self . _orientation_repr = config [ \"orientation_repr\" ] if self . _orientation_repr == \"discretized\" : self . _orientation_grid = so3grid . SO3Grid ( config [ \"orientation_grid_resolution\" ] ) self . _load_annotations () self . _camera = Camera ( width = 640 , height = 480 , fx = 525 , fy = 525 , cx = 319.5 , cy = 239.5 ) __len__ __len__ () -> int Return number of sample in dataset. Source code in sdfest/initialization/datasets/redwood_dataset.py 186 187 188 def __len__ ( self ) -> int : \"\"\"Return number of sample in dataset.\"\"\" return len ( self . _raw_samples ) __getitem__ __getitem__ ( idx : int ) -> dict Return a sample of the dataset. PARAMETER DESCRIPTION idx Index of the instance. TYPE: int RETURNS DESCRIPTION dict Sample containing the following keys: \"color\" \"depth\" \"mask\" \"pointset\" \"position\" \"orientation\" \"quaternion\" \"scale\" \"color_path\" \"obj_path\" \"category_id\" \"category_str\" Source code in sdfest/initialization/datasets/redwood_dataset.py 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 def __getitem__ ( self , idx : int ) -> dict : \"\"\"Return a sample of the dataset. Args: idx: Index of the instance. Returns: Sample containing the following keys: \"color\" \"depth\" \"mask\" \"pointset\" \"position\" \"orientation\" \"quaternion\" \"scale\" \"color_path\" \"obj_path\" \"category_id\" \"category_str\" \"\"\" raw_sample = self . _raw_samples [ idx ] color = torch . from_numpy ( np . asarray ( Image . open ( raw_sample [ \"color_path\" ]), dtype = np . float32 ) / 255 ) depth = self . _load_depth ( raw_sample [ \"depth_path\" ]) instance_mask = self . _compute_mask ( depth , raw_sample ) pointcloud_mask = instance_mask if self . _mask_pointcloud else None pointcloud = pointset_utils . depth_to_pointcloud ( depth , self . _camera , mask = pointcloud_mask , convention = self . _camera_convention , ) # adjust camera convention for position, orientation and scale position = pointset_utils . change_position_camera_convention ( raw_sample [ \"position\" ], \"opencv\" , self . _camera_convention ) # orientation / scale orientation_q , extents = self . _change_axis_convention ( raw_sample [ \"orientation_q\" ], raw_sample [ \"extents\" ] ) orientation_q = pointset_utils . change_orientation_camera_convention ( orientation_q , \"opencv\" , self . _camera_convention ) orientation = self . _quat_to_orientation_repr ( orientation_q ) scale = self . _get_scale ( extents ) # normalize pointcloud & position if self . _normalize_pointcloud : pointcloud , centroid = pointset_utils . normalize_points ( pointcloud ) position = position - centroid category_str = raw_sample [ \"category_str\" ] sample = { \"color\" : color , \"depth\" : depth , \"pointset\" : pointcloud , \"mask\" : instance_mask , \"position\" : position , \"orientation\" : orientation , \"quaternion\" : orientation_q , \"scale\" : scale , \"color_path\" : raw_sample [ \"color_path\" ], \"obj_path\" : raw_sample [ \"mesh_path\" ], \"category_id\" : self . category_str_to_id [ category_str ], \"category_str\" : category_str , } return sample load_mesh load_mesh ( object_path : str ) -> o3d . geometry . TriangleMesh Load an object mesh and adjust its object frame convention. Source code in sdfest/initialization/datasets/redwood_dataset.py 390 391 392 393 394 395 396 397 398 399 400 401 402 403 def load_mesh ( self , object_path : str ) -> o3d . geometry . TriangleMesh : \"\"\"Load an object mesh and adjust its object frame convention.\"\"\" mesh = o3d . io . read_triangle_mesh ( object_path ) if self . _remap_y_axis is None and self . _remap_x_axis is None : return mesh elif self . _remap_y_axis is None or self . _remap_x_axis is None : raise ValueError ( \"Either both or none of remap_{y,x}_axis have to be None.\" ) rotation_o2n = self . _get_o2n_object_rotation_matrix () mesh . rotate ( rotation_o2n , center = np . array ([ 0.0 , 0.0 , 0.0 ])[:, None ], ) return mesh ObjectError Bases: Exception Error if something with the mesh is wrong. Source code in sdfest/initialization/datasets/redwood_dataset.py 406 407 408 409 class ObjectError ( Exception ): \"\"\"Error if something with the mesh is wrong.\"\"\" pass","title":"redwood_dataset"},{"location":"reference/initialization/datasets/redwood_dataset/#sdfest.initialization.datasets.redwood_dataset","text":"Module providing dataset class for annotated Redwood dataset.","title":"redwood_dataset"},{"location":"reference/initialization/datasets/redwood_dataset/#sdfest.initialization.datasets.redwood_dataset.AnnotatedRedwoodDataset","text":"Bases: torch . utils . data . Dataset Dataset class for annotated Redwood dataset. Data can be found here: http://redwood-data.org/3dscan/index.html Annotations are part SDFEst repo. Expected directory format {root_dir}/{category_str}/rgbd/{sequence_id}/... {ann_dir}/{sequence_id}.obj {ann_dir}/annotations.json Source code in sdfest/initialization/datasets/redwood_dataset.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 class AnnotatedRedwoodDataset ( torch . utils . data . Dataset ): \"\"\"Dataset class for annotated Redwood dataset. Data can be found here: http://redwood-data.org/3dscan/index.html Annotations are part SDFEst repo. Expected directory format: {root_dir}/{category_str}/rgbd/{sequence_id}/... {ann_dir}/{sequence_id}.obj {ann_dir}/annotations.json \"\"\" num_categories = 3 category_id_to_str = { 0 : \"bottle\" , 1 : \"bowl\" , 2 : \"mug\" , } category_str_to_id = { v : k for k , v in category_id_to_str . items ()} class Config ( TypedDict , total = False ): \"\"\"Configuration dictionary for annoated Redwood dataset. Attributes: root_dir: See AnnotatedRedwoodDataset docstring. ann_dir: See AnnotatedRedwoodDataset docstring. mask_pointcloud: Whether the returned pointcloud will be masked. normalize_pointcloud: Whether the returned pointcloud and position will be normalized, such that pointcloud centroid is at the origin. scale_convention: Which scale is returned. The following strings are supported: \"diagonal\": Length of bounding box' diagonal. This is what NOCS uses. \"max\": Maximum side length of bounding box. \"half_max\": Half maximum side length of bounding box. \"full\": Bounding box side lengths. Shape (3,). camera_convention: Which camera convention is used for position and orientation. One of: \"opengl\": x right, y up, z back \"opencv\": x right, y down, z forward Note that this does not influence how the dataset is processed, only the returned position and quaternion. orientation_repr: Which orientation representation is used. One of: \"quaternion\" \"discretized\" orientation_grid_resolution: Resolution of the orientation grid. Only used if orientation_repr is \"discretized\". remap_y_axis: If not None, the Redwood y-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. This is typically the up-axis. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: -y One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\" remap_x_axis: If not None, the original x-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: z One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\" category_str: If not None, only samples from the matching category will be returned. See AnnotatedRedwoodDataset.category_id_to_str for admissible category strings. \"\"\" root_dir : str ann_dir : str split : str mask_pointcloud : bool normalize_pointcloud : bool scale_convention : str camera_convention : str orientation_repr : str orientation_grid_resolution : int remap_y_axis : Optional [ str ] remap_x_axis : Optional [ str ] category_str : Optional [ str ] default_config : Config = { \"root_dir\" : None , \"ann_dir\" : None , \"mask_pointcloud\" : False , \"normalize_pointcloud\" : False , \"camera_convention\" : \"opengl\" , \"scale_convention\" : \"half_max\" , \"orientation_repr\" : \"quaternion\" , \"orientation_grid_resolution\" : None , \"category_str\" : None , \"remap_y_axis\" : None , \"remap_x_axis\" : None , } def __init__ ( self , config : Config , ) -> None : \"\"\"Initialize the dataset. Args: config: Configuration dictionary of dataset. Provided dictionary will be merged with default_dict. See AnnotatedRedwoodDataset.Config for keys. \"\"\" config = yoco . load_config ( config , current_dict = AnnotatedRedwoodDataset . default_config ) self . _root_dir = config [ \"root_dir\" ] self . _ann_dir = config [ \"ann_dir\" ] self . _camera_convention = config [ \"camera_convention\" ] self . _mask_pointcloud = config [ \"mask_pointcloud\" ] self . _normalize_pointcloud = config [ \"normalize_pointcloud\" ] self . _scale_convention = config [ \"scale_convention\" ] self . _remap_y_axis = config [ \"remap_y_axis\" ] self . _remap_x_axis = config [ \"remap_x_axis\" ] self . _orientation_repr = config [ \"orientation_repr\" ] if self . _orientation_repr == \"discretized\" : self . _orientation_grid = so3grid . SO3Grid ( config [ \"orientation_grid_resolution\" ] ) self . _load_annotations () self . _camera = Camera ( width = 640 , height = 480 , fx = 525 , fy = 525 , cx = 319.5 , cy = 239.5 ) def _load_annotations ( self ) -> None : \"\"\"Load annotations into memory.\"\"\" ann_json = os . path . join ( self . _ann_dir , \"annotations.json\" ) with open ( ann_json , \"r\" ) as f : anns_dict = json . load ( f ) self . _raw_samples = [] for seq_id , seq_anns in anns_dict . items (): for pose_ann in seq_anns [ \"pose_anns\" ]: self . _raw_samples . append ( self . _create_raw_sample ( seq_id , seq_anns , pose_ann ) ) def _create_raw_sample ( self , seq_id : str , sequence_dict : dict , annotation_dict : dict ) -> dict : \"\"\"Create raw sample from information in annotations file.\"\"\" position = torch . tensor ( annotation_dict [ \"position\" ]) orientation_q = torch . tensor ( annotation_dict [ \"orientation\" ]) rgb_filename = annotation_dict [ \"rgb_file\" ] depth_filename = annotation_dict [ \"depth_file\" ] mesh_filename = sequence_dict [ \"mesh\" ] mesh_path = os . path . join ( self . _ann_dir , mesh_filename ) category_str = sequence_dict [ \"category\" ] color_path = os . path . join ( self . _root_dir , category_str , \"rgbd\" , seq_id , \"rgb\" , rgb_filename ) depth_path = os . path . join ( self . _root_dir , category_str , \"rgbd\" , seq_id , \"depth\" , depth_filename ) extents = torch . tensor ( sequence_dict [ \"scale\" ]) * 2 return { \"position\" : position , \"orientation_q\" : orientation_q , \"extents\" : extents , \"color_path\" : color_path , \"depth_path\" : depth_path , \"mesh_path\" : mesh_path , \"category_str\" : category_str , } def __len__ ( self ) -> int : \"\"\"Return number of sample in dataset.\"\"\" return len ( self . _raw_samples ) def __getitem__ ( self , idx : int ) -> dict : \"\"\"Return a sample of the dataset. Args: idx: Index of the instance. Returns: Sample containing the following keys: \"color\" \"depth\" \"mask\" \"pointset\" \"position\" \"orientation\" \"quaternion\" \"scale\" \"color_path\" \"obj_path\" \"category_id\" \"category_str\" \"\"\" raw_sample = self . _raw_samples [ idx ] color = torch . from_numpy ( np . asarray ( Image . open ( raw_sample [ \"color_path\" ]), dtype = np . float32 ) / 255 ) depth = self . _load_depth ( raw_sample [ \"depth_path\" ]) instance_mask = self . _compute_mask ( depth , raw_sample ) pointcloud_mask = instance_mask if self . _mask_pointcloud else None pointcloud = pointset_utils . depth_to_pointcloud ( depth , self . _camera , mask = pointcloud_mask , convention = self . _camera_convention , ) # adjust camera convention for position, orientation and scale position = pointset_utils . change_position_camera_convention ( raw_sample [ \"position\" ], \"opencv\" , self . _camera_convention ) # orientation / scale orientation_q , extents = self . _change_axis_convention ( raw_sample [ \"orientation_q\" ], raw_sample [ \"extents\" ] ) orientation_q = pointset_utils . change_orientation_camera_convention ( orientation_q , \"opencv\" , self . _camera_convention ) orientation = self . _quat_to_orientation_repr ( orientation_q ) scale = self . _get_scale ( extents ) # normalize pointcloud & position if self . _normalize_pointcloud : pointcloud , centroid = pointset_utils . normalize_points ( pointcloud ) position = position - centroid category_str = raw_sample [ \"category_str\" ] sample = { \"color\" : color , \"depth\" : depth , \"pointset\" : pointcloud , \"mask\" : instance_mask , \"position\" : position , \"orientation\" : orientation , \"quaternion\" : orientation_q , \"scale\" : scale , \"color_path\" : raw_sample [ \"color_path\" ], \"obj_path\" : raw_sample [ \"mesh_path\" ], \"category_id\" : self . category_str_to_id [ category_str ], \"category_str\" : category_str , } return sample def _compute_mask ( self , depth : torch . Tensor , raw_sample : dict ) -> torch . Tensor : mesh = synthetic . Mesh ( path = raw_sample [ \"mesh_path\" ], scale = 1.0 , # do not resize mesh, as it is already at right size rel_scale = True , center = False , ) mesh . position = raw_sample [ \"position\" ] mesh . orientation = raw_sample [ \"orientation_q\" ] gt_depth = torch . from_numpy ( synthetic . draw_depth_geometry ( mesh , self . _camera )) mask = gt_depth != 0 # exclude occluded parts from mask mask [( depth != 0 ) * ( depth < gt_depth - 0.01 )] = 0 return mask def _load_depth ( self , depth_path : str ) -> torch . Tensor : \"\"\"Load depth from depth filepath.\"\"\" depth = torch . from_numpy ( np . asarray ( Image . open ( depth_path ), dtype = np . float32 ) * 0.001 ) return depth def _get_scale ( self , extents : torch . Tensor ) -> float : \"\"\"Return scale from stored sample data and extents.\"\"\" if self . _scale_convention == \"diagonal\" : return torch . linalg . norm ( extents ) elif self . _scale_convention == \"max\" : return extents . max () elif self . _scale_convention == \"half_max\" : return 0.5 * extents . max () elif self . _scale_convention == \"full\" : return extents else : raise ValueError ( f \"Specified scale convention { self . _scale_convention } not supported.\" ) def _change_axis_convention ( self , orientation_q : torch . Tensor , extents : torch . Tensor ) -> tuple : \"\"\"Adjust up-axis for orientation and extents. Returns: Tuple of position, orienation_q and extents, with specified up-axis. \"\"\" if self . _remap_y_axis is None and self . _remap_x_axis is None : return orientation_q , extents elif self . _remap_y_axis is None or self . _remap_x_axis is None : raise ValueError ( \"Either both or none of remap_{y,x}_axis have to be None.\" ) rotation_o2n = self . _get_o2n_object_rotation_matrix () remapped_extents = torch . abs ( torch . Tensor ( rotation_o2n ) @ extents ) # quaternion so far: original -> camera # we want a quaternion: new -> camera rotation_n2o = rotation_o2n . T quaternion_n2o = torch . from_numpy ( Rotation . from_matrix ( rotation_n2o ) . as_quat ()) remapped_orientation_q = quaternion_utils . quaternion_multiply ( orientation_q , quaternion_n2o ) # new -> original -> camera return remapped_orientation_q , remapped_extents def _get_o2n_object_rotation_matrix ( self ) -> np . ndarray : \"\"\"Compute rotation matrix which rotates original to new object coordinates.\"\"\" rotation_o2n = np . zeros (( 3 , 3 )) # original to new object convention if self . _remap_y_axis == \"x\" : rotation_o2n [ 0 , 1 ] = 1 elif self . _remap_y_axis == \"-x\" : rotation_o2n [ 0 , 1 ] = - 1 elif self . _remap_y_axis == \"y\" : rotation_o2n [ 1 , 1 ] = 1 elif self . _remap_y_axis == \"-y\" : rotation_o2n [ 1 , 1 ] = - 1 elif self . _remap_y_axis == \"z\" : rotation_o2n [ 2 , 1 ] = 1 elif self . _remap_y_axis == \"-z\" : rotation_o2n [ 2 , 1 ] = - 1 else : raise ValueError ( \"Unsupported remap_y_axis {self.remap_y} \" ) if self . _remap_x_axis == \"x\" : rotation_o2n [ 0 , 0 ] = 1 elif self . _remap_x_axis == \"-x\" : rotation_o2n [ 0 , 0 ] = - 1 elif self . _remap_x_axis == \"y\" : rotation_o2n [ 1 , 0 ] = 1 elif self . _remap_x_axis == \"-y\" : rotation_o2n [ 1 , 0 ] = - 1 elif self . _remap_x_axis == \"z\" : rotation_o2n [ 2 , 0 ] = 1 elif self . _remap_x_axis == \"-z\" : rotation_o2n [ 2 , 0 ] = - 1 else : raise ValueError ( \"Unsupported remap_x_axis {self.remap_y} \" ) # infer last column rotation_o2n [:, 2 ] = 1 - np . abs ( np . sum ( rotation_o2n , 1 )) # rows must sum to +-1 rotation_o2n [:, 2 ] *= np . linalg . det ( rotation_o2n ) # make special orthogonal if np . linalg . det ( rotation_o2n ) != 1.0 : # check if special orthogonal raise ValueError ( \"Unsupported combination of remap_{y,x}_axis. det != 1\" ) return rotation_o2n def _quat_to_orientation_repr ( self , quaternion : torch . Tensor ) -> torch . Tensor : \"\"\"Convert quaternion to selected orientation representation. Args: quaternion: The quaternion to convert, scalar-last, shape (4,). Returns: The same orientation as represented by the quaternion in the chosen orientation representation. \"\"\" if self . _orientation_repr == \"quaternion\" : return quaternion elif self . _orientation_repr == \"discretized\" : index = self . _orientation_grid . quat_to_index ( quaternion . numpy ()) return torch . tensor ( index , dtype = torch . long , ) else : raise NotImplementedError ( f \"Orientation representation { self . _orientation_repr } is not supported.\" ) def load_mesh ( self , object_path : str ) -> o3d . geometry . TriangleMesh : \"\"\"Load an object mesh and adjust its object frame convention.\"\"\" mesh = o3d . io . read_triangle_mesh ( object_path ) if self . _remap_y_axis is None and self . _remap_x_axis is None : return mesh elif self . _remap_y_axis is None or self . _remap_x_axis is None : raise ValueError ( \"Either both or none of remap_{y,x}_axis have to be None.\" ) rotation_o2n = self . _get_o2n_object_rotation_matrix () mesh . rotate ( rotation_o2n , center = np . array ([ 0.0 , 0.0 , 0.0 ])[:, None ], ) return mesh","title":"AnnotatedRedwoodDataset"},{"location":"reference/initialization/datasets/redwood_dataset/#sdfest.initialization.datasets.redwood_dataset.AnnotatedRedwoodDataset.Config","text":"Bases: TypedDict Configuration dictionary for annoated Redwood dataset. ATTRIBUTE DESCRIPTION root_dir See AnnotatedRedwoodDataset docstring. TYPE: str ann_dir See AnnotatedRedwoodDataset docstring. TYPE: str mask_pointcloud Whether the returned pointcloud will be masked. TYPE: bool normalize_pointcloud Whether the returned pointcloud and position will be normalized, such that pointcloud centroid is at the origin. TYPE: bool scale_convention Which scale is returned. The following strings are supported: \"diagonal\": Length of bounding box' diagonal. This is what NOCS uses. \"max\": Maximum side length of bounding box. \"half_max\": Half maximum side length of bounding box. \"full\": Bounding box side lengths. Shape (3,). TYPE: str camera_convention Which camera convention is used for position and orientation. One of: \"opengl\": x right, y up, z back \"opencv\": x right, y down, z forward Note that this does not influence how the dataset is processed, only the returned position and quaternion. TYPE: str orientation_repr Which orientation representation is used. One of: \"quaternion\" \"discretized\" TYPE: str orientation_grid_resolution Resolution of the orientation grid. Only used if orientation_repr is \"discretized\". TYPE: int remap_y_axis If not None, the Redwood y-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. This is typically the up-axis. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: -y One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\" TYPE: Optional [ str ] remap_x_axis If not None, the original x-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: z One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\" TYPE: Optional [ str ] category_str If not None, only samples from the matching category will be returned. See AnnotatedRedwoodDataset.category_id_to_str for admissible category strings. TYPE: Optional [ str ] Source code in sdfest/initialization/datasets/redwood_dataset.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 class Config ( TypedDict , total = False ): \"\"\"Configuration dictionary for annoated Redwood dataset. Attributes: root_dir: See AnnotatedRedwoodDataset docstring. ann_dir: See AnnotatedRedwoodDataset docstring. mask_pointcloud: Whether the returned pointcloud will be masked. normalize_pointcloud: Whether the returned pointcloud and position will be normalized, such that pointcloud centroid is at the origin. scale_convention: Which scale is returned. The following strings are supported: \"diagonal\": Length of bounding box' diagonal. This is what NOCS uses. \"max\": Maximum side length of bounding box. \"half_max\": Half maximum side length of bounding box. \"full\": Bounding box side lengths. Shape (3,). camera_convention: Which camera convention is used for position and orientation. One of: \"opengl\": x right, y up, z back \"opencv\": x right, y down, z forward Note that this does not influence how the dataset is processed, only the returned position and quaternion. orientation_repr: Which orientation representation is used. One of: \"quaternion\" \"discretized\" orientation_grid_resolution: Resolution of the orientation grid. Only used if orientation_repr is \"discretized\". remap_y_axis: If not None, the Redwood y-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. This is typically the up-axis. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: -y One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\" remap_x_axis: If not None, the original x-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: z One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\" category_str: If not None, only samples from the matching category will be returned. See AnnotatedRedwoodDataset.category_id_to_str for admissible category strings. \"\"\" root_dir : str ann_dir : str split : str mask_pointcloud : bool normalize_pointcloud : bool scale_convention : str camera_convention : str orientation_repr : str orientation_grid_resolution : int remap_y_axis : Optional [ str ] remap_x_axis : Optional [ str ] category_str : Optional [ str ]","title":"Config"},{"location":"reference/initialization/datasets/redwood_dataset/#sdfest.initialization.datasets.redwood_dataset.AnnotatedRedwoodDataset.__init__","text":"__init__ ( config : Config ) -> None Initialize the dataset. PARAMETER DESCRIPTION config Configuration dictionary of dataset. Provided dictionary will be merged with default_dict. See AnnotatedRedwoodDataset.Config for keys. TYPE: Config Source code in sdfest/initialization/datasets/redwood_dataset.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def __init__ ( self , config : Config , ) -> None : \"\"\"Initialize the dataset. Args: config: Configuration dictionary of dataset. Provided dictionary will be merged with default_dict. See AnnotatedRedwoodDataset.Config for keys. \"\"\" config = yoco . load_config ( config , current_dict = AnnotatedRedwoodDataset . default_config ) self . _root_dir = config [ \"root_dir\" ] self . _ann_dir = config [ \"ann_dir\" ] self . _camera_convention = config [ \"camera_convention\" ] self . _mask_pointcloud = config [ \"mask_pointcloud\" ] self . _normalize_pointcloud = config [ \"normalize_pointcloud\" ] self . _scale_convention = config [ \"scale_convention\" ] self . _remap_y_axis = config [ \"remap_y_axis\" ] self . _remap_x_axis = config [ \"remap_x_axis\" ] self . _orientation_repr = config [ \"orientation_repr\" ] if self . _orientation_repr == \"discretized\" : self . _orientation_grid = so3grid . SO3Grid ( config [ \"orientation_grid_resolution\" ] ) self . _load_annotations () self . _camera = Camera ( width = 640 , height = 480 , fx = 525 , fy = 525 , cx = 319.5 , cy = 239.5 )","title":"__init__()"},{"location":"reference/initialization/datasets/redwood_dataset/#sdfest.initialization.datasets.redwood_dataset.AnnotatedRedwoodDataset.__len__","text":"__len__ () -> int Return number of sample in dataset. Source code in sdfest/initialization/datasets/redwood_dataset.py 186 187 188 def __len__ ( self ) -> int : \"\"\"Return number of sample in dataset.\"\"\" return len ( self . _raw_samples )","title":"__len__()"},{"location":"reference/initialization/datasets/redwood_dataset/#sdfest.initialization.datasets.redwood_dataset.AnnotatedRedwoodDataset.__getitem__","text":"__getitem__ ( idx : int ) -> dict Return a sample of the dataset. PARAMETER DESCRIPTION idx Index of the instance. TYPE: int RETURNS DESCRIPTION dict Sample containing the following keys: \"color\" \"depth\" \"mask\" \"pointset\" \"position\" \"orientation\" \"quaternion\" \"scale\" \"color_path\" \"obj_path\" \"category_id\" \"category_str\" Source code in sdfest/initialization/datasets/redwood_dataset.py 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 def __getitem__ ( self , idx : int ) -> dict : \"\"\"Return a sample of the dataset. Args: idx: Index of the instance. Returns: Sample containing the following keys: \"color\" \"depth\" \"mask\" \"pointset\" \"position\" \"orientation\" \"quaternion\" \"scale\" \"color_path\" \"obj_path\" \"category_id\" \"category_str\" \"\"\" raw_sample = self . _raw_samples [ idx ] color = torch . from_numpy ( np . asarray ( Image . open ( raw_sample [ \"color_path\" ]), dtype = np . float32 ) / 255 ) depth = self . _load_depth ( raw_sample [ \"depth_path\" ]) instance_mask = self . _compute_mask ( depth , raw_sample ) pointcloud_mask = instance_mask if self . _mask_pointcloud else None pointcloud = pointset_utils . depth_to_pointcloud ( depth , self . _camera , mask = pointcloud_mask , convention = self . _camera_convention , ) # adjust camera convention for position, orientation and scale position = pointset_utils . change_position_camera_convention ( raw_sample [ \"position\" ], \"opencv\" , self . _camera_convention ) # orientation / scale orientation_q , extents = self . _change_axis_convention ( raw_sample [ \"orientation_q\" ], raw_sample [ \"extents\" ] ) orientation_q = pointset_utils . change_orientation_camera_convention ( orientation_q , \"opencv\" , self . _camera_convention ) orientation = self . _quat_to_orientation_repr ( orientation_q ) scale = self . _get_scale ( extents ) # normalize pointcloud & position if self . _normalize_pointcloud : pointcloud , centroid = pointset_utils . normalize_points ( pointcloud ) position = position - centroid category_str = raw_sample [ \"category_str\" ] sample = { \"color\" : color , \"depth\" : depth , \"pointset\" : pointcloud , \"mask\" : instance_mask , \"position\" : position , \"orientation\" : orientation , \"quaternion\" : orientation_q , \"scale\" : scale , \"color_path\" : raw_sample [ \"color_path\" ], \"obj_path\" : raw_sample [ \"mesh_path\" ], \"category_id\" : self . category_str_to_id [ category_str ], \"category_str\" : category_str , } return sample","title":"__getitem__()"},{"location":"reference/initialization/datasets/redwood_dataset/#sdfest.initialization.datasets.redwood_dataset.AnnotatedRedwoodDataset.load_mesh","text":"load_mesh ( object_path : str ) -> o3d . geometry . TriangleMesh Load an object mesh and adjust its object frame convention. Source code in sdfest/initialization/datasets/redwood_dataset.py 390 391 392 393 394 395 396 397 398 399 400 401 402 403 def load_mesh ( self , object_path : str ) -> o3d . geometry . TriangleMesh : \"\"\"Load an object mesh and adjust its object frame convention.\"\"\" mesh = o3d . io . read_triangle_mesh ( object_path ) if self . _remap_y_axis is None and self . _remap_x_axis is None : return mesh elif self . _remap_y_axis is None or self . _remap_x_axis is None : raise ValueError ( \"Either both or none of remap_{y,x}_axis have to be None.\" ) rotation_o2n = self . _get_o2n_object_rotation_matrix () mesh . rotate ( rotation_o2n , center = np . array ([ 0.0 , 0.0 , 0.0 ])[:, None ], ) return mesh","title":"load_mesh()"},{"location":"reference/initialization/datasets/redwood_dataset/#sdfest.initialization.datasets.redwood_dataset.ObjectError","text":"Bases: Exception Error if something with the mesh is wrong. Source code in sdfest/initialization/datasets/redwood_dataset.py 406 407 408 409 class ObjectError ( Exception ): \"\"\"Error if something with the mesh is wrong.\"\"\" pass","title":"ObjectError"},{"location":"reference/initialization/scripts/train/","text":"sdfest.initialization.scripts.train Script to train initialization model. Trainer Trainer for single shot pose and shape estimation network. Source code in sdfest/initialization/scripts/train.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 class Trainer : \"\"\"Trainer for single shot pose and shape estimation network.\"\"\" def __init__ ( self , config : dict ) -> None : \"\"\"Construct trainer. Args: config: The configuration for model and training. \"\"\" self . _read_config ( config ) def _read_config ( self , config : dict ) -> None : self . _config = config self . _validation_iteration = config [ \"validation_iteration\" ] self . _visualization_iteration = config [ \"visualization_iteration\" ] self . _checkpoint_iteration = config [ \"checkpoint_iteration\" ] self . _iterations = config [ \"iterations\" ] self . _init_weights_path = ( config [ \"init_weights\" ] if \"init_weights\" in config else None ) # propagate orientation representation and category to datasets datasets = list ( self . _config [ \"datasets\" ] . values ()) + list ( self . _config [ \"validation_datasets\" ] . values () ) for dataset in datasets : dataset [ \"config_dict\" ][ \"orientation_repr\" ] = config [ \"orientation_repr\" ] if \"orientation_grid_resolution\" in config : dataset [ \"config_dict\" ][ \"orientation_grid_resolution\" ] = config [ \"orientation_grid_resolution\" ] if \"category_str\" in config : dataset [ \"config_dict\" ][ \"category_str\" ] = config [ \"category_str\" ] # propagate orientation representation to init head self . _config [ \"head\" ][ \"orientation_repr\" ] = config [ \"orientation_repr\" ] if \"orientation_grid_resolution\" in config : self . _config [ \"head\" ][ \"orientation_grid_resolution\" ] = config [ \"orientation_grid_resolution\" ] def run ( self ) -> None : \"\"\"Train the model.\"\"\" wandb . init ( project = \"sdfest.initialization\" , config = self . _config ) self . _device = self . get_device () # init dataset and dataloader self . vae = self . create_sdfvae () # init model to train self . _sdf_pose_net = SDFPoseNet ( backbone = MODULE_DICT [ self . _config [ \"backbone_type\" ]]( ** self . _config [ \"backbone\" ] ), head = MODULE_DICT [ self . _config [ \"head_type\" ]]( shape_dimension = self . _config [ \"vae\" ][ \"latent_size\" ], ** self . _config [ \"head\" ], ), ) . to ( self . _device ) self . _sdf_pose_net . train () # deterministic samples (needs to be done after model initialization, as it # can have varying number of parameters) torch . manual_seed ( 0 ) random . seed ( torch . initial_seed ()) # to get deterministic examples # print network summary torchinfo . summary ( self . _sdf_pose_net , ( 1 , 500 , 3 ), device = self . _device ) # init optimizer self . _optimizer = torch . optim . Adam ( self . _sdf_pose_net . parameters (), lr = self . _config [ \"learning_rate\" ] ) # load weights if provided if self . _init_weights_path is not None : state_dict = torch . load ( self . _init_weights_path , map_location = self . _device ) self . _sdf_pose_net . load_state_dict ( state_dict ) self . _current_iteration = 0 self . _run_name = ( f \"sdfest.initialization_ { datetime . now () . strftime ( '%Y-%m- %d _%H-%M-%S- %f ' ) } \" ) wandb . config . run_name = ( self . _run_name # to allow association of pt files with wandb runs ) self . _model_base_path = os . path . join ( os . getcwd (), \"models\" , self . _run_name ) self . _multi_data_loader = self . _create_multi_data_loader () self . _validation_data_loader_dict = self . _create_validation_data_loader_dict () # backup config to model directory os . makedirs ( self . _model_base_path , exist_ok = True ) config_path = os . path . join ( self . _model_base_path , \"config.yaml\" ) yoco . save_config_to_file ( config_path , self . _config ) self . _start_time = time . time () for samples in self . _multi_data_loader : self . _current_iteration += 1 print ( f \"Current iteration: { self . _current_iteration } \\033 [K\" , end = \" \\r \" ) self . _update_progress () samples = utils . dict_to ( samples , self . _device ) latent_shape , position , scale , orientation = self . _sdf_pose_net ( samples [ \"pointset\" ] ) predictions = { \"latent_shape\" : latent_shape , \"position\" : position , \"scale\" : scale , \"orientation\" : orientation , } loss = self . _compute_loss ( samples , predictions ) self . _optimizer . zero_grad () loss . backward () self . _optimizer . step () with torch . no_grad (): # samples_dict = defaultdict(lambda: dict()) # for k,vs in samples.items(): # for i, v in enumerate(vs): # samples_dict[i][k] = v # for sample in samples_dict.values(): # utils.visualize_sample(sample, None) self . _compute_metrics ( samples , predictions ) if self . _current_iteration % self . _visualization_iteration == 0 : self . _generate_visualizations () if self . _current_iteration % self . _validation_iteration == 0 : self . _compute_validation_metrics () if self . _current_iteration % self . _checkpoint_iteration == 0 : self . _save_checkpoint () if self . _current_iteration >= self . _iterations : break now = time . time () print ( f \"Training finished after { now - self . _start_time } seconds.\" ) # save the final model torch . save ( self . _sdf_pose_net . state_dict (), os . path . join ( wandb . run . dir , f \" { wandb . run . name } .pt\" ), ) config_path = os . path . join ( wandb . run . dir , f \" { wandb . run . name } .yaml\" ) self . _config [ \"model\" ] = os . path . join ( \".\" , f \" { wandb . run . name } .pt\" ) yoco . save_config_to_file ( config_path , self . _config ) def get_device ( self ) -> torch . device : \"\"\"Create device based on config.\"\"\" if \"device\" not in self . _config or self . _config [ \"device\" ] is None : return torch . device ( \"cuda\" if torch . cuda . is_available () else \"cpu\" ) return torch . device ( self . _config [ \"device\" ]) def create_sdfvae ( self ) -> SDFVAE : \"\"\"Create SDFVAE based on config. Returns: The SDFVAE on the specified device, with weights from specified model. \"\"\" model_url = self . _config [ \"vae\" ] . get ( \"model_url\" ) device = self . get_device () vae = SDFVAE ( sdf_size = 64 , latent_size = self . _config [ \"vae\" ][ \"latent_size\" ], encoder_dict = self . _config [ \"vae\" ][ \"encoder\" ], decoder_dict = self . _config [ \"vae\" ][ \"decoder\" ], device = device , ) . to ( device ) load_model_weights ( self . _config [ \"vae\" ][ \"model\" ], vae , device , model_url ) vae . eval () return vae def _compute_loss ( self , samples : dict , predictions : dict , ) -> torch . Tensor : \"\"\"Compute total loss. Args: samples: Samples dictionary containing a subset of the following keys: \"latent_shape\": Shape (N,D). \"position\": Shape (N,3). \"scale\": Shape (N,). \"orientation\": Shape (N,4) for quaternion representation. Shape (N,) for discretized representation. predictions: Dictionary containing the following keys: \"latent_shape\": Shape (N,D). \"position\": Shape (N,3). \"scale\": Shape (N,). \"orientation\": Shape (N,4) for quaternion representation. Shape (N,R) for discretized representation. Returns: The combined loss. Scalar. \"\"\" log_dict = {} loss = 0 if \"latent_shape\" in samples : loss_latent_l2 = torch . nn . functional . mse_loss ( predictions [ \"latent_shape\" ], samples [ \"latent_shape\" ] ) log_dict [ \"loss latent\" ] = loss_latent_l2 . item () loss = loss + self . _config [ \"latent_weight\" ] * loss_latent_l2 if \"position\" in samples : loss_position_l2 = torch . nn . functional . mse_loss ( predictions [ \"position\" ], samples [ \"position\" ] ) log_dict [ \"loss position\" ] = loss_position_l2 . item () loss = loss + self . _config [ \"position_weight\" ] * loss_position_l2 if \"scale\" in samples : loss_scale_l2 = torch . nn . functional . mse_loss ( predictions [ \"scale\" ], samples [ \"scale\" ] ) log_dict [ \"loss scale\" ] = loss_scale_l2 . item () loss = loss + self . _config [ \"scale_weight\" ] * loss_scale_l2 if \"orientation\" in samples : if self . _config [ \"head\" ][ \"orientation_repr\" ] == \"quaternion\" : loss_orientation = quaternion_utils . simple_quaternion_loss ( predictions [ \"orientation\" ], samples [ \"orientation\" ] ) elif self . _config [ \"head\" ][ \"orientation_repr\" ] == \"discretized\" : loss_orientation = torch . nn . functional . cross_entropy ( predictions [ \"orientation\" ], samples [ \"orientation\" ] ) else : raise NotImplementedError ( \"Orientation repr \" f \" { self . _config [ 'head' ][ 'orientation_repr' ] } \" \" not supported.\" ) log_dict [ \"loss orientation\" ] = loss_orientation . item () loss = loss + self . _config [ \"orientation_weight\" ] * loss_orientation log_dict [ \"total loss\" ] = loss . item () wandb . log ( log_dict , step = self . _current_iteration , ) return loss def _create_multi_data_loader ( self ) -> dataset_utils . MultiDataLoader : data_loaders = [] probabilities = [] for dataset_dict in self . _config [ \"datasets\" ] . values (): if dataset_dict [ \"probability\" ] == 0.0 : continue dataset = self . _create_dataset ( dataset_dict [ \"type\" ], dataset_dict [ \"config_dict\" ] ) num_workers = 8 if dataset_dict [ \"type\" ] != \"SDFVAEViewDataset\" else 0 shuffle = ( False if isinstance ( dataset , torch . utils . data . IterableDataset ) else True ) probabilities . append ( dataset_dict [ \"probability\" ]) data_loader = torch . utils . data . DataLoader ( dataset = dataset , batch_size = self . _config [ \"batch_size\" ], collate_fn = dataset_utils . collate_samples , drop_last = True , shuffle = shuffle , num_workers = num_workers , ) data_loaders . append ( data_loader ) return dataset_utils . MultiDataLoader ( data_loaders , probabilities ) def _create_validation_data_loader_dict ( self ) -> dict : data_loader_dict = {} for dataset_name , dataset_dict in self . _config [ \"validation_datasets\" ] . items (): dataset = self . _create_dataset ( dataset_dict [ \"type\" ], dataset_dict [ \"config_dict\" ] ) data_loader = torch . utils . data . DataLoader ( dataset = dataset , batch_size = self . _config [ \"batch_size\" ], collate_fn = dataset_utils . collate_samples , num_workers = 12 , ) data_loader_dict [ dataset_name ] = data_loader return data_loader_dict def _create_dataset ( self , type_str : str , config_dict : dict ) -> torch . utils . data . Dataset : dataset_type = utils . str_to_object ( type_str ) if dataset_type == SDFVAEViewDataset : dataset = dataset_type ( config = config_dict , vae = self . vae , ) elif dataset_type is not None : dataset = dataset_type ( config = config_dict ) else : raise NotImplementedError ( f \"Dataset type { type_str } not supported.\" ) return dataset def _mean_geodesic_distance ( self , samples : dict , predictions : dict ) -> torch . Tensor : target_quaternions = samples [ \"quaternion\" ] if self . _config [ \"head\" ][ \"orientation_repr\" ] == \"quaternion\" : predicted_quaternions = predictions [ \"orientation\" ] elif self . _config [ \"head\" ][ \"orientation_repr\" ] == \"discretized\" : predicted_quaternions = torch . empty_like ( target_quaternions ) for i , v in enumerate ( predictions [ \"orientation\" ]): index = v . argmax () . item () quat = self . _sdf_pose_net . _head . _grid . index_to_quat ( index ) predicted_quaternions [ i , :] = torch . tensor ( quat ) else : raise NotImplementedError ( \"Orientation representation \" f \" { self . _config [ 'head' ][ 'orientation_repr' ] } \" \" is not supported\" ) geodesic_distances = quaternion_utils . geodesic_distance ( target_quaternions , predicted_quaternions ) return torch . mean ( geodesic_distances ) def _compute_metrics ( self , samples : dict , predictions : dict ) -> None : # compute metrics / i.e., loss and representation independent metrics # extract quaternion from orientation representation geodesic_distance = self . _mean_geodesic_distance ( samples , predictions ) wandb . log ( { \"metric geodesic distance\" : geodesic_distance . item (), }, step = self . _current_iteration , ) def _generate_visualizations ( self ) -> None : # generate visualizations if self . _current_iteration % self . _visualization_iteration == 0 : # generate unseen input and target samples = next ( iter ( self . _multi_data_loader )) samples = utils . dict_to ( samples , self . _device ) predictions = self . _sdf_pose_net ( samples [ \"pointset\" ]) input_pointcloud = samples [ \"pointset\" ][ 0 ] . detach () . cpu () . numpy () input_pointcloud = np . hstack ( ( input_pointcloud , np . full (( input_pointcloud . shape [ 0 ], 1 ), 0 ), ) ) output_sdfs = self . vae . decode ( predictions [ 0 ]) output_sdf = output_sdfs [ 0 ][ 0 ] . detach () . cpu () . numpy () output_position = predictions [ 1 ][ 0 ] . detach () . cpu () . numpy () output_scale = predictions [ 2 ][ 0 ] . detach () . cpu () . numpy () if self . _config [ \"head\" ][ \"orientation_repr\" ] == \"quaternion\" : output_quaternion = predictions [ 3 ][ 0 ] . detach () . cpu () . numpy () elif self . _config [ \"head\" ][ \"orientation_repr\" ] == \"discretized\" : index = predictions [ 3 ][ 0 ] . argmax () . item () output_quaternion = self . _sdf_pose_net . _head . _grid . index_to_quat ( index ) else : raise NotImplementedError ( \"Orientation representation \" f \" { self . _config [ 'head' ][ 'orientation_repr' ] } \" \" is not supported\" ) output_pointcloud = sdf_utils . sdf_to_pointcloud ( output_sdf , output_position , output_quaternion , output_scale ) output_pointcloud = np . hstack ( ( output_pointcloud , np . full (( output_pointcloud . shape [ 0 ], 1 ), 1 ), ) ) pointcloud = np . vstack (( input_pointcloud , output_pointcloud )) wandb . log ( { \"point_cloud\" : wandb . Object3D ( pointcloud )}, step = self . _current_iteration , ) output_pointcloud = sdf_utils . sdf_to_pointcloud ( output_sdf , samples [ \"position\" ][ 0 ] . detach () . cpu () . numpy (), samples [ \"quaternion\" ][ 0 ] . detach () . cpu () . numpy (), samples [ \"scale\" ][ 0 ] . detach () . cpu () . numpy (), ) output_pointcloud = np . hstack ( ( output_pointcloud , np . full (( output_pointcloud . shape [ 0 ], 1 ), 1 ), ) ) pointcloud = np . vstack (( input_pointcloud , output_pointcloud )) wandb . log ( { \"point_cloud gt pose\" : wandb . Object3D ( pointcloud )}, step = self . _current_iteration , ) def _compute_validation_metrics ( self ) -> None : self . _sdf_pose_net . eval () for name , data_loader in self . _validation_data_loader_dict . items (): metrics_dict = defaultdict ( lambda : 0 ) sample_count = 0 for samples in tqdm ( data_loader , desc = \"Validation\" ): batch_size = samples [ \"position\" ] . shape [ 0 ] samples = utils . dict_to ( samples , self . _device ) latent_shape , position , scale , orientation = self . _sdf_pose_net ( samples [ \"pointset\" ] ) predictions = { \"latent_shape\" : latent_shape , \"position\" : position , \"scale\" : scale , \"orientation\" : orientation , } euclidean_distance = torch . linalg . norm ( predictions [ \"position\" ] - samples [ \"position\" ], dim = 1 ) metrics_dict [ f \" { name } validation mean position error / m\" ] += torch . sum ( euclidean_distance ) . item () metrics_dict [ f \" { name } validation mean scale error / m\" ] += torch . sum ( torch . abs ( predictions [ \"scale\" ] - samples [ \"scale\" ]) ) . item () metrics_dict [ f \" { name } validation mean geodesic_distance / rad\" ] += ( self . _mean_geodesic_distance ( samples , predictions ) . item () * batch_size ) sample_count += batch_size if self . _config [ \"head\" ][ \"orientation_repr\" ] == \"discretized\" : metrics_dict [ f \" { name } validation orientation mean NLL\" ] += torch . nn . functional . cross_entropy ( predictions [ \"orientation\" ], samples [ \"orientation\" ], reduction = \"sum\" , ) for metric_name in metrics_dict : metrics_dict [ metric_name ] /= sample_count wandb . log ( metrics_dict , step = self . _current_iteration ) self . _sdf_pose_net . train () def _save_checkpoint ( self ) -> None : checkpoint_path = os . path . join ( self . _model_base_path , f \" { self . _current_iteration } .pt\" ) torch . save ( self . _sdf_pose_net . state_dict (), checkpoint_path , ) def _update_progress ( self ) -> None : current_time = time . time () duration = current_time - self . _start_time iterations_per_sec = self . _current_iteration / duration if self . _current_iteration > 10 : remaining_iterations = self . _iterations - self . _current_iteration remaining_secs = remaining_iterations / iterations_per_sec remaining_time_str = str ( timedelta ( seconds = round ( remaining_secs ))) else : remaining_time_str = \"N/A\" print ( f \"Current iteration: { self . _current_iteration : >10 } / { self . _iterations } \" f \" { self . _current_iteration / self . _iterations * 100 : >6.2f } %\" f \" Remaining time: { remaining_time_str } \" # remaining time \" \\033 [K\" , # clear until end of line end = \" \\r \" , # overwrite previous ) __init__ __init__ ( config : dict ) -> None Construct trainer. PARAMETER DESCRIPTION config The configuration for model and training. TYPE: dict Source code in sdfest/initialization/scripts/train.py 33 34 35 36 37 38 39 def __init__ ( self , config : dict ) -> None : \"\"\"Construct trainer. Args: config: The configuration for model and training. \"\"\" self . _read_config ( config ) run run () -> None Train the model. Source code in sdfest/initialization/scripts/train.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 def run ( self ) -> None : \"\"\"Train the model.\"\"\" wandb . init ( project = \"sdfest.initialization\" , config = self . _config ) self . _device = self . get_device () # init dataset and dataloader self . vae = self . create_sdfvae () # init model to train self . _sdf_pose_net = SDFPoseNet ( backbone = MODULE_DICT [ self . _config [ \"backbone_type\" ]]( ** self . _config [ \"backbone\" ] ), head = MODULE_DICT [ self . _config [ \"head_type\" ]]( shape_dimension = self . _config [ \"vae\" ][ \"latent_size\" ], ** self . _config [ \"head\" ], ), ) . to ( self . _device ) self . _sdf_pose_net . train () # deterministic samples (needs to be done after model initialization, as it # can have varying number of parameters) torch . manual_seed ( 0 ) random . seed ( torch . initial_seed ()) # to get deterministic examples # print network summary torchinfo . summary ( self . _sdf_pose_net , ( 1 , 500 , 3 ), device = self . _device ) # init optimizer self . _optimizer = torch . optim . Adam ( self . _sdf_pose_net . parameters (), lr = self . _config [ \"learning_rate\" ] ) # load weights if provided if self . _init_weights_path is not None : state_dict = torch . load ( self . _init_weights_path , map_location = self . _device ) self . _sdf_pose_net . load_state_dict ( state_dict ) self . _current_iteration = 0 self . _run_name = ( f \"sdfest.initialization_ { datetime . now () . strftime ( '%Y-%m- %d _%H-%M-%S- %f ' ) } \" ) wandb . config . run_name = ( self . _run_name # to allow association of pt files with wandb runs ) self . _model_base_path = os . path . join ( os . getcwd (), \"models\" , self . _run_name ) self . _multi_data_loader = self . _create_multi_data_loader () self . _validation_data_loader_dict = self . _create_validation_data_loader_dict () # backup config to model directory os . makedirs ( self . _model_base_path , exist_ok = True ) config_path = os . path . join ( self . _model_base_path , \"config.yaml\" ) yoco . save_config_to_file ( config_path , self . _config ) self . _start_time = time . time () for samples in self . _multi_data_loader : self . _current_iteration += 1 print ( f \"Current iteration: { self . _current_iteration } \\033 [K\" , end = \" \\r \" ) self . _update_progress () samples = utils . dict_to ( samples , self . _device ) latent_shape , position , scale , orientation = self . _sdf_pose_net ( samples [ \"pointset\" ] ) predictions = { \"latent_shape\" : latent_shape , \"position\" : position , \"scale\" : scale , \"orientation\" : orientation , } loss = self . _compute_loss ( samples , predictions ) self . _optimizer . zero_grad () loss . backward () self . _optimizer . step () with torch . no_grad (): # samples_dict = defaultdict(lambda: dict()) # for k,vs in samples.items(): # for i, v in enumerate(vs): # samples_dict[i][k] = v # for sample in samples_dict.values(): # utils.visualize_sample(sample, None) self . _compute_metrics ( samples , predictions ) if self . _current_iteration % self . _visualization_iteration == 0 : self . _generate_visualizations () if self . _current_iteration % self . _validation_iteration == 0 : self . _compute_validation_metrics () if self . _current_iteration % self . _checkpoint_iteration == 0 : self . _save_checkpoint () if self . _current_iteration >= self . _iterations : break now = time . time () print ( f \"Training finished after { now - self . _start_time } seconds.\" ) # save the final model torch . save ( self . _sdf_pose_net . state_dict (), os . path . join ( wandb . run . dir , f \" { wandb . run . name } .pt\" ), ) config_path = os . path . join ( wandb . run . dir , f \" { wandb . run . name } .yaml\" ) self . _config [ \"model\" ] = os . path . join ( \".\" , f \" { wandb . run . name } .pt\" ) yoco . save_config_to_file ( config_path , self . _config ) get_device get_device () -> torch . device Create device based on config. Source code in sdfest/initialization/scripts/train.py 186 187 188 189 190 def get_device ( self ) -> torch . device : \"\"\"Create device based on config.\"\"\" if \"device\" not in self . _config or self . _config [ \"device\" ] is None : return torch . device ( \"cuda\" if torch . cuda . is_available () else \"cpu\" ) return torch . device ( self . _config [ \"device\" ]) create_sdfvae create_sdfvae () -> SDFVAE Create SDFVAE based on config. RETURNS DESCRIPTION SDFVAE The SDFVAE on the specified device, with weights from specified model. Source code in sdfest/initialization/scripts/train.py 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 def create_sdfvae ( self ) -> SDFVAE : \"\"\"Create SDFVAE based on config. Returns: The SDFVAE on the specified device, with weights from specified model. \"\"\" model_url = self . _config [ \"vae\" ] . get ( \"model_url\" ) device = self . get_device () vae = SDFVAE ( sdf_size = 64 , latent_size = self . _config [ \"vae\" ][ \"latent_size\" ], encoder_dict = self . _config [ \"vae\" ][ \"encoder\" ], decoder_dict = self . _config [ \"vae\" ][ \"decoder\" ], device = device , ) . to ( device ) load_model_weights ( self . _config [ \"vae\" ][ \"model\" ], vae , device , model_url ) vae . eval () return vae main main () -> None Entry point of the program. Source code in sdfest/initialization/scripts/train.py 511 512 513 514 515 516 517 518 519 520 def main () -> None : \"\"\"Entry point of the program.\"\"\" # define the arguments parser = argparse . ArgumentParser ( description = \"Training script for init network.\" ) parser . add_argument ( \"--config\" , default = \"configs/default.yaml\" , nargs = \"+\" ) config = yoco . load_config_from_args ( parser , search_paths = [ \".\" , \"~/.sdfest/\" , sdfest . __path__ [ 0 ]] ) trainer = Trainer ( config ) trainer . run ()","title":"train"},{"location":"reference/initialization/scripts/train/#sdfest.initialization.scripts.train","text":"Script to train initialization model.","title":"train"},{"location":"reference/initialization/scripts/train/#sdfest.initialization.scripts.train.Trainer","text":"Trainer for single shot pose and shape estimation network. Source code in sdfest/initialization/scripts/train.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 class Trainer : \"\"\"Trainer for single shot pose and shape estimation network.\"\"\" def __init__ ( self , config : dict ) -> None : \"\"\"Construct trainer. Args: config: The configuration for model and training. \"\"\" self . _read_config ( config ) def _read_config ( self , config : dict ) -> None : self . _config = config self . _validation_iteration = config [ \"validation_iteration\" ] self . _visualization_iteration = config [ \"visualization_iteration\" ] self . _checkpoint_iteration = config [ \"checkpoint_iteration\" ] self . _iterations = config [ \"iterations\" ] self . _init_weights_path = ( config [ \"init_weights\" ] if \"init_weights\" in config else None ) # propagate orientation representation and category to datasets datasets = list ( self . _config [ \"datasets\" ] . values ()) + list ( self . _config [ \"validation_datasets\" ] . values () ) for dataset in datasets : dataset [ \"config_dict\" ][ \"orientation_repr\" ] = config [ \"orientation_repr\" ] if \"orientation_grid_resolution\" in config : dataset [ \"config_dict\" ][ \"orientation_grid_resolution\" ] = config [ \"orientation_grid_resolution\" ] if \"category_str\" in config : dataset [ \"config_dict\" ][ \"category_str\" ] = config [ \"category_str\" ] # propagate orientation representation to init head self . _config [ \"head\" ][ \"orientation_repr\" ] = config [ \"orientation_repr\" ] if \"orientation_grid_resolution\" in config : self . _config [ \"head\" ][ \"orientation_grid_resolution\" ] = config [ \"orientation_grid_resolution\" ] def run ( self ) -> None : \"\"\"Train the model.\"\"\" wandb . init ( project = \"sdfest.initialization\" , config = self . _config ) self . _device = self . get_device () # init dataset and dataloader self . vae = self . create_sdfvae () # init model to train self . _sdf_pose_net = SDFPoseNet ( backbone = MODULE_DICT [ self . _config [ \"backbone_type\" ]]( ** self . _config [ \"backbone\" ] ), head = MODULE_DICT [ self . _config [ \"head_type\" ]]( shape_dimension = self . _config [ \"vae\" ][ \"latent_size\" ], ** self . _config [ \"head\" ], ), ) . to ( self . _device ) self . _sdf_pose_net . train () # deterministic samples (needs to be done after model initialization, as it # can have varying number of parameters) torch . manual_seed ( 0 ) random . seed ( torch . initial_seed ()) # to get deterministic examples # print network summary torchinfo . summary ( self . _sdf_pose_net , ( 1 , 500 , 3 ), device = self . _device ) # init optimizer self . _optimizer = torch . optim . Adam ( self . _sdf_pose_net . parameters (), lr = self . _config [ \"learning_rate\" ] ) # load weights if provided if self . _init_weights_path is not None : state_dict = torch . load ( self . _init_weights_path , map_location = self . _device ) self . _sdf_pose_net . load_state_dict ( state_dict ) self . _current_iteration = 0 self . _run_name = ( f \"sdfest.initialization_ { datetime . now () . strftime ( '%Y-%m- %d _%H-%M-%S- %f ' ) } \" ) wandb . config . run_name = ( self . _run_name # to allow association of pt files with wandb runs ) self . _model_base_path = os . path . join ( os . getcwd (), \"models\" , self . _run_name ) self . _multi_data_loader = self . _create_multi_data_loader () self . _validation_data_loader_dict = self . _create_validation_data_loader_dict () # backup config to model directory os . makedirs ( self . _model_base_path , exist_ok = True ) config_path = os . path . join ( self . _model_base_path , \"config.yaml\" ) yoco . save_config_to_file ( config_path , self . _config ) self . _start_time = time . time () for samples in self . _multi_data_loader : self . _current_iteration += 1 print ( f \"Current iteration: { self . _current_iteration } \\033 [K\" , end = \" \\r \" ) self . _update_progress () samples = utils . dict_to ( samples , self . _device ) latent_shape , position , scale , orientation = self . _sdf_pose_net ( samples [ \"pointset\" ] ) predictions = { \"latent_shape\" : latent_shape , \"position\" : position , \"scale\" : scale , \"orientation\" : orientation , } loss = self . _compute_loss ( samples , predictions ) self . _optimizer . zero_grad () loss . backward () self . _optimizer . step () with torch . no_grad (): # samples_dict = defaultdict(lambda: dict()) # for k,vs in samples.items(): # for i, v in enumerate(vs): # samples_dict[i][k] = v # for sample in samples_dict.values(): # utils.visualize_sample(sample, None) self . _compute_metrics ( samples , predictions ) if self . _current_iteration % self . _visualization_iteration == 0 : self . _generate_visualizations () if self . _current_iteration % self . _validation_iteration == 0 : self . _compute_validation_metrics () if self . _current_iteration % self . _checkpoint_iteration == 0 : self . _save_checkpoint () if self . _current_iteration >= self . _iterations : break now = time . time () print ( f \"Training finished after { now - self . _start_time } seconds.\" ) # save the final model torch . save ( self . _sdf_pose_net . state_dict (), os . path . join ( wandb . run . dir , f \" { wandb . run . name } .pt\" ), ) config_path = os . path . join ( wandb . run . dir , f \" { wandb . run . name } .yaml\" ) self . _config [ \"model\" ] = os . path . join ( \".\" , f \" { wandb . run . name } .pt\" ) yoco . save_config_to_file ( config_path , self . _config ) def get_device ( self ) -> torch . device : \"\"\"Create device based on config.\"\"\" if \"device\" not in self . _config or self . _config [ \"device\" ] is None : return torch . device ( \"cuda\" if torch . cuda . is_available () else \"cpu\" ) return torch . device ( self . _config [ \"device\" ]) def create_sdfvae ( self ) -> SDFVAE : \"\"\"Create SDFVAE based on config. Returns: The SDFVAE on the specified device, with weights from specified model. \"\"\" model_url = self . _config [ \"vae\" ] . get ( \"model_url\" ) device = self . get_device () vae = SDFVAE ( sdf_size = 64 , latent_size = self . _config [ \"vae\" ][ \"latent_size\" ], encoder_dict = self . _config [ \"vae\" ][ \"encoder\" ], decoder_dict = self . _config [ \"vae\" ][ \"decoder\" ], device = device , ) . to ( device ) load_model_weights ( self . _config [ \"vae\" ][ \"model\" ], vae , device , model_url ) vae . eval () return vae def _compute_loss ( self , samples : dict , predictions : dict , ) -> torch . Tensor : \"\"\"Compute total loss. Args: samples: Samples dictionary containing a subset of the following keys: \"latent_shape\": Shape (N,D). \"position\": Shape (N,3). \"scale\": Shape (N,). \"orientation\": Shape (N,4) for quaternion representation. Shape (N,) for discretized representation. predictions: Dictionary containing the following keys: \"latent_shape\": Shape (N,D). \"position\": Shape (N,3). \"scale\": Shape (N,). \"orientation\": Shape (N,4) for quaternion representation. Shape (N,R) for discretized representation. Returns: The combined loss. Scalar. \"\"\" log_dict = {} loss = 0 if \"latent_shape\" in samples : loss_latent_l2 = torch . nn . functional . mse_loss ( predictions [ \"latent_shape\" ], samples [ \"latent_shape\" ] ) log_dict [ \"loss latent\" ] = loss_latent_l2 . item () loss = loss + self . _config [ \"latent_weight\" ] * loss_latent_l2 if \"position\" in samples : loss_position_l2 = torch . nn . functional . mse_loss ( predictions [ \"position\" ], samples [ \"position\" ] ) log_dict [ \"loss position\" ] = loss_position_l2 . item () loss = loss + self . _config [ \"position_weight\" ] * loss_position_l2 if \"scale\" in samples : loss_scale_l2 = torch . nn . functional . mse_loss ( predictions [ \"scale\" ], samples [ \"scale\" ] ) log_dict [ \"loss scale\" ] = loss_scale_l2 . item () loss = loss + self . _config [ \"scale_weight\" ] * loss_scale_l2 if \"orientation\" in samples : if self . _config [ \"head\" ][ \"orientation_repr\" ] == \"quaternion\" : loss_orientation = quaternion_utils . simple_quaternion_loss ( predictions [ \"orientation\" ], samples [ \"orientation\" ] ) elif self . _config [ \"head\" ][ \"orientation_repr\" ] == \"discretized\" : loss_orientation = torch . nn . functional . cross_entropy ( predictions [ \"orientation\" ], samples [ \"orientation\" ] ) else : raise NotImplementedError ( \"Orientation repr \" f \" { self . _config [ 'head' ][ 'orientation_repr' ] } \" \" not supported.\" ) log_dict [ \"loss orientation\" ] = loss_orientation . item () loss = loss + self . _config [ \"orientation_weight\" ] * loss_orientation log_dict [ \"total loss\" ] = loss . item () wandb . log ( log_dict , step = self . _current_iteration , ) return loss def _create_multi_data_loader ( self ) -> dataset_utils . MultiDataLoader : data_loaders = [] probabilities = [] for dataset_dict in self . _config [ \"datasets\" ] . values (): if dataset_dict [ \"probability\" ] == 0.0 : continue dataset = self . _create_dataset ( dataset_dict [ \"type\" ], dataset_dict [ \"config_dict\" ] ) num_workers = 8 if dataset_dict [ \"type\" ] != \"SDFVAEViewDataset\" else 0 shuffle = ( False if isinstance ( dataset , torch . utils . data . IterableDataset ) else True ) probabilities . append ( dataset_dict [ \"probability\" ]) data_loader = torch . utils . data . DataLoader ( dataset = dataset , batch_size = self . _config [ \"batch_size\" ], collate_fn = dataset_utils . collate_samples , drop_last = True , shuffle = shuffle , num_workers = num_workers , ) data_loaders . append ( data_loader ) return dataset_utils . MultiDataLoader ( data_loaders , probabilities ) def _create_validation_data_loader_dict ( self ) -> dict : data_loader_dict = {} for dataset_name , dataset_dict in self . _config [ \"validation_datasets\" ] . items (): dataset = self . _create_dataset ( dataset_dict [ \"type\" ], dataset_dict [ \"config_dict\" ] ) data_loader = torch . utils . data . DataLoader ( dataset = dataset , batch_size = self . _config [ \"batch_size\" ], collate_fn = dataset_utils . collate_samples , num_workers = 12 , ) data_loader_dict [ dataset_name ] = data_loader return data_loader_dict def _create_dataset ( self , type_str : str , config_dict : dict ) -> torch . utils . data . Dataset : dataset_type = utils . str_to_object ( type_str ) if dataset_type == SDFVAEViewDataset : dataset = dataset_type ( config = config_dict , vae = self . vae , ) elif dataset_type is not None : dataset = dataset_type ( config = config_dict ) else : raise NotImplementedError ( f \"Dataset type { type_str } not supported.\" ) return dataset def _mean_geodesic_distance ( self , samples : dict , predictions : dict ) -> torch . Tensor : target_quaternions = samples [ \"quaternion\" ] if self . _config [ \"head\" ][ \"orientation_repr\" ] == \"quaternion\" : predicted_quaternions = predictions [ \"orientation\" ] elif self . _config [ \"head\" ][ \"orientation_repr\" ] == \"discretized\" : predicted_quaternions = torch . empty_like ( target_quaternions ) for i , v in enumerate ( predictions [ \"orientation\" ]): index = v . argmax () . item () quat = self . _sdf_pose_net . _head . _grid . index_to_quat ( index ) predicted_quaternions [ i , :] = torch . tensor ( quat ) else : raise NotImplementedError ( \"Orientation representation \" f \" { self . _config [ 'head' ][ 'orientation_repr' ] } \" \" is not supported\" ) geodesic_distances = quaternion_utils . geodesic_distance ( target_quaternions , predicted_quaternions ) return torch . mean ( geodesic_distances ) def _compute_metrics ( self , samples : dict , predictions : dict ) -> None : # compute metrics / i.e., loss and representation independent metrics # extract quaternion from orientation representation geodesic_distance = self . _mean_geodesic_distance ( samples , predictions ) wandb . log ( { \"metric geodesic distance\" : geodesic_distance . item (), }, step = self . _current_iteration , ) def _generate_visualizations ( self ) -> None : # generate visualizations if self . _current_iteration % self . _visualization_iteration == 0 : # generate unseen input and target samples = next ( iter ( self . _multi_data_loader )) samples = utils . dict_to ( samples , self . _device ) predictions = self . _sdf_pose_net ( samples [ \"pointset\" ]) input_pointcloud = samples [ \"pointset\" ][ 0 ] . detach () . cpu () . numpy () input_pointcloud = np . hstack ( ( input_pointcloud , np . full (( input_pointcloud . shape [ 0 ], 1 ), 0 ), ) ) output_sdfs = self . vae . decode ( predictions [ 0 ]) output_sdf = output_sdfs [ 0 ][ 0 ] . detach () . cpu () . numpy () output_position = predictions [ 1 ][ 0 ] . detach () . cpu () . numpy () output_scale = predictions [ 2 ][ 0 ] . detach () . cpu () . numpy () if self . _config [ \"head\" ][ \"orientation_repr\" ] == \"quaternion\" : output_quaternion = predictions [ 3 ][ 0 ] . detach () . cpu () . numpy () elif self . _config [ \"head\" ][ \"orientation_repr\" ] == \"discretized\" : index = predictions [ 3 ][ 0 ] . argmax () . item () output_quaternion = self . _sdf_pose_net . _head . _grid . index_to_quat ( index ) else : raise NotImplementedError ( \"Orientation representation \" f \" { self . _config [ 'head' ][ 'orientation_repr' ] } \" \" is not supported\" ) output_pointcloud = sdf_utils . sdf_to_pointcloud ( output_sdf , output_position , output_quaternion , output_scale ) output_pointcloud = np . hstack ( ( output_pointcloud , np . full (( output_pointcloud . shape [ 0 ], 1 ), 1 ), ) ) pointcloud = np . vstack (( input_pointcloud , output_pointcloud )) wandb . log ( { \"point_cloud\" : wandb . Object3D ( pointcloud )}, step = self . _current_iteration , ) output_pointcloud = sdf_utils . sdf_to_pointcloud ( output_sdf , samples [ \"position\" ][ 0 ] . detach () . cpu () . numpy (), samples [ \"quaternion\" ][ 0 ] . detach () . cpu () . numpy (), samples [ \"scale\" ][ 0 ] . detach () . cpu () . numpy (), ) output_pointcloud = np . hstack ( ( output_pointcloud , np . full (( output_pointcloud . shape [ 0 ], 1 ), 1 ), ) ) pointcloud = np . vstack (( input_pointcloud , output_pointcloud )) wandb . log ( { \"point_cloud gt pose\" : wandb . Object3D ( pointcloud )}, step = self . _current_iteration , ) def _compute_validation_metrics ( self ) -> None : self . _sdf_pose_net . eval () for name , data_loader in self . _validation_data_loader_dict . items (): metrics_dict = defaultdict ( lambda : 0 ) sample_count = 0 for samples in tqdm ( data_loader , desc = \"Validation\" ): batch_size = samples [ \"position\" ] . shape [ 0 ] samples = utils . dict_to ( samples , self . _device ) latent_shape , position , scale , orientation = self . _sdf_pose_net ( samples [ \"pointset\" ] ) predictions = { \"latent_shape\" : latent_shape , \"position\" : position , \"scale\" : scale , \"orientation\" : orientation , } euclidean_distance = torch . linalg . norm ( predictions [ \"position\" ] - samples [ \"position\" ], dim = 1 ) metrics_dict [ f \" { name } validation mean position error / m\" ] += torch . sum ( euclidean_distance ) . item () metrics_dict [ f \" { name } validation mean scale error / m\" ] += torch . sum ( torch . abs ( predictions [ \"scale\" ] - samples [ \"scale\" ]) ) . item () metrics_dict [ f \" { name } validation mean geodesic_distance / rad\" ] += ( self . _mean_geodesic_distance ( samples , predictions ) . item () * batch_size ) sample_count += batch_size if self . _config [ \"head\" ][ \"orientation_repr\" ] == \"discretized\" : metrics_dict [ f \" { name } validation orientation mean NLL\" ] += torch . nn . functional . cross_entropy ( predictions [ \"orientation\" ], samples [ \"orientation\" ], reduction = \"sum\" , ) for metric_name in metrics_dict : metrics_dict [ metric_name ] /= sample_count wandb . log ( metrics_dict , step = self . _current_iteration ) self . _sdf_pose_net . train () def _save_checkpoint ( self ) -> None : checkpoint_path = os . path . join ( self . _model_base_path , f \" { self . _current_iteration } .pt\" ) torch . save ( self . _sdf_pose_net . state_dict (), checkpoint_path , ) def _update_progress ( self ) -> None : current_time = time . time () duration = current_time - self . _start_time iterations_per_sec = self . _current_iteration / duration if self . _current_iteration > 10 : remaining_iterations = self . _iterations - self . _current_iteration remaining_secs = remaining_iterations / iterations_per_sec remaining_time_str = str ( timedelta ( seconds = round ( remaining_secs ))) else : remaining_time_str = \"N/A\" print ( f \"Current iteration: { self . _current_iteration : >10 } / { self . _iterations } \" f \" { self . _current_iteration / self . _iterations * 100 : >6.2f } %\" f \" Remaining time: { remaining_time_str } \" # remaining time \" \\033 [K\" , # clear until end of line end = \" \\r \" , # overwrite previous )","title":"Trainer"},{"location":"reference/initialization/scripts/train/#sdfest.initialization.scripts.train.Trainer.__init__","text":"__init__ ( config : dict ) -> None Construct trainer. PARAMETER DESCRIPTION config The configuration for model and training. TYPE: dict Source code in sdfest/initialization/scripts/train.py 33 34 35 36 37 38 39 def __init__ ( self , config : dict ) -> None : \"\"\"Construct trainer. Args: config: The configuration for model and training. \"\"\" self . _read_config ( config )","title":"__init__()"},{"location":"reference/initialization/scripts/train/#sdfest.initialization.scripts.train.Trainer.run","text":"run () -> None Train the model. Source code in sdfest/initialization/scripts/train.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 def run ( self ) -> None : \"\"\"Train the model.\"\"\" wandb . init ( project = \"sdfest.initialization\" , config = self . _config ) self . _device = self . get_device () # init dataset and dataloader self . vae = self . create_sdfvae () # init model to train self . _sdf_pose_net = SDFPoseNet ( backbone = MODULE_DICT [ self . _config [ \"backbone_type\" ]]( ** self . _config [ \"backbone\" ] ), head = MODULE_DICT [ self . _config [ \"head_type\" ]]( shape_dimension = self . _config [ \"vae\" ][ \"latent_size\" ], ** self . _config [ \"head\" ], ), ) . to ( self . _device ) self . _sdf_pose_net . train () # deterministic samples (needs to be done after model initialization, as it # can have varying number of parameters) torch . manual_seed ( 0 ) random . seed ( torch . initial_seed ()) # to get deterministic examples # print network summary torchinfo . summary ( self . _sdf_pose_net , ( 1 , 500 , 3 ), device = self . _device ) # init optimizer self . _optimizer = torch . optim . Adam ( self . _sdf_pose_net . parameters (), lr = self . _config [ \"learning_rate\" ] ) # load weights if provided if self . _init_weights_path is not None : state_dict = torch . load ( self . _init_weights_path , map_location = self . _device ) self . _sdf_pose_net . load_state_dict ( state_dict ) self . _current_iteration = 0 self . _run_name = ( f \"sdfest.initialization_ { datetime . now () . strftime ( '%Y-%m- %d _%H-%M-%S- %f ' ) } \" ) wandb . config . run_name = ( self . _run_name # to allow association of pt files with wandb runs ) self . _model_base_path = os . path . join ( os . getcwd (), \"models\" , self . _run_name ) self . _multi_data_loader = self . _create_multi_data_loader () self . _validation_data_loader_dict = self . _create_validation_data_loader_dict () # backup config to model directory os . makedirs ( self . _model_base_path , exist_ok = True ) config_path = os . path . join ( self . _model_base_path , \"config.yaml\" ) yoco . save_config_to_file ( config_path , self . _config ) self . _start_time = time . time () for samples in self . _multi_data_loader : self . _current_iteration += 1 print ( f \"Current iteration: { self . _current_iteration } \\033 [K\" , end = \" \\r \" ) self . _update_progress () samples = utils . dict_to ( samples , self . _device ) latent_shape , position , scale , orientation = self . _sdf_pose_net ( samples [ \"pointset\" ] ) predictions = { \"latent_shape\" : latent_shape , \"position\" : position , \"scale\" : scale , \"orientation\" : orientation , } loss = self . _compute_loss ( samples , predictions ) self . _optimizer . zero_grad () loss . backward () self . _optimizer . step () with torch . no_grad (): # samples_dict = defaultdict(lambda: dict()) # for k,vs in samples.items(): # for i, v in enumerate(vs): # samples_dict[i][k] = v # for sample in samples_dict.values(): # utils.visualize_sample(sample, None) self . _compute_metrics ( samples , predictions ) if self . _current_iteration % self . _visualization_iteration == 0 : self . _generate_visualizations () if self . _current_iteration % self . _validation_iteration == 0 : self . _compute_validation_metrics () if self . _current_iteration % self . _checkpoint_iteration == 0 : self . _save_checkpoint () if self . _current_iteration >= self . _iterations : break now = time . time () print ( f \"Training finished after { now - self . _start_time } seconds.\" ) # save the final model torch . save ( self . _sdf_pose_net . state_dict (), os . path . join ( wandb . run . dir , f \" { wandb . run . name } .pt\" ), ) config_path = os . path . join ( wandb . run . dir , f \" { wandb . run . name } .yaml\" ) self . _config [ \"model\" ] = os . path . join ( \".\" , f \" { wandb . run . name } .pt\" ) yoco . save_config_to_file ( config_path , self . _config )","title":"run()"},{"location":"reference/initialization/scripts/train/#sdfest.initialization.scripts.train.Trainer.get_device","text":"get_device () -> torch . device Create device based on config. Source code in sdfest/initialization/scripts/train.py 186 187 188 189 190 def get_device ( self ) -> torch . device : \"\"\"Create device based on config.\"\"\" if \"device\" not in self . _config or self . _config [ \"device\" ] is None : return torch . device ( \"cuda\" if torch . cuda . is_available () else \"cpu\" ) return torch . device ( self . _config [ \"device\" ])","title":"get_device()"},{"location":"reference/initialization/scripts/train/#sdfest.initialization.scripts.train.Trainer.create_sdfvae","text":"create_sdfvae () -> SDFVAE Create SDFVAE based on config. RETURNS DESCRIPTION SDFVAE The SDFVAE on the specified device, with weights from specified model. Source code in sdfest/initialization/scripts/train.py 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 def create_sdfvae ( self ) -> SDFVAE : \"\"\"Create SDFVAE based on config. Returns: The SDFVAE on the specified device, with weights from specified model. \"\"\" model_url = self . _config [ \"vae\" ] . get ( \"model_url\" ) device = self . get_device () vae = SDFVAE ( sdf_size = 64 , latent_size = self . _config [ \"vae\" ][ \"latent_size\" ], encoder_dict = self . _config [ \"vae\" ][ \"encoder\" ], decoder_dict = self . _config [ \"vae\" ][ \"decoder\" ], device = device , ) . to ( device ) load_model_weights ( self . _config [ \"vae\" ][ \"model\" ], vae , device , model_url ) vae . eval () return vae","title":"create_sdfvae()"},{"location":"reference/initialization/scripts/train/#sdfest.initialization.scripts.train.main","text":"main () -> None Entry point of the program. Source code in sdfest/initialization/scripts/train.py 511 512 513 514 515 516 517 518 519 520 def main () -> None : \"\"\"Entry point of the program.\"\"\" # define the arguments parser = argparse . ArgumentParser ( description = \"Training script for init network.\" ) parser . add_argument ( \"--config\" , default = \"configs/default.yaml\" , nargs = \"+\" ) config = yoco . load_config_from_args ( parser , search_paths = [ \".\" , \"~/.sdfest/\" , sdfest . __path__ [ 0 ]] ) trainer = Trainer ( config ) trainer . run ()","title":"main()"},{"location":"reference/vae/sdf_dataset/","text":"sdfest.vae.sdf_dataset Module which provides SDFDataset class. SDFDataset Bases: torch . utils . data . Dataset Dataset of SDF volumes stored in .npy format. Expected dataset format {sdf_folder}/00000.npy {sdf_folder}/00001.npy ... Source code in sdfest/vae/sdf_dataset.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class SDFDataset ( torch . utils . data . Dataset ): \"\"\"Dataset of SDF volumes stored in .npy format. Expected dataset format: {sdf_folder}/00000.npy {sdf_folder}/00001.npy ... \"\"\" def __init__ ( self , sdf_folder : str ): \"\"\"Construct the dataset. Args: sdf_folder: The folder containing the npy files. \"\"\" self . path = sdf_folder self . size = len ([ f for f in os . listdir ( sdf_folder ) if f . endswith ( \".npy\" )]) def __len__ ( self ): \"\"\"Return the number of images in the dataset.\"\"\" return self . size def __getitem__ ( self , idx : int ) -> torch . Tensor : \"\"\"Return SDF volume at a specific index. Args: idx: The index of the sdf file to retrieve. Returns: The loaded SDF volume. \"\"\" sdf_path = os . path . join ( self . path , f \" { idx : 05 } .npy\" ) sdf_np = np . load ( sdf_path ) return torch . as_tensor ( sdf_np ) . unsqueeze ( 0 ) __init__ __init__ ( sdf_folder : str ) Construct the dataset. PARAMETER DESCRIPTION sdf_folder The folder containing the npy files. TYPE: str Source code in sdfest/vae/sdf_dataset.py 16 17 18 19 20 21 22 23 24 def __init__ ( self , sdf_folder : str ): \"\"\"Construct the dataset. Args: sdf_folder: The folder containing the npy files. \"\"\" self . path = sdf_folder self . size = len ([ f for f in os . listdir ( sdf_folder ) if f . endswith ( \".npy\" )]) __len__ __len__ () Return the number of images in the dataset. Source code in sdfest/vae/sdf_dataset.py 26 27 28 def __len__ ( self ): \"\"\"Return the number of images in the dataset.\"\"\" return self . size __getitem__ __getitem__ ( idx : int ) -> torch . Tensor Return SDF volume at a specific index. PARAMETER DESCRIPTION idx The index of the sdf file to retrieve. TYPE: int RETURNS DESCRIPTION torch . Tensor The loaded SDF volume. Source code in sdfest/vae/sdf_dataset.py 30 31 32 33 34 35 36 37 38 39 40 def __getitem__ ( self , idx : int ) -> torch . Tensor : \"\"\"Return SDF volume at a specific index. Args: idx: The index of the sdf file to retrieve. Returns: The loaded SDF volume. \"\"\" sdf_path = os . path . join ( self . path , f \" { idx : 05 } .npy\" ) sdf_np = np . load ( sdf_path ) return torch . as_tensor ( sdf_np ) . unsqueeze ( 0 )","title":"sdf_dataset"},{"location":"reference/vae/sdf_dataset/#sdfest.vae.sdf_dataset","text":"Module which provides SDFDataset class.","title":"sdf_dataset"},{"location":"reference/vae/sdf_dataset/#sdfest.vae.sdf_dataset.SDFDataset","text":"Bases: torch . utils . data . Dataset Dataset of SDF volumes stored in .npy format. Expected dataset format {sdf_folder}/00000.npy {sdf_folder}/00001.npy ... Source code in sdfest/vae/sdf_dataset.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class SDFDataset ( torch . utils . data . Dataset ): \"\"\"Dataset of SDF volumes stored in .npy format. Expected dataset format: {sdf_folder}/00000.npy {sdf_folder}/00001.npy ... \"\"\" def __init__ ( self , sdf_folder : str ): \"\"\"Construct the dataset. Args: sdf_folder: The folder containing the npy files. \"\"\" self . path = sdf_folder self . size = len ([ f for f in os . listdir ( sdf_folder ) if f . endswith ( \".npy\" )]) def __len__ ( self ): \"\"\"Return the number of images in the dataset.\"\"\" return self . size def __getitem__ ( self , idx : int ) -> torch . Tensor : \"\"\"Return SDF volume at a specific index. Args: idx: The index of the sdf file to retrieve. Returns: The loaded SDF volume. \"\"\" sdf_path = os . path . join ( self . path , f \" { idx : 05 } .npy\" ) sdf_np = np . load ( sdf_path ) return torch . as_tensor ( sdf_np ) . unsqueeze ( 0 )","title":"SDFDataset"},{"location":"reference/vae/sdf_dataset/#sdfest.vae.sdf_dataset.SDFDataset.__init__","text":"__init__ ( sdf_folder : str ) Construct the dataset. PARAMETER DESCRIPTION sdf_folder The folder containing the npy files. TYPE: str Source code in sdfest/vae/sdf_dataset.py 16 17 18 19 20 21 22 23 24 def __init__ ( self , sdf_folder : str ): \"\"\"Construct the dataset. Args: sdf_folder: The folder containing the npy files. \"\"\" self . path = sdf_folder self . size = len ([ f for f in os . listdir ( sdf_folder ) if f . endswith ( \".npy\" )])","title":"__init__()"},{"location":"reference/vae/sdf_dataset/#sdfest.vae.sdf_dataset.SDFDataset.__len__","text":"__len__ () Return the number of images in the dataset. Source code in sdfest/vae/sdf_dataset.py 26 27 28 def __len__ ( self ): \"\"\"Return the number of images in the dataset.\"\"\" return self . size","title":"__len__()"},{"location":"reference/vae/sdf_dataset/#sdfest.vae.sdf_dataset.SDFDataset.__getitem__","text":"__getitem__ ( idx : int ) -> torch . Tensor Return SDF volume at a specific index. PARAMETER DESCRIPTION idx The index of the sdf file to retrieve. TYPE: int RETURNS DESCRIPTION torch . Tensor The loaded SDF volume. Source code in sdfest/vae/sdf_dataset.py 30 31 32 33 34 35 36 37 38 39 40 def __getitem__ ( self , idx : int ) -> torch . Tensor : \"\"\"Return SDF volume at a specific index. Args: idx: The index of the sdf file to retrieve. Returns: The loaded SDF volume. \"\"\" sdf_path = os . path . join ( self . path , f \" { idx : 05 } .npy\" ) sdf_np = np . load ( sdf_path ) return torch . as_tensor ( sdf_np ) . unsqueeze ( 0 )","title":"__getitem__()"},{"location":"reference/vae/sdf_utils/","text":"sdfest.vae.sdf_utils This module provides utility functions for working with SDF volumes. mesh_to_sdf mesh_to_sdf ( mesh : Trimesh , cells_per_dim : int , padding : Optional [ int ] = 0 ) Convert mesh to discretized signed distance field. The mesh will be stretched, so that its longest extend fills out the unit cube leaving the specified padding empty. PARAMETER DESCRIPTION mesh The mesh to convert. TYPE: Trimesh cells_per_dim The number of cells along each dimension. TYPE: int padding Number of empty space cells. TYPE: Optional [ int ] DEFAULT: 0 RETURNS DESCRIPTION The discretized signed distance field. Source code in sdfest/vae/sdf_utils.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def mesh_to_sdf ( mesh : Trimesh , cells_per_dim : int , padding : Optional [ int ] = 0 ): \"\"\"Convert mesh to discretized signed distance field. The mesh will be stretched, so that its longest extend fills out the unit cube leaving the specified padding empty. Args: mesh: The mesh to convert. cells_per_dim: The number of cells along each dimension. padding: Number of empty space cells. Returns: The discretized signed distance field. \"\"\" surface_point_method = \"scan\" sign_method = \"depth\" scaled_mesh = mts . utils . scale_to_unit_cube ( mesh ) scaled_mesh . vertices *= ( cells_per_dim - 2 * padding ) / cells_per_dim surface_point_cloud = mts . get_surface_point_cloud ( scaled_mesh , surface_point_method , calculate_normals = sign_method == \"normal\" ) try : return surface_point_cloud . get_voxels ( cells_per_dim , check_result = True , use_depth_buffer = sign_method == \"depth\" ) except mts . BadMeshException : print ( \"Bad mesh detected. Skipping.\" ) return None mesh_from_sdf mesh_from_sdf ( sdf_volume : np . array , level : Optional [ float ] = 0 , complete_mesh : bool = False , ) -> Trimesh Compute mesh from sdf using marching cubes algorithm. PARAMETER DESCRIPTION sdf_volume the SDF volume to convert, shape (M, M, M) TYPE: np . array level the isosurface level to extract the mesh for TYPE: Optional [ float ] DEFAULT: 0 complete_mesh if True, the SDF will be padded with positive values prior to converting it to a mesh. This ensures a watertight mesh is created. TYPE: bool DEFAULT: False RETURNS DESCRIPTION Trimesh The resulting mesh. Source code in sdfest/vae/sdf_utils.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def mesh_from_sdf ( sdf_volume : np . array , level : Optional [ float ] = 0 , complete_mesh : bool = False ) -> Trimesh : \"\"\"Compute mesh from sdf using marching cubes algorithm. Args: sdf_volume: the SDF volume to convert, shape (M, M, M) level: the isosurface level to extract the mesh for complete_mesh: if True, the SDF will be padded with positive values prior to converting it to a mesh. This ensures a watertight mesh is created. Returns: The resulting mesh. \"\"\" try : if complete_mesh : sdf_volume = np . pad ( sdf_volume , pad_width = 1 , constant_values = 1.0 ) sdf_volume . shape vertices , faces , normals , _ = marching_cubes ( sdf_volume , spacing = 2 / np . array ( sdf_volume . shape ), level = level ) vertices -= 1 except ValueError : return None return Trimesh ( vertices , faces , vertex_normals = normals , visual = trimesh . visual . TextureVisuals ( material = SimpleMaterial ()), ) plot_mesh plot_mesh ( mesh : Trimesh , polar_angle = np . pi / 4 , azimuth = 0 , camera_distance = 2.5 , plot_object : Optional [ Axes ] = None , transform : Optional [ np . array ] = None , ) Render a mesh with camera pointing at its center. Note that in pyrender z-axis is up, x,y form the polar_angle=0 plane. PARAMETER DESCRIPTION mesh The mesh to render. TYPE: Trimesh polar_angle Polar angle of the camera. For 0 the camera will look down the z-axis. DEFAULT: np.pi / 4 azimuth Azimuth of the camera. For 0, polar_anlge=pi/2 the camera will look down the x axis. DEFAULT: 0 camera_distance Distance of camera to the origin. DEFAULT: 2.5 plot_object Axis to plot the image. Will use plt if not provided. TYPE: Optional [ Axes ] DEFAULT: None transform Transform of the object. Identity by default. TYPE: Optional [ np . array ] DEFAULT: None Source code in sdfest/vae/sdf_utils.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def plot_mesh ( mesh : Trimesh , polar_angle = np . pi / 4 , azimuth = 0 , camera_distance = 2.5 , plot_object : Optional [ Axes ] = None , transform : Optional [ np . array ] = None , ): \"\"\"Render a mesh with camera pointing at its center. Note that in pyrender z-axis is up, x,y form the polar_angle=0 plane. Args: mesh: The mesh to render. polar_angle: Polar angle of the camera. For 0 the camera will look down the z-axis. azimuth: Azimuth of the camera. For 0, polar_anlge=pi/2 the camera will look down the x axis. camera_distance: Distance of camera to the origin. plot_object: Axis to plot the image. Will use plt if not provided. transform: Transform of the object. Identity by default. \"\"\" if plot_object is None : plot_object = plt if transform is None : transform = np . eye ( 4 , 4 )[ None ] elif transform . ndim == 2 : transform = transform [ None ] pyrender_mesh = pyrender . Mesh . from_trimesh ( mesh , poses = transform , smooth = False ) scene = pyrender . Scene () scene . add ( pyrender_mesh ) camera = pyrender . PerspectiveCamera ( yfov = np . pi / 3.0 , aspectRatio = 1.0 ) # position camera on sphere centered and pointing at centroid camera_unit_vector = np . array ( [ np . sin ( polar_angle ) * np . cos ( azimuth ), np . sin ( polar_angle ) * np . sin ( azimuth ), np . cos ( polar_angle ), ] ) camera_position = camera_distance * camera_unit_vector camera_pose = np . array ( [ [ - np . sin ( azimuth ), - np . cos ( azimuth ) * np . cos ( polar_angle ), np . cos ( azimuth ) * np . sin ( polar_angle ), camera_position [ 0 ], ], [ np . cos ( azimuth ), - np . sin ( azimuth ) * np . cos ( polar_angle ), np . sin ( azimuth ) * np . sin ( polar_angle ), camera_position [ 1 ], ], [ 0 , np . sin ( polar_angle ), np . cos ( polar_angle ), camera_position [ 2 ]], [ 0.0 , 0.0 , 0.0 , 1.0 ], ] ) scene . add ( camera , pose = camera_pose ) light = pyrender . PointLight ( color = [ 1.0 , 1.0 , 1.0 ], intensity = 45.0 ) light_pose = np . array ( [ [ np . cos ( azimuth ) * np . cos ( polar_angle ), - np . sin ( azimuth ), np . cos ( azimuth ) * np . sin ( polar_angle ), camera_position [ 0 ], ], [ np . sin ( azimuth ) * np . cos ( polar_angle ), np . cos ( azimuth ), np . sin ( azimuth ) * np . sin ( polar_angle ), camera_position [ 1 ], ], [ - np . sin ( polar_angle ), 0 , np . cos ( polar_angle ), camera_position [ 2 ]], [ 0.0 , 0.0 , 0.0 , 1.0 ], ] ) # scene.add(light, pose=camera_pose) scene . add_node ( pyrender . Node ( light = light , matrix = light_pose )) flags = RenderFlags . RGBA | RenderFlags . ALL_SOLID r = pyrender . OffscreenRenderer ( 1000 , 1000 , flags ) color , _ = r . render ( scene ) plot_object . axis ( \"off\" ) plot_object . imshow ( color , interpolation = \"none\" ) visualize_sdf_batch_columns visualize_sdf_batch_columns ( sdfs : np . array , show : bool = False ) Visualize batch of sdfs, with one per column (mesh + cross-views). Source code in sdfest/vae/sdf_utils.py 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 def visualize_sdf_batch_columns ( sdfs : np . array , show : bool = False ): \"\"\"Visualize batch of sdfs, with one per column (mesh + cross-views).\"\"\" fig = plt . figure () num_sdfs = sdfs . shape [ 0 ] center = np . array ( sdfs . shape )[ 2 ] // 2 level = 1.0 / sdfs . shape [ - 1 ] for c in range ( num_sdfs ): min = np . min ( sdfs [ c ]) max = np . max ( sdfs [ c ]) plt . subplot ( 4 , num_sdfs , 0 * num_sdfs + c + 1 ) mesh = mesh_from_sdf ( sdfs [ c ], level = level ) if mesh is not None : plot_mesh ( mesh ) plt . subplot ( 4 , num_sdfs , 1 * num_sdfs + c + 1 ) plt . imshow ( sdfs [ c , center , :, :] . T , origin = \"lower\" , vmin = min , vmax = max ) plt . xlabel ( \"y\" ) plt . ylabel ( \"z\" ) plt . subplot ( 4 , num_sdfs , 2 * num_sdfs + c + 1 ) plt . imshow ( sdfs [ c , :, center , :] . T , origin = \"lower\" , vmin = min , vmax = max ) plt . xlabel ( \"x\" ) plt . ylabel ( \"z\" ) plt . subplot ( 4 , num_sdfs , 3 * num_sdfs + c + 1 ) plt . imshow ( sdfs [ c , :, :, center ] . T , origin = \"lower\" , vmin = min , vmax = max ) plt . xlabel ( \"x\" ) plt . ylabel ( \"y\" ) if show : plt . show () return fig","title":"sdf_utils"},{"location":"reference/vae/sdf_utils/#sdfest.vae.sdf_utils","text":"This module provides utility functions for working with SDF volumes.","title":"sdf_utils"},{"location":"reference/vae/sdf_utils/#sdfest.vae.sdf_utils.mesh_to_sdf","text":"mesh_to_sdf ( mesh : Trimesh , cells_per_dim : int , padding : Optional [ int ] = 0 ) Convert mesh to discretized signed distance field. The mesh will be stretched, so that its longest extend fills out the unit cube leaving the specified padding empty. PARAMETER DESCRIPTION mesh The mesh to convert. TYPE: Trimesh cells_per_dim The number of cells along each dimension. TYPE: int padding Number of empty space cells. TYPE: Optional [ int ] DEFAULT: 0 RETURNS DESCRIPTION The discretized signed distance field. Source code in sdfest/vae/sdf_utils.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def mesh_to_sdf ( mesh : Trimesh , cells_per_dim : int , padding : Optional [ int ] = 0 ): \"\"\"Convert mesh to discretized signed distance field. The mesh will be stretched, so that its longest extend fills out the unit cube leaving the specified padding empty. Args: mesh: The mesh to convert. cells_per_dim: The number of cells along each dimension. padding: Number of empty space cells. Returns: The discretized signed distance field. \"\"\" surface_point_method = \"scan\" sign_method = \"depth\" scaled_mesh = mts . utils . scale_to_unit_cube ( mesh ) scaled_mesh . vertices *= ( cells_per_dim - 2 * padding ) / cells_per_dim surface_point_cloud = mts . get_surface_point_cloud ( scaled_mesh , surface_point_method , calculate_normals = sign_method == \"normal\" ) try : return surface_point_cloud . get_voxels ( cells_per_dim , check_result = True , use_depth_buffer = sign_method == \"depth\" ) except mts . BadMeshException : print ( \"Bad mesh detected. Skipping.\" ) return None","title":"mesh_to_sdf()"},{"location":"reference/vae/sdf_utils/#sdfest.vae.sdf_utils.mesh_from_sdf","text":"mesh_from_sdf ( sdf_volume : np . array , level : Optional [ float ] = 0 , complete_mesh : bool = False , ) -> Trimesh Compute mesh from sdf using marching cubes algorithm. PARAMETER DESCRIPTION sdf_volume the SDF volume to convert, shape (M, M, M) TYPE: np . array level the isosurface level to extract the mesh for TYPE: Optional [ float ] DEFAULT: 0 complete_mesh if True, the SDF will be padded with positive values prior to converting it to a mesh. This ensures a watertight mesh is created. TYPE: bool DEFAULT: False RETURNS DESCRIPTION Trimesh The resulting mesh. Source code in sdfest/vae/sdf_utils.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def mesh_from_sdf ( sdf_volume : np . array , level : Optional [ float ] = 0 , complete_mesh : bool = False ) -> Trimesh : \"\"\"Compute mesh from sdf using marching cubes algorithm. Args: sdf_volume: the SDF volume to convert, shape (M, M, M) level: the isosurface level to extract the mesh for complete_mesh: if True, the SDF will be padded with positive values prior to converting it to a mesh. This ensures a watertight mesh is created. Returns: The resulting mesh. \"\"\" try : if complete_mesh : sdf_volume = np . pad ( sdf_volume , pad_width = 1 , constant_values = 1.0 ) sdf_volume . shape vertices , faces , normals , _ = marching_cubes ( sdf_volume , spacing = 2 / np . array ( sdf_volume . shape ), level = level ) vertices -= 1 except ValueError : return None return Trimesh ( vertices , faces , vertex_normals = normals , visual = trimesh . visual . TextureVisuals ( material = SimpleMaterial ()), )","title":"mesh_from_sdf()"},{"location":"reference/vae/sdf_utils/#sdfest.vae.sdf_utils.plot_mesh","text":"plot_mesh ( mesh : Trimesh , polar_angle = np . pi / 4 , azimuth = 0 , camera_distance = 2.5 , plot_object : Optional [ Axes ] = None , transform : Optional [ np . array ] = None , ) Render a mesh with camera pointing at its center. Note that in pyrender z-axis is up, x,y form the polar_angle=0 plane. PARAMETER DESCRIPTION mesh The mesh to render. TYPE: Trimesh polar_angle Polar angle of the camera. For 0 the camera will look down the z-axis. DEFAULT: np.pi / 4 azimuth Azimuth of the camera. For 0, polar_anlge=pi/2 the camera will look down the x axis. DEFAULT: 0 camera_distance Distance of camera to the origin. DEFAULT: 2.5 plot_object Axis to plot the image. Will use plt if not provided. TYPE: Optional [ Axes ] DEFAULT: None transform Transform of the object. Identity by default. TYPE: Optional [ np . array ] DEFAULT: None Source code in sdfest/vae/sdf_utils.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def plot_mesh ( mesh : Trimesh , polar_angle = np . pi / 4 , azimuth = 0 , camera_distance = 2.5 , plot_object : Optional [ Axes ] = None , transform : Optional [ np . array ] = None , ): \"\"\"Render a mesh with camera pointing at its center. Note that in pyrender z-axis is up, x,y form the polar_angle=0 plane. Args: mesh: The mesh to render. polar_angle: Polar angle of the camera. For 0 the camera will look down the z-axis. azimuth: Azimuth of the camera. For 0, polar_anlge=pi/2 the camera will look down the x axis. camera_distance: Distance of camera to the origin. plot_object: Axis to plot the image. Will use plt if not provided. transform: Transform of the object. Identity by default. \"\"\" if plot_object is None : plot_object = plt if transform is None : transform = np . eye ( 4 , 4 )[ None ] elif transform . ndim == 2 : transform = transform [ None ] pyrender_mesh = pyrender . Mesh . from_trimesh ( mesh , poses = transform , smooth = False ) scene = pyrender . Scene () scene . add ( pyrender_mesh ) camera = pyrender . PerspectiveCamera ( yfov = np . pi / 3.0 , aspectRatio = 1.0 ) # position camera on sphere centered and pointing at centroid camera_unit_vector = np . array ( [ np . sin ( polar_angle ) * np . cos ( azimuth ), np . sin ( polar_angle ) * np . sin ( azimuth ), np . cos ( polar_angle ), ] ) camera_position = camera_distance * camera_unit_vector camera_pose = np . array ( [ [ - np . sin ( azimuth ), - np . cos ( azimuth ) * np . cos ( polar_angle ), np . cos ( azimuth ) * np . sin ( polar_angle ), camera_position [ 0 ], ], [ np . cos ( azimuth ), - np . sin ( azimuth ) * np . cos ( polar_angle ), np . sin ( azimuth ) * np . sin ( polar_angle ), camera_position [ 1 ], ], [ 0 , np . sin ( polar_angle ), np . cos ( polar_angle ), camera_position [ 2 ]], [ 0.0 , 0.0 , 0.0 , 1.0 ], ] ) scene . add ( camera , pose = camera_pose ) light = pyrender . PointLight ( color = [ 1.0 , 1.0 , 1.0 ], intensity = 45.0 ) light_pose = np . array ( [ [ np . cos ( azimuth ) * np . cos ( polar_angle ), - np . sin ( azimuth ), np . cos ( azimuth ) * np . sin ( polar_angle ), camera_position [ 0 ], ], [ np . sin ( azimuth ) * np . cos ( polar_angle ), np . cos ( azimuth ), np . sin ( azimuth ) * np . sin ( polar_angle ), camera_position [ 1 ], ], [ - np . sin ( polar_angle ), 0 , np . cos ( polar_angle ), camera_position [ 2 ]], [ 0.0 , 0.0 , 0.0 , 1.0 ], ] ) # scene.add(light, pose=camera_pose) scene . add_node ( pyrender . Node ( light = light , matrix = light_pose )) flags = RenderFlags . RGBA | RenderFlags . ALL_SOLID r = pyrender . OffscreenRenderer ( 1000 , 1000 , flags ) color , _ = r . render ( scene ) plot_object . axis ( \"off\" ) plot_object . imshow ( color , interpolation = \"none\" )","title":"plot_mesh()"},{"location":"reference/vae/sdf_utils/#sdfest.vae.sdf_utils.visualize_sdf_batch_columns","text":"visualize_sdf_batch_columns ( sdfs : np . array , show : bool = False ) Visualize batch of sdfs, with one per column (mesh + cross-views). Source code in sdfest/vae/sdf_utils.py 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 def visualize_sdf_batch_columns ( sdfs : np . array , show : bool = False ): \"\"\"Visualize batch of sdfs, with one per column (mesh + cross-views).\"\"\" fig = plt . figure () num_sdfs = sdfs . shape [ 0 ] center = np . array ( sdfs . shape )[ 2 ] // 2 level = 1.0 / sdfs . shape [ - 1 ] for c in range ( num_sdfs ): min = np . min ( sdfs [ c ]) max = np . max ( sdfs [ c ]) plt . subplot ( 4 , num_sdfs , 0 * num_sdfs + c + 1 ) mesh = mesh_from_sdf ( sdfs [ c ], level = level ) if mesh is not None : plot_mesh ( mesh ) plt . subplot ( 4 , num_sdfs , 1 * num_sdfs + c + 1 ) plt . imshow ( sdfs [ c , center , :, :] . T , origin = \"lower\" , vmin = min , vmax = max ) plt . xlabel ( \"y\" ) plt . ylabel ( \"z\" ) plt . subplot ( 4 , num_sdfs , 2 * num_sdfs + c + 1 ) plt . imshow ( sdfs [ c , :, center , :] . T , origin = \"lower\" , vmin = min , vmax = max ) plt . xlabel ( \"x\" ) plt . ylabel ( \"z\" ) plt . subplot ( 4 , num_sdfs , 3 * num_sdfs + c + 1 ) plt . imshow ( sdfs [ c , :, :, center ] . T , origin = \"lower\" , vmin = min , vmax = max ) plt . xlabel ( \"x\" ) plt . ylabel ( \"y\" ) if show : plt . show () return fig","title":"visualize_sdf_batch_columns()"},{"location":"reference/vae/sdf_vae/","text":"sdfest.vae.sdf_vae This module provides various PyTorch modules for working with SDFs. SDFVAE Bases: nn . Module Variational Autoencoder for Signed Distance Fields. Source code in sdfest/vae/sdf_vae.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 class SDFVAE ( nn . Module ): \"\"\"Variational Autoencoder for Signed Distance Fields.\"\"\" def __init__ ( self , sdf_size : int , latent_size : int , encoder_dict : dict , decoder_dict : dict , device : torch . device , tsdf : Optional [ Union [ bool , float ]] = False , ): \"\"\"Initialize the SDFVAE module. Args: sdf_size: depth/width/length of the sdf latent_size: dimensions of latent representation encoder_dict: Arguments passed to encoder constructor. See SDFEncoder.__init__ for details. decoder_dict: Arguments passed to decoder constructor. See SDFDecoder.__init__ for details. device: The device this model will work on. This is used for tensors created inside this model for sampling. tsdf: Value to truncate the SDF at. False for untruncated SDF. For the input this is only done in prepare_input, not in the forward pass. The output is truncated in the forward pass. \"\"\" super () . __init__ () self . latent_size = latent_size self . _device = device self . encoder = SDFEncoder ( sdf_size , latent_size , tsdf = tsdf , ** encoder_dict ) self . decoder = SDFDecoder ( sdf_size , latent_size , tsdf = tsdf , ** decoder_dict ) self . sdf_size = sdf_size self . _tsdf = tsdf def forward ( self , x , enforce_tsdf = False ): z , means , log_var = self . encode ( x ) recon_x = self . decoder ( z , enforce_tsdf ) return recon_x , means , log_var , z def sample ( self , n = 1 ): z = torch . randn ([ n , self . latent_size ]) . to ( self . _device ) return z def encode ( self , x ): means , log_var = self . encoder ( x ) std = torch . exp ( 0.5 * log_var ) eps = torch . randn ([ x . shape [ 0 ], self . latent_size ]) . to ( self . _device ) z = eps * std + means return z , means , log_var def inference ( self , n = 1 , enforce_tsdf = False ): z = self . sample ( n ) recon_x = self . decoder ( z , enforce_tsdf ) return recon_x , z def decode ( self , z , enforce_tsdf = False ): \"\"\"Returns decoded SDF. Args: Batch of latent shapes. Shape (N, L). Returns: The decoded SDF. Shape (N, C, D, D, D). \"\"\" return self . decoder ( z , enforce_tsdf ) def prepare_input ( self , sdfs : torch . Tensor ) -> None : \"\"\"Convert batched SDFs to expected input format. This will transform inputs as defined by the decoder. See SDFEncoder.prepare_input for details. Will be done in place without gradients. Args: sdfs: Batched SDFs, expected shape (N,C,D,D,D). \"\"\" self . encoder . prepare_input ( sdfs ) __init__ __init__ ( sdf_size : int , latent_size : int , encoder_dict : dict , decoder_dict : dict , device : torch . device , tsdf : Optional [ Union [ bool , float ]] = False , ) Initialize the SDFVAE module. PARAMETER DESCRIPTION sdf_size depth/width/length of the sdf TYPE: int latent_size dimensions of latent representation TYPE: int encoder_dict Arguments passed to encoder constructor. See SDFEncoder. init for details. TYPE: dict decoder_dict Arguments passed to decoder constructor. See SDFDecoder. init for details. TYPE: dict device The device this model will work on. This is used for tensors created inside this model for sampling. TYPE: torch . device tsdf Value to truncate the SDF at. False for untruncated SDF. For the input this is only done in prepare_input, not in the forward pass. The output is truncated in the forward pass. TYPE: Optional [ Union [ bool , float ]] DEFAULT: False Source code in sdfest/vae/sdf_vae.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def __init__ ( self , sdf_size : int , latent_size : int , encoder_dict : dict , decoder_dict : dict , device : torch . device , tsdf : Optional [ Union [ bool , float ]] = False , ): \"\"\"Initialize the SDFVAE module. Args: sdf_size: depth/width/length of the sdf latent_size: dimensions of latent representation encoder_dict: Arguments passed to encoder constructor. See SDFEncoder.__init__ for details. decoder_dict: Arguments passed to decoder constructor. See SDFDecoder.__init__ for details. device: The device this model will work on. This is used for tensors created inside this model for sampling. tsdf: Value to truncate the SDF at. False for untruncated SDF. For the input this is only done in prepare_input, not in the forward pass. The output is truncated in the forward pass. \"\"\" super () . __init__ () self . latent_size = latent_size self . _device = device self . encoder = SDFEncoder ( sdf_size , latent_size , tsdf = tsdf , ** encoder_dict ) self . decoder = SDFDecoder ( sdf_size , latent_size , tsdf = tsdf , ** decoder_dict ) self . sdf_size = sdf_size self . _tsdf = tsdf decode decode ( z , enforce_tsdf = False ) Returns decoded SDF. RETURNS DESCRIPTION The decoded SDF. Shape (N, C, D, D, D). Source code in sdfest/vae/sdf_vae.py 79 80 81 82 83 84 85 86 87 def decode ( self , z , enforce_tsdf = False ): \"\"\"Returns decoded SDF. Args: Batch of latent shapes. Shape (N, L). Returns: The decoded SDF. Shape (N, C, D, D, D). \"\"\" return self . decoder ( z , enforce_tsdf ) prepare_input prepare_input ( sdfs : torch . Tensor ) -> None Convert batched SDFs to expected input format. This will transform inputs as defined by the decoder. See SDFEncoder.prepare_input for details. Will be done in place without gradients. PARAMETER DESCRIPTION sdfs Batched SDFs, expected shape (N,C,D,D,D). TYPE: torch . Tensor Source code in sdfest/vae/sdf_vae.py 89 90 91 92 93 94 95 96 97 98 99 100 def prepare_input ( self , sdfs : torch . Tensor ) -> None : \"\"\"Convert batched SDFs to expected input format. This will transform inputs as defined by the decoder. See SDFEncoder.prepare_input for details. Will be done in place without gradients. Args: sdfs: Batched SDFs, expected shape (N,C,D,D,D). \"\"\" self . encoder . prepare_input ( sdfs ) SDFEncoder Bases: nn . Module Source code in sdfest/vae/sdf_vae.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 class SDFEncoder ( nn . Module ): def __init__ ( self , volume_size , latent_size , layer_infos , tsdf = False ): \"\"\"Create SDFEncoder, i.e., define trainable layers. Args: volume_size: Input size D of the volume. (i.e., input tensor is expected to be Nx1xDxDxD) latent_size: Dimensionality of the latent representation. layers: Dictionaries defining the layers before the final output layers. Required fields: - fully qualified type (str) - args (dict): params passed to init of type These layers need to construct an 2D tensor (including batch dimension). \"\"\" super () . __init__ () in_channels = 1 # define layers layers = [] for layer_info in layer_infos : t = locate ( layer_info [ \"type\" ]) layers . append ( t ( ** layer_info [ \"args\" ])) self . _features = torch . nn . Sequential ( * layers ) with torch . no_grad (): output_size = self . _features ( torch . zeros ( 1 , in_channels , volume_size , volume_size , volume_size ) ) . shape self . linear_means = nn . Linear ( output_size [ 1 ], latent_size ) self . linear_log_var = nn . Linear ( output_size [ 1 ], latent_size ) self . _tsdf = tsdf def forward ( self , x ): \"\"\"Forward pass of the module. Args: Returns: \"\"\" out = self . _features ( x ) means = self . linear_means ( out ) log_vars = self . linear_log_var ( out ) return means , log_vars def prepare_input ( self , sdfs : torch . Tensor ) -> None : \"\"\"Convert batched SDFs to expected input format. This will truncate the SDFs based on tsdf argument passed to constructor. Will be done in place without gradients. Args: sdfs: Batched SDFs, expected shape (N,C,D,D,D). \"\"\" if self . _tsdf is not False : with torch . no_grad (): sdfs . clamp_ ( - self . _tsdf , self . _tsdf ) __init__ __init__ ( volume_size , latent_size , layer_infos , tsdf = False ) Create SDFEncoder, i.e., define trainable layers. PARAMETER DESCRIPTION volume_size Input size D of the volume. (i.e., input tensor is expected to be Nx1xDxDxD) latent_size Dimensionality of the latent representation. layers Dictionaries defining the layers before the final output layers. Required fields: - fully qualified type (str) - args (dict): params passed to init of type These layers need to construct an 2D tensor (including batch dimension). Source code in sdfest/vae/sdf_vae.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 def __init__ ( self , volume_size , latent_size , layer_infos , tsdf = False ): \"\"\"Create SDFEncoder, i.e., define trainable layers. Args: volume_size: Input size D of the volume. (i.e., input tensor is expected to be Nx1xDxDxD) latent_size: Dimensionality of the latent representation. layers: Dictionaries defining the layers before the final output layers. Required fields: - fully qualified type (str) - args (dict): params passed to init of type These layers need to construct an 2D tensor (including batch dimension). \"\"\" super () . __init__ () in_channels = 1 # define layers layers = [] for layer_info in layer_infos : t = locate ( layer_info [ \"type\" ]) layers . append ( t ( ** layer_info [ \"args\" ])) self . _features = torch . nn . Sequential ( * layers ) with torch . no_grad (): output_size = self . _features ( torch . zeros ( 1 , in_channels , volume_size , volume_size , volume_size ) ) . shape self . linear_means = nn . Linear ( output_size [ 1 ], latent_size ) self . linear_log_var = nn . Linear ( output_size [ 1 ], latent_size ) self . _tsdf = tsdf forward forward ( x ) Forward pass of the module. Source code in sdfest/vae/sdf_vae.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def forward ( self , x ): \"\"\"Forward pass of the module. Args: Returns: \"\"\" out = self . _features ( x ) means = self . linear_means ( out ) log_vars = self . linear_log_var ( out ) return means , log_vars prepare_input prepare_input ( sdfs : torch . Tensor ) -> None Convert batched SDFs to expected input format. This will truncate the SDFs based on tsdf argument passed to constructor. Will be done in place without gradients. PARAMETER DESCRIPTION sdfs Batched SDFs, expected shape (N,C,D,D,D). TYPE: torch . Tensor Source code in sdfest/vae/sdf_vae.py 155 156 157 158 159 160 161 162 163 164 165 166 167 def prepare_input ( self , sdfs : torch . Tensor ) -> None : \"\"\"Convert batched SDFs to expected input format. This will truncate the SDFs based on tsdf argument passed to constructor. Will be done in place without gradients. Args: sdfs: Batched SDFs, expected shape (N,C,D,D,D). \"\"\" if self . _tsdf is not False : with torch . no_grad (): sdfs . clamp_ ( - self . _tsdf , self . _tsdf ) SDFDecoder Bases: nn . Module Source code in sdfest/vae/sdf_vae.py 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 class SDFDecoder ( nn . Module ): def __init__ ( self , volume_size : int , latent_size : int , fc_layers : list , conv_layers : list , tsdf : Optional [ Union [ bool , float ]] = False , ): super () . __init__ () # sanity checks self . sanity_check ( volume_size , fc_layers , conv_layers ) # create layers and store params self . _volume_size = volume_size in_size = latent_size self . _fc_layers = torch . nn . ModuleList () for fc_layer in fc_layers : self . _fc_layers . append ( nn . Linear ( in_size , fc_layer [ \"out\" ])) in_size = fc_layer [ \"out\" ] self . _fc_info = fc_layers self . _conv_layers = torch . nn . ModuleList () for conv_layer in conv_layers : self . _conv_layers . append ( nn . Conv3d ( conv_layer [ \"in_channels\" ], conv_layer [ \"out_channels\" ], conv_layer [ \"kernel_size\" ], ) ) self . _conv_info = conv_layers self . _tsdf = tsdf def sanity_check ( self , volume_size , fc_dicts , conv_dicts ): assert fc_dicts [ - 1 ][ \"out\" ] == ( conv_dicts [ 0 ][ \"in_channels\" ] * conv_dicts [ 0 ][ \"in_size\" ] ** 3 ) for i , conv_dict in enumerate ( conv_dicts [: - 1 ]): assert conv_dict [ \"out_channels\" ] == conv_dicts [ i + 1 ][ \"in_channels\" ] assert conv_dicts [ - 1 ][ \"out_channels\" ] == 1 def forward ( self , z , enforce_tsdf = False ): \"\"\"Decode latent vectors to SDFs. Args: z: Batch of latent vectors. Expected shape of (N,latent_size). \"\"\" out = z for fc_layer in self . _fc_layers : out = nn . functional . relu ( fc_layer ( out )) out = out . view ( - 1 , self . _conv_info [ 0 ][ \"in_channels\" ], self . _conv_info [ 0 ][ \"in_size\" ], self . _conv_info [ 0 ][ \"in_size\" ], self . _conv_info [ 0 ][ \"in_size\" ], ) for info , layer in zip ( self . _conv_info , self . _conv_layers ): # interpolate to match next input if out . shape [ 2 ] != info [ \"in_size\" ]: out = torch . nn . functional . interpolate ( out , size = ( info [ \"in_size\" ], info [ \"in_size\" ], info [ \"in_size\" ]), mode = \"trilinear\" , align_corners = False , ) out = layer ( out ) if info [ \"relu\" ]: out = nn . functional . relu ( out ) if out . shape [ 2 ] != self . _volume_size : out = torch . nn . functional . interpolate ( out , size = ( self . _volume_size , self . _volume_size , self . _volume_size ), mode = \"trilinear\" , align_corners = False , ) if self . _tsdf is not False and enforce_tsdf : out = out . clamp ( - self . _tsdf , self . _tsdf ) return out forward forward ( z , enforce_tsdf = False ) Decode latent vectors to SDFs. PARAMETER DESCRIPTION z Batch of latent vectors. Expected shape of (N,latent_size). Source code in sdfest/vae/sdf_vae.py 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 def forward ( self , z , enforce_tsdf = False ): \"\"\"Decode latent vectors to SDFs. Args: z: Batch of latent vectors. Expected shape of (N,latent_size). \"\"\" out = z for fc_layer in self . _fc_layers : out = nn . functional . relu ( fc_layer ( out )) out = out . view ( - 1 , self . _conv_info [ 0 ][ \"in_channels\" ], self . _conv_info [ 0 ][ \"in_size\" ], self . _conv_info [ 0 ][ \"in_size\" ], self . _conv_info [ 0 ][ \"in_size\" ], ) for info , layer in zip ( self . _conv_info , self . _conv_layers ): # interpolate to match next input if out . shape [ 2 ] != info [ \"in_size\" ]: out = torch . nn . functional . interpolate ( out , size = ( info [ \"in_size\" ], info [ \"in_size\" ], info [ \"in_size\" ]), mode = \"trilinear\" , align_corners = False , ) out = layer ( out ) if info [ \"relu\" ]: out = nn . functional . relu ( out ) if out . shape [ 2 ] != self . _volume_size : out = torch . nn . functional . interpolate ( out , size = ( self . _volume_size , self . _volume_size , self . _volume_size ), mode = \"trilinear\" , align_corners = False , ) if self . _tsdf is not False and enforce_tsdf : out = out . clamp ( - self . _tsdf , self . _tsdf ) return out","title":"sdf_vae"},{"location":"reference/vae/sdf_vae/#sdfest.vae.sdf_vae","text":"This module provides various PyTorch modules for working with SDFs.","title":"sdf_vae"},{"location":"reference/vae/sdf_vae/#sdfest.vae.sdf_vae.SDFVAE","text":"Bases: nn . Module Variational Autoencoder for Signed Distance Fields. Source code in sdfest/vae/sdf_vae.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 class SDFVAE ( nn . Module ): \"\"\"Variational Autoencoder for Signed Distance Fields.\"\"\" def __init__ ( self , sdf_size : int , latent_size : int , encoder_dict : dict , decoder_dict : dict , device : torch . device , tsdf : Optional [ Union [ bool , float ]] = False , ): \"\"\"Initialize the SDFVAE module. Args: sdf_size: depth/width/length of the sdf latent_size: dimensions of latent representation encoder_dict: Arguments passed to encoder constructor. See SDFEncoder.__init__ for details. decoder_dict: Arguments passed to decoder constructor. See SDFDecoder.__init__ for details. device: The device this model will work on. This is used for tensors created inside this model for sampling. tsdf: Value to truncate the SDF at. False for untruncated SDF. For the input this is only done in prepare_input, not in the forward pass. The output is truncated in the forward pass. \"\"\" super () . __init__ () self . latent_size = latent_size self . _device = device self . encoder = SDFEncoder ( sdf_size , latent_size , tsdf = tsdf , ** encoder_dict ) self . decoder = SDFDecoder ( sdf_size , latent_size , tsdf = tsdf , ** decoder_dict ) self . sdf_size = sdf_size self . _tsdf = tsdf def forward ( self , x , enforce_tsdf = False ): z , means , log_var = self . encode ( x ) recon_x = self . decoder ( z , enforce_tsdf ) return recon_x , means , log_var , z def sample ( self , n = 1 ): z = torch . randn ([ n , self . latent_size ]) . to ( self . _device ) return z def encode ( self , x ): means , log_var = self . encoder ( x ) std = torch . exp ( 0.5 * log_var ) eps = torch . randn ([ x . shape [ 0 ], self . latent_size ]) . to ( self . _device ) z = eps * std + means return z , means , log_var def inference ( self , n = 1 , enforce_tsdf = False ): z = self . sample ( n ) recon_x = self . decoder ( z , enforce_tsdf ) return recon_x , z def decode ( self , z , enforce_tsdf = False ): \"\"\"Returns decoded SDF. Args: Batch of latent shapes. Shape (N, L). Returns: The decoded SDF. Shape (N, C, D, D, D). \"\"\" return self . decoder ( z , enforce_tsdf ) def prepare_input ( self , sdfs : torch . Tensor ) -> None : \"\"\"Convert batched SDFs to expected input format. This will transform inputs as defined by the decoder. See SDFEncoder.prepare_input for details. Will be done in place without gradients. Args: sdfs: Batched SDFs, expected shape (N,C,D,D,D). \"\"\" self . encoder . prepare_input ( sdfs )","title":"SDFVAE"},{"location":"reference/vae/sdf_vae/#sdfest.vae.sdf_vae.SDFVAE.__init__","text":"__init__ ( sdf_size : int , latent_size : int , encoder_dict : dict , decoder_dict : dict , device : torch . device , tsdf : Optional [ Union [ bool , float ]] = False , ) Initialize the SDFVAE module. PARAMETER DESCRIPTION sdf_size depth/width/length of the sdf TYPE: int latent_size dimensions of latent representation TYPE: int encoder_dict Arguments passed to encoder constructor. See SDFEncoder. init for details. TYPE: dict decoder_dict Arguments passed to decoder constructor. See SDFDecoder. init for details. TYPE: dict device The device this model will work on. This is used for tensors created inside this model for sampling. TYPE: torch . device tsdf Value to truncate the SDF at. False for untruncated SDF. For the input this is only done in prepare_input, not in the forward pass. The output is truncated in the forward pass. TYPE: Optional [ Union [ bool , float ]] DEFAULT: False Source code in sdfest/vae/sdf_vae.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def __init__ ( self , sdf_size : int , latent_size : int , encoder_dict : dict , decoder_dict : dict , device : torch . device , tsdf : Optional [ Union [ bool , float ]] = False , ): \"\"\"Initialize the SDFVAE module. Args: sdf_size: depth/width/length of the sdf latent_size: dimensions of latent representation encoder_dict: Arguments passed to encoder constructor. See SDFEncoder.__init__ for details. decoder_dict: Arguments passed to decoder constructor. See SDFDecoder.__init__ for details. device: The device this model will work on. This is used for tensors created inside this model for sampling. tsdf: Value to truncate the SDF at. False for untruncated SDF. For the input this is only done in prepare_input, not in the forward pass. The output is truncated in the forward pass. \"\"\" super () . __init__ () self . latent_size = latent_size self . _device = device self . encoder = SDFEncoder ( sdf_size , latent_size , tsdf = tsdf , ** encoder_dict ) self . decoder = SDFDecoder ( sdf_size , latent_size , tsdf = tsdf , ** decoder_dict ) self . sdf_size = sdf_size self . _tsdf = tsdf","title":"__init__()"},{"location":"reference/vae/sdf_vae/#sdfest.vae.sdf_vae.SDFVAE.decode","text":"decode ( z , enforce_tsdf = False ) Returns decoded SDF. RETURNS DESCRIPTION The decoded SDF. Shape (N, C, D, D, D). Source code in sdfest/vae/sdf_vae.py 79 80 81 82 83 84 85 86 87 def decode ( self , z , enforce_tsdf = False ): \"\"\"Returns decoded SDF. Args: Batch of latent shapes. Shape (N, L). Returns: The decoded SDF. Shape (N, C, D, D, D). \"\"\" return self . decoder ( z , enforce_tsdf )","title":"decode()"},{"location":"reference/vae/sdf_vae/#sdfest.vae.sdf_vae.SDFVAE.prepare_input","text":"prepare_input ( sdfs : torch . Tensor ) -> None Convert batched SDFs to expected input format. This will transform inputs as defined by the decoder. See SDFEncoder.prepare_input for details. Will be done in place without gradients. PARAMETER DESCRIPTION sdfs Batched SDFs, expected shape (N,C,D,D,D). TYPE: torch . Tensor Source code in sdfest/vae/sdf_vae.py 89 90 91 92 93 94 95 96 97 98 99 100 def prepare_input ( self , sdfs : torch . Tensor ) -> None : \"\"\"Convert batched SDFs to expected input format. This will transform inputs as defined by the decoder. See SDFEncoder.prepare_input for details. Will be done in place without gradients. Args: sdfs: Batched SDFs, expected shape (N,C,D,D,D). \"\"\" self . encoder . prepare_input ( sdfs )","title":"prepare_input()"},{"location":"reference/vae/sdf_vae/#sdfest.vae.sdf_vae.SDFEncoder","text":"Bases: nn . Module Source code in sdfest/vae/sdf_vae.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 class SDFEncoder ( nn . Module ): def __init__ ( self , volume_size , latent_size , layer_infos , tsdf = False ): \"\"\"Create SDFEncoder, i.e., define trainable layers. Args: volume_size: Input size D of the volume. (i.e., input tensor is expected to be Nx1xDxDxD) latent_size: Dimensionality of the latent representation. layers: Dictionaries defining the layers before the final output layers. Required fields: - fully qualified type (str) - args (dict): params passed to init of type These layers need to construct an 2D tensor (including batch dimension). \"\"\" super () . __init__ () in_channels = 1 # define layers layers = [] for layer_info in layer_infos : t = locate ( layer_info [ \"type\" ]) layers . append ( t ( ** layer_info [ \"args\" ])) self . _features = torch . nn . Sequential ( * layers ) with torch . no_grad (): output_size = self . _features ( torch . zeros ( 1 , in_channels , volume_size , volume_size , volume_size ) ) . shape self . linear_means = nn . Linear ( output_size [ 1 ], latent_size ) self . linear_log_var = nn . Linear ( output_size [ 1 ], latent_size ) self . _tsdf = tsdf def forward ( self , x ): \"\"\"Forward pass of the module. Args: Returns: \"\"\" out = self . _features ( x ) means = self . linear_means ( out ) log_vars = self . linear_log_var ( out ) return means , log_vars def prepare_input ( self , sdfs : torch . Tensor ) -> None : \"\"\"Convert batched SDFs to expected input format. This will truncate the SDFs based on tsdf argument passed to constructor. Will be done in place without gradients. Args: sdfs: Batched SDFs, expected shape (N,C,D,D,D). \"\"\" if self . _tsdf is not False : with torch . no_grad (): sdfs . clamp_ ( - self . _tsdf , self . _tsdf )","title":"SDFEncoder"},{"location":"reference/vae/sdf_vae/#sdfest.vae.sdf_vae.SDFEncoder.__init__","text":"__init__ ( volume_size , latent_size , layer_infos , tsdf = False ) Create SDFEncoder, i.e., define trainable layers. PARAMETER DESCRIPTION volume_size Input size D of the volume. (i.e., input tensor is expected to be Nx1xDxDxD) latent_size Dimensionality of the latent representation. layers Dictionaries defining the layers before the final output layers. Required fields: - fully qualified type (str) - args (dict): params passed to init of type These layers need to construct an 2D tensor (including batch dimension). Source code in sdfest/vae/sdf_vae.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 def __init__ ( self , volume_size , latent_size , layer_infos , tsdf = False ): \"\"\"Create SDFEncoder, i.e., define trainable layers. Args: volume_size: Input size D of the volume. (i.e., input tensor is expected to be Nx1xDxDxD) latent_size: Dimensionality of the latent representation. layers: Dictionaries defining the layers before the final output layers. Required fields: - fully qualified type (str) - args (dict): params passed to init of type These layers need to construct an 2D tensor (including batch dimension). \"\"\" super () . __init__ () in_channels = 1 # define layers layers = [] for layer_info in layer_infos : t = locate ( layer_info [ \"type\" ]) layers . append ( t ( ** layer_info [ \"args\" ])) self . _features = torch . nn . Sequential ( * layers ) with torch . no_grad (): output_size = self . _features ( torch . zeros ( 1 , in_channels , volume_size , volume_size , volume_size ) ) . shape self . linear_means = nn . Linear ( output_size [ 1 ], latent_size ) self . linear_log_var = nn . Linear ( output_size [ 1 ], latent_size ) self . _tsdf = tsdf","title":"__init__()"},{"location":"reference/vae/sdf_vae/#sdfest.vae.sdf_vae.SDFEncoder.forward","text":"forward ( x ) Forward pass of the module. Source code in sdfest/vae/sdf_vae.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def forward ( self , x ): \"\"\"Forward pass of the module. Args: Returns: \"\"\" out = self . _features ( x ) means = self . linear_means ( out ) log_vars = self . linear_log_var ( out ) return means , log_vars","title":"forward()"},{"location":"reference/vae/sdf_vae/#sdfest.vae.sdf_vae.SDFEncoder.prepare_input","text":"prepare_input ( sdfs : torch . Tensor ) -> None Convert batched SDFs to expected input format. This will truncate the SDFs based on tsdf argument passed to constructor. Will be done in place without gradients. PARAMETER DESCRIPTION sdfs Batched SDFs, expected shape (N,C,D,D,D). TYPE: torch . Tensor Source code in sdfest/vae/sdf_vae.py 155 156 157 158 159 160 161 162 163 164 165 166 167 def prepare_input ( self , sdfs : torch . Tensor ) -> None : \"\"\"Convert batched SDFs to expected input format. This will truncate the SDFs based on tsdf argument passed to constructor. Will be done in place without gradients. Args: sdfs: Batched SDFs, expected shape (N,C,D,D,D). \"\"\" if self . _tsdf is not False : with torch . no_grad (): sdfs . clamp_ ( - self . _tsdf , self . _tsdf )","title":"prepare_input()"},{"location":"reference/vae/sdf_vae/#sdfest.vae.sdf_vae.SDFDecoder","text":"Bases: nn . Module Source code in sdfest/vae/sdf_vae.py 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 class SDFDecoder ( nn . Module ): def __init__ ( self , volume_size : int , latent_size : int , fc_layers : list , conv_layers : list , tsdf : Optional [ Union [ bool , float ]] = False , ): super () . __init__ () # sanity checks self . sanity_check ( volume_size , fc_layers , conv_layers ) # create layers and store params self . _volume_size = volume_size in_size = latent_size self . _fc_layers = torch . nn . ModuleList () for fc_layer in fc_layers : self . _fc_layers . append ( nn . Linear ( in_size , fc_layer [ \"out\" ])) in_size = fc_layer [ \"out\" ] self . _fc_info = fc_layers self . _conv_layers = torch . nn . ModuleList () for conv_layer in conv_layers : self . _conv_layers . append ( nn . Conv3d ( conv_layer [ \"in_channels\" ], conv_layer [ \"out_channels\" ], conv_layer [ \"kernel_size\" ], ) ) self . _conv_info = conv_layers self . _tsdf = tsdf def sanity_check ( self , volume_size , fc_dicts , conv_dicts ): assert fc_dicts [ - 1 ][ \"out\" ] == ( conv_dicts [ 0 ][ \"in_channels\" ] * conv_dicts [ 0 ][ \"in_size\" ] ** 3 ) for i , conv_dict in enumerate ( conv_dicts [: - 1 ]): assert conv_dict [ \"out_channels\" ] == conv_dicts [ i + 1 ][ \"in_channels\" ] assert conv_dicts [ - 1 ][ \"out_channels\" ] == 1 def forward ( self , z , enforce_tsdf = False ): \"\"\"Decode latent vectors to SDFs. Args: z: Batch of latent vectors. Expected shape of (N,latent_size). \"\"\" out = z for fc_layer in self . _fc_layers : out = nn . functional . relu ( fc_layer ( out )) out = out . view ( - 1 , self . _conv_info [ 0 ][ \"in_channels\" ], self . _conv_info [ 0 ][ \"in_size\" ], self . _conv_info [ 0 ][ \"in_size\" ], self . _conv_info [ 0 ][ \"in_size\" ], ) for info , layer in zip ( self . _conv_info , self . _conv_layers ): # interpolate to match next input if out . shape [ 2 ] != info [ \"in_size\" ]: out = torch . nn . functional . interpolate ( out , size = ( info [ \"in_size\" ], info [ \"in_size\" ], info [ \"in_size\" ]), mode = \"trilinear\" , align_corners = False , ) out = layer ( out ) if info [ \"relu\" ]: out = nn . functional . relu ( out ) if out . shape [ 2 ] != self . _volume_size : out = torch . nn . functional . interpolate ( out , size = ( self . _volume_size , self . _volume_size , self . _volume_size ), mode = \"trilinear\" , align_corners = False , ) if self . _tsdf is not False and enforce_tsdf : out = out . clamp ( - self . _tsdf , self . _tsdf ) return out","title":"SDFDecoder"},{"location":"reference/vae/sdf_vae/#sdfest.vae.sdf_vae.SDFDecoder.forward","text":"forward ( z , enforce_tsdf = False ) Decode latent vectors to SDFs. PARAMETER DESCRIPTION z Batch of latent vectors. Expected shape of (N,latent_size). Source code in sdfest/vae/sdf_vae.py 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 def forward ( self , z , enforce_tsdf = False ): \"\"\"Decode latent vectors to SDFs. Args: z: Batch of latent vectors. Expected shape of (N,latent_size). \"\"\" out = z for fc_layer in self . _fc_layers : out = nn . functional . relu ( fc_layer ( out )) out = out . view ( - 1 , self . _conv_info [ 0 ][ \"in_channels\" ], self . _conv_info [ 0 ][ \"in_size\" ], self . _conv_info [ 0 ][ \"in_size\" ], self . _conv_info [ 0 ][ \"in_size\" ], ) for info , layer in zip ( self . _conv_info , self . _conv_layers ): # interpolate to match next input if out . shape [ 2 ] != info [ \"in_size\" ]: out = torch . nn . functional . interpolate ( out , size = ( info [ \"in_size\" ], info [ \"in_size\" ], info [ \"in_size\" ]), mode = \"trilinear\" , align_corners = False , ) out = layer ( out ) if info [ \"relu\" ]: out = nn . functional . relu ( out ) if out . shape [ 2 ] != self . _volume_size : out = torch . nn . functional . interpolate ( out , size = ( self . _volume_size , self . _volume_size , self . _volume_size ), mode = \"trilinear\" , align_corners = False , ) if self . _tsdf is not False and enforce_tsdf : out = out . clamp ( - self . _tsdf , self . _tsdf ) return out","title":"forward()"},{"location":"reference/vae/torch_utils/","text":"sdfest.vae.torch_utils Generally useful modules for pytorch. View Bases: torch . nn . Module Wrapper of torch's view method to use with nn.Sequential. Source code in sdfest/vae/torch_utils.py 5 6 7 8 9 10 11 12 13 14 15 class View ( torch . nn . Module ): \"\"\"Wrapper of torch's view method to use with nn.Sequential.\"\"\" def __init__ ( self , * shape ): \"\"\"Construct the module.\"\"\" super ( View , self ) . __init__ () self . shape = shape def forward ( self , x ): \"\"\"Reshape the tensor.\"\"\" return x . view ( * self . shape ) __init__ __init__ ( * shape ) Construct the module. Source code in sdfest/vae/torch_utils.py 8 9 10 11 def __init__ ( self , * shape ): \"\"\"Construct the module.\"\"\" super ( View , self ) . __init__ () self . shape = shape forward forward ( x ) Reshape the tensor. Source code in sdfest/vae/torch_utils.py 13 14 15 def forward ( self , x ): \"\"\"Reshape the tensor.\"\"\" return x . view ( * self . shape )","title":"torch_utils"},{"location":"reference/vae/torch_utils/#sdfest.vae.torch_utils","text":"Generally useful modules for pytorch.","title":"torch_utils"},{"location":"reference/vae/torch_utils/#sdfest.vae.torch_utils.View","text":"Bases: torch . nn . Module Wrapper of torch's view method to use with nn.Sequential. Source code in sdfest/vae/torch_utils.py 5 6 7 8 9 10 11 12 13 14 15 class View ( torch . nn . Module ): \"\"\"Wrapper of torch's view method to use with nn.Sequential.\"\"\" def __init__ ( self , * shape ): \"\"\"Construct the module.\"\"\" super ( View , self ) . __init__ () self . shape = shape def forward ( self , x ): \"\"\"Reshape the tensor.\"\"\" return x . view ( * self . shape )","title":"View"},{"location":"reference/vae/torch_utils/#sdfest.vae.torch_utils.View.__init__","text":"__init__ ( * shape ) Construct the module. Source code in sdfest/vae/torch_utils.py 8 9 10 11 def __init__ ( self , * shape ): \"\"\"Construct the module.\"\"\" super ( View , self ) . __init__ () self . shape = shape","title":"__init__()"},{"location":"reference/vae/torch_utils/#sdfest.vae.torch_utils.View.forward","text":"forward ( x ) Reshape the tensor. Source code in sdfest/vae/torch_utils.py 13 14 15 def forward ( self , x ): \"\"\"Reshape the tensor.\"\"\" return x . view ( * self . shape )","title":"forward()"},{"location":"reference/vae/utils/","text":"sdfest.vae.utils General functions for experiments and pytorch. View Bases: torch . nn . Module Wrapper of torch's view method to use with nn.Sequential. Source code in sdfest/vae/utils.py 97 98 99 100 101 102 103 104 105 106 107 class View ( torch . nn . Module ): \"\"\"Wrapper of torch's view method to use with nn.Sequential.\"\"\" def __init__ ( self , shape ): \"\"\"Construct the module.\"\"\" super ( View , self ) . __init__ () self . shape = shape def forward ( self , x ): \"\"\"Reshape the tensor.\"\"\" return x . view ( * self . shape ) __init__ __init__ ( shape ) Construct the module. Source code in sdfest/vae/utils.py 100 101 102 103 def __init__ ( self , shape ): \"\"\"Construct the module.\"\"\" super ( View , self ) . __init__ () self . shape = shape forward forward ( x ) Reshape the tensor. Source code in sdfest/vae/utils.py 105 106 107 def forward ( self , x ): \"\"\"Reshape the tensor.\"\"\" return x . view ( * self . shape ) save_checkpoint save_checkpoint ( path : str , model : torch . nn . Module , optimizer , iteration , epoch , run_name , ) Save a checkpoint during training. Source code in sdfest/vae/utils.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def save_checkpoint ( path : str , model : torch . nn . Module , optimizer , iteration , epoch , run_name ): \"\"\"Save a checkpoint during training. Args: \"\"\" torch . save ( { \"iteration\" : iteration , \"epoch\" : epoch , \"model_state_dict\" : model . state_dict (), \"optimizer_state_dict\" : optimizer . state_dict (), \"run_name\" : run_name , }, path , ) load_checkpoint load_checkpoint ( path , model , optimizer ) Load a checkpoint during training. Source code in sdfest/vae/utils.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def load_checkpoint ( path , model , optimizer ): \"\"\"Load a checkpoint during training. Args: \"\"\" print ( f \"Loading checkpoint at { path } ...\" ) checkpoint = torch . load ( path ) model . load_state_dict ( checkpoint [ \"model_state_dict\" ]) optimizer . load_state_dict ( checkpoint [ \"optimizer_state_dict\" ]) iteration = checkpoint [ \"iteration\" ] epoch = checkpoint [ \"epoch\" ] run_name = checkpoint [ \"run_name\" ] print ( \"Checkpoint loaded\" ) model . train () # training mode return model , optimizer , iteration , run_name , epoch str_to_tsdf str_to_tsdf ( x : str ) -> Union [ bool , float ] Convert string to expected values for tsdf setting. PARAMETER DESCRIPTION x A string containing either some representation of False or a float. TYPE: str RETURNS DESCRIPTION Union [ bool , float ] False or float. Source code in sdfest/vae/utils.py 84 85 86 87 88 89 90 91 92 93 94 def str_to_tsdf ( x : str ) -> Union [ bool , float ]: \"\"\"Convert string to expected values for tsdf setting. Args: x: A string containing either some representation of False or a float. Returns: False or float. \"\"\" if x . lower () in ( \"no\" , \"false\" , \"f\" , \"n\" , \"0\" ): return False return float ( x )","title":"utils"},{"location":"reference/vae/utils/#sdfest.vae.utils","text":"General functions for experiments and pytorch.","title":"utils"},{"location":"reference/vae/utils/#sdfest.vae.utils.View","text":"Bases: torch . nn . Module Wrapper of torch's view method to use with nn.Sequential. Source code in sdfest/vae/utils.py 97 98 99 100 101 102 103 104 105 106 107 class View ( torch . nn . Module ): \"\"\"Wrapper of torch's view method to use with nn.Sequential.\"\"\" def __init__ ( self , shape ): \"\"\"Construct the module.\"\"\" super ( View , self ) . __init__ () self . shape = shape def forward ( self , x ): \"\"\"Reshape the tensor.\"\"\" return x . view ( * self . shape )","title":"View"},{"location":"reference/vae/utils/#sdfest.vae.utils.View.__init__","text":"__init__ ( shape ) Construct the module. Source code in sdfest/vae/utils.py 100 101 102 103 def __init__ ( self , shape ): \"\"\"Construct the module.\"\"\" super ( View , self ) . __init__ () self . shape = shape","title":"__init__()"},{"location":"reference/vae/utils/#sdfest.vae.utils.View.forward","text":"forward ( x ) Reshape the tensor. Source code in sdfest/vae/utils.py 105 106 107 def forward ( self , x ): \"\"\"Reshape the tensor.\"\"\" return x . view ( * self . shape )","title":"forward()"},{"location":"reference/vae/utils/#sdfest.vae.utils.save_checkpoint","text":"save_checkpoint ( path : str , model : torch . nn . Module , optimizer , iteration , epoch , run_name , ) Save a checkpoint during training. Source code in sdfest/vae/utils.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def save_checkpoint ( path : str , model : torch . nn . Module , optimizer , iteration , epoch , run_name ): \"\"\"Save a checkpoint during training. Args: \"\"\" torch . save ( { \"iteration\" : iteration , \"epoch\" : epoch , \"model_state_dict\" : model . state_dict (), \"optimizer_state_dict\" : optimizer . state_dict (), \"run_name\" : run_name , }, path , )","title":"save_checkpoint()"},{"location":"reference/vae/utils/#sdfest.vae.utils.load_checkpoint","text":"load_checkpoint ( path , model , optimizer ) Load a checkpoint during training. Source code in sdfest/vae/utils.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def load_checkpoint ( path , model , optimizer ): \"\"\"Load a checkpoint during training. Args: \"\"\" print ( f \"Loading checkpoint at { path } ...\" ) checkpoint = torch . load ( path ) model . load_state_dict ( checkpoint [ \"model_state_dict\" ]) optimizer . load_state_dict ( checkpoint [ \"optimizer_state_dict\" ]) iteration = checkpoint [ \"iteration\" ] epoch = checkpoint [ \"epoch\" ] run_name = checkpoint [ \"run_name\" ] print ( \"Checkpoint loaded\" ) model . train () # training mode return model , optimizer , iteration , run_name , epoch","title":"load_checkpoint()"},{"location":"reference/vae/utils/#sdfest.vae.utils.str_to_tsdf","text":"str_to_tsdf ( x : str ) -> Union [ bool , float ] Convert string to expected values for tsdf setting. PARAMETER DESCRIPTION x A string containing either some representation of False or a float. TYPE: str RETURNS DESCRIPTION Union [ bool , float ] False or float. Source code in sdfest/vae/utils.py 84 85 86 87 88 89 90 91 92 93 94 def str_to_tsdf ( x : str ) -> Union [ bool , float ]: \"\"\"Convert string to expected values for tsdf setting. Args: x: A string containing either some representation of False or a float. Returns: False or float. \"\"\" if x . lower () in ( \"no\" , \"false\" , \"f\" , \"n\" , \"0\" ): return False return float ( x )","title":"str_to_tsdf()"},{"location":"reference/vae/scripts/benchmark/","text":"sdfest.vae.scripts.benchmark","title":"benchmark"},{"location":"reference/vae/scripts/benchmark/#sdfest.vae.scripts.benchmark","text":"","title":"benchmark"},{"location":"reference/vae/scripts/process_shapenet/","text":"sdfest.vae.scripts.process_shapenet Script to preprocess shapenet like dataset. This script allows to interactively go through a ShapeNet dataset and decide whether to keep or remove a mesh. All textures are removed, only the obj file and a converted SDF volume are stored in the output folder. Object3D Representation of a 3D object. Source code in sdfest/vae/scripts/process_shapenet.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 class Object3D : \"\"\"Representation of a 3D object.\"\"\" def __init__ ( self , mesh_path : str ): \"\"\"Initialize the 3D object. Args: mesh_path: the full file path of the mesh \"\"\" self . mesh_path = mesh_path self . simplified_mesh = None self . sdf_volume = None self . reconstructed_mesh = None def convert_to_sdf ( self , cells_per_dim , padding ): self . sdf_volume = sdf_utils . mesh_to_sdf ( self . simplified_mesh , cells_per_dim , padding ) return self def load_mesh ( self ): loaded_obj = trimesh . load ( self . mesh_path , process = False ) if isinstance ( loaded_obj , Trimesh ): self . simplified_mesh = Trimesh ( loaded_obj . vertices , loaded_obj . faces , vertex_normals = loaded_obj . vertex_normals , visual = trimesh . visual . TextureVisuals ( material = SimpleMaterial ()), ) elif isinstance ( loaded_obj , Scene ): mesh = trimesh . util . concatenate ( tuple ( trimesh . Trimesh ( vertices = g . vertices , faces = g . faces , vertex_normals = g . vertex_normals , ) for g in loaded_obj . geometry . values () ) ) self . simplified_mesh = Trimesh ( mesh . vertices , mesh . faces , vertex_normals = mesh . vertex_normals , visual = trimesh . visual . TextureVisuals ( material = SimpleMaterial ()), ) else : print ( f \"Not supported: { type ( loaded_obj ) } \" ) return False return True def reconstruct_from_sdf ( self ): level = 1.0 / self . sdf_volume . shape [ - 1 ] self . reconstructed_mesh = sdf_utils . mesh_from_sdf ( self . sdf_volume , level ) return self __init__ __init__ ( mesh_path : str ) Initialize the 3D object. PARAMETER DESCRIPTION mesh_path the full file path of the mesh TYPE: str Source code in sdfest/vae/scripts/process_shapenet.py 28 29 30 31 32 33 34 35 36 37 def __init__ ( self , mesh_path : str ): \"\"\"Initialize the 3D object. Args: mesh_path: the full file path of the mesh \"\"\" self . mesh_path = mesh_path self . simplified_mesh = None self . sdf_volume = None self . reconstructed_mesh = None trimesh_decision_viewer trimesh_decision_viewer ( mesh , return_dict ) View a trimesh with pyrender adding support to press left and right. Source code in sdfest/vae/scripts/process_shapenet.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 def trimesh_decision_viewer ( mesh , return_dict ): \"\"\"View a trimesh with pyrender adding support to press left and right.\"\"\" def on_press ( key ): nonlocal inp try : inp = key . char # single-char keys except AttributeError : inp = key . name pyrender_mesh = pyrender . Mesh . from_trimesh ( mesh ) scene = pyrender . Scene ( ambient_light = [ 0.3 , 0.3 , 0.3 , 1.0 ]) scene . add ( pyrender_mesh ) v = pyrender . Viewer ( scene , run_in_thread = True , use_raymond_lighting = True ) inp = None from pynput import keyboard decision = None listener = keyboard . Listener ( on_press = on_press ) listener . start () # start to listen on a separate thread while True : time . sleep ( 0.1 ) if inp == \"left\" : decision = \"remove\" break elif inp == \"right\" : decision = \"keep\" break elif inp == \"down\" : decision = \"stop\" break # TODO: listener.stop() hangs randomly (especially under load) v . close_external () return_dict [ \"decision\" ] = decision return decision","title":"process_shapenet"},{"location":"reference/vae/scripts/process_shapenet/#sdfest.vae.scripts.process_shapenet","text":"Script to preprocess shapenet like dataset. This script allows to interactively go through a ShapeNet dataset and decide whether to keep or remove a mesh. All textures are removed, only the obj file and a converted SDF volume are stored in the output folder.","title":"process_shapenet"},{"location":"reference/vae/scripts/process_shapenet/#sdfest.vae.scripts.process_shapenet.Object3D","text":"Representation of a 3D object. Source code in sdfest/vae/scripts/process_shapenet.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 class Object3D : \"\"\"Representation of a 3D object.\"\"\" def __init__ ( self , mesh_path : str ): \"\"\"Initialize the 3D object. Args: mesh_path: the full file path of the mesh \"\"\" self . mesh_path = mesh_path self . simplified_mesh = None self . sdf_volume = None self . reconstructed_mesh = None def convert_to_sdf ( self , cells_per_dim , padding ): self . sdf_volume = sdf_utils . mesh_to_sdf ( self . simplified_mesh , cells_per_dim , padding ) return self def load_mesh ( self ): loaded_obj = trimesh . load ( self . mesh_path , process = False ) if isinstance ( loaded_obj , Trimesh ): self . simplified_mesh = Trimesh ( loaded_obj . vertices , loaded_obj . faces , vertex_normals = loaded_obj . vertex_normals , visual = trimesh . visual . TextureVisuals ( material = SimpleMaterial ()), ) elif isinstance ( loaded_obj , Scene ): mesh = trimesh . util . concatenate ( tuple ( trimesh . Trimesh ( vertices = g . vertices , faces = g . faces , vertex_normals = g . vertex_normals , ) for g in loaded_obj . geometry . values () ) ) self . simplified_mesh = Trimesh ( mesh . vertices , mesh . faces , vertex_normals = mesh . vertex_normals , visual = trimesh . visual . TextureVisuals ( material = SimpleMaterial ()), ) else : print ( f \"Not supported: { type ( loaded_obj ) } \" ) return False return True def reconstruct_from_sdf ( self ): level = 1.0 / self . sdf_volume . shape [ - 1 ] self . reconstructed_mesh = sdf_utils . mesh_from_sdf ( self . sdf_volume , level ) return self","title":"Object3D"},{"location":"reference/vae/scripts/process_shapenet/#sdfest.vae.scripts.process_shapenet.Object3D.__init__","text":"__init__ ( mesh_path : str ) Initialize the 3D object. PARAMETER DESCRIPTION mesh_path the full file path of the mesh TYPE: str Source code in sdfest/vae/scripts/process_shapenet.py 28 29 30 31 32 33 34 35 36 37 def __init__ ( self , mesh_path : str ): \"\"\"Initialize the 3D object. Args: mesh_path: the full file path of the mesh \"\"\" self . mesh_path = mesh_path self . simplified_mesh = None self . sdf_volume = None self . reconstructed_mesh = None","title":"__init__()"},{"location":"reference/vae/scripts/process_shapenet/#sdfest.vae.scripts.process_shapenet.trimesh_decision_viewer","text":"trimesh_decision_viewer ( mesh , return_dict ) View a trimesh with pyrender adding support to press left and right. Source code in sdfest/vae/scripts/process_shapenet.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 def trimesh_decision_viewer ( mesh , return_dict ): \"\"\"View a trimesh with pyrender adding support to press left and right.\"\"\" def on_press ( key ): nonlocal inp try : inp = key . char # single-char keys except AttributeError : inp = key . name pyrender_mesh = pyrender . Mesh . from_trimesh ( mesh ) scene = pyrender . Scene ( ambient_light = [ 0.3 , 0.3 , 0.3 , 1.0 ]) scene . add ( pyrender_mesh ) v = pyrender . Viewer ( scene , run_in_thread = True , use_raymond_lighting = True ) inp = None from pynput import keyboard decision = None listener = keyboard . Listener ( on_press = on_press ) listener . start () # start to listen on a separate thread while True : time . sleep ( 0.1 ) if inp == \"left\" : decision = \"remove\" break elif inp == \"right\" : decision = \"keep\" break elif inp == \"down\" : decision = \"stop\" break # TODO: listener.stop() hangs randomly (especially under load) v . close_external () return_dict [ \"decision\" ] = decision return decision","title":"trimesh_decision_viewer()"},{"location":"reference/vae/scripts/train/","text":"sdfest.vae.scripts.train Script to train model. pc_loss pc_loss ( points , position , orientation , scale , sdf ) Compute trilinerly interpolated SDF value at the points positions. PARAMETER DESCRIPTION points Mx4 pointcloud in camera frame. position Position of SDF center in camera frame. orientation Quaternion representing orientation of SDF. scale Half-width of SDF volume. sdf Volumetric signed distance field. RETURNS DESCRIPTION Trilinearly interpolated distance at the passed points 0 if outside of SDF volume. Source code in sdfest/vae/scripts/train.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def pc_loss ( points , position , orientation , scale , sdf ): \"\"\"Compute trilinerly interpolated SDF value at the points positions. Args: points: Mx4 pointcloud in camera frame. position: Position of SDF center in camera frame. orientation: Quaternion representing orientation of SDF. scale: Half-width of SDF volume. sdf: Volumetric signed distance field. Returns: Trilinearly interpolated distance at the passed points 0 if outside of SDF volume. \"\"\" q = orientation / torch . norm ( orientation ) # to get normalization gradients obj_points = points - position . unsqueeze ( 0 ) # Quaternion to rotation matrix # Note that we use conjugate here since we want to transform points from # world to object coordinates and the quaternion describes rotation of # object coordinate system in world coordinates. R = obj_points . new_zeros ( 3 , 3 ) R [ 0 , 0 ] = 1 - 2 * ( q [ 1 ] * q [ 1 ] + q [ 2 ] * q [ 2 ]) R [ 0 , 1 ] = 2 * ( q [ 0 ] * q [ 1 ] + q [ 2 ] * q [ 3 ]) R [ 0 , 2 ] = 2 * ( q [ 0 ] * q [ 2 ] - q [ 3 ] * q [ 1 ]) R [ 1 , 0 ] = 2 * ( q [ 0 ] * q [ 1 ] - q [ 2 ] * q [ 3 ]) R [ 1 , 1 ] = 1 - 2 * ( q [ 0 ] * q [ 0 ] + q [ 2 ] * q [ 2 ]) R [ 1 , 2 ] = 2 * ( q [ 1 ] * q [ 2 ] + q [ 3 ] * q [ 0 ]) R [ 2 , 0 ] = 2 * ( q [ 0 ] * q [ 2 ] + q [ 3 ] * q [ 1 ]) R [ 2 , 1 ] = 2 * ( q [ 1 ] * q [ 2 ] - q [ 3 ] * q [ 0 ]) R [ 2 , 2 ] = 1 - 2 * ( q [ 0 ] * q [ 0 ] + q [ 1 ] * q [ 1 ]) obj_points = ( R @ obj_points . T ) . T # Transform to canonical coordintes obj_point in [-1,1]^3 obj_point = obj_points / scale # Compute cell and cell position res = sdf . shape [ 0 ] # assuming same resolution along each axis grid_size = 2.0 / ( res - 1 ) c = torch . floor (( obj_point + 1.0 ) * ( res - 1 ) * 0.5 ) mask = torch . logical_or ( torch . min ( c , dim = 1 )[ 0 ] < 0 , torch . max ( c , dim = 1 )[ 0 ] > res - 2 ) c = torch . clip ( c , 0 , res - 2 ) # base cell index of each point cell_position = c * grid_size - 1.0 # base cell position of each point sdf_indices = c . new_empty (( obj_point . shape [ 0 ], 8 ), dtype = torch . long ) sdf_indices [:, 0 ] = c [:, 0 ] * res ** 2 + c [:, 1 ] * res + c [:, 2 ] sdf_indices [:, 1 ] = c [:, 0 ] * res ** 2 + c [:, 1 ] * res + c [:, 2 ] + 1 sdf_indices [:, 2 ] = c [:, 0 ] * res ** 2 + ( c [:, 1 ] + 1 ) * res + c [:, 2 ] sdf_indices [:, 3 ] = c [:, 0 ] * res ** 2 + ( c [:, 1 ] + 1 ) * res + c [:, 2 ] + 1 sdf_indices [:, 4 ] = ( c [:, 0 ] + 1 ) * res ** 2 + c [:, 1 ] * res + c [:, 2 ] sdf_indices [:, 5 ] = ( c [:, 0 ] + 1 ) * res ** 2 + c [:, 1 ] * res + c [:, 2 ] + 1 sdf_indices [:, 6 ] = ( c [:, 0 ] + 1 ) * res ** 2 + ( c [:, 1 ] + 1 ) * res + c [:, 2 ] sdf_indices [:, 7 ] = ( c [:, 0 ] + 1 ) * res ** 2 + ( c [:, 1 ] + 1 ) * res + c [:, 2 ] + 1 sdf_view = sdf . view ([ - 1 ]) point_cell_position = ( obj_point - cell_position ) / grid_size # [0,1]^3 sdf_values = torch . take ( sdf_view , sdf_indices ) # trilinear interpolation sdf_value = sdf_values . new_empty ( obj_points . shape [ 0 ]) # sdf_value = obj_point[:, 0] sdf_value = ( ( sdf_values [:, 0 ] * ( 1 - point_cell_position [:, 0 ]) + sdf_values [:, 4 ] * point_cell_position [:, 0 ] ) * ( 1 - point_cell_position [:, 1 ]) + ( sdf_values [:, 2 ] * ( 1 - point_cell_position [:, 0 ]) + sdf_values [:, 6 ] * point_cell_position [:, 0 ] ) * point_cell_position [:, 1 ] ) * ( 1 - point_cell_position [:, 2 ]) + ( ( sdf_values [:, 1 ] * ( 1 - point_cell_position [:, 0 ]) + sdf_values [:, 5 ] * point_cell_position [:, 0 ] ) * ( 1 - point_cell_position [:, 1 ]) + ( sdf_values [:, 3 ] * ( 1 - point_cell_position [:, 0 ]) + sdf_values [:, 7 ] * point_cell_position [:, 0 ] ) * point_cell_position [:, 1 ] ) * point_cell_position [ :, 2 ] sdf_value [ mask ] = 0 return sdf_value train train ( config ) Train the model. PARAMETER DESCRIPTION config The training and model config. Source code in sdfest/vae/scripts/train.py 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 def train ( config ): \"\"\"Train the model. Args: config: The training and model config. \"\"\" batch_size = wandb . config . batch_size = config [ \"batch_size\" ] iterations = wandb . config . iterations = config [ \"iterations\" ] learning_rate = wandb . config . learning_rate = config [ \"learning_rate\" ] latent_size = wandb . config . latent_size = config [ \"latent_size\" ] l2_large_weight = wandb . config . l2_large_weight = config [ \"l2_large_weight\" ] l2_small_weight = wandb . config . l2_small_weight = config [ \"l2_small_weight\" ] l1_large_weight = wandb . config . l1_large_weight = config [ \"l1_large_weight\" ] l1_small_weight = wandb . config . l1_small_weight = config [ \"l1_small_weight\" ] kld_weight = wandb . config . kld_weight = config [ \"kld_weight\" ] pc_weight = wandb . config . pc_weight = config [ \"pc_weight\" ] encoder_dict = wandb . config . encoder_dict = config [ \"encoder\" ] decoder_dict = wandb . config . decoder_dict = config [ \"decoder\" ] dataset_path = wandb . config . dataset_path = config [ \"dataset_path\" ] tsdf = wandb . config . tsdf = config [ \"tsdf\" ] # init dataset, dataloader, model and optimizer dataset = SDFDataset ( dataset_path ) data_loader = torch . utils . data . DataLoader ( dataset = dataset , batch_size = batch_size , shuffle = True , drop_last = True ) camera = Camera ( 640 , 480 , 320 , 320 , 320 , 240 , pixel_center = 0.5 ) if \"device\" not in config or config [ \"device\" ] is None : device = torch . device ( \"cuda\" if torch . cuda . is_available () else \"cpu\" ) else : device = torch . device ( config [ \"device\" ]) sdfvae = SDFVAE ( sdf_size = 64 , latent_size = latent_size , encoder_dict = encoder_dict , decoder_dict = decoder_dict , device = device , tsdf = tsdf , ) . to ( device ) print ( str ( sdfvae )) summary ( sdfvae , ( 1 , 1 , 64 , 64 , 64 ), device = device ) optimizer = torch . optim . Adam ( sdfvae . parameters (), lr = learning_rate ) # load checkpoint if provided if \"checkpoint\" in config and config [ \"checkpoint\" ] is not None : # TODO: checkpoint should always go together with model config! model , optimizer , current_iteration , run_name , epoch = utils . load_checkpoint ( config [ \"checkpoint\" ], sdfvae , optimizer ) else : current_iteration = 0 current_epoch = 0 run_name = f \"sdfvae_ { datetime . now () . strftime ( '%Y-%m- %d _%H-%M-%S- %f ' ) } \" # create SummaryWriter for logging and intermediate output writer = torch . utils . tensorboard . SummaryWriter ( f \"runs/\" + run_name ) model_base_path = os . path . join ( os . getcwd (), \"models\" , run_name ) program_starts = time . time () warm_up_iterations = 1000 stop = False while current_iteration <= iterations : current_epoch += 1 for sdf_volumes in data_loader : sdf_volumes = sdf_volumes . to ( device ) # train first N iters with SDF instead of TSDF to stabilize early training if current_iteration > warm_up_iterations : sdfvae . prepare_input ( sdf_volumes ) recon_sdf_volumes , mean , log_var , z = sdfvae ( sdf_volumes , enforce_tsdf = False ) if tsdf is not False and current_iteration > warm_up_iterations : # during training, clamp the reconstructed SDF only where both target # and output are outside the TSDF range mask = torch . logical_and ( torch . abs ( sdf_volumes ) >= tsdf , torch . abs ( recon_sdf_volumes ) >= tsdf ) recon_sdf_volumes_temp = recon_sdf_volumes recon_sdf_volumes = recon_sdf_volumes_temp . clone () recon_sdf_volumes [ mask ] = recon_sdf_volumes_temp [ mask ] . clamp ( - tsdf , tsdf ) # Compute losses, use negative log-likelihood here # Note: for average negative log-likelihood all of these have to be divided # by the batch size. Probably would be better to keep losses comparable for # varying batch size. l1_error = torch . abs ( recon_sdf_volumes - sdf_volumes ) l2_error = l1_error ** 2 loss_l2_small = torch . sum ( l2_error [ torch . abs ( sdf_volumes ) < 0.1 ]) loss_l2_large = torch . sum ( l2_error [ torch . abs ( sdf_volumes ) >= 0.1 ]) loss_l1_small = torch . sum ( l1_error [ torch . abs ( sdf_volumes ) < 0.1 ]) loss_l1_large = torch . sum ( l1_error [ torch . abs ( sdf_volumes ) >= 0.1 ]) loss_pc = 0 depth_images = torch . empty (( batch_size , 480 , 640 ), device = device ) pointclouds = [] # compute point clouds from sdf volumes at zero crossings for recon_sdf_volume , sdf_volume , depth_image in zip ( recon_sdf_volumes , sdf_volumes , depth_images ): with torch . no_grad (): import random import math u1 , u2 , u3 = random . random (), random . random (), random . random () q = ( torch . tensor ( [ math . sqrt ( 1 - u1 ) * math . sin ( 2 * math . pi * u2 ), math . sqrt ( 1 - u1 ) * math . cos ( 2 * math . pi * u2 ), math . sqrt ( u1 ) * math . sin ( 2 * math . pi * u3 ), math . sqrt ( u1 ) * math . cos ( 2 * math . pi * u3 ), ] ) . unsqueeze ( 0 ) . to ( device ) ) s = torch . Tensor ([ 1.0 ]) . to ( device ) p = torch . Tensor ([ 0.0 , 0.0 , - 5.0 ]) . to ( device ) depth_image = render_depth_gpu ( sdf_volume [ 0 ], p , q , s , threshold = 0.01 , camera = camera , ) pointcloud = pointset_utils . depth_to_pointcloud ( depth_image , camera ) loss_pc = loss_pc + torch . sum ( pc_loss ( pointcloud , p , q [ 0 ], s , recon_sdf_volume [ 0 ]) ** 2 ) loss_kld = - 0.5 * torch . sum ( 1 + log_var - mean . pow ( 2 ) - log_var . exp ()) loss = ( l2_small_weight * loss_l2_small + l2_large_weight * loss_l2_large + l1_small_weight * loss_l1_small + l1_large_weight * loss_l1_large + pc_weight * loss_pc + loss_kld * ( kld_weight if current_iteration > warm_up_iterations else 0 ) ) print ( f \"Iteration { current_iteration } , epoch { current_epoch } , loss { loss } \" ) optimizer . zero_grad () loss . backward () optimizer . step () writer . add_scalar ( \"loss\" , loss . item (), current_iteration ) writer . add_scalar ( \"loss l2 small\" , loss_l2_small . item (), current_iteration ) writer . add_scalar ( \"loss l2 large\" , loss_l2_large . item (), current_iteration ) writer . add_scalar ( \"loss l1 small\" , loss_l1_small . item (), current_iteration ) writer . add_scalar ( \"loss l1 large\" , loss_l1_large . item (), current_iteration ) writer . add_scalar ( \"loss pc\" , loss_pc . item (), current_iteration ) writer . add_scalar ( \"loss kl\" , loss_kld , current_iteration ) wandb . log ( { \"total loss\" : loss . item (), \"loss l2 small\" : loss_l2_small . item (), \"loss l2 large\" : loss_l2_large . item (), \"loss l1 small\" : loss_l1_small . item (), \"loss l1 large\" : loss_l1_large . item (), \"loss kl\" : loss_kld . item (), \"loss pc\" : loss_pc . item (), }, step = current_iteration , ) current_iteration += 1 with torch . no_grad (): if current_iteration % 1000 == 0 : # show one reconstruction mean = mean [ 0 ] . cpu () figure = sdf_utils . visualize_sdf_reconstruction ( sdf_volumes [ 0 , 0 ] . cpu () . numpy (), recon_sdf_volumes [ 0 , 0 ] . cpu () . numpy (), ) if figure . gca () . has_data (): writer . add_figure ( tag = \"reconstruction\" , figure = figure , global_step = current_iteration , ) wandb . log ({ \"reconstruction\" : figure }, step = current_iteration ) # generate 8 samples from the prior output , _ = sdfvae . inference ( n = 8 , enforce_tsdf = True ) figure = sdf_utils . visualize_sdf_batch ( output . squeeze () . cpu () . numpy () ) if figure . gca () . has_data (): writer . add_figure ( tag = \"samples from prior\" , figure = figure , global_step = current_iteration , ) wandb . log ( { \"samples from prior\" : figure }, step = current_iteration ) # generate 4 samples from prior output , _ = sdfvae . inference ( n = 4 , enforce_tsdf = True ) figure = sdf_utils . visualize_sdf_batch_columns ( output . squeeze () . cpu () . numpy () ) if figure . gca () . has_data (): writer . add_figure ( tag = \"samples from prior, columns\" , figure = figure , global_step = current_iteration , ) wandb . log ( { \"samples from prior, columns\" : figure }, step = current_iteration , ) # if current_iteration % 10000 == 0: # os.makedirs(model_base_path, exist_ok=True) # checkpoint_path = os.path.join(model_base_path, F\"{current_iteration}.ckp\") # utils.save_checkpoint( # path=checkpoint_path, # model=sdfvae, # optimizer=optimizer, # iteration=current_iteration, # run_name=run_name) if current_iteration > iterations : break if current_iteration > iterations : break now = time . time () print ( \"It has been {0} seconds since the loop started\" . format ( now - program_starts )) # save the final model torch . save ( sdfvae . state_dict (), os . path . join ( wandb . run . dir , f \" { wandb . run . name } .pt\" )) config_path = os . path . join ( wandb . run . dir , f \" { wandb . run . name } .yaml\" ) config [ \"model\" ] = os . path . join ( \".\" , f \" { wandb . run . name } .pt\" ) yoco . save_config_to_file ( config_path , config ) main main () Entry point of the program. Source code in sdfest/vae/scripts/train.py 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 def main (): \"\"\"Entry point of the program.\"\"\" # define the arguments parser = argparse . ArgumentParser ( description = \"Training script for SDFVAE.\" ) # parse arguments parser . add_argument ( \"--checkpoint\" ) parser . add_argument ( \"--dataset_path\" , required = True ) parser . add_argument ( \"--batch_size\" , type = lambda x : int ( float ( x ))) parser . add_argument ( \"--iterations\" , type = lambda x : int ( float ( x ))) parser . add_argument ( \"--latent_size\" , type = lambda x : int ( float ( x ))) parser . add_argument ( \"--tsdf\" , type = utils . str_to_tsdf , default = False ) parser . add_argument ( \"--kld_weight\" , type = lambda x : float ( x )) parser . add_argument ( \"--l2_large_weight\" , type = lambda x : float ( x )) parser . add_argument ( \"--l2_small_weight\" , type = lambda x : float ( x )) parser . add_argument ( \"--l1_large_weight\" , type = lambda x : float ( x )) parser . add_argument ( \"--l1_small_weight\" , type = lambda x : float ( x )) parser . add_argument ( \"--pc_weight\" , type = lambda x : float ( x )) parser . add_argument ( \"--sign_weight\" , type = lambda x : float ( x )) parser . add_argument ( \"--bce_weight\" , type = lambda x : float ( x )) parser . add_argument ( \"--learning_rate\" , type = lambda x : float ( x )) parser . add_argument ( \"--device\" ) parser . add_argument ( \"--config\" , default = \"configs/default.yaml\" , nargs = \"+\" ) config = yoco . load_config_from_args ( parser , search_paths = [ \".\" , \"~/.sdfest/\" , sdfest . __path__ [ 0 ]] ) train ( config )","title":"train"},{"location":"reference/vae/scripts/train/#sdfest.vae.scripts.train","text":"Script to train model.","title":"train"},{"location":"reference/vae/scripts/train/#sdfest.vae.scripts.train.pc_loss","text":"pc_loss ( points , position , orientation , scale , sdf ) Compute trilinerly interpolated SDF value at the points positions. PARAMETER DESCRIPTION points Mx4 pointcloud in camera frame. position Position of SDF center in camera frame. orientation Quaternion representing orientation of SDF. scale Half-width of SDF volume. sdf Volumetric signed distance field. RETURNS DESCRIPTION Trilinearly interpolated distance at the passed points 0 if outside of SDF volume. Source code in sdfest/vae/scripts/train.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def pc_loss ( points , position , orientation , scale , sdf ): \"\"\"Compute trilinerly interpolated SDF value at the points positions. Args: points: Mx4 pointcloud in camera frame. position: Position of SDF center in camera frame. orientation: Quaternion representing orientation of SDF. scale: Half-width of SDF volume. sdf: Volumetric signed distance field. Returns: Trilinearly interpolated distance at the passed points 0 if outside of SDF volume. \"\"\" q = orientation / torch . norm ( orientation ) # to get normalization gradients obj_points = points - position . unsqueeze ( 0 ) # Quaternion to rotation matrix # Note that we use conjugate here since we want to transform points from # world to object coordinates and the quaternion describes rotation of # object coordinate system in world coordinates. R = obj_points . new_zeros ( 3 , 3 ) R [ 0 , 0 ] = 1 - 2 * ( q [ 1 ] * q [ 1 ] + q [ 2 ] * q [ 2 ]) R [ 0 , 1 ] = 2 * ( q [ 0 ] * q [ 1 ] + q [ 2 ] * q [ 3 ]) R [ 0 , 2 ] = 2 * ( q [ 0 ] * q [ 2 ] - q [ 3 ] * q [ 1 ]) R [ 1 , 0 ] = 2 * ( q [ 0 ] * q [ 1 ] - q [ 2 ] * q [ 3 ]) R [ 1 , 1 ] = 1 - 2 * ( q [ 0 ] * q [ 0 ] + q [ 2 ] * q [ 2 ]) R [ 1 , 2 ] = 2 * ( q [ 1 ] * q [ 2 ] + q [ 3 ] * q [ 0 ]) R [ 2 , 0 ] = 2 * ( q [ 0 ] * q [ 2 ] + q [ 3 ] * q [ 1 ]) R [ 2 , 1 ] = 2 * ( q [ 1 ] * q [ 2 ] - q [ 3 ] * q [ 0 ]) R [ 2 , 2 ] = 1 - 2 * ( q [ 0 ] * q [ 0 ] + q [ 1 ] * q [ 1 ]) obj_points = ( R @ obj_points . T ) . T # Transform to canonical coordintes obj_point in [-1,1]^3 obj_point = obj_points / scale # Compute cell and cell position res = sdf . shape [ 0 ] # assuming same resolution along each axis grid_size = 2.0 / ( res - 1 ) c = torch . floor (( obj_point + 1.0 ) * ( res - 1 ) * 0.5 ) mask = torch . logical_or ( torch . min ( c , dim = 1 )[ 0 ] < 0 , torch . max ( c , dim = 1 )[ 0 ] > res - 2 ) c = torch . clip ( c , 0 , res - 2 ) # base cell index of each point cell_position = c * grid_size - 1.0 # base cell position of each point sdf_indices = c . new_empty (( obj_point . shape [ 0 ], 8 ), dtype = torch . long ) sdf_indices [:, 0 ] = c [:, 0 ] * res ** 2 + c [:, 1 ] * res + c [:, 2 ] sdf_indices [:, 1 ] = c [:, 0 ] * res ** 2 + c [:, 1 ] * res + c [:, 2 ] + 1 sdf_indices [:, 2 ] = c [:, 0 ] * res ** 2 + ( c [:, 1 ] + 1 ) * res + c [:, 2 ] sdf_indices [:, 3 ] = c [:, 0 ] * res ** 2 + ( c [:, 1 ] + 1 ) * res + c [:, 2 ] + 1 sdf_indices [:, 4 ] = ( c [:, 0 ] + 1 ) * res ** 2 + c [:, 1 ] * res + c [:, 2 ] sdf_indices [:, 5 ] = ( c [:, 0 ] + 1 ) * res ** 2 + c [:, 1 ] * res + c [:, 2 ] + 1 sdf_indices [:, 6 ] = ( c [:, 0 ] + 1 ) * res ** 2 + ( c [:, 1 ] + 1 ) * res + c [:, 2 ] sdf_indices [:, 7 ] = ( c [:, 0 ] + 1 ) * res ** 2 + ( c [:, 1 ] + 1 ) * res + c [:, 2 ] + 1 sdf_view = sdf . view ([ - 1 ]) point_cell_position = ( obj_point - cell_position ) / grid_size # [0,1]^3 sdf_values = torch . take ( sdf_view , sdf_indices ) # trilinear interpolation sdf_value = sdf_values . new_empty ( obj_points . shape [ 0 ]) # sdf_value = obj_point[:, 0] sdf_value = ( ( sdf_values [:, 0 ] * ( 1 - point_cell_position [:, 0 ]) + sdf_values [:, 4 ] * point_cell_position [:, 0 ] ) * ( 1 - point_cell_position [:, 1 ]) + ( sdf_values [:, 2 ] * ( 1 - point_cell_position [:, 0 ]) + sdf_values [:, 6 ] * point_cell_position [:, 0 ] ) * point_cell_position [:, 1 ] ) * ( 1 - point_cell_position [:, 2 ]) + ( ( sdf_values [:, 1 ] * ( 1 - point_cell_position [:, 0 ]) + sdf_values [:, 5 ] * point_cell_position [:, 0 ] ) * ( 1 - point_cell_position [:, 1 ]) + ( sdf_values [:, 3 ] * ( 1 - point_cell_position [:, 0 ]) + sdf_values [:, 7 ] * point_cell_position [:, 0 ] ) * point_cell_position [:, 1 ] ) * point_cell_position [ :, 2 ] sdf_value [ mask ] = 0 return sdf_value","title":"pc_loss()"},{"location":"reference/vae/scripts/train/#sdfest.vae.scripts.train.train","text":"train ( config ) Train the model. PARAMETER DESCRIPTION config The training and model config. Source code in sdfest/vae/scripts/train.py 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 def train ( config ): \"\"\"Train the model. Args: config: The training and model config. \"\"\" batch_size = wandb . config . batch_size = config [ \"batch_size\" ] iterations = wandb . config . iterations = config [ \"iterations\" ] learning_rate = wandb . config . learning_rate = config [ \"learning_rate\" ] latent_size = wandb . config . latent_size = config [ \"latent_size\" ] l2_large_weight = wandb . config . l2_large_weight = config [ \"l2_large_weight\" ] l2_small_weight = wandb . config . l2_small_weight = config [ \"l2_small_weight\" ] l1_large_weight = wandb . config . l1_large_weight = config [ \"l1_large_weight\" ] l1_small_weight = wandb . config . l1_small_weight = config [ \"l1_small_weight\" ] kld_weight = wandb . config . kld_weight = config [ \"kld_weight\" ] pc_weight = wandb . config . pc_weight = config [ \"pc_weight\" ] encoder_dict = wandb . config . encoder_dict = config [ \"encoder\" ] decoder_dict = wandb . config . decoder_dict = config [ \"decoder\" ] dataset_path = wandb . config . dataset_path = config [ \"dataset_path\" ] tsdf = wandb . config . tsdf = config [ \"tsdf\" ] # init dataset, dataloader, model and optimizer dataset = SDFDataset ( dataset_path ) data_loader = torch . utils . data . DataLoader ( dataset = dataset , batch_size = batch_size , shuffle = True , drop_last = True ) camera = Camera ( 640 , 480 , 320 , 320 , 320 , 240 , pixel_center = 0.5 ) if \"device\" not in config or config [ \"device\" ] is None : device = torch . device ( \"cuda\" if torch . cuda . is_available () else \"cpu\" ) else : device = torch . device ( config [ \"device\" ]) sdfvae = SDFVAE ( sdf_size = 64 , latent_size = latent_size , encoder_dict = encoder_dict , decoder_dict = decoder_dict , device = device , tsdf = tsdf , ) . to ( device ) print ( str ( sdfvae )) summary ( sdfvae , ( 1 , 1 , 64 , 64 , 64 ), device = device ) optimizer = torch . optim . Adam ( sdfvae . parameters (), lr = learning_rate ) # load checkpoint if provided if \"checkpoint\" in config and config [ \"checkpoint\" ] is not None : # TODO: checkpoint should always go together with model config! model , optimizer , current_iteration , run_name , epoch = utils . load_checkpoint ( config [ \"checkpoint\" ], sdfvae , optimizer ) else : current_iteration = 0 current_epoch = 0 run_name = f \"sdfvae_ { datetime . now () . strftime ( '%Y-%m- %d _%H-%M-%S- %f ' ) } \" # create SummaryWriter for logging and intermediate output writer = torch . utils . tensorboard . SummaryWriter ( f \"runs/\" + run_name ) model_base_path = os . path . join ( os . getcwd (), \"models\" , run_name ) program_starts = time . time () warm_up_iterations = 1000 stop = False while current_iteration <= iterations : current_epoch += 1 for sdf_volumes in data_loader : sdf_volumes = sdf_volumes . to ( device ) # train first N iters with SDF instead of TSDF to stabilize early training if current_iteration > warm_up_iterations : sdfvae . prepare_input ( sdf_volumes ) recon_sdf_volumes , mean , log_var , z = sdfvae ( sdf_volumes , enforce_tsdf = False ) if tsdf is not False and current_iteration > warm_up_iterations : # during training, clamp the reconstructed SDF only where both target # and output are outside the TSDF range mask = torch . logical_and ( torch . abs ( sdf_volumes ) >= tsdf , torch . abs ( recon_sdf_volumes ) >= tsdf ) recon_sdf_volumes_temp = recon_sdf_volumes recon_sdf_volumes = recon_sdf_volumes_temp . clone () recon_sdf_volumes [ mask ] = recon_sdf_volumes_temp [ mask ] . clamp ( - tsdf , tsdf ) # Compute losses, use negative log-likelihood here # Note: for average negative log-likelihood all of these have to be divided # by the batch size. Probably would be better to keep losses comparable for # varying batch size. l1_error = torch . abs ( recon_sdf_volumes - sdf_volumes ) l2_error = l1_error ** 2 loss_l2_small = torch . sum ( l2_error [ torch . abs ( sdf_volumes ) < 0.1 ]) loss_l2_large = torch . sum ( l2_error [ torch . abs ( sdf_volumes ) >= 0.1 ]) loss_l1_small = torch . sum ( l1_error [ torch . abs ( sdf_volumes ) < 0.1 ]) loss_l1_large = torch . sum ( l1_error [ torch . abs ( sdf_volumes ) >= 0.1 ]) loss_pc = 0 depth_images = torch . empty (( batch_size , 480 , 640 ), device = device ) pointclouds = [] # compute point clouds from sdf volumes at zero crossings for recon_sdf_volume , sdf_volume , depth_image in zip ( recon_sdf_volumes , sdf_volumes , depth_images ): with torch . no_grad (): import random import math u1 , u2 , u3 = random . random (), random . random (), random . random () q = ( torch . tensor ( [ math . sqrt ( 1 - u1 ) * math . sin ( 2 * math . pi * u2 ), math . sqrt ( 1 - u1 ) * math . cos ( 2 * math . pi * u2 ), math . sqrt ( u1 ) * math . sin ( 2 * math . pi * u3 ), math . sqrt ( u1 ) * math . cos ( 2 * math . pi * u3 ), ] ) . unsqueeze ( 0 ) . to ( device ) ) s = torch . Tensor ([ 1.0 ]) . to ( device ) p = torch . Tensor ([ 0.0 , 0.0 , - 5.0 ]) . to ( device ) depth_image = render_depth_gpu ( sdf_volume [ 0 ], p , q , s , threshold = 0.01 , camera = camera , ) pointcloud = pointset_utils . depth_to_pointcloud ( depth_image , camera ) loss_pc = loss_pc + torch . sum ( pc_loss ( pointcloud , p , q [ 0 ], s , recon_sdf_volume [ 0 ]) ** 2 ) loss_kld = - 0.5 * torch . sum ( 1 + log_var - mean . pow ( 2 ) - log_var . exp ()) loss = ( l2_small_weight * loss_l2_small + l2_large_weight * loss_l2_large + l1_small_weight * loss_l1_small + l1_large_weight * loss_l1_large + pc_weight * loss_pc + loss_kld * ( kld_weight if current_iteration > warm_up_iterations else 0 ) ) print ( f \"Iteration { current_iteration } , epoch { current_epoch } , loss { loss } \" ) optimizer . zero_grad () loss . backward () optimizer . step () writer . add_scalar ( \"loss\" , loss . item (), current_iteration ) writer . add_scalar ( \"loss l2 small\" , loss_l2_small . item (), current_iteration ) writer . add_scalar ( \"loss l2 large\" , loss_l2_large . item (), current_iteration ) writer . add_scalar ( \"loss l1 small\" , loss_l1_small . item (), current_iteration ) writer . add_scalar ( \"loss l1 large\" , loss_l1_large . item (), current_iteration ) writer . add_scalar ( \"loss pc\" , loss_pc . item (), current_iteration ) writer . add_scalar ( \"loss kl\" , loss_kld , current_iteration ) wandb . log ( { \"total loss\" : loss . item (), \"loss l2 small\" : loss_l2_small . item (), \"loss l2 large\" : loss_l2_large . item (), \"loss l1 small\" : loss_l1_small . item (), \"loss l1 large\" : loss_l1_large . item (), \"loss kl\" : loss_kld . item (), \"loss pc\" : loss_pc . item (), }, step = current_iteration , ) current_iteration += 1 with torch . no_grad (): if current_iteration % 1000 == 0 : # show one reconstruction mean = mean [ 0 ] . cpu () figure = sdf_utils . visualize_sdf_reconstruction ( sdf_volumes [ 0 , 0 ] . cpu () . numpy (), recon_sdf_volumes [ 0 , 0 ] . cpu () . numpy (), ) if figure . gca () . has_data (): writer . add_figure ( tag = \"reconstruction\" , figure = figure , global_step = current_iteration , ) wandb . log ({ \"reconstruction\" : figure }, step = current_iteration ) # generate 8 samples from the prior output , _ = sdfvae . inference ( n = 8 , enforce_tsdf = True ) figure = sdf_utils . visualize_sdf_batch ( output . squeeze () . cpu () . numpy () ) if figure . gca () . has_data (): writer . add_figure ( tag = \"samples from prior\" , figure = figure , global_step = current_iteration , ) wandb . log ( { \"samples from prior\" : figure }, step = current_iteration ) # generate 4 samples from prior output , _ = sdfvae . inference ( n = 4 , enforce_tsdf = True ) figure = sdf_utils . visualize_sdf_batch_columns ( output . squeeze () . cpu () . numpy () ) if figure . gca () . has_data (): writer . add_figure ( tag = \"samples from prior, columns\" , figure = figure , global_step = current_iteration , ) wandb . log ( { \"samples from prior, columns\" : figure }, step = current_iteration , ) # if current_iteration % 10000 == 0: # os.makedirs(model_base_path, exist_ok=True) # checkpoint_path = os.path.join(model_base_path, F\"{current_iteration}.ckp\") # utils.save_checkpoint( # path=checkpoint_path, # model=sdfvae, # optimizer=optimizer, # iteration=current_iteration, # run_name=run_name) if current_iteration > iterations : break if current_iteration > iterations : break now = time . time () print ( \"It has been {0} seconds since the loop started\" . format ( now - program_starts )) # save the final model torch . save ( sdfvae . state_dict (), os . path . join ( wandb . run . dir , f \" { wandb . run . name } .pt\" )) config_path = os . path . join ( wandb . run . dir , f \" { wandb . run . name } .yaml\" ) config [ \"model\" ] = os . path . join ( \".\" , f \" { wandb . run . name } .pt\" ) yoco . save_config_to_file ( config_path , config )","title":"train()"},{"location":"reference/vae/scripts/train/#sdfest.vae.scripts.train.main","text":"main () Entry point of the program. Source code in sdfest/vae/scripts/train.py 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 def main (): \"\"\"Entry point of the program.\"\"\" # define the arguments parser = argparse . ArgumentParser ( description = \"Training script for SDFVAE.\" ) # parse arguments parser . add_argument ( \"--checkpoint\" ) parser . add_argument ( \"--dataset_path\" , required = True ) parser . add_argument ( \"--batch_size\" , type = lambda x : int ( float ( x ))) parser . add_argument ( \"--iterations\" , type = lambda x : int ( float ( x ))) parser . add_argument ( \"--latent_size\" , type = lambda x : int ( float ( x ))) parser . add_argument ( \"--tsdf\" , type = utils . str_to_tsdf , default = False ) parser . add_argument ( \"--kld_weight\" , type = lambda x : float ( x )) parser . add_argument ( \"--l2_large_weight\" , type = lambda x : float ( x )) parser . add_argument ( \"--l2_small_weight\" , type = lambda x : float ( x )) parser . add_argument ( \"--l1_large_weight\" , type = lambda x : float ( x )) parser . add_argument ( \"--l1_small_weight\" , type = lambda x : float ( x )) parser . add_argument ( \"--pc_weight\" , type = lambda x : float ( x )) parser . add_argument ( \"--sign_weight\" , type = lambda x : float ( x )) parser . add_argument ( \"--bce_weight\" , type = lambda x : float ( x )) parser . add_argument ( \"--learning_rate\" , type = lambda x : float ( x )) parser . add_argument ( \"--device\" ) parser . add_argument ( \"--config\" , default = \"configs/default.yaml\" , nargs = \"+\" ) config = yoco . load_config_from_args ( parser , search_paths = [ \".\" , \"~/.sdfest/\" , sdfest . __path__ [ 0 ]] ) train ( config )","title":"main()"},{"location":"reference/vae/scripts/visualizer/","text":"sdfest.vae.scripts.visualizer Application to explore and visualize VAE. VAEVisualizer Bases: QDialog User interface to explore VAE model. Source code in sdfest/vae/scripts/visualizer.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 class VAEVisualizer ( QDialog ): \"\"\"User interface to explore VAE model.\"\"\" LATENT_ABS_MAX = 4 LATENT_TICKS = 100 def __init__ ( self , parent = None ): super ( VAEVisualizer , self ) . __init__ ( parent ) # define attributes self . _single_sdf_input = None self . _single_sdf_latent = None self . _single_sdf_output = None self . _transition_sdf_first = None self . _transition_sdf_first_latent = None self . _transition_sdf_first_out = None self . _transition_sdf_second = None self . _transition_sdf_second_latent = None self . _transition_sdf_second_out = None self . _transition_sdf_out = None self . _config = None self . _state_dict = None self . _model = None self . _latent_sliders = [] self . _isosurface_level = 0 # split layout into two halfs # (left for loading model, right is for visualization) layout = QHBoxLayout () model_layout = QVBoxLayout () # Load model button load_model_button = QPushButton ( \"Load model/config\" ) load_model_button . clicked . connect ( self . load_model ) load_model_button . setSizePolicy ( QSizePolicy . Policy . Expanding , QSizePolicy . Policy . Maximum ) model_layout . addWidget ( load_model_button ) # Confg line edit self . _config_line_edit = QLineEdit () self . _config_line_edit . textChanged . connect ( self . config_changed ) self . _config_line_edit . setSizePolicy ( QSizePolicy . Policy . Expanding , QSizePolicy . Policy . Maximum ) model_layout . addWidget ( self . _config_line_edit ) # Isosurface leve spin box self . _iso_level_spinbox = QDoubleSpinBox () self . _iso_level_spinbox . setDecimals ( 2 ) self . _iso_level_spinbox . setSingleStep ( 0.01 ) self . _iso_level_spinbox . valueChanged . connect ( self . update_isosurface_level ) model_layout . addWidget ( self . _iso_level_spinbox ) # Model status self . _model_status_label = QLabel () model_layout . addWidget ( self . _model_status_label ) # Current config scroll_area = QScrollArea ( widgetResizable = True ) scroll_area . setSizePolicy ( QSizePolicy . Policy . Expanding , QSizePolicy . Policy . Expanding ) scroll_area . setMinimumWidth ( 270 ) self . _config_label = QLabel () self . _config_label . setAlignment ( QtCore . Qt . AlignTop ) scroll_area . setWidget ( self . _config_label ) model_layout . addWidget ( scroll_area ) # Single object explorer single_object_widget = QWidget () single_object_widget_layout = QVBoxLayout () single_object_widget_layout . setAlignment ( QtCore . Qt . AlignTop ) single_object_widget_bottom = QWidget () single_object_widget_bottom_layout = QHBoxLayout () single_object_save_buttons_widget = QWidget () single_object_save_buttons_layout = QHBoxLayout () self . slider_group_widget = QWidget () self . slider_group_widget_layout = QVBoxLayout () load_sdf_button = QPushButton ( \"Load SDF\" ) load_sdf_button . clicked . connect ( self . load_single_sdf ) generate_sdf_button = QPushButton ( \"Generate SDF from prior\" ) generate_sdf_button . clicked . connect ( self . generate_sdf ) save_figure_button = QPushButton ( \"Save figure\" ) save_figure_button . clicked . connect ( self . save_figure ) save_mesh_button = QPushButton ( \"Save mesh\" ) save_mesh_button . clicked . connect ( self . save_mesh ) save_sdf_button = QPushButton ( \"Save sdf\" ) save_sdf_button . clicked . connect ( self . save_sdf ) self . single_object_figure = Figure ( dpi = 85 , facecolor = ( 1 , 1 , 1 ), edgecolor = ( 0 , 0 , 0 ), tight_layout = True ) single_object_canvas = FigureCanvas ( self . single_object_figure ) single_object_canvas . setSizePolicy ( QSizePolicy . Expanding , QSizePolicy . Expanding ) self . scroll_area_sliders = QScrollArea ( widgetResizable = True ) # widgetResizable indicates that the widget insided can be resized self . scroll_area_sliders . setFixedWidth ( 130 ) self . slider_group_widget . setLayout ( self . slider_group_widget_layout ) self . scroll_area_sliders . setWidget ( self . slider_group_widget ) self . scroll_area_sliders . show () # Single object bottom single_object_widget_bottom_layout . addWidget ( single_object_canvas ) single_object_widget_bottom_layout . addWidget ( self . scroll_area_sliders ) single_object_widget_bottom . setLayout ( single_object_widget_bottom_layout ) # Single object save buttons single_object_save_buttons_layout . addWidget ( save_figure_button ) single_object_save_buttons_layout . addWidget ( save_mesh_button ) single_object_save_buttons_layout . addWidget ( save_sdf_button ) single_object_save_buttons_widget . setLayout ( single_object_save_buttons_layout ) single_object_save_buttons_layout . setMargin ( 0 ) # Single object tab single_object_widget_layout . addWidget ( load_sdf_button ) single_object_widget_layout . addWidget ( generate_sdf_button ) single_object_widget_layout . addWidget ( single_object_save_buttons_widget ) single_object_widget_layout . addWidget ( single_object_widget_bottom ) single_object_widget . setLayout ( single_object_widget_layout ) # Transition widget transition_widget = QWidget () transition_widget_layout = QVBoxLayout () transition_widget_layout . setAlignment ( QtCore . Qt . AlignTop ) transition_widget_button_layout = QHBoxLayout () load_first_sdf_button = QPushButton ( \"Load SDF 1\" ) load_first_sdf_button . clicked . connect ( lambda : self . load_transition_sdf ( \"first\" )) load_second_sdf_button = QPushButton ( \"Load SDF 2\" ) load_second_sdf_button . clicked . connect ( lambda : self . load_transition_sdf ( \"second\" ) ) self . transition_figure = Figure ( dpi = 85 , facecolor = ( 1 , 1 , 1 ), edgecolor = ( 0 , 0 , 0 ) ) transition_canvas = FigureCanvas ( self . transition_figure ) transition_canvas . setSizePolicy ( QSizePolicy . Expanding , QSizePolicy . Expanding ) self . transition_figure_axes = self . transition_figure . subplots ( 2 , 3 ) self . transition_figure . delaxes ( self . transition_figure_axes [ 0 , 1 ]) self . transition_slider = QSlider ( QtCore . Qt . Horizontal ) self . transition_slider . setMinimum ( 0 ) self . transition_slider . setMaximum ( 100 ) self . transition_slider . valueChanged . connect ( self . update_transition_slider ) transition_widget_button_layout . addWidget ( load_first_sdf_button ) transition_widget_button_layout . addWidget ( load_second_sdf_button ) transition_widget_layout . addLayout ( transition_widget_button_layout ) transition_widget_layout . addWidget ( transition_canvas ) transition_widget_layout . addWidget ( self . transition_slider ) transition_widget . setLayout ( transition_widget_layout ) # Animation widget animation_widget = QWidget () animation_widget_layout = QVBoxLayout () animation_widget_layout . setAlignment ( QtCore . Qt . AlignTop ) add_kf_from_single_view_button = QPushButton ( \"Add from single view\" ) add_kf_from_single_view_button . clicked . connect ( lambda : self . add_kf_from_single_view () ) delete_kf_button = QPushButton ( \"Delete keyframe\" ) delete_kf_button . clicked . connect ( lambda : self . delete_kf ()) generate_animation_button = QPushButton ( \"Generate animation\" ) generate_animation_button . clicked . connect ( lambda : self . generate_animation ()) self . _keyframe_list = QListWidget () animation_widget . setLayout ( animation_widget_layout ) animation_widget_layout . addWidget ( add_kf_from_single_view_button ) animation_widget_layout . addWidget ( delete_kf_button ) animation_widget_layout . addWidget ( self . _keyframe_list ) animation_widget_layout . addWidget ( generate_animation_button ) # Tab widget tab_widget = QTabWidget () tab_widget . addTab ( single_object_widget , \"Single SDF\" ) tab_widget . addTab ( transition_widget , \"Transition\" ) tab_widget . addTab ( animation_widget , \"Animation\" ) layout . addLayout ( model_layout ) layout . addWidget ( tab_widget ) layout . setStretch ( 0 , 0 ) layout . setStretch ( 1 , 2 ) self . setLayout ( layout ) self . parse_config () self . update_model () # TODO make these adjustable from GUI self . transform = np . array ( [ [ 0 , - 1 , 0 , 0 ], [ 0 , 0 , 1 , 0 ], [ - 1 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 1 ], ] ) self . azimuth = 0 self . polar_angle = - np . pi / 4 def reset_output ( self ) -> None : self . _single_sdf_latent = None self . _single_sdf_output = None self . _transition_sdf_first_latent = None self . _transition_sdf_first_out = None self . _transition_sdf_second_latent = None self . _transition_sdf_second_out = None def add_kf_from_single_view ( self ) -> None : \"\"\"Add keyframe based on current latent in Single SDF tab.\"\"\" if self . _single_sdf_latent is not None : item = QListWidgetItem () item . setText ( str ( self . _single_sdf_latent )) item . setData ( QtCore . Qt . UserRole , self . _single_sdf_latent ) self . _keyframe_list . addItem ( item ) else : print ( \"No valid latent currently set in Single SDF tab\" ) def delete_kf ( self ) -> None : \"\"\"Delete currently selected keyframe.\"\"\" row = self . _keyframe_list . currentRow () self . _keyframe_list . takeItem ( row ) def generate_animation ( self ) -> None : \"\"\"Generate animation based on keyframes.\"\"\" fps = 30 distance_per_frame = 0.1 loop = True keyframes = [] for i in range ( self . _keyframe_list . count ()): keyframes . append ( self . _keyframe_list . item ( i ) . data ( QtCore . Qt . UserRole )) if loop : keyframes . append ( keyframes [ 0 ]) # generate frames with torch . no_grad (): fig = Figure ( dpi = 85 , facecolor = ( 1 , 1 , 1 ), edgecolor = ( 0 , 0 , 0 )) now = datetime . datetime . now () folder = now . strftime ( \"%Y-%m- %d _%H-%M-%S\" ) os . makedirs ( folder ) frame_number = 0 current = keyframes . pop ( 0 ) . clone () remaining_distance = distance_per_frame while keyframes : next_target = keyframes [ 0 ] delta = next_target - current distance = torch . linalg . norm ( delta ) direction = delta / torch . linalg . norm ( delta ) if distance > distance_per_frame : current += direction * distance_per_frame remaining_distance = 0 else : current = keyframes . pop ( 0 ) . clone () remaining_distance -= distance if remaining_distance <= 0 : # generate frame sdf = self . _model . decode ( current ) mesh = sdf_utils . mesh_from_sdf ( sdf [ 0 , 0 ] . cpu () . numpy (), self . _isosurface_level , complete_mesh = True , ) if mesh is None : print ( \"No surface boundaries in SDF.\" ) else : ax = fig . subplots ( 1 , 1 ) sdf_utils . plot_mesh ( mesh , plot_object = ax , transform = self . transform , azimuth = self . azimuth , polar_angle = self . polar_angle , ) ax . xaxis . set_major_locator ( NullLocator ()) ax . yaxis . set_major_locator ( NullLocator ()) fig . savefig ( os . path . join ( folder , f \" { frame_number : 05d } .png\" ), dpi = 200 ) frame_number += 1 fig . clear () # convert frames to video video_name = os . path . join ( f \" { folder } .mp4\" ) ffmpeg . input ( os . path . join ( folder , \"*.png\" ), pattern_type = \"glob\" , framerate = fps ) . output ( video_name ) . run () def update_model ( self ): if self . _config is None : return try : self . _model = SDFVAE ( sdf_size = 64 , latent_size = self . _config [ \"latent_size\" ], encoder_dict = self . _config [ \"encoder\" ], decoder_dict = self . _config [ \"decoder\" ], device = \"cpu\" , tsdf = self . _config [ \"tsdf\" ], ) if self . _state_dict is not None : self . _model . load_state_dict ( self . _state_dict ) self . _model_status_label . setText ( \"Model (with weights) successfully loaded.\" ) else : self . _model_status_label . setText ( \"Model (no weights) successfully loaded.\" ) self . _model_status_label . setStyleSheet ( \"color: green\" ) self . reset_output () except RuntimeError : self . _model_status_label . setText ( \"Model config and weights do not match.\" ) self . _model_status_label . setStyleSheet ( \"color: red\" ) self . _config_label . setText ( \"\" ) self . _model = None except KeyError : self . _model_status_label . setText ( \"Model config incomplete.\" ) self . _model_status_label . setStyleSheet ( \"color: red\" ) self . _config_label . setText ( \"\" ) self . _model = None self . update_single () self . update_transition () def load_sdf ( self ): path , _ = QFileDialog . getOpenFileName ( self , \"Open file\" , filter = \"*.npy\" ) if path == \"\" : return None try : sdf_np = np . load ( path ) return torch . as_tensor ( sdf_np ) . unsqueeze ( 0 ) . unsqueeze ( 0 ) except FileNotFoundError : print ( \"Error loading SDF.\" ) return None def parse_config ( self ): try : arg_list = self . _config_line_edit . text () . split ( \" \" ) arg_list = [ x for x in arg_list if x != \"\" ] parser = ThrowingArgumentParser () parser . add_argument ( \"--tsdf\" , type = utils . str_to_tsdf , default = False ) parser . add_argument ( \"--latent_size\" , type = lambda x : int ( float ( x ))) parser . add_argument ( \"--config\" , default = \"configs/default.yaml\" , nargs = \"+\" ) self . _config = yoco . load_config_from_args ( parser , arg_list ) self . _config_label . setText ( json . dumps ( self . _config , indent = 4 )) except ArgumentParserError : self . _model_status_label . setText ( \"Error in config string.\" ) self . _model_status_label . setStyleSheet ( \"color: red\" ) self . _config_label . setText ( \"\" ) self . _model = None self . _config = None except ( FileNotFoundError , IsADirectoryError ): self . _model_status_label . setText ( \"Specified config file not found.\" ) self . _model_status_label . setStyleSheet ( \"color: red\" ) self . _config_label . setText ( \"\" ) self . _model = None self . _config = None except RuntimeError : self . _model_status_label . setText ( \"Model config and weights do not match.\" ) self . _model_status_label . setStyleSheet ( \"color: red\" ) self . _config_label . setText ( \"\" ) self . _model = None self . _config = None def load_transition_sdf ( self , which ): sdf = self . load_sdf () if which == \"first\" : self . _transition_sdf_first = sdf self . _transition_sdf_first_out = None self . _transition_sdf_first_latent = None elif which == \"second\" : self . _transition_sdf_second = sdf self . _transition_sdf_second_out = None self . _transition_sdf_second_latent = None else : raise ValueError ( \"which must be either 'first' or 'second'.\" ) self . update_transition () def update_single ( self , evaluate = True ): # Visualize input sdf self . single_object_figure . clear () if self . _single_sdf_input is not None : ax1 , ax2 = self . single_object_figure . subplots ( 1 , 2 ) self . render_sdf ( ax1 , self . _single_sdf_input [ 0 , 0 ] . numpy ()) else : ax2 = self . single_object_figure . subplots ( 1 , 1 ) if self . _model is not None : with torch . no_grad (): if ( self . _single_sdf_input is not None and self . _single_sdf_latent is None ): inp = self . _single_sdf_input . clone () self . _model . prepare_input ( inp ) self . _single_sdf_latent , _ , _ = self . _model . encode ( inp ) self . update_sliders () if self . _single_sdf_latent is not None : if evaluate : t1 = time . time () self . _single_sdf_output = self . _model . decode ( self . _single_sdf_latent , enforce_tsdf = True ) print ( f \"Decoding took { time . time () - t1 } s\" ) t1 = time . time () self . render_sdf ( ax2 , self . _single_sdf_output [ 0 , 0 ] . detach () . numpy ()) print ( f \"Rendering took { time . time () - t1 } s\" ) self . single_object_figure . canvas . draw () def update_sliders ( self ): for current_slider in self . _latent_sliders : self . slider_group_widget_layout . removeWidget ( current_slider ) current_slider . deleteLater () self . _latent_sliders = [] for latent in self . _single_sdf_latent . view ( - 1 ) . numpy (): slider = QSlider ( QtCore . Qt . Horizontal ) slider . setMinimum ( - self . LATENT_ABS_MAX * self . LATENT_TICKS / 2.0 ) slider . setMaximum ( + self . LATENT_ABS_MAX * self . LATENT_TICKS / 2.0 ) slider . setValue ( latent * self . LATENT_TICKS / 2.0 ) slider . valueChanged . connect ( self . update_latent ) self . _latent_sliders . append ( slider ) self . slider_group_widget_layout . addWidget ( slider ) def update_latent ( self ): for i , current_slider in enumerate ( self . _latent_sliders ): self . _single_sdf_latent [ 0 , i ] = ( current_slider . value () * 2.0 / self . LATENT_TICKS ) self . update_single () def update_transition_slider ( self ): self . update_transition () def update_transition ( self , update_iso_level_only = False ): if update_iso_level_only : if self . _transition_sdf_first_out is not None : self . render_sdf ( self . transition_figure_axes [ 1 , 0 ], self . _transition_sdf_first_out [ 0 , 0 ] . detach () . numpy (), ) if self . _transition_sdf_second_out is not None : self . render_sdf ( self . transition_figure_axes [ 1 , 2 ], self . _transition_sdf_second_out [ 0 , 0 ] . detach () . numpy (), ) if self . _transition_sdf_out is not None : self . render_sdf ( self . transition_figure_axes [ 1 , 1 ], self . _transition_sdf_out [ 0 , 0 ] . detach () . numpy (), ) self . transition_figure . canvas . draw () return with torch . no_grad (): if ( self . _transition_sdf_first is not None and self . _transition_sdf_first_out is None ): print ( \"render first sdf\" ) self . render_sdf ( self . transition_figure_axes [ 0 , 0 ], self . _transition_sdf_first [ 0 , 0 ] . detach () . numpy (), ) if ( self . _model is not None and self . _transition_sdf_first_latent is None ): inp = self . _transition_sdf_first . clone () self . _model . prepare_input ( inp ) self . _transition_sdf_first_latent , _ , _ = self . _model . encode ( inp ) self . _transition_sdf_first_out = self . _model . decode ( self . _transition_sdf_first_latent ) self . render_sdf ( self . transition_figure_axes [ 1 , 0 ], self . _transition_sdf_first_out [ 0 , 0 ] . detach () . numpy (), ) if ( self . _transition_sdf_second is not None and self . _transition_sdf_second_out is None ): self . render_sdf ( self . transition_figure_axes [ 0 , 2 ], self . _transition_sdf_second [ 0 , 0 ] . detach () . numpy (), ) if ( self . _model is not None and self . _transition_sdf_second_latent is None ): inp = self . _transition_sdf_second . clone () self . _model . prepare_input ( inp ) self . _transition_sdf_second_latent , _ , _ = self . _model . encode ( inp ) self . _transition_sdf_second_out = self . _model . decode ( self . _transition_sdf_second_latent ) self . render_sdf ( self . transition_figure_axes [ 1 , 2 ], self . _transition_sdf_second_out [ 0 , 0 ] . detach () . numpy (), ) if ( self . _transition_sdf_first_latent is not None and self . _transition_sdf_second_latent is not None ): t = self . transition_slider . value () / 100.0 interpolated_latent = ( 1 - t ) * self . _transition_sdf_first_latent + ( t ) * self . _transition_sdf_second_latent self . _transition_sdf_out = self . _model . decode ( interpolated_latent ) self . render_sdf ( self . transition_figure_axes [ 1 , 1 ], self . _transition_sdf_out [ 0 , 0 ] . detach () . numpy (), ) self . transition_figure . canvas . draw () def load_single_sdf ( self ): sdf = self . load_sdf () self . _single_sdf_input = sdf self . _single_sdf_latent = None self . update_single () def generate_sdf ( self ): self . _single_sdf_input = None if self . _model is not None : self . _single_sdf_latent = self . _model . sample ( 1 ) * 0.5 self . update_single () self . update_sliders () else : print ( \"Can't generate without loaded model.\" ) def save_figure ( self ): \"\"\"Save current single sample figure.\"\"\" now = datetime . datetime . now () filename = now . strftime ( \"%Y-%m- %d _%H-%M-%S.png\" ) self . single_object_figure . savefig ( filename , pad_inches = 0 , dpi = 200 ) print ( f \"Saved as { filename } \" ) def save_mesh ( self ): \"\"\"Save current object mesh.\"\"\" if self . _single_sdf_output is None : print ( \"Can't save mesh without generated SDF.\" ) return sdf = self . _single_sdf_output [ 0 , 0 ] . detach () . numpy () mesh = sdf_utils . mesh_from_sdf ( sdf , self . _isosurface_level , complete_mesh = True ) mesh_filename = ( f \"mesh_ { datetime . datetime . now () . strftime ( '%Y-%m- %d _%H-%M-%S- %f ' ) } .obj\" ) mesh_path = os . path . join ( os . getcwd (), mesh_filename ) with open ( mesh_path , \"w\" ) as f : f . write ( trimesh . exchange . obj . export_obj ( mesh )) def save_sdf ( self ): \"\"\"Save current object sdf.\"\"\" if self . _single_sdf_output is None : print ( \"Can't save SDF without generated SDF.\" ) return sdf = self . _single_sdf_output [ 0 , 0 ] . detach () . numpy () sdf_filename = ( f \"sdf_ { datetime . datetime . now () . strftime ( '%Y-%m- %d _%H-%M-%S- %f ' ) } .npy\" ) sdf_path = os . path . join ( os . getcwd (), sdf_filename ) np . save ( sdf_path , sdf ) def config_changed ( self , string ): self . parse_config () self . update_model () def load_model ( self ): path , _ = QFileDialog . getOpenFileName ( self , \"Open file\" ) if path . endswith ( \".yml\" ) or path . endswith ( \".yaml\" ): self . _config_line_edit . setText ( f \"--config { path } \" ) self . parse_config () print ( self . _config ) if \"model\" in self . _config : self . _state_dict = torch . load ( self . _config [ \"model\" ], map_location = \"cpu\" ) self . update_model () elif path . endswith ( \".pt\" ): self . _state_dict = torch . load ( path , map_location = \"cpu\" ) self . update_model () else : return def render_sdf ( self , ax , sdf ): mesh = sdf_utils . mesh_from_sdf ( sdf , self . _isosurface_level , complete_mesh = True ) if mesh is None : print ( \"No surface boundaries in SDF.\" ) else : sdf_utils . plot_mesh ( mesh , plot_object = ax , transform = self . transform , azimuth = self . azimuth , polar_angle = self . polar_angle , ) def update_isosurface_level ( self , d ): self . _isosurface_level = d self . update_single ( False ) self . update_transition ( True ) add_kf_from_single_view add_kf_from_single_view () -> None Add keyframe based on current latent in Single SDF tab. Source code in sdfest/vae/scripts/visualizer.py 276 277 278 279 280 281 282 283 284 def add_kf_from_single_view ( self ) -> None : \"\"\"Add keyframe based on current latent in Single SDF tab.\"\"\" if self . _single_sdf_latent is not None : item = QListWidgetItem () item . setText ( str ( self . _single_sdf_latent )) item . setData ( QtCore . Qt . UserRole , self . _single_sdf_latent ) self . _keyframe_list . addItem ( item ) else : print ( \"No valid latent currently set in Single SDF tab\" ) delete_kf delete_kf () -> None Delete currently selected keyframe. Source code in sdfest/vae/scripts/visualizer.py 286 287 288 289 def delete_kf ( self ) -> None : \"\"\"Delete currently selected keyframe.\"\"\" row = self . _keyframe_list . currentRow () self . _keyframe_list . takeItem ( row ) generate_animation generate_animation () -> None Generate animation based on keyframes. Source code in sdfest/vae/scripts/visualizer.py 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 def generate_animation ( self ) -> None : \"\"\"Generate animation based on keyframes.\"\"\" fps = 30 distance_per_frame = 0.1 loop = True keyframes = [] for i in range ( self . _keyframe_list . count ()): keyframes . append ( self . _keyframe_list . item ( i ) . data ( QtCore . Qt . UserRole )) if loop : keyframes . append ( keyframes [ 0 ]) # generate frames with torch . no_grad (): fig = Figure ( dpi = 85 , facecolor = ( 1 , 1 , 1 ), edgecolor = ( 0 , 0 , 0 )) now = datetime . datetime . now () folder = now . strftime ( \"%Y-%m- %d _%H-%M-%S\" ) os . makedirs ( folder ) frame_number = 0 current = keyframes . pop ( 0 ) . clone () remaining_distance = distance_per_frame while keyframes : next_target = keyframes [ 0 ] delta = next_target - current distance = torch . linalg . norm ( delta ) direction = delta / torch . linalg . norm ( delta ) if distance > distance_per_frame : current += direction * distance_per_frame remaining_distance = 0 else : current = keyframes . pop ( 0 ) . clone () remaining_distance -= distance if remaining_distance <= 0 : # generate frame sdf = self . _model . decode ( current ) mesh = sdf_utils . mesh_from_sdf ( sdf [ 0 , 0 ] . cpu () . numpy (), self . _isosurface_level , complete_mesh = True , ) if mesh is None : print ( \"No surface boundaries in SDF.\" ) else : ax = fig . subplots ( 1 , 1 ) sdf_utils . plot_mesh ( mesh , plot_object = ax , transform = self . transform , azimuth = self . azimuth , polar_angle = self . polar_angle , ) ax . xaxis . set_major_locator ( NullLocator ()) ax . yaxis . set_major_locator ( NullLocator ()) fig . savefig ( os . path . join ( folder , f \" { frame_number : 05d } .png\" ), dpi = 200 ) frame_number += 1 fig . clear () # convert frames to video video_name = os . path . join ( f \" { folder } .mp4\" ) ffmpeg . input ( os . path . join ( folder , \"*.png\" ), pattern_type = \"glob\" , framerate = fps ) . output ( video_name ) . run () save_figure save_figure () Save current single sample figure. Source code in sdfest/vae/scripts/visualizer.py 602 603 604 605 606 607 def save_figure ( self ): \"\"\"Save current single sample figure.\"\"\" now = datetime . datetime . now () filename = now . strftime ( \"%Y-%m- %d _%H-%M-%S.png\" ) self . single_object_figure . savefig ( filename , pad_inches = 0 , dpi = 200 ) print ( f \"Saved as { filename } \" ) save_mesh save_mesh () Save current object mesh. Source code in sdfest/vae/scripts/visualizer.py 609 610 611 612 613 614 615 616 617 618 619 620 621 def save_mesh ( self ): \"\"\"Save current object mesh.\"\"\" if self . _single_sdf_output is None : print ( \"Can't save mesh without generated SDF.\" ) return sdf = self . _single_sdf_output [ 0 , 0 ] . detach () . numpy () mesh = sdf_utils . mesh_from_sdf ( sdf , self . _isosurface_level , complete_mesh = True ) mesh_filename = ( f \"mesh_ { datetime . datetime . now () . strftime ( '%Y-%m- %d _%H-%M-%S- %f ' ) } .obj\" ) mesh_path = os . path . join ( os . getcwd (), mesh_filename ) with open ( mesh_path , \"w\" ) as f : f . write ( trimesh . exchange . obj . export_obj ( mesh )) save_sdf save_sdf () Save current object sdf. Source code in sdfest/vae/scripts/visualizer.py 623 624 625 626 627 628 629 630 631 632 633 def save_sdf ( self ): \"\"\"Save current object sdf.\"\"\" if self . _single_sdf_output is None : print ( \"Can't save SDF without generated SDF.\" ) return sdf = self . _single_sdf_output [ 0 , 0 ] . detach () . numpy () sdf_filename = ( f \"sdf_ { datetime . datetime . now () . strftime ( '%Y-%m- %d _%H-%M-%S- %f ' ) } .npy\" ) sdf_path = os . path . join ( os . getcwd (), sdf_filename ) np . save ( sdf_path , sdf ) main main () Start the UI. Source code in sdfest/vae/scripts/visualizer.py 673 674 675 676 677 678 679 680 def main (): \"\"\"Start the UI.\"\"\" app = QApplication ( sys . argv ) gui = VAEVisualizer () gui . show () app . exec_ ()","title":"visualizer"},{"location":"reference/vae/scripts/visualizer/#sdfest.vae.scripts.visualizer","text":"Application to explore and visualize VAE.","title":"visualizer"},{"location":"reference/vae/scripts/visualizer/#sdfest.vae.scripts.visualizer.VAEVisualizer","text":"Bases: QDialog User interface to explore VAE model. Source code in sdfest/vae/scripts/visualizer.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 class VAEVisualizer ( QDialog ): \"\"\"User interface to explore VAE model.\"\"\" LATENT_ABS_MAX = 4 LATENT_TICKS = 100 def __init__ ( self , parent = None ): super ( VAEVisualizer , self ) . __init__ ( parent ) # define attributes self . _single_sdf_input = None self . _single_sdf_latent = None self . _single_sdf_output = None self . _transition_sdf_first = None self . _transition_sdf_first_latent = None self . _transition_sdf_first_out = None self . _transition_sdf_second = None self . _transition_sdf_second_latent = None self . _transition_sdf_second_out = None self . _transition_sdf_out = None self . _config = None self . _state_dict = None self . _model = None self . _latent_sliders = [] self . _isosurface_level = 0 # split layout into two halfs # (left for loading model, right is for visualization) layout = QHBoxLayout () model_layout = QVBoxLayout () # Load model button load_model_button = QPushButton ( \"Load model/config\" ) load_model_button . clicked . connect ( self . load_model ) load_model_button . setSizePolicy ( QSizePolicy . Policy . Expanding , QSizePolicy . Policy . Maximum ) model_layout . addWidget ( load_model_button ) # Confg line edit self . _config_line_edit = QLineEdit () self . _config_line_edit . textChanged . connect ( self . config_changed ) self . _config_line_edit . setSizePolicy ( QSizePolicy . Policy . Expanding , QSizePolicy . Policy . Maximum ) model_layout . addWidget ( self . _config_line_edit ) # Isosurface leve spin box self . _iso_level_spinbox = QDoubleSpinBox () self . _iso_level_spinbox . setDecimals ( 2 ) self . _iso_level_spinbox . setSingleStep ( 0.01 ) self . _iso_level_spinbox . valueChanged . connect ( self . update_isosurface_level ) model_layout . addWidget ( self . _iso_level_spinbox ) # Model status self . _model_status_label = QLabel () model_layout . addWidget ( self . _model_status_label ) # Current config scroll_area = QScrollArea ( widgetResizable = True ) scroll_area . setSizePolicy ( QSizePolicy . Policy . Expanding , QSizePolicy . Policy . Expanding ) scroll_area . setMinimumWidth ( 270 ) self . _config_label = QLabel () self . _config_label . setAlignment ( QtCore . Qt . AlignTop ) scroll_area . setWidget ( self . _config_label ) model_layout . addWidget ( scroll_area ) # Single object explorer single_object_widget = QWidget () single_object_widget_layout = QVBoxLayout () single_object_widget_layout . setAlignment ( QtCore . Qt . AlignTop ) single_object_widget_bottom = QWidget () single_object_widget_bottom_layout = QHBoxLayout () single_object_save_buttons_widget = QWidget () single_object_save_buttons_layout = QHBoxLayout () self . slider_group_widget = QWidget () self . slider_group_widget_layout = QVBoxLayout () load_sdf_button = QPushButton ( \"Load SDF\" ) load_sdf_button . clicked . connect ( self . load_single_sdf ) generate_sdf_button = QPushButton ( \"Generate SDF from prior\" ) generate_sdf_button . clicked . connect ( self . generate_sdf ) save_figure_button = QPushButton ( \"Save figure\" ) save_figure_button . clicked . connect ( self . save_figure ) save_mesh_button = QPushButton ( \"Save mesh\" ) save_mesh_button . clicked . connect ( self . save_mesh ) save_sdf_button = QPushButton ( \"Save sdf\" ) save_sdf_button . clicked . connect ( self . save_sdf ) self . single_object_figure = Figure ( dpi = 85 , facecolor = ( 1 , 1 , 1 ), edgecolor = ( 0 , 0 , 0 ), tight_layout = True ) single_object_canvas = FigureCanvas ( self . single_object_figure ) single_object_canvas . setSizePolicy ( QSizePolicy . Expanding , QSizePolicy . Expanding ) self . scroll_area_sliders = QScrollArea ( widgetResizable = True ) # widgetResizable indicates that the widget insided can be resized self . scroll_area_sliders . setFixedWidth ( 130 ) self . slider_group_widget . setLayout ( self . slider_group_widget_layout ) self . scroll_area_sliders . setWidget ( self . slider_group_widget ) self . scroll_area_sliders . show () # Single object bottom single_object_widget_bottom_layout . addWidget ( single_object_canvas ) single_object_widget_bottom_layout . addWidget ( self . scroll_area_sliders ) single_object_widget_bottom . setLayout ( single_object_widget_bottom_layout ) # Single object save buttons single_object_save_buttons_layout . addWidget ( save_figure_button ) single_object_save_buttons_layout . addWidget ( save_mesh_button ) single_object_save_buttons_layout . addWidget ( save_sdf_button ) single_object_save_buttons_widget . setLayout ( single_object_save_buttons_layout ) single_object_save_buttons_layout . setMargin ( 0 ) # Single object tab single_object_widget_layout . addWidget ( load_sdf_button ) single_object_widget_layout . addWidget ( generate_sdf_button ) single_object_widget_layout . addWidget ( single_object_save_buttons_widget ) single_object_widget_layout . addWidget ( single_object_widget_bottom ) single_object_widget . setLayout ( single_object_widget_layout ) # Transition widget transition_widget = QWidget () transition_widget_layout = QVBoxLayout () transition_widget_layout . setAlignment ( QtCore . Qt . AlignTop ) transition_widget_button_layout = QHBoxLayout () load_first_sdf_button = QPushButton ( \"Load SDF 1\" ) load_first_sdf_button . clicked . connect ( lambda : self . load_transition_sdf ( \"first\" )) load_second_sdf_button = QPushButton ( \"Load SDF 2\" ) load_second_sdf_button . clicked . connect ( lambda : self . load_transition_sdf ( \"second\" ) ) self . transition_figure = Figure ( dpi = 85 , facecolor = ( 1 , 1 , 1 ), edgecolor = ( 0 , 0 , 0 ) ) transition_canvas = FigureCanvas ( self . transition_figure ) transition_canvas . setSizePolicy ( QSizePolicy . Expanding , QSizePolicy . Expanding ) self . transition_figure_axes = self . transition_figure . subplots ( 2 , 3 ) self . transition_figure . delaxes ( self . transition_figure_axes [ 0 , 1 ]) self . transition_slider = QSlider ( QtCore . Qt . Horizontal ) self . transition_slider . setMinimum ( 0 ) self . transition_slider . setMaximum ( 100 ) self . transition_slider . valueChanged . connect ( self . update_transition_slider ) transition_widget_button_layout . addWidget ( load_first_sdf_button ) transition_widget_button_layout . addWidget ( load_second_sdf_button ) transition_widget_layout . addLayout ( transition_widget_button_layout ) transition_widget_layout . addWidget ( transition_canvas ) transition_widget_layout . addWidget ( self . transition_slider ) transition_widget . setLayout ( transition_widget_layout ) # Animation widget animation_widget = QWidget () animation_widget_layout = QVBoxLayout () animation_widget_layout . setAlignment ( QtCore . Qt . AlignTop ) add_kf_from_single_view_button = QPushButton ( \"Add from single view\" ) add_kf_from_single_view_button . clicked . connect ( lambda : self . add_kf_from_single_view () ) delete_kf_button = QPushButton ( \"Delete keyframe\" ) delete_kf_button . clicked . connect ( lambda : self . delete_kf ()) generate_animation_button = QPushButton ( \"Generate animation\" ) generate_animation_button . clicked . connect ( lambda : self . generate_animation ()) self . _keyframe_list = QListWidget () animation_widget . setLayout ( animation_widget_layout ) animation_widget_layout . addWidget ( add_kf_from_single_view_button ) animation_widget_layout . addWidget ( delete_kf_button ) animation_widget_layout . addWidget ( self . _keyframe_list ) animation_widget_layout . addWidget ( generate_animation_button ) # Tab widget tab_widget = QTabWidget () tab_widget . addTab ( single_object_widget , \"Single SDF\" ) tab_widget . addTab ( transition_widget , \"Transition\" ) tab_widget . addTab ( animation_widget , \"Animation\" ) layout . addLayout ( model_layout ) layout . addWidget ( tab_widget ) layout . setStretch ( 0 , 0 ) layout . setStretch ( 1 , 2 ) self . setLayout ( layout ) self . parse_config () self . update_model () # TODO make these adjustable from GUI self . transform = np . array ( [ [ 0 , - 1 , 0 , 0 ], [ 0 , 0 , 1 , 0 ], [ - 1 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 1 ], ] ) self . azimuth = 0 self . polar_angle = - np . pi / 4 def reset_output ( self ) -> None : self . _single_sdf_latent = None self . _single_sdf_output = None self . _transition_sdf_first_latent = None self . _transition_sdf_first_out = None self . _transition_sdf_second_latent = None self . _transition_sdf_second_out = None def add_kf_from_single_view ( self ) -> None : \"\"\"Add keyframe based on current latent in Single SDF tab.\"\"\" if self . _single_sdf_latent is not None : item = QListWidgetItem () item . setText ( str ( self . _single_sdf_latent )) item . setData ( QtCore . Qt . UserRole , self . _single_sdf_latent ) self . _keyframe_list . addItem ( item ) else : print ( \"No valid latent currently set in Single SDF tab\" ) def delete_kf ( self ) -> None : \"\"\"Delete currently selected keyframe.\"\"\" row = self . _keyframe_list . currentRow () self . _keyframe_list . takeItem ( row ) def generate_animation ( self ) -> None : \"\"\"Generate animation based on keyframes.\"\"\" fps = 30 distance_per_frame = 0.1 loop = True keyframes = [] for i in range ( self . _keyframe_list . count ()): keyframes . append ( self . _keyframe_list . item ( i ) . data ( QtCore . Qt . UserRole )) if loop : keyframes . append ( keyframes [ 0 ]) # generate frames with torch . no_grad (): fig = Figure ( dpi = 85 , facecolor = ( 1 , 1 , 1 ), edgecolor = ( 0 , 0 , 0 )) now = datetime . datetime . now () folder = now . strftime ( \"%Y-%m- %d _%H-%M-%S\" ) os . makedirs ( folder ) frame_number = 0 current = keyframes . pop ( 0 ) . clone () remaining_distance = distance_per_frame while keyframes : next_target = keyframes [ 0 ] delta = next_target - current distance = torch . linalg . norm ( delta ) direction = delta / torch . linalg . norm ( delta ) if distance > distance_per_frame : current += direction * distance_per_frame remaining_distance = 0 else : current = keyframes . pop ( 0 ) . clone () remaining_distance -= distance if remaining_distance <= 0 : # generate frame sdf = self . _model . decode ( current ) mesh = sdf_utils . mesh_from_sdf ( sdf [ 0 , 0 ] . cpu () . numpy (), self . _isosurface_level , complete_mesh = True , ) if mesh is None : print ( \"No surface boundaries in SDF.\" ) else : ax = fig . subplots ( 1 , 1 ) sdf_utils . plot_mesh ( mesh , plot_object = ax , transform = self . transform , azimuth = self . azimuth , polar_angle = self . polar_angle , ) ax . xaxis . set_major_locator ( NullLocator ()) ax . yaxis . set_major_locator ( NullLocator ()) fig . savefig ( os . path . join ( folder , f \" { frame_number : 05d } .png\" ), dpi = 200 ) frame_number += 1 fig . clear () # convert frames to video video_name = os . path . join ( f \" { folder } .mp4\" ) ffmpeg . input ( os . path . join ( folder , \"*.png\" ), pattern_type = \"glob\" , framerate = fps ) . output ( video_name ) . run () def update_model ( self ): if self . _config is None : return try : self . _model = SDFVAE ( sdf_size = 64 , latent_size = self . _config [ \"latent_size\" ], encoder_dict = self . _config [ \"encoder\" ], decoder_dict = self . _config [ \"decoder\" ], device = \"cpu\" , tsdf = self . _config [ \"tsdf\" ], ) if self . _state_dict is not None : self . _model . load_state_dict ( self . _state_dict ) self . _model_status_label . setText ( \"Model (with weights) successfully loaded.\" ) else : self . _model_status_label . setText ( \"Model (no weights) successfully loaded.\" ) self . _model_status_label . setStyleSheet ( \"color: green\" ) self . reset_output () except RuntimeError : self . _model_status_label . setText ( \"Model config and weights do not match.\" ) self . _model_status_label . setStyleSheet ( \"color: red\" ) self . _config_label . setText ( \"\" ) self . _model = None except KeyError : self . _model_status_label . setText ( \"Model config incomplete.\" ) self . _model_status_label . setStyleSheet ( \"color: red\" ) self . _config_label . setText ( \"\" ) self . _model = None self . update_single () self . update_transition () def load_sdf ( self ): path , _ = QFileDialog . getOpenFileName ( self , \"Open file\" , filter = \"*.npy\" ) if path == \"\" : return None try : sdf_np = np . load ( path ) return torch . as_tensor ( sdf_np ) . unsqueeze ( 0 ) . unsqueeze ( 0 ) except FileNotFoundError : print ( \"Error loading SDF.\" ) return None def parse_config ( self ): try : arg_list = self . _config_line_edit . text () . split ( \" \" ) arg_list = [ x for x in arg_list if x != \"\" ] parser = ThrowingArgumentParser () parser . add_argument ( \"--tsdf\" , type = utils . str_to_tsdf , default = False ) parser . add_argument ( \"--latent_size\" , type = lambda x : int ( float ( x ))) parser . add_argument ( \"--config\" , default = \"configs/default.yaml\" , nargs = \"+\" ) self . _config = yoco . load_config_from_args ( parser , arg_list ) self . _config_label . setText ( json . dumps ( self . _config , indent = 4 )) except ArgumentParserError : self . _model_status_label . setText ( \"Error in config string.\" ) self . _model_status_label . setStyleSheet ( \"color: red\" ) self . _config_label . setText ( \"\" ) self . _model = None self . _config = None except ( FileNotFoundError , IsADirectoryError ): self . _model_status_label . setText ( \"Specified config file not found.\" ) self . _model_status_label . setStyleSheet ( \"color: red\" ) self . _config_label . setText ( \"\" ) self . _model = None self . _config = None except RuntimeError : self . _model_status_label . setText ( \"Model config and weights do not match.\" ) self . _model_status_label . setStyleSheet ( \"color: red\" ) self . _config_label . setText ( \"\" ) self . _model = None self . _config = None def load_transition_sdf ( self , which ): sdf = self . load_sdf () if which == \"first\" : self . _transition_sdf_first = sdf self . _transition_sdf_first_out = None self . _transition_sdf_first_latent = None elif which == \"second\" : self . _transition_sdf_second = sdf self . _transition_sdf_second_out = None self . _transition_sdf_second_latent = None else : raise ValueError ( \"which must be either 'first' or 'second'.\" ) self . update_transition () def update_single ( self , evaluate = True ): # Visualize input sdf self . single_object_figure . clear () if self . _single_sdf_input is not None : ax1 , ax2 = self . single_object_figure . subplots ( 1 , 2 ) self . render_sdf ( ax1 , self . _single_sdf_input [ 0 , 0 ] . numpy ()) else : ax2 = self . single_object_figure . subplots ( 1 , 1 ) if self . _model is not None : with torch . no_grad (): if ( self . _single_sdf_input is not None and self . _single_sdf_latent is None ): inp = self . _single_sdf_input . clone () self . _model . prepare_input ( inp ) self . _single_sdf_latent , _ , _ = self . _model . encode ( inp ) self . update_sliders () if self . _single_sdf_latent is not None : if evaluate : t1 = time . time () self . _single_sdf_output = self . _model . decode ( self . _single_sdf_latent , enforce_tsdf = True ) print ( f \"Decoding took { time . time () - t1 } s\" ) t1 = time . time () self . render_sdf ( ax2 , self . _single_sdf_output [ 0 , 0 ] . detach () . numpy ()) print ( f \"Rendering took { time . time () - t1 } s\" ) self . single_object_figure . canvas . draw () def update_sliders ( self ): for current_slider in self . _latent_sliders : self . slider_group_widget_layout . removeWidget ( current_slider ) current_slider . deleteLater () self . _latent_sliders = [] for latent in self . _single_sdf_latent . view ( - 1 ) . numpy (): slider = QSlider ( QtCore . Qt . Horizontal ) slider . setMinimum ( - self . LATENT_ABS_MAX * self . LATENT_TICKS / 2.0 ) slider . setMaximum ( + self . LATENT_ABS_MAX * self . LATENT_TICKS / 2.0 ) slider . setValue ( latent * self . LATENT_TICKS / 2.0 ) slider . valueChanged . connect ( self . update_latent ) self . _latent_sliders . append ( slider ) self . slider_group_widget_layout . addWidget ( slider ) def update_latent ( self ): for i , current_slider in enumerate ( self . _latent_sliders ): self . _single_sdf_latent [ 0 , i ] = ( current_slider . value () * 2.0 / self . LATENT_TICKS ) self . update_single () def update_transition_slider ( self ): self . update_transition () def update_transition ( self , update_iso_level_only = False ): if update_iso_level_only : if self . _transition_sdf_first_out is not None : self . render_sdf ( self . transition_figure_axes [ 1 , 0 ], self . _transition_sdf_first_out [ 0 , 0 ] . detach () . numpy (), ) if self . _transition_sdf_second_out is not None : self . render_sdf ( self . transition_figure_axes [ 1 , 2 ], self . _transition_sdf_second_out [ 0 , 0 ] . detach () . numpy (), ) if self . _transition_sdf_out is not None : self . render_sdf ( self . transition_figure_axes [ 1 , 1 ], self . _transition_sdf_out [ 0 , 0 ] . detach () . numpy (), ) self . transition_figure . canvas . draw () return with torch . no_grad (): if ( self . _transition_sdf_first is not None and self . _transition_sdf_first_out is None ): print ( \"render first sdf\" ) self . render_sdf ( self . transition_figure_axes [ 0 , 0 ], self . _transition_sdf_first [ 0 , 0 ] . detach () . numpy (), ) if ( self . _model is not None and self . _transition_sdf_first_latent is None ): inp = self . _transition_sdf_first . clone () self . _model . prepare_input ( inp ) self . _transition_sdf_first_latent , _ , _ = self . _model . encode ( inp ) self . _transition_sdf_first_out = self . _model . decode ( self . _transition_sdf_first_latent ) self . render_sdf ( self . transition_figure_axes [ 1 , 0 ], self . _transition_sdf_first_out [ 0 , 0 ] . detach () . numpy (), ) if ( self . _transition_sdf_second is not None and self . _transition_sdf_second_out is None ): self . render_sdf ( self . transition_figure_axes [ 0 , 2 ], self . _transition_sdf_second [ 0 , 0 ] . detach () . numpy (), ) if ( self . _model is not None and self . _transition_sdf_second_latent is None ): inp = self . _transition_sdf_second . clone () self . _model . prepare_input ( inp ) self . _transition_sdf_second_latent , _ , _ = self . _model . encode ( inp ) self . _transition_sdf_second_out = self . _model . decode ( self . _transition_sdf_second_latent ) self . render_sdf ( self . transition_figure_axes [ 1 , 2 ], self . _transition_sdf_second_out [ 0 , 0 ] . detach () . numpy (), ) if ( self . _transition_sdf_first_latent is not None and self . _transition_sdf_second_latent is not None ): t = self . transition_slider . value () / 100.0 interpolated_latent = ( 1 - t ) * self . _transition_sdf_first_latent + ( t ) * self . _transition_sdf_second_latent self . _transition_sdf_out = self . _model . decode ( interpolated_latent ) self . render_sdf ( self . transition_figure_axes [ 1 , 1 ], self . _transition_sdf_out [ 0 , 0 ] . detach () . numpy (), ) self . transition_figure . canvas . draw () def load_single_sdf ( self ): sdf = self . load_sdf () self . _single_sdf_input = sdf self . _single_sdf_latent = None self . update_single () def generate_sdf ( self ): self . _single_sdf_input = None if self . _model is not None : self . _single_sdf_latent = self . _model . sample ( 1 ) * 0.5 self . update_single () self . update_sliders () else : print ( \"Can't generate without loaded model.\" ) def save_figure ( self ): \"\"\"Save current single sample figure.\"\"\" now = datetime . datetime . now () filename = now . strftime ( \"%Y-%m- %d _%H-%M-%S.png\" ) self . single_object_figure . savefig ( filename , pad_inches = 0 , dpi = 200 ) print ( f \"Saved as { filename } \" ) def save_mesh ( self ): \"\"\"Save current object mesh.\"\"\" if self . _single_sdf_output is None : print ( \"Can't save mesh without generated SDF.\" ) return sdf = self . _single_sdf_output [ 0 , 0 ] . detach () . numpy () mesh = sdf_utils . mesh_from_sdf ( sdf , self . _isosurface_level , complete_mesh = True ) mesh_filename = ( f \"mesh_ { datetime . datetime . now () . strftime ( '%Y-%m- %d _%H-%M-%S- %f ' ) } .obj\" ) mesh_path = os . path . join ( os . getcwd (), mesh_filename ) with open ( mesh_path , \"w\" ) as f : f . write ( trimesh . exchange . obj . export_obj ( mesh )) def save_sdf ( self ): \"\"\"Save current object sdf.\"\"\" if self . _single_sdf_output is None : print ( \"Can't save SDF without generated SDF.\" ) return sdf = self . _single_sdf_output [ 0 , 0 ] . detach () . numpy () sdf_filename = ( f \"sdf_ { datetime . datetime . now () . strftime ( '%Y-%m- %d _%H-%M-%S- %f ' ) } .npy\" ) sdf_path = os . path . join ( os . getcwd (), sdf_filename ) np . save ( sdf_path , sdf ) def config_changed ( self , string ): self . parse_config () self . update_model () def load_model ( self ): path , _ = QFileDialog . getOpenFileName ( self , \"Open file\" ) if path . endswith ( \".yml\" ) or path . endswith ( \".yaml\" ): self . _config_line_edit . setText ( f \"--config { path } \" ) self . parse_config () print ( self . _config ) if \"model\" in self . _config : self . _state_dict = torch . load ( self . _config [ \"model\" ], map_location = \"cpu\" ) self . update_model () elif path . endswith ( \".pt\" ): self . _state_dict = torch . load ( path , map_location = \"cpu\" ) self . update_model () else : return def render_sdf ( self , ax , sdf ): mesh = sdf_utils . mesh_from_sdf ( sdf , self . _isosurface_level , complete_mesh = True ) if mesh is None : print ( \"No surface boundaries in SDF.\" ) else : sdf_utils . plot_mesh ( mesh , plot_object = ax , transform = self . transform , azimuth = self . azimuth , polar_angle = self . polar_angle , ) def update_isosurface_level ( self , d ): self . _isosurface_level = d self . update_single ( False ) self . update_transition ( True )","title":"VAEVisualizer"},{"location":"reference/vae/scripts/visualizer/#sdfest.vae.scripts.visualizer.VAEVisualizer.add_kf_from_single_view","text":"add_kf_from_single_view () -> None Add keyframe based on current latent in Single SDF tab. Source code in sdfest/vae/scripts/visualizer.py 276 277 278 279 280 281 282 283 284 def add_kf_from_single_view ( self ) -> None : \"\"\"Add keyframe based on current latent in Single SDF tab.\"\"\" if self . _single_sdf_latent is not None : item = QListWidgetItem () item . setText ( str ( self . _single_sdf_latent )) item . setData ( QtCore . Qt . UserRole , self . _single_sdf_latent ) self . _keyframe_list . addItem ( item ) else : print ( \"No valid latent currently set in Single SDF tab\" )","title":"add_kf_from_single_view()"},{"location":"reference/vae/scripts/visualizer/#sdfest.vae.scripts.visualizer.VAEVisualizer.delete_kf","text":"delete_kf () -> None Delete currently selected keyframe. Source code in sdfest/vae/scripts/visualizer.py 286 287 288 289 def delete_kf ( self ) -> None : \"\"\"Delete currently selected keyframe.\"\"\" row = self . _keyframe_list . currentRow () self . _keyframe_list . takeItem ( row )","title":"delete_kf()"},{"location":"reference/vae/scripts/visualizer/#sdfest.vae.scripts.visualizer.VAEVisualizer.generate_animation","text":"generate_animation () -> None Generate animation based on keyframes. Source code in sdfest/vae/scripts/visualizer.py 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 def generate_animation ( self ) -> None : \"\"\"Generate animation based on keyframes.\"\"\" fps = 30 distance_per_frame = 0.1 loop = True keyframes = [] for i in range ( self . _keyframe_list . count ()): keyframes . append ( self . _keyframe_list . item ( i ) . data ( QtCore . Qt . UserRole )) if loop : keyframes . append ( keyframes [ 0 ]) # generate frames with torch . no_grad (): fig = Figure ( dpi = 85 , facecolor = ( 1 , 1 , 1 ), edgecolor = ( 0 , 0 , 0 )) now = datetime . datetime . now () folder = now . strftime ( \"%Y-%m- %d _%H-%M-%S\" ) os . makedirs ( folder ) frame_number = 0 current = keyframes . pop ( 0 ) . clone () remaining_distance = distance_per_frame while keyframes : next_target = keyframes [ 0 ] delta = next_target - current distance = torch . linalg . norm ( delta ) direction = delta / torch . linalg . norm ( delta ) if distance > distance_per_frame : current += direction * distance_per_frame remaining_distance = 0 else : current = keyframes . pop ( 0 ) . clone () remaining_distance -= distance if remaining_distance <= 0 : # generate frame sdf = self . _model . decode ( current ) mesh = sdf_utils . mesh_from_sdf ( sdf [ 0 , 0 ] . cpu () . numpy (), self . _isosurface_level , complete_mesh = True , ) if mesh is None : print ( \"No surface boundaries in SDF.\" ) else : ax = fig . subplots ( 1 , 1 ) sdf_utils . plot_mesh ( mesh , plot_object = ax , transform = self . transform , azimuth = self . azimuth , polar_angle = self . polar_angle , ) ax . xaxis . set_major_locator ( NullLocator ()) ax . yaxis . set_major_locator ( NullLocator ()) fig . savefig ( os . path . join ( folder , f \" { frame_number : 05d } .png\" ), dpi = 200 ) frame_number += 1 fig . clear () # convert frames to video video_name = os . path . join ( f \" { folder } .mp4\" ) ffmpeg . input ( os . path . join ( folder , \"*.png\" ), pattern_type = \"glob\" , framerate = fps ) . output ( video_name ) . run ()","title":"generate_animation()"},{"location":"reference/vae/scripts/visualizer/#sdfest.vae.scripts.visualizer.VAEVisualizer.save_figure","text":"save_figure () Save current single sample figure. Source code in sdfest/vae/scripts/visualizer.py 602 603 604 605 606 607 def save_figure ( self ): \"\"\"Save current single sample figure.\"\"\" now = datetime . datetime . now () filename = now . strftime ( \"%Y-%m- %d _%H-%M-%S.png\" ) self . single_object_figure . savefig ( filename , pad_inches = 0 , dpi = 200 ) print ( f \"Saved as { filename } \" )","title":"save_figure()"},{"location":"reference/vae/scripts/visualizer/#sdfest.vae.scripts.visualizer.VAEVisualizer.save_mesh","text":"save_mesh () Save current object mesh. Source code in sdfest/vae/scripts/visualizer.py 609 610 611 612 613 614 615 616 617 618 619 620 621 def save_mesh ( self ): \"\"\"Save current object mesh.\"\"\" if self . _single_sdf_output is None : print ( \"Can't save mesh without generated SDF.\" ) return sdf = self . _single_sdf_output [ 0 , 0 ] . detach () . numpy () mesh = sdf_utils . mesh_from_sdf ( sdf , self . _isosurface_level , complete_mesh = True ) mesh_filename = ( f \"mesh_ { datetime . datetime . now () . strftime ( '%Y-%m- %d _%H-%M-%S- %f ' ) } .obj\" ) mesh_path = os . path . join ( os . getcwd (), mesh_filename ) with open ( mesh_path , \"w\" ) as f : f . write ( trimesh . exchange . obj . export_obj ( mesh ))","title":"save_mesh()"},{"location":"reference/vae/scripts/visualizer/#sdfest.vae.scripts.visualizer.VAEVisualizer.save_sdf","text":"save_sdf () Save current object sdf. Source code in sdfest/vae/scripts/visualizer.py 623 624 625 626 627 628 629 630 631 632 633 def save_sdf ( self ): \"\"\"Save current object sdf.\"\"\" if self . _single_sdf_output is None : print ( \"Can't save SDF without generated SDF.\" ) return sdf = self . _single_sdf_output [ 0 , 0 ] . detach () . numpy () sdf_filename = ( f \"sdf_ { datetime . datetime . now () . strftime ( '%Y-%m- %d _%H-%M-%S- %f ' ) } .npy\" ) sdf_path = os . path . join ( os . getcwd (), sdf_filename ) np . save ( sdf_path , sdf )","title":"save_sdf()"},{"location":"reference/vae/scripts/visualizer/#sdfest.vae.scripts.visualizer.main","text":"main () Start the UI. Source code in sdfest/vae/scripts/visualizer.py 673 674 675 676 677 678 679 680 def main (): \"\"\"Start the UI.\"\"\" app = QApplication ( sys . argv ) gui = VAEVisualizer () gui . show () app . exec_ ()","title":"main()"}]}