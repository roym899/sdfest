<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN""http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
    <title>Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation, CVPR 2019
        (Oral)</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <!-- Meta tags for Zotero grab citation -->
    <meta name="citation_title"
          content="Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation">
    <meta name="citation_author" content="Wang, He">
    <meta name="citation_author" content="Sridhar, Srinath">
    <meta name="citation_author" content="Huang, Jingwei">
    <meta name="citation_author" content="Valentin, Julien">
    <meta name="citation_author" content="Song, Shuran">
    <meta name="citation_author" content="Guibas, Leonidas">
    <meta name="citation_publication_date" content="2019">
    <meta name="citation_conference_title" content="CVPR 2019">
    <!-- <meta name="citation_volume" content="271"> -->
    <!-- <meta name="citation_issue" content="20"> -->
    <!-- <meta name="citation_firstpage" content="11761"> -->
    <!-- <meta name="citation_lastpage" content="11766"> -->
    <meta name="citation_pdf_url" content="http://gvv.mpi-inf.mpg.de/projects/VNect/content/VNect_SIGGRAPH2017.pdf">

    <!-- Meta tags for search engines to crawl -->
    <meta name="robots" content="index,follow">
    <meta name="description"
          content="The goal of this paper is to estimate the 6D pose and dimensions of unseen object instances in an RGB-D image. Contrary to “instance-level” 6D pose estimation tasks, our problem assumes that no exact object CAD models are available during either training or testing time. To handle different and unseen object instances in a given category, we introduce Normalized Object Coordinate Space (NOCS)—a shared canonical representation for all possible object instances within a category. Our region-based neural network is then trained to directly infer the correspondence from observed pixels to this shared object representation (NOCS) along with other object information such as class label and instance mask. These predictions can be combined with the depth map to jointly estimate the metric 6D pose and dimensions of multiple objects in a cluttered scene. To train our network, we present a new context-aware technique to generate large amounts of fully annotated mixed reality data. To further improve our model and evaluate its performance on real data, we also provide a fully annotated real-world dataset with large environment and instance variation. Extensive experiments demonstrate that the proposed method is able to robustly estimate the pose and size of unseen object instances in real environments while also achieving state-of-the-art performance on standard 6D pose estimation benchmarks.">
    <meta name="keywords" content="6D pose estimationl category-level; object detection; instance mask; RGB-D">
    <link rel="author" href="http://http://srinathsridhar.com"/>

    <!-- Fonts and stuff -->
    <!--<link href='http://fonts.googleapis.com/css?family=Open+Sans:400italic,700italic,800italic,400,700,800' rel='stylesheet' type='text/css'>-->
    <link href='https://fonts.googleapis.com/css?family=Source+Sans+Pro:400italic,700italic,800italic,400,700,800'
          rel='stylesheet' type='text/css'/>
    <link rel="stylesheet" type="text/css" href="css/project.css" media="screen"/>
    <link rel="stylesheet" type="text/css" media="screen" href="css/iconize.css"/>
    <!--<script src="js/google-code-prettify/prettify.js"></script>-->


</head>

<body>
<div id="content">
    <div class="section logos">
        <a href="http://cvpr2019.thecvf.com/" target="_blank"><img width="28%" height="28%" src="images/cvpr.png" align="left"></a>
        <a href="http://ai.stanford.edu/" target="_blank"><img width="4.5%" height="4.5%" src="images/stanford.png" ></a>
        <a href="https://ai.google/" target="_blank"><img width="6%" height="6%" src="images/google.png"></a>
        <a href="http://www.cs.princeton.edu/" target="_blank"><img width="5%" height="5%"
                                                                    src="images/princeton.png"></a>
        <a href="https://research.fb.com/" target="_blank"><img width="10%" height="10%" src="images/fb.png"></a>
    </div>

    <div class="section head">
        <h1>Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation</h1>

        <div class="authors">
            <a href="https://geometry.stanford.edu/member/hewang/index.html" target="_blank">He Wang</a><sup>1</sup>&#160;&#160;
            <a href="http://srinathsridhar.com/" target="_blank">Srinath Sridhar</a><sup>1</sup>&#160;&#160;
            <a href="http://stanford.edu/~jingweih/" target="_blank">Jingwei Huang</a><sup>1</sup>&#160;&#160;
            <a href="https://scholar.google.co.uk/citations?user=pZPD0hMAAAAJ&hl=en" target="_blank">Julien Valentin</a><sup>2</sup>&#160;&#160;
            <a href="https://shurans.github.io/publication.html" target="_blank">Shuran Song</a><sup>3</sup>&#160;&#160;
            <a href="https://geometry.stanford.edu/member/guibas/" target="_blank">Leonidas J. Guibas</a><sup>1,4</sup>
        </div>

        <div class="affiliations">
            <sup>1</sup><a href="http://ai.stanford.edu/" target="_blank">Stanford University</a>&#160;&#160;
            <sup>2</sup><a href="https://ai.google/" target="_blank">Google Inc.</a>&#160;&#160;
            <sup>3</sup><a href="http://www.cs.princeton.edu/" target="_blank">Princeton University</a>&#160;&#160;
            <sup>4</sup><a href="https://research.fb.com/" target="_blank">Facebook AI Research</a>&#160;&#160;<br/>
        </div>

        <div class="venue"><a href="http://cvpr2019.thecvf.com/" target="_blank">CVPR 2019</a> [<b>Oral Presentation</b>]
        </div>
    </div>

    <div id="content-inner">
        <div class="section abstract">

            <div class="section teaser">
                <iframe width="640" height="360"
                        src="https://www.youtube-nocookie.com/embed/UsfoLcInlhM?showinfo=0&controls=1&modestbranding=0&autohide=1&theme=light"
                        frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
            </div>

            <div class="section abstract">
                <h2>Abstract</h2>
                <p>
                    The goal of this paper is to estimate the 6D pose and dimensions of unseen object instances in an
                    RGB-D image. Contrary to “instance-level” 6D pose estimation tasks, our problem assumes that no
                    exact object CAD models are available during either training or testing time. To handle different
                    and unseen object instances in a given category, we introduce Normalized Object Coordinate Space
                    (NOCS)—a shared canonical representation for all possible object instances within a category. Our
                    region-based neural network is then trained to directly infer the correspondence from observed
                    pixels to this shared object representation (NOCS) along with other object information such as class
                    label and instance mask. These predictions can be combined with the depth map to jointly estimate
                    the metric 6D pose and dimensions of multiple objects in a cluttered scene. To train our network, we
                    present a new context-aware technique to generate large amounts of fully annotated mixed reality
                    data. To further improve our model and evaluate its performance on real data, we also provide a
                    fully annotated real-world dataset with large environment and instance variation. Extensive
                    experiments demonstrate that the proposed method is able to robustly estimate the pose and size of
                    unseen object instances in real environments while also achieving state-of-the-art performance on
                    standard 6D pose estimation benchmarks.
                </p>
            </div>

            <div class="section downloads">
                <h2>Downloads</h2>
                <center>
                    <ul>
                        <li class="grid">
                            <div class="griditem">
                                <a href="pub/NOCS_CVPR2019.pdf" target="_blank" class="imageLink"><img
                                        src="images/NOCS_CVPR2019_thumb.png"></a><br/>
                                Paper<br/><a href="content/NOCS_CVPR2019.pdf" target="_blank">PDF</a>
                            </div>
                        </li>

                        <li class="grid">
                            <div class="griditem">
                                <a href="pub/NOCS_CVPR2019_Supp.pdf" target="_blank" class="imageLink"><img
                                        src="images/NOCS_CVPR2019_supp_thumb.png"></a><br/>
                                Supplementary Document<br/><a href="content/NOCS_CVPR2019_Supp.pdf" target="_blank">PDF</a>
                            </div>
                        </li>

                        <li class="grid">
                            <div class="griditem">
                                <a href="https://github.com/hughw19/NOCS_CVPR2019" target="_blank"
                                   class="imageLink">
                                    <img src="images/code.png">
                                </a><br/>
                                Code & Datasets<br/>
                                <a
                                        href="https://github.com/hughw19/NOCS_CVPR2019" target="_blank">on Github</a>
                            </div>
                        </li>
                    </ul>
                </center>
            </div>
            <br/>
            <div class="section list">
                <h2>Citation</h2>
                <p><a href="pub/NOCS_CVPR2019.bib" target="_blank">BibTeX, 1 KB</a></p>
                <div class="section bibtex">
                    <center>
	      <pre>
@InProceedings{Wang_2019_CVPR,
author = {Wang, He and Sridhar, Srinath and Huang, Jingwei and Valentin, Julien and Song, Shuran and Guibas, Leonidas J.},
title = {Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}
      </pre>
                    </center>
                </div>
            </div>


            <div class="section acknowledgments">
                <h2>Acknowledgments</h2>
                <p>
                    This research was supported by a grant from Toyota-Stanford Center for AI Research, NSF grant
                    IIS-1763268, a gift from Google, and a Vannevar Bush Faculty Fellowship. We thank Xin Wang, Shengjun
                    Qin, Anastasia Dubrovina, Davis Rempe, Li Yi, and Vignesh Ganapathi-Subramanian.
                </p>
            </div>

            <div class="section contact">
                <h2>Contact</h2>
                Srinath Sridhar<br/><a href="mailto:ssrinath@cs.stanford.edu">ssrinath@cs.stanford.edu</a>
            </div>

            <div class="section">
                <hr class="smooth">
                This page is <a href="http://www.zotero.org" target="_blank">Zotero</a> translator friendly.
                <!--Page last updated-->
                <!--#config timefmt="%d-%b-%Y" --><!--#echo var="LAST_MODIFIED" -->
            </div>

        </div>
    </div>
</div>

</body>
</html>

