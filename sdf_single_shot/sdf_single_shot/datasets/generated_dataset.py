"""Module which provides SDFDataset class."""
import math
from typing import Tuple, List, Optional, Iterator, TypedDict
import random

import torch
import torchvision.transforms as T
import yoco
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D  # unused, but needed for 3D projection

from sdf_differentiable_renderer.sdf_renderer import render_depth_gpu
from sdf_vae.sdf_vae import SDFVAE
from sdf_single_shot import pointset_utils, so3grid


# TODO: refactor this code to use namedtuple for targets


def collate_pointsets(pointset_data: List[Tuple]) -> Tuple:
    """Collate pointsets by taking subset of points.

    Based on the reduces all pointset to a common size based on the smallest set.

    Args:
        Pointsets with pose and shape information.

        Expected to be Tuple as returned by SDFVAEViewDataset.generate_sample

    Returns:
        Tensor of size (N, M_min, D) where N is the batch size, M_min the number of
        points in the smallest pointset and D the number of channels per point.
    """
    # remove empty pointsets
    pointset_data = [x for x in pointset_data if x[0].shape[0] > 0]

    smallest_set = min(pointset.shape[0] for pointset, _ in pointset_data)
    smallest_set = min(
        smallest_set, 2500
    )  # limit number of points to prevent rare memory problems
    channels = pointset_data[0][0].shape[-1]
    device = pointset_data[0][0].device
    output = torch.empty(len(pointset_data), smallest_set, channels, device=device)
    for i, (pointset, _) in enumerate(pointset_data):
        num_points = pointset.shape[0]
        point_indices = random.sample(range(0, num_points), smallest_set)
        output[i] = pointset[point_indices]

    shape = torch.stack([targets[0] for _, targets in pointset_data])
    positions = torch.stack([targets[1] for _, targets in pointset_data])
    scales = torch.stack([targets[2] for _, targets in pointset_data])
    orientations = torch.stack([targets[3] for _, targets in pointset_data])
    quaternions = torch.stack([targets[4] for _, targets in pointset_data])
    targets = shape, positions, scales, orientations, quaternions
    return output, targets


class SDFVAEViewDataset(torch.utils.data.IterableDataset):
    """Dataset of SDF views generated by VAE and renderer from a random view."""

    class Config(TypedDict, total=False):
        """Configuration dictionary for SDFVAEViewDataset.

        Attributes:
            width: The width of the generated images in px.
            height: The height of the generated images in px.
            fov_deg: The horizontal fov in deg.
            z_min:
                Minimum z value (i.e., distance from camera) for the SDF.
                Note that positive z means in front of the camera, hence z_sampler
                should in most cases return positive values.
            z_max:
                Maximum z value (i.e., distance from camera) for the SDF.
            extent_mean:
                Mean scale of the SDF.
                Extent is the total size of an SDF.
            extent_std:
                Standard deviation of the SDF scale.
            pointcloud: Whether to generate pointcloud or depth image.
            normalize_pose:
                Whether to center the augmented pointcloud at 0,0,0.
                Ignored if pointcloud=False
            orientation_repr:
                Which orientation representation is used. One of:
                    "quaternion"
                    "discretized"
            orientation_grid_resolution:
                Resolution of the orientation grid.
                Only used if orientation_repr is "discretized".
            mask_noise:
                Whether the mask should be perturbed to simulate noisy segmentation.
                If True a random, small, affine transform will be applied to the correct
                mask. The outliers will be filled with a random value sampled between
                mask_noise_min, and mask_noise_max.
            mask_noise_min:
                Minimum value to fill in for noisy mask.
                Only used if mask_noise is True.
            mask_noise_max:
                Maximum value to fill in for noisy mask.
                Only used if mask_noise is True.
        """

        width: int
        height: int
        fov_deg: float
        z_min: float
        z_max: float
        extent_mean: float
        extent_std: float
        pointcloud: bool
        normalize_pose: bool
        render_threshold: float
        orientation_repr: str
        orientation_grid_resolution: Optional[int]
        mask_noise: bool
        mask_noise_min: Optional[float]
        mask_noise_max: Optional[float]

    default_config: Config = {
        "device": "cuda",
        "orientation_repr": "quaternion",
        "orientation_grid_resolution": None,
        "mask_noise": False,
        "mask_noise_min": 0.1,
        "mask_noise_max": 2.0,
    }

    def __init__(
        self,
        config: dict,
        vae: SDFVAE,
    ) -> None:
        """Initialize the dataset.

        Args:
            config:
                Configuration dictionary of dataset. Provided dictionary will be merged
                with default_dict. See SDFVAEViewDataset.Config for supported keys.
            vae: The variational autoencoder used to create training samples.
        """
        config = yoco.load_config(config, default_dict=SDFVAEViewDataset.default_config)
        self._vae = vae
        self._device = next(self._vae.parameters()).device
        self._width = config["width"]
        self._height = config["height"]
        self._fov_deg = config["fov_deg"]
        self._z_sampler = lambda: random.uniform(config["z_min"], config["z_max"])
        self._scale_sampler = (
            lambda: random.gauss(config["extent_mean"], config["extent_std"]) / 2.0
        )
        self._mask_noise = config["mask_noise"]
        self._mask_noise_sampler = lambda: random.uniform(
            config["mask_noise_min"], config["mask_noise_max"]
        )
        self._pointcloud = config["pointcloud"]
        self._normalize_pose = config["normalize_pose"]
        self._render_threshold = config["render_threshold"]
        self._orientation_repr = config["orientation_repr"]
        if self._orientation_repr == "discretized":
            self._orientation_grid = so3grid.SO3Grid(
                config["orientation_grid_resolution"]
            )

    def __iter__(self) -> Iterator:
        """Return SDF volume at a specific index.

        Returns:
            Infinite iterator, generating SDFs.
        """
        # this is an infinite iterator as the sentinel False will never be returned
        while True:
            yield self.generate_valid_sample()

    def generate_uniform_quaternion(self) -> torch.Tensor:
        """Generate a uniform quaternion.

        Following the method from K. Shoemake, Uniform Random Rotations, 1992.

        See: http://planning.cs.uiuc.edu/node198.html

        Returns:
            Uniformly distributed unit quaternion on the dataset's device.
        """
        u1, u2, u3 = random.random(), random.random(), random.random()
        return torch.tensor(
            [
                math.sqrt(1 - u1) * math.sin(2 * math.pi * u2),
                math.sqrt(1 - u1) * math.cos(2 * math.pi * u2),
                math.sqrt(u1) * math.sin(2 * math.pi * u3),
                math.sqrt(u1) * math.cos(2 * math.pi * u3),
            ]
        ).to(self._device)

    def is_valid(self, inp: torch.Tensor) -> bool:
        """Check whether a generated input is empty or only zero."""
        if self._pointcloud and inp.shape[0] == 0:
            return False
        elif not self._pointcloud and inp.max() == 0:
            return False
        return True

    def generate_valid_sample(self) -> Tuple:
        """Generate a single non-zero sample.

        Return:
            Generated sample and targets based on the settings of the dataset.
        """
        inp, targets = self.generate_sample()
        while not self.is_valid(inp):
            inp, targets = self.generate_sample()
            print("Warning: invalid sample, this should only happen very infrequently.")
            # if this happens often, either the SDF does not have any zero crossings
            # or the object pose is completely outside the frustum
        return inp, targets

    def _perturb_mask(self, mask: torch.Tensor) -> torch.Tensor:
        """Perturb mask by applying small random affine transform to it.

        Args:
            mask: The mask to perturb.
        Returns:
            The perturbed mask. Same shape as mask.
        """
        affine_transfomer = T.RandomAffine(
            degrees=(0, 1), translate=(0.00, 0.01), scale=(0.999, 1.001)
        )
        return affine_transfomer(mask.unsqueeze(0))[0]

    def generate_sample(self) -> Tuple:
        """Generate a single sample. Possibly (normally very unlikely) zero / empty.

        Return:
            Generated sample and targets based on the settings of the dataset.
        """
        with torch.no_grad():
            f = self._width / math.tan(self._fov_deg * math.pi / 180.0 / 2.0) / 2.0
            latent = self._vae.sample()  # will be 1xlatent_size (batch size 1 here)
            sdf = self._vae.decode(latent)
            z = self._z_sampler()
            # generate x, y s.t. center is inside frustum
            x = random.uniform(-self._width / 2, self._width / 2) / f * z
            y = random.uniform(-self._height / 2, self._height / 2) / f * z
            position = torch.tensor([x, y, -z]).to(self._device)
            quaternion = self.generate_uniform_quaternion()
            scale = torch.tensor(self._scale_sampler()).to(self._device)
            inv_scale = 1.0 / scale

            orientation = self._quat_to_orientation_repr(quaternion)
            targets = latent.squeeze(), position, scale, orientation, quaternion

            depth_image = render_depth_gpu(
                sdf[0, 0],
                position,
                quaternion,
                inv_scale,
                self._width,
                self._height,
                self._fov_deg,
                self._render_threshold,
            )

            if self._mask_noise:
                mask = depth_image != 0
                perturbed_mask = self._perturb_mask(mask)
                depth_image[
                    mask * (mask != perturbed_mask)
                ] = self._mask_noise_sampler()

            if self._pointcloud:
                indices = torch.nonzero(depth_image, as_tuple=True)
                depth_values = depth_image[indices]
                points = torch.cat(
                    (
                        indices[1][:, None].float(),
                        indices[0][:, None].float(),
                        depth_values[:, None],
                    ),
                    dim=1,
                )

                # OpenGL coordinate system
                points[:, 0] = (points[:, 0] - self._width / 2) * points[:, 2] / f
                points[:, 1] = (self._height / 2 - points[:, 1]) * points[:, 2] / f
                points[:, 2] *= -1.0

                if self._normalize_pose:
                    normalized_points, centroid = pointset_utils.normalize_points(
                        points
                    )
                    position -= centroid  # adjust target
                    return normalized_points, targets
                return points, targets
            else:
                return depth_image, targets

        # TODO: add augmentation

    def _quat_to_orientation_repr(self, quaternion: torch.Tensor) -> torch.Tensor:
        """Convert quaternion to selected orientation representation.

        Args:
            quaternion:
                The quaternion to convert, scalar-last, shape (4,).
        Returns:
            The same orientation as represented by the quaternion in the chosen
            orientation representation.
        """
        if self._orientation_repr == "quaternion":
            return quaternion
        elif self._orientation_repr == "discretized":
            index = self._orientation_grid.quat_to_index(quaternion.cpu().numpy())
            return torch.tensor(
                index,
                device=self._device,
                dtype=torch.long,
            )
        else:
            raise NotImplementedError(
                f"Orientation representation {self._orientation_repr} is not supported."
            )


if __name__ == "__main__":
    # test the dataset
    vae_config = yoco.load_config_from_file("../vae_models/mug.yaml")
    device = "cuda"
    vae = SDFVAE(
        sdf_size=64,
        latent_size=vae_config["latent_size"],
        encoder_dict=vae_config["encoder"],
        decoder_dict=vae_config["decoder"],
        device=device,
    ).to(device)
    state_dict = torch.load(vae_config["model"], map_location=device)
    vae.load_state_dict(state_dict)
    z = vae.sample()
    sdf = vae.decode(z)
    pointcloud = False
    normalize_pose = True
    vae_dataset = SDFVAEViewDataset(
        vae,
        device=device,
        width=640,
        height=480,
        fov_deg=90,
        z_min=0.2,
        z_max=0.5,
        extent_mean=0.11,
        extent_std=0.01,
        pointcloud=pointcloud,
        normalize_pose=normalize_pose,
        render_threshold=0.01,
        orientation_repr="discretized",
        orientation_grid=so3grid.SO3Grid(0),
        mask_noise=True,
    )
    vae_dataset_iter = iter(vae_dataset)
    if pointcloud:
        for pointcloud, _ in vae_dataset:
            fig = plt.figure()
            ax = fig.add_subplot(111, projection="3d")
            pointcloud = pointcloud.detach().cpu().numpy()
            ax.scatter(
                pointcloud[:, 0],
                pointcloud[:, 1],
                pointcloud[:, 2],
                s=0.5,
                c="b",
            )
            plt.show()
    else:
        for depth_image, _ in vae_dataset:
            plt.imshow(depth_image.detach().cpu().numpy(), cmap="gist_heat_r")
            plt.show()
