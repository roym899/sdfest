"""Module which provides SDFDataset class."""
import math
import random
import torch
import yoco
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D  # unused, but needed for 3D projection
from typing import Tuple, Callable, List, Optional, Iterator
from sdf_differentiable_renderer.sdf_renderer import render_depth_gpu
from sdf_vae.sdf_vae import SDFVAE
from sdf_single_shot import pointset_utils
from sdf_single_shot.so3grid import SO3Grid


# TODO: refactor this code to use namedtuple for targets


def collate_pointsets(pointset_data: List[Tuple]) -> Tuple:
    """Collate pointsets by taking subset of points.

    Based on the reduces all pointset to a common size based on the smallest set.

    Args:
        Pointsets with pose and shape information.

        Expected to be Tuple as returned by SDFVAEViewDataset.generate_sample

    Returns:
        Tensor of size (N, M_min, D) where N is the batch size, M_min the number of
        points in the smallest pointset and D the number of channels per point.
    """
    # remove empty pointsets
    pointset_data = [x for x in pointset_data if x[0].shape[0] > 0]

    smallest_set = min(pointset.shape[0] for pointset, _ in pointset_data)
    smallest_set = min(
        smallest_set, 2500
    )  # limit number of points to prevent rare memory problems
    channels = pointset_data[0][0].shape[-1]
    device = pointset_data[0][0].device
    output = torch.empty(len(pointset_data), smallest_set, channels, device=device)
    for i, (pointset, _) in enumerate(pointset_data):
        num_points = pointset.shape[0]
        point_indices = random.sample(range(0, num_points), smallest_set)
        output[i] = pointset[point_indices]

    shape = torch.stack([targets[0] for _, targets in pointset_data])
    positions = torch.stack([targets[1] for _, targets in pointset_data])
    scales = torch.stack([targets[2] for _, targets in pointset_data])
    orientations = torch.stack([targets[3] for _, targets in pointset_data])
    quaternions = torch.stack([targets[4] for _, targets in pointset_data])
    targets = shape, positions, scales, orientations, quaternions
    return output, targets


class SDFVAEViewDataset(torch.utils.data.IterableDataset):
    """Dataset of SDF views generated by VAE and renderer from a random view."""

    def __init__(
        self,
        vae: SDFVAE,
        device: torch.device,
        width: int,
        height: int,
        fov_deg: float,
        z_min: float,
        z_max: float,
        extent_mean: float,
        extent_std: float,
        pointcloud: bool,
        normalize_pose: bool,
        render_threshold: float,
        orientation_repr: Optional[str] = "quaternion",
        orientation_grid: Optional[SO3Grid] = None,
    ):
        """Construct the dataset.

        Args:
            vae: The variational autoencoder used to create training samples.
            width: The width of the generated images in px.
            height: The height of the generated images in px.
            fov_deg: The horizontal fov in deg.
            z_min:
                Minimum z value (i.e., distance from camera) for the SDF.
                Note that positive z means in front of the camera, hence z_sampler
                should in most cases return positive values.
            z_max:
                Maximum z value (i.e., distance from camera) for the SDF.
            extent_mean:
                Mean scale of the SDF.
                Extent is the total size of an SDF.
            extent_std:
                Standard deviation of the SDF scale.
            pointcloud: Whether to generate pointcloud or depth image.
            normalize_pose:
                Whether to center the augmented pointcloud at 0,0,0.
                Ignored if pointcloud=False
            orientation_repr:
                The orientation representation used for the target.
                One of "quaternion"|"discretized".
            orientation_grid:
                The grid used to discretize SO3.
                Only used if orientation_repr == "discretized".
        """
        self._vae = vae
        self._device = device
        self._width = width
        self._height = height
        self._fov_deg = fov_deg
        self._z_sampler = lambda: random.uniform(z_min, z_max)
        self._scale_sampler = lambda: random.gauss(extent_mean, extent_std) / 2.0
        self._pointcloud = pointcloud
        self._normalize_pose = normalize_pose
        self._render_threshold = render_threshold
        self._orientation_repr = orientation_repr
        self._orientation_grid = orientation_grid

    def __iter__(self) -> Iterator:
        """Return SDF volume at a specific index.

        Returns:
            Infinite iterator, generating SDFs.
        """
        # this is an infinite iterator as the sentinel False will never be returned
        while True:
            yield self.generate_valid_sample()

    def generate_uniform_quaternion(self) -> torch.tensor:
        """Generate a uniform quaternion.

        Following the method from K. Shoemake, Uniform Random Rotations, 1992.

        See: http://planning.cs.uiuc.edu/node198.html

        Returns:
            Uniformly distributed unit quaternion on the dataset's device.
        """
        u1, u2, u3 = random.random(), random.random(), random.random()
        return torch.tensor(
            [
                math.sqrt(1 - u1) * math.sin(2 * math.pi * u2),
                math.sqrt(1 - u1) * math.cos(2 * math.pi * u2),
                math.sqrt(u1) * math.sin(2 * math.pi * u3),
                math.sqrt(u1) * math.cos(2 * math.pi * u3),
            ]
        ).to(self._device)

    def is_valid(self, inp: torch.Tensor) -> bool:
        """Check whether a generated input is empty or only zero."""
        if self._pointcloud and inp.shape[0] == 0:
            return False
        elif not self._pointcloud and inp.max() == 0:
            return False
        return True

    def generate_valid_sample(self) -> Tuple:
        """Generate a single non-zero sample.

        Return:
            Generated sample and targets based on the settings of the dataset.
        """
        inp, targets = self.generate_sample()
        while not self.is_valid(inp):
            inp, targets = self.generate_sample()
            print("Warning: invalid sample, this should only happen very infrequently.")
            # if this happens often, either the SDF does not have any zero crossings
            # or the object pose is completely outside the frustum
        return inp, targets

    def generate_sample(self) -> Tuple:
        """Generate a single sample. Possibly (normally very unlikely) zero / empty.

        Return:
            Generated sample and targets based on the settings of the dataset.
        """
        with torch.no_grad():
            f = self._width / math.tan(self._fov_deg * math.pi / 180.0 / 2.0) / 2.0
            latent = self._vae.sample()  # will be 1xlatent_size (batch size 1 here)
            sdf = self._vae.decode(latent)
            z = self._z_sampler()
            # generate x, y s.t. center is inside frustum
            x = random.uniform(-self._width / 2, self._width / 2) / f * z
            y = random.uniform(-self._height / 2, self._height / 2) / f * z
            position = torch.tensor([x, y, -z]).to(self._device)
            quaternion = self.generate_uniform_quaternion()
            scale = torch.tensor(self._scale_sampler()).to(self._device)
            inv_scale = 1.0 / scale

            orientation = self._quat_to_orientation_repr(quaternion)
            targets = latent.squeeze(), position, scale, orientation, quaternion

            depth_image = render_depth_gpu(
                sdf[0, 0],
                position,
                quaternion,
                inv_scale,
                self._width,
                self._height,
                self._fov_deg,
                self._render_threshold,
            )

            if self._pointcloud:
                indices = torch.nonzero(depth_image, as_tuple=True)
                depth_values = depth_image[indices]
                points = torch.cat(
                    (
                        indices[1][:, None].float(),
                        indices[0][:, None].float(),
                        depth_values[:, None],
                    ),
                    dim=1,
                )

                # OpenGL coordinate system
                points[:, 0] = (points[:, 0] - self._width / 2) * points[:, 2] / f
                points[:, 1] = (self._height / 2 - points[:, 1]) * points[:, 2] / f
                points[:, 2] *= -1.0

                if self._normalize_pose:
                    normalized_points, centroid = pointset_utils.normalize_points(
                        points
                    )
                    position -= centroid  # adjust target
                    return normalized_points, targets
                return points, targets
            else:
                return depth_image, targets

        # TODO: add augmentation

    def _quat_to_orientation_repr(self, quat):
        """Convert quaternion to selected orientation representation.

        Args:
            quat: The quaternion to convert.
        Returns:
            The same orientation as represented by the quaternion in the chosen
            orientation representation.
        """
        if self._orientation_repr == "quaternion":
            return quat
        elif self._orientation_repr == "discretized":
            index = self._orientation_grid.quat_to_index(quat.cpu().numpy())
            return torch.tensor(
                index,
                device=self._device,
                dtype=torch.long,
            )
        else:
            raise NotImplementedError(
                f"Orientation representation {self._orientation_repr} is not supported."
            )


if __name__ == "__main__":
    # test the dataset
    vae_config = yoco.load_config_from_file("./configs/vae.yaml")
    device = "cuda"
    vae = SDFVAE(
        sdf_size=64,
        latent_size=vae_config["latent_size"],
        encoder_dict=vae_config["encoder"],
        decoder_dict=vae_config["decoder"],
        device=device,
    ).to(device)
    state_dict = torch.load(vae_config["vae_model"], map_location=device)
    vae.load_state_dict(state_dict)
    z = vae.sample()
    sdf = vae.decode(z)
    pointcloud = False
    normalize_pose = True
    vae_dataset = SDFVAEViewDataset(
        vae,
        device=device,
        width=640,
        height=480,
        fov_deg=90,
        z_min=0.2,
        z_max=0.5,
        extent_mean=0.11,
        extent_std=0.01,
        pointcloud=pointcloud,
        normalize_pose=normalize_pose,
        render_threshold=0.01,
        orientation_repr="discretized",
        orientation_grid=SO3Grid(0),
    )
    vae_dataset_iter = iter(vae_dataset)
    if pointcloud:
        for pointcloud, _ in vae_dataset:
            fig = plt.figure()
            ax = fig.add_subplot(111, projection="3d")
            pointcloud = pointcloud.detach().cpu().numpy()
            ax.scatter(
                pointcloud[:, 0],
                pointcloud[:, 1],
                pointcloud[:, 2],
                s=0.5,
                c="b",
            )
            plt.show()
    else:
        for depth_image, _ in vae_dataset:
            plt.imshow(depth_image.detach().cpu().numpy(), cmap="gist_heat_r")
            plt.show()
